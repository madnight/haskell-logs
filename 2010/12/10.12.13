00:01:22 <Cale> If we evaluate foo 0 100000, it'll go something like foo 0 100000 -> foo (0+100000) (100000-1) -> foo (0+100000) 99999 -> foo ((0+100000)+99999) (99999-1) -> ... -> ((...(0+100000)+99999)+...+1)
00:01:36 <Cale> and then it'll stack overflow, trying to evaluate that huge sum :)
00:01:47 <dibblego> !seen bos
00:01:52 <dskippy> Cale, how can it be true that the Haskell compiler doesn't optimize tail calls?
00:03:07 <Cale> dskippy: The whole manner in which expressions are evaluated is so different that 'calls' don't really make sense as such.
00:03:16 <dskippy> Tail call optimization simply means that if there's a function call in the tail position, the compile realizes it doesn't need to return the code pointer to the calling function. 
00:03:40 <Cale> That description is based on assumptions about how a strict evaluator works
00:04:14 <Cale> "return the code pointer to the calling function" doesn't make any sense in the context of a lazy evaluator
00:04:32 * dibblego gives up trying to use filemanip
00:05:27 <Cale> Evaluation proceeds outermost-first
00:05:49 <dskippy> Cale, I am not convinced. You can get stack overflow for building up too man expressions that need to be evaluated. 
00:06:14 <Cale> The stack basically consists of case expressions which are waiting for their parameter to be evaluated enough to pattern match
00:06:17 <dibblego> that's why you have strictness
00:06:30 <Cale> That's not the same thing as a call stack
00:06:31 <dskippy> Cale, sure.
00:06:44 <dskippy> No but the optimization works the same way.
00:06:52 <Cale> I don't think that it does.
00:06:57 <dskippy> Alright.
00:07:14 <Cale> Let's evaluate foldl (+) 0 [1,2,3]
00:07:24 <Cale> The way that a lazy evaluator would
00:07:28 <Cale> @src foldl
00:07:28 <lambdabot> foldl f z []     = z
00:07:28 <lambdabot> foldl f z (x:xs) = foldl f (f z x) xs
00:07:38 <mauke> ^ tail recursion!
00:07:42 <Cale> yep
00:07:51 <Cale> So we match the second pattern
00:08:00 <Cale> and the expression becomes
00:08:13 <Cale> foldl (+) (0 + 1) [2,3]
00:08:46 <Cale> The stack might have consisted of one element for a moment there, but by the time we're here, it's back down to 0
00:09:04 <Cale> Depending on how the syntax sugar for [1,2,3] works
00:09:27 <Cale> If we assume that [1,2,3] is identical to (1 : (2 : (3 : []))), then no stack has been used at all to this point
00:09:50 <Cale> because we were able to match the pattern so it didn't have to wait
00:09:55 <Cale> and then
00:10:06 <Cale> foldl (+) ((0 + 1) + 2) [3]
00:10:13 <Cale> foldl (+) (((0 + 1) + 2) + 3) []
00:10:16 <Cale> and finally
00:10:20 <Cale> ((0 + 1) + 2) + 3
00:10:27 <Cale> Now here's where we need the stack
00:10:46 <Cale> The outermost function application is (...) + 3
00:11:03 <Cale> and the left parameter ((0 + 1) + 2) isn't evaluated enough for (+) to pattern match
00:11:22 <Cale> So (...) + 3 (or really the case expression inside (+)) goes on the stack
00:11:35 <Cale> and we start evaluating (0 + 1) + 2
00:11:41 <Cale> and (...) + 2 goes on the stack
00:11:47 <Cale> and then we evaluate 0 + 1
00:11:52 <Cale> and that evaluates to 1
00:11:56 <Cale> and then 1 + 2 evaluates to 3
00:12:01 <Cale> and 3 + 3 evaluates to 6
00:12:27 <Cale> There's no way to do tail-call optimisation there.
00:12:39 <Cale> What you have to do instead is to use a stricter foldl
00:13:16 <Cale> one which forces the accumulating parameter to be evaluated at each step
00:15:09 <Cale> dskippy: Does that make sense?
00:16:15 <Cale> Even though the foldl is strictified, it still doesn't work quite the same way as foldl would in a strict evaluator with tail-call optimisation, but the result is the same.
00:17:39 <Cale> In fact, foldl' isn't even quite in tail recursive form
00:18:26 <Cale> foldl' f z (x:xs) = let y = f z x in seq y (foldl' f y xs)
00:19:10 * Cale doesn't know if everyone went to sleep or what :)
00:20:09 <nsmryan> I'm not. Thank you for explaining that.
01:11:10 <spvensko> Hi, I am a biologist by profession but do mostly computational work with Python. I am interested in learning Haskell due, from what I've heard, different perspectives it allows through using it. Assuming Python works perfectly well for my purposes now, would you guys suggest still taking time to learn Haskell just to aid in how I approach problems?
01:12:03 <axman> sure, i think learning haskell will make you a better programmer in practically any language
01:12:21 <axman> plus we have some very nice tools for doing high performance work, you may find those useful to you
01:13:23 <spvensko> axman, thank you for the answer, do you think Learn You a Haskell is a reasonable source for learning?
01:13:43 <Nibble> spvensko: for a start, yes
01:13:48 <axman> i think it's the best place to start, i recommend to newbies all the time :)
01:13:51 <Nibble> spvensko: Real world haskell is great too
01:13:58 <spvensko> Nibble, ty
01:14:35 <axman> real world haskell is great once you know a little haskell. i usually recommend lyah followed by rwh
01:14:36 <osfameron> win 29
01:14:39 <osfameron> (oops)
01:15:51 <spvensko> just curious but is the haskell community increasing in size? As a rather newbie programmer (I can get stuff done, just not very efficiently), I really enjoy the size of Python's community. I expect Haskell's community is rather small compared to Python but is there still reasonable support?
01:16:09 <axman> have you seen the size of this channel?
01:16:16 <spvensko> whoa, good point
01:16:22 <axman> ;)
01:16:34 <axman> nd we're much much nicer and more helpful than ##python
01:16:36 <axman> and8
01:16:38 <axman> **
01:16:39 <axman> -_-
01:16:55 <Nibble> In fact, haskell is a huge language, many many universities teach it
01:16:55 <spvensko> python and perl seem to be the most common languages in my specific field so i feel like using haskell would may introduce some complications
01:17:10 <spvensko> still, my advisor and I don't even use the same main language so I guess I can survive :)
01:17:36 <Nibble> spvensko: also, haskell's speed >> python's
01:17:50 <axman> yeah, we smash python on speed
01:17:54 <axman> BAM!
01:17:57 <Nibble> that means less time for sword fights
01:18:06 <osfameron> spvensko: I use mostly Perl myself, but it's still been really useful to do some things in haskell.  go for it, it's not such a risk, you can always re-write in python if you have to
01:18:09 <spvensko> interesting
01:18:47 <Figs> axman & Nibble: Is that including library speed for libraries like SciPy? ;)
01:19:06 <spvensko> speed is always a plus but I always hear C++ as being the standard in fast computing (sorry, I'm not old enough for assembly, etc to be familiar), how does haskell compare to C++?
01:19:44 <axman> we need a nice SciPy replacement. there's a few things that are close, but not as easy to use imo
01:20:12 <Figs> spvensko: What are you working on that requires speed?
01:20:19 <axman> spvensko: you can write faster haskell than C++, but it takes work and experience (not huge amounts though)
01:20:47 <spvensko> Figs, at the moment I work with hundreds of Drosophila genomes
01:20:52 <spvensko> several terrabytes of data
01:21:08 <axman> there's some genomics libraries for haskell i believe
01:21:08 <Nibble> spvensko: sounds like something for haskell!
01:21:27 <spvensko> most of my scripts run in under a few hours however I've noticed that even SLIGHT tweaking in Python greatly reduces the runtime
01:22:16 <axman> heh, sounds like haskell
01:22:39 <spvensko> my only concern with haskell is the lack of users at my university but honestly I don't think that'll be a major problem with the web resources available
01:22:50 <Chaze> i am kinda lost on this one: http://projecteuler.net/index.php?section=problems&id=39
01:23:05 <spvensko> i've gotten a reasonable grasp on Python (with only a single C++ class under my belt from highschool) using a "sink or swim" mentality 
01:23:21 <Chaze> brute forcing this would result in something O(n^3), but i have no idea how to calculate the number of integer solutions in another way
01:24:51 <Figs> spvensko: Don't give up on learning Python if you haven't mastered it already. Haskell may provide you with some insights to it that you might not notice as quickly (if at all) though.
01:25:16 <Chaze> (in fact, brute force ran 5 minutes just now - but i'm interested on how to solve this more elegantly)
01:25:22 <spvensko> Figs, i've definitely not given up on python, as i've said, it's served its purpose well
01:25:30 <Figs> They are both useful languages. Python's good for quickly throwing things together and experimenting with, but Haskell will change the way you think.
01:25:53 <spvensko> i wouldn't say i've mastered it (i still have to google rather silly questions every now and then) but i can definitely do what i need to do in it
01:26:09 <spvensko> i'm interested in speed quite honestly
01:26:30 <axman> well, haskell is for you then
01:26:35 <spvensko> i imagine a nice course in algorithms would help me out quite a bit but unfortunately that's not on my calendar until fall of next year
01:27:08 <axman> you'll find it'll blow your mind while learning it, but #haskell is probably the most helpful irc channel in the internet*, and someone should always be able to help you
01:27:21 <axman> * maybe
01:27:31 <Chaze> spvensko: courses on algorithms sadly are far away from haskell, most of the time
01:27:42 <Figs> spvensko: Have you learned how to work with functional programming in Python yet?
01:27:56 <axman> yeah, algorithms courses assume everyone uses imperative algorithms
01:28:11 <spvensko> Figs, no, I simply read through a fairly introductory book a year or so ago and just google anything i get stuck on
01:28:24 <spvensko> Chaze and axman, can you explain more?
01:28:33 <axman> spvensko: my advice, start rteading and following LYAH, and then decide later if it's for you. we can't accurately tell you that
01:29:03 <Chaze> spvensko: axman said it: algorithm courses assume imperative programming
01:29:38 <Chaze> in fact, most people's understanding of 'algorithm' does
01:29:46 <spvensko> i see
01:30:32 <axman> which doesn't make as much sense for a language like haskell, where you write what a program does, not how it does it
01:30:46 <Figs> spvensko: http://pastebin.com/8qynHJwM
01:31:24 <spvensko> Figs, i follow what is going on but I've never scripted in that manner
01:32:13 <tsbo> If I wanted to represent network addresses (not use, just parsing and showing really), what would be the best package to use?
01:33:01 <spvensko> axman, finals are almost over and i have a number of research projects that i need to be working on, i may try them in haskell and see how they work out for me
01:33:05 <axman> Figs: well, that's just currying, not really particular to haskell or functional languages
01:33:20 <axman> spvensko: sounds like a good plan :)
01:33:48 <axman> bare in mind, haskell is very different to anything you've used before, you'll have to thiunk about programming in a different way (it's not too hard)
01:34:07 <Figs> axman: I was curious how he'd respond to it, since a lot of people are confused when they see that.
01:34:53 <spvensko> axman, i have a few (online) buddies in other channels that use it and have given some reasonable insight on it
01:34:58 <spvensko> i'm prepared to have my mind warped
01:37:44 <sipa> axman: haskell describes data flow (mostly), imperative languages describe control flow (mostly)
01:38:18 <axman> yes?
01:38:42 <sipa> you write how a value is derived from other values, most of the time
01:38:58 <axman> are you sure you mean me and not spvensko?
01:39:19 <sipa> axman: ah, i just comment on your earlier statement 
01:40:33 <Jafet> Haskell is so different, you'll never really get it until you unlearn Haskell.
01:41:05 <sipa> is that possible?
01:43:38 <spvensko> thanks everyone for the advice, i'll continue with LYAH and hopefully make some progess
01:52:48 <Nibble> sipa: it is indeed possible
01:53:07 <Nibble> like when I programmed some C and used XCB
01:53:30 <Nibble> I couldn't if my life depended on it decide what variables that I passed to a function would get changed by it
01:54:44 <spvensko> one last thing before i go, a guilty pleasure of mine is thinking of ideas for nethack-ish (CLI based) games, is Haskell a suitable language for coding such things?
01:55:06 <Nibble> spvensko: sure
01:55:08 <Nibble> why not
01:55:26 <spvensko> no reason, just don't know the limitations of the language at all yet
01:55:32 <spvensko> i suppose the limitation is your imagination
01:56:22 <blackh> spvensko: I don't think many people know what the limitations of Haskell are.
01:56:52 <blackh> I'm half joking.
01:57:05 <theorbtwo> Languages don't exactly have limitations -- any decent language is turing-complete, and is capable of expressing anything that any other turing-complete language can.
01:57:24 <blackh> Generally I've found that the thing Haskell isn't good for are fairly specialized things, or things where the library support is lacking.
01:57:36 <Nibble> theorbtwo: languages can have limitations in how you do stuff
01:57:36 <spvensko> i guess i meant practical limitations
01:57:54 <theorbtwo> Some things are easier to express in one language, some things in another.
01:57:55 <spvensko> ie sure i could program such a game in haskell but would i die of old age first sort of thing
01:58:30 <blackh> The thing is, in Haskell you can always write imperative-style state-mutating code, and sometimes that's the right thing to do.  And Haskell does that better than most languages do.
01:58:41 <Jafet> You might quit out of boredom, or out of frustration when you get to the DEC VT codes.
01:59:19 <blackh> spvensko: I don't really understand Nethack well enough to comment specifically.
01:59:57 <spvensko> blackh, i know there is at least one python library to create similar games and i got started with that but nothing ever came of it due to classes/research
01:59:58 <kosmikus> Jafet: there are libraries for that (vty for example)
02:00:22 <blackh> spvensko: Let me do some "research".
02:00:45 <Jafet> I believe vty is incomplete, at least for nethack's purposes.
02:01:18 <kosmikus> Jafet: good enough to get started
02:01:40 <blackh> spvensko: Haskell is really good for writing games.  I should know.  I'm writing a commercial game in it.
02:02:02 <blackh> I've also written a couple of toy adventure games (just to figure out what techniques are good).
02:02:32 <spvensko> blackh, interesting, is haskell commonly used for commercial products?
02:02:38 <kosmikus> spvensko: I've written LambdaHack a while ago. Only proof of concept, and a bit bitrotted in the Hackage version.
02:02:43 <Figs> blackh: Really? My impression of it so far is that it makes writing games rather more difficult than they need to be. What do you use?
02:02:50 <kosmikus> spvensko: current development branch: https://github.com/kosmikus/LambdaHack
02:02:58 <kosmikus> spvensko: there's also a fork with continued development
02:02:59 <blackh> spvensko: No, it isn't, but I think that will change.
02:03:36 <spvensko> kosmikus, very cool, ty
02:04:05 <spvensko> blackh, honestly, as someone that doesn't have a huge background in programming, i've always perceived haskell as a toy language
02:04:08 <blackh> Figs: The thing with writing games in Haskell is that you have to do things quite a bit differently.  You can't really take an OO approach and modify it slightly.  That doesn't really work.
02:04:16 <spvensko> not sure why, maybe just impressions from others
02:04:56 <Jafet> If BASIC is a pencil, Haskell would be a vibrator...
02:05:05 <blackh> spvensko: Haskell is a better language for doing real work than the mainstream languages are, and its primary implementation is completely production ready.
02:05:40 <spvensko> Jafet, i can stick it up my nose but won't require a trip to the hospital?
02:05:47 <blackh> Haskell is poised to do some serious damage to its competition, but it is a very different language in a (commercial) world that is conservative.
02:06:49 <spvensko> blackh, i see, that's good to hear
02:06:57 <blackh> For big projects, any company that adopts Haskell has a competitive advantage.
02:07:18 <osfameron> blackh: that's not a given
02:07:29 <Jafet> blackh: get your head out of the blub!
02:07:56 <blackh> I am speaking purely from a technical standpoint.
02:09:03 <Jafet> You sounded very much like you were speaking from a marketing standpoint, and that's rather redundant around here
02:09:35 <blackh> Well, customers buy products. They don't buy software.
02:09:49 <blackh> They only care how good it is, not how it was engineered.
02:10:51 <Nibble> What is the best way to deal with mutable variables in haskell? Tvar? Mvar?
02:11:22 <mauke> MVar isn't a normal mutable variable
02:11:57 <Jafet> Hrm, "mutable variable"
02:12:01 <Nibble> yeah yeah
02:12:07 <Nibble> skip the terminology bullshit
02:12:09 <blackh> What do you want to do?
02:12:14 <axman> IORefs are like normal mutable variables
02:12:31 <mauke> also, STRefs
02:12:36 <axman> but should be avoided... because they're like normal mutable variables :P
02:13:45 <quicksilver> Nibble: if you are sharing between multiple threads, then MVar if you want the blocking semantics, TVar if you want the transaction semantics
02:14:10 <quicksilver> Nibble: if you're using them in one thread only you probably dont' need mutable references, just pass values around.
02:14:12 <Nibble> axman: well, any better approach if I want to represent ~10000 or more structures(not structures, but you get the idea) of data
02:14:24 <Nibble> and all of their properties can change
02:15:34 <axman> we'd need to know about what you want to do to help you
02:15:38 <Jafet> Why, what's changing them?
02:15:50 <Nibble> coordinates, heat, pressure etc etc
02:16:03 <Nibble> lets put it this way, interractions
02:16:14 <quicksilver> Nibble: normally you just use a good persistent structure like a Map or a Trie
02:16:16 <Jafet> A physical simulation?
02:16:17 <Nibble> s/interractions/interactions
02:16:32 <quicksilver> you don't need true mutation there, these structures admit efficient updates.
02:16:44 <Nibble> oh
02:18:47 <Jafet> A big simulation might use arrays, although a pure algorithm can sometimes be optimized in strange ways
02:19:30 <Nibble> Well, doesn't this mean a _lot_ of memory deletion and allocation?
02:20:58 <flux> well, if your simulation would lend itself to having a 10000^3 array of floats, you're surely not better off switching into a tree structure :)
02:21:00 <mauke> Nibble: why a lot?
02:21:22 <blackh> There are certain problems where using pure data structures is significantly less efficient than using mutable ones, but that's relatively unusual.
02:21:53 <quicksilver> Nibble: yes.
02:21:57 <Nibble> mauke: lots of changed needed to be done
02:22:04 <quicksilver> Nibble: memory deletion and allocation is what GHC does rather well.
02:22:30 <quicksilver> memory allocation in the nursery, which fits in a cache line, is essentially free
02:22:45 <quicksilver> (assuming the rest of your program bottlenecks somewhere else on memory bandwidth)
02:23:23 <Jafet> It's the runs in allocation and deallocation that are a problem.
02:23:42 <Jafet> RWH has some example graphs, chapter 25
03:06:35 <poseidon> g time
03:06:48 <poseidon> oops, sorry.  thought I was in firefox :)
03:08:04 <Janni> Hello.
03:08:16 <blackh> Hello!
03:11:39 <Janni> Shouldn't "until" be defined differently from how it is defined in the base libraries? "until p f = let rec x = if p x then x else rec (f x)" should be much more efficient, right?
03:12:20 <Janni> (I should know, because I am in the course of writing a paper for performing these kind of optimisations automatically)
03:13:12 <Janni> But I was suprised to find this unoptimised version of until in the standard libraries, while e.g. "repeat" is already given in the optimised form.
03:13:24 <mauke> why would it be much more efficient?
03:13:24 <Janni> So, I'm just making sure...
03:13:53 <Janni> Because the recursion involves fewer beta-reductions. You only have to pass one parameter instead of three.
03:13:56 <benmachine> Janni: er, do you mean to have 'in rec' at the end?
03:14:10 <mauke> Janni: why would that be faster?
03:14:12 <Janni> Oh sorry. Missed that. "in rec x", that is.
03:14:51 <benmachine> Janni: compilers work in strange ways. if that's really a big speedup, I imagine a compiler could do it automatically
03:15:19 <mauke> I don't think it is a speedup
03:15:47 <Janni> benmachine, mauke: Have a look at the definition of repeat. The optimisation has been applied there manually.
03:15:49 <byorgey> it may or may not be a speedup depending on the operational model.
03:15:52 <int-e> Janni: but you pay for that by accessing more of the context. And with the spineless G-machine, you can have several "beta steps" (which don't really exist, what you have is a saturated function application) at once.
03:15:52 <Nibble> @src until
03:15:52 <lambdabot> until p f x | p x       = x
03:15:52 <lambdabot>             | otherwise = until p f (f x)
03:15:56 <mauke> Janni: what optimization?
03:16:04 <benmachine> Janni: do you mean let xs = x:xs in xs?
03:16:23 <Janni> Lifting parameters out of recursive positions.
03:16:34 <benmachine> Janni: in the case of repeat, that's not why it's like that
03:16:36 <Janni> Have a look at: http://www.cs.yale.edu/~hl293/download/leak.pdf
03:16:36 <int-e> Janni: it's really hard to guess which version is actually faster for ghc.
03:17:04 <Janni> The paper is not _about_ the optimisation, but it does argue why a space leak occurs (AFAIR)
03:17:21 <benmachine> Janni: the advantage of let xs = x : xs in xs is that it creates a circular data structure
03:17:30 <benmachine> instead of constantly building an infinite one
03:17:38 <mauke> Janni: repeat doesn't lift a parameter
03:17:50 <mauke> xs is the *result*
03:18:53 <Janni> mauke: No, what I meant is: From "repeat x = x : repeat x" the version found in the base libraries can be obtained by lifting the parameter x out of the recursion.
03:19:20 <mauke> I don't see it
03:19:31 <mauke> well, I sort of do
03:19:31 <Janni> benmachine: Yes and "building" happens by beta reductions. But you're right, I should do some benchmarking...
03:19:38 <mauke> it's still a different quality
03:19:48 <benmachine> Janni: it's not done to avoid extra reductions
03:19:49 <mauke> you're converting a function call into a non-call
03:20:13 <Janni> benmachine: Yes, just as I did above for the "until" function.
03:20:45 <benmachine> Janni: that's not the purpose of writing it like that
03:20:53 <Janni> Well, _if_ we consider a "function call" as _multiple_ reductions. 
03:21:01 <Janni> benmachine: I understand your point.
03:21:05 <mauke> why would we do that?
03:21:16 <mauke> machines don't perform multiple reductions in reality
03:21:48 <benmachine> Janni: I think you should profile both definitions
03:22:12 <benmachine> if we're right, it'll turn out to be the same
03:22:34 <benmachine> if you're right, that's something interesting and you can authoritatively tell everyone that there's some speed to be gained here
03:22:36 <mauke> Janni: what would happen if you transform Prelude.until to a loop via tail call elimination?
03:22:42 <Janni> Yes, you're right. My perspective is atm rather theoretic.
03:23:11 * Janni attempts to do some benchmarking
03:24:17 <byorgey> you might also be interested in looking at the GHC Core code which gets generated for both definitions.
03:24:25 <mauke> answer: it becomes something like: while (not (p x)) { x = f x; }
03:24:42 <byorgey> If your version is indeed an improvement, it may be that GHC is already doing the transformation.
03:25:11 <byorgey> whereas I could see why it would not perform the transformation on the naive definition of repeat (since it changes the sharing properties).
03:26:19 <mauke> ocaml explicitly recommends against doing that transformation manually (parameter lifting)
03:26:46 <Janni> Turns out that my version of "until" performs much worse in GHC.
03:26:47 <mauke> because passing one or two more parameters via registers is free, but creating a new closure is work
03:27:21 <Janni> I see.
03:29:45 <Janni> Ah no, made a mistake... It is more efficient, at least for my trivial example here *relieved*
03:29:55 <Janni> main = print $ until (==0) (\x->x-1) 100000000
03:29:57 <Janni> versus
03:30:15 <Janni> let until' p f x = let rec x = if p x then x else rec (f x) in rec x in print $ until' (==0) (\x->x-1) 100000000
03:30:52 <Janni> Although, just using the "time" command on such an example can hardly be called benchmarking :)
03:31:11 <Nibble> time command is the best benchmark
03:31:25 <Janni> But at least there's _potential_ in the transformation, and the paper is not _merely_ of theoretical interest.
03:31:51 <blackh> With 'print' in there, the answer will be extremely inaccurate.
03:31:58 <blackh> Oh ...
03:32:06 <blackh> It's just print one number.  Ignore me.
03:32:18 <Janni> \ignore blackh
03:32:26 <blackh> \ignore self
03:32:30 <byorgey> Janni: are you compiling with -O2 ?
03:33:15 <Janni> byorgey: Now I did. Both version are now faster, but still the second one is superior.
03:33:22 <byorgey> ok, nice
03:33:30 <Janni> 1.9s vs 0.85s
03:34:16 <benmachine> Janni: which compiler version?
03:34:21 <Janni> 6.12
03:35:19 <Janni> If anyone would care to test with 7, I'd be interested.
03:35:24 <benmachine> working on it
03:35:41 <Nibble> Janni: arch?
03:36:01 <Janni> i686
03:36:08 <Nibble> I meant arch linux
03:36:23 <benmachine> why is that relevant?
03:36:25 <Janni> Debian.
03:36:35 <Janni> Yeah why?
03:37:07 <Nibble> The GHC version
03:37:08 <Janni> OK, I see why. Some linux distributions might already ship GHC 7.
03:37:17 <benmachine> arch doesn't
03:37:22 <Nibble> Anyone in here interested in maintaining the arch linux ghc packages?
03:37:43 <benmachine> Nibble: I thought dons did that
03:37:46 <JuanDaugherty> what's interesting about arch linux?
03:37:58 <Nibble> JuanDaugherty: no mainitainer for GHC
03:38:03 <benmachine> JuanDaugherty: it tends to be pretty fast to adopt new versions of things
03:38:24 <JuanDaugherty> so it's constantly broken, like sid debian?
03:38:29 <benmachine> uhm
03:38:37 <benmachine> it's more often broken than most other distributions
03:39:02 <byorgey> I find it is not broken all that often.
03:39:05 <benmachine> but I've been using it for a good while without anything catastrophic happening
03:39:08 * JuanDaugherty is headed in the other direction, i.e. to NixOS.
03:39:41 <JuanDaugherty> b0rken shit I don't need.
03:40:22 <benmachine> Janni: I can confirm I get a speedup with GHC 7
03:40:44 <benmachine> Janni: which, I admit, surprises me, but I guess until isn't used all that often :)
03:41:10 <benmachine> Janni: I suppose the proper thing to do is submit a bug or something
03:41:20 <Janni> Yeah. Still, the optimisation will not only work for "until" :-)
03:41:50 <benmachine> until' p f = rec where rec x | p x = x | otherwise = rec (f x)
03:42:00 <benmachine> pretty version
03:42:16 <benmachine> runs just as fast
03:42:18 <Janni> benmachine: If you're okay with waiting for at most 2 or 3 months, this optimisation (after it has been thoroughly tested and benchmarked) will be part of GHC anyway.
03:42:53 <benmachine> Janni: I actually don't use haskell for anything that requires high performance so I'm okay waiting forever :P
03:43:15 <benmachine> Janni: but it's possible the base libraries will be used by compilers other than GHC
03:43:37 <Janni> benmachine: That's true.
03:44:09 <Janni> Then again, the optimisation will _first_ flow into UHC anyway (I'm working in Utrecht).
03:44:24 <benmachine> ah, okay
03:44:35 <benmachine> what about poor little jhc :P
03:45:20 <Janni> Well, I guess it's time to decompose all those optimisations into libraries, so that they can be used by different compilers....
03:45:44 <Janni> (not that I think that's easy)
03:45:48 <benmachine> that would be nice, but yeah
03:45:53 <benmachine> non-simple
03:46:05 <byorgey> JuanDaugherty: fair enough, I think NixOS is always an acceptable choice around here =)
03:46:18 <byorgey> in fact, I might try it out next time I need to install an OS
03:46:38 <benmachine> but in this case there's a transformation that can be applied to the library code, that's simple and non-API-changing
03:46:48 <benmachine> so why not?
03:47:25 <Nibble> I never heard of NixOS
03:47:44 <JuanDaugherty> byorgey, I've been using since about March. It's my distro of choice now and it is listed on Haskell Platform as a supported distro.
03:48:16 <benmachine> what distinguishes nixOS from the crows?
03:48:17 <benmachine> er
03:48:19 <benmachine> crowd
03:48:23 <benmachine> <_<
03:49:15 <byorgey> purely declarative system configuration, with non-destructive update
03:49:28 <benmachine> oh, neat
03:49:48 <JuanDaugherty> in essence it's the only really distinct linux distro, excluding android and the like
03:49:54 <byorgey> http://nixos.org/nixos/
03:50:36 <benmachine> my phone's linux confuses all kinds of hell out of me
03:51:12 <byorgey> JuanDaugherty: how hard is it to create new packages for stuff?
03:51:37 <byorgey> i.e. if there's something you want to install that isn't in the existing package repository
03:51:45 <Janni> benmachine: fscks? Application configuration based on text files? (just kidding)
03:51:48 <JuanDaugherty> once you learn the expression lang and the lack of LSB, it's fairly easy
03:52:00 <byorgey> LSB?
03:52:08 <JuanDaugherty> that's why they've been able to crank out like 3K packages
03:52:16 <JuanDaugherty> Linux Standard Base
03:52:18 <byorgey> cool
03:52:59 <byorgey> JuanDaugherty: and it's still actively developed?
03:53:21 <JuanDaugherty> of course, it's the hobby horse some dutch academics
03:53:29 <benmachine> can someone give me an example of a single-param class instance that requires FlexibleInstances?
03:53:46 <JuanDaugherty> and it has a following outside that base group who do a lot of the work
03:53:58 <JuanDaugherty> *of some
03:54:11 <byorgey> benmachine: sure,  instance Foo [Int] where ...
03:55:05 <byorgey> because Int is not a type var
03:56:04 <benmachine> byorgey: mm, it seems like some of FlexibleInstances is provided by TypeSynonymInstances
03:56:55 <benmachine> TypeSynonymInstances lets me define instance Foo String
03:57:02 <benmachine> even though instance Foo [Char] would be illegal
03:57:44 <benmachine> ok now *that* is more worrying
03:57:54 <benmachine> it lets me define instance C String *and* instance C [a]
03:57:57 * benmachine fiddles
03:58:38 <quicksilver> but it will give an error if you use them, right?
03:58:48 <quicksilver> overlaps are detected "lazily"
03:59:27 <benmachine> quicksilver: hmm
03:59:36 <benmachine> quicksilver: I thought they were detected when defined
03:59:50 <benmachine> quicksilver: I can use the [a] instance as long as a /= Char
04:00:35 <quicksilver> well, I said what I think.
04:00:41 <quicksilver> I think they're detected at use.
04:00:50 <quicksilver> (because they might not be defined in the same file)
04:00:56 <benmachine> but they are :P
04:01:05 <accel> how hard would it be to make xmobar multi line?
04:01:29 <benmachine> quicksilver: huh, I guess you're right
04:01:43 <benmachine> with flexibleinstances I can define instances for [Char] and [a]
04:01:55 * benmachine surprised a little
04:02:04 <kosmikus> quicksilver is right, afaik
04:02:16 <benmachine> but still, I shouldn't be able to define an instance for String without FlexibleInstances
04:02:21 <benmachine> so my original complaint is valid
04:02:42 <quicksilver> I think you're probably right.
04:02:56 <quicksilver> but FlexibleInstances is no more controversial than TypeSynonymInstances IMO
04:03:41 <benmachine> well, it's quite neat that without FlexibleInstances, instances are either non-overlapping or identical
04:03:44 <benmachine> imo
04:03:56 <kosmikus> perhaps TypeSynonymInstances simply implies FlexibleInstances
04:04:02 <benmachine> kosmikus: it doesn't
04:04:04 <quicksilver> benmachine: sure
04:04:13 <quicksilver> benmachine: but the restriction is too great.
04:04:16 <benmachine> kosmikus: with just TypeSynonymInstances I can declare instance C String but not instance C [Char]
04:04:21 <kosmikus> ok
04:04:26 <benmachine> quicksilver: mm, I suppose I'd agree with that
04:04:34 <quicksilver> benmachine: there is nothign wrong with wanting to make an instance for "Either String a" or "(a,Int)"
04:04:45 <quicksilver> both illegal under haskell98
04:05:00 <quicksilver> you're right to remark that the automatic no-overlap result is neat
04:05:01 <benmachine> quicksilver: you can still do those with newtypes, although admittedly it's awkward
04:05:24 <byorgey> presumably the automatic no-overlap was the reason for the original restruction.
04:05:29 <byorgey> *restriction
04:05:32 <benmachine> yeah
04:05:40 <benmachine> there's no need to define what overlap means precisely
04:05:46 <benmachine> or how to detect it
04:06:00 <benmachine> makes things simpler
04:06:19 <benmachine> I wish there was just a better way of using newtypes
04:06:41 <benmachine> they're useful for so many things but the syntactic noise tends to dissuade people
04:07:01 <benmachine> I think they're important enough that some special sugar for them wouldn't be amiss
04:07:18 <kosmikus> there's already newtype deriving, and that helps a lot
04:07:23 <benmachine> but I don't have concrete ideas as to what to actually use
04:09:32 <quicksilver> the TypeCompose newtype feels clumsy to use, especially considering how incredibly important it is
04:09:39 <quicksilver> that's the only newtype that really bugs me to be honest.
04:10:52 <Jafet> Sometimes I want a derive everything extension, like right now
04:10:59 <Jafet> (Although I'm not sure what exactly that should mean)
04:13:06 <Jafet> newtype BB = BB Word64 deriving (Eq, Ord, Show, Num, Enum, Real, Integral, Bits) -- note that half of these are noise: required dependencies of other classes
04:16:39 <benmachine> Jafet: half of 8 is not many :P
04:16:54 <opqdonut> we need a new extension, how about "newtype BB = BB Word64 deriving (..)" ?-)
04:17:10 <opqdonut> (half-serious)
04:17:15 <sipa> where .. means "every derivable instance you know about" ?
04:17:18 <Nibble> Does deriving a class derive its subclasses?
04:17:28 <sipa> no
04:17:38 <opqdonut> sipa: that's what I thought yeah
04:17:38 <benmachine> I think one thing I would like would be a compiler feature that said "just for now, if you find a derivable instance is missing, derive it, and if an entity is not in scope, hoogle it and import it"
04:17:44 <Nibble> derive the classes it has derived*
04:17:46 <sipa> that's the 'noise' Jafet talked about
04:17:52 <benmachine> (and probably whine that you are doing so)
04:19:17 <Jafet> preflex, seen jafet
04:19:18 <preflex>  jafet was last seen on #haskell 6 minutes and 12 seconds ago, saying: newtype BB = BB Word64 deriving (Eq, Ord, Show, Num, Enum, Real, Integral, Bits) -- note that half of these are noise: required dependencies of other classes
04:21:59 <edon> hi, how do i fix this cabal dependency/package error: http://hpaste.org/42289/a ?
04:23:08 <Nibble> edon: you don't
04:23:59 <edon> Nibble: so i can't, for the moment, install some packages using cabal ?
04:24:33 <Nibble> edon: Ask star wars kid
04:24:36 <Nibble> he has the lightsaber
04:25:07 <quicksilver> Nibble: if you're not going to be helpful, please shush
04:25:51 <Nibble> http://www.youtube.com/watch?v=yPQq0Bz6QQc&NR=1
04:25:53 <Nibble> lol
04:27:05 <dcoutts_> edon: solution is not to reinstall the haskell98 package, you don't want to do that
04:27:41 <Nibble> I did once, look what happened to me
04:28:34 <edon> dcoutts_: yes i tried reinstalling it, didn't help obviously
04:28:38 <Nibble> also
04:28:46 <Nibble> GLfloat not compatible with regular floats??? wtf???
04:29:17 <mauke> Nibble: eat type safety, villain!
04:33:41 <Nibble> @type GLfloat
04:33:43 <lambdabot> Not in scope: data constructor `GLfloat'
04:34:09 <Nibble> actually GLfloat is just a "type" of Float.
04:34:47 <Nibble> rotate xrot (Vector3 1 0 0 :: Vector3 GLfloat)
04:35:05 <Nibble> couldn't match expected type Float against inferred type GLfloat
04:35:53 <dcoutts_> edon: right, you need to remove the instance that you reinstalled and just use the one that comes with ghc
04:36:27 <quicksilver> Nibble: just use GLfloat everywhere and/or convert with realToFrac
04:37:13 <Nibble> why can't it just accept floats.....
04:37:29 <Nibble> that is how it is done in C
04:37:47 <quicksilver> no it's not.
04:37:56 <quicksilver> in C, GLfloat is not guaranteed to be the same type as C float
04:38:12 <sipa> type GLfloat = Float
04:38:17 <sipa> no?
04:38:25 <quicksilver> it just happens to be in the C compiler you used.
04:38:26 <quicksilver> sipa: no.
04:38:29 <Nibble> quicksilver: it is
04:38:46 <quicksilver> Nibble: it is not. The OpenGL spec makes it quite clear that GLfloat is its own type.
04:38:54 <Nibble> quicksilver: wasn't talking about that.
04:39:23 <Axman6> i thought that GLFloat was Float in haskell's package
04:39:29 <Nibble> it is
04:39:39 <Nibble> that is why I think this error makes no sense
04:39:41 <quicksilver> Axman6: since version 2.3 (I think) it's a newtype.
04:41:14 <Nibble> what version is haskell's opengl package at?
04:41:43 <Nibble> http://cvs.haskell.org/Hugs/pages/libraries/OpenGL/Graphics-Rendering-OpenGL-GL-BasicTypes.html
04:42:06 <quicksilver> Nibble: that's a manual page for an older version
04:42:44 <quicksilver> the more recent version is http://hackage.haskell.org/packages/archive/OpenGLRaw/1.1.0.1/doc/html/Graphics-Rendering-OpenGL-Raw-Core31.html#t:GLint
04:42:47 <sipa> if it were type GLfloat = Float, then both are actually the same type, and you couldn't possibly get "couldn't match expected type Float against inferred type GLfloat"
04:43:02 <quicksilver> right. It used to be a simple synonym and no longer is.
04:43:08 <quicksilver> it's easy to convert, though
04:43:19 <quicksilver> and probably better just to use the GL types directly, if your program's main purpose is opengl
04:43:19 <Nibble> it is indeed
04:43:24 <Nibble> I have to go now, bbiab
04:47:34 <int-e> does Fedora ship the pretty package? https://admin.fedoraproject.org/pkgdb/acls/list/?searchwords=*pretty* doesn't list it, so I guess the answer is "no".
04:47:52 <ziman> when generating random numbers { x <- getRandom; y <- getRandom; stuff x y }, I don't care about the order of the getRandom "calls", I just want them to be different and get (last stdgen, stuff); the monadic notation introduces unnecessary ordering. Is there a way to express that I don't care about the actual order?
04:48:30 <Axman6> why does the ordering matter?
04:49:22 <mauke> replicateM n getRandom
04:49:23 <ziman> Well, I perceive is as a redundancy in my code and I'd like to get rid of it.
04:49:54 <mercury^> ziman: but the ordering does matter.
04:49:57 <quicksilver> ziman: no.
04:50:09 <quicksilver> the ordering might not matter to you, but it does matter to the code.
04:50:22 <quicksilver> however, if you're used to conventional function call notation
04:50:37 <quicksilver> you may find "stuff <*> getRandom <*> getRandom" feels less ordered.
04:50:37 <ziman> ...I can use Applicative.
04:50:45 <int-e> on the other hand it may be part of the ghc package itself. hmmm.
04:50:45 <quicksilver> of course, it isn't any less ordered. But it feels it.
04:50:57 <ziman> hm, yes.
04:52:02 <tsbo> Am I crazy or is <?> from Attoparsec really just a no-op?
04:52:52 <Botje> yes
04:52:56 <Jafet> If the generator is perfectly uniform, I'm not sure if the ordering does matter.
04:52:58 <Botje> it just attaches documentation to a parser
04:53:17 <quicksilver> Jafet: it doesn't matter in some notational sense. It matters entirely in a practical sense.
04:53:19 <quicksilver> erm
04:53:21 <romildo> @pl \x y -> not (x == y)
04:53:21 <lambdabot> (/=)
04:53:22 <quicksilver> operational sense.
04:53:44 <romildo> @pl \x y -> not (isPrefixOf x y)
04:53:44 <lambdabot> (not .) . isPrefixOf
04:53:58 <quicksilver> Jafet: if the state of the stdgen is, say '42', then calling them in that order will get x = 17 and y = 961; Reversing the order will get the opposite x and y. The compiler has to generate one code or the other.
04:54:06 <Jafet> If the semantics of your program wants n uniformly random numbers and doesn't care what they are, it might not matter. I'll have to check the math, of course.
04:54:24 <quicksilver> of course, if you only care about it being pseudo-random and not what the values are, it doesn't matter.
04:54:41 <quicksilver> but if you carea bout it being *reproducible* - a common useful property of PRNGs - then you do need to expect it to be consistent
04:54:43 <Jafet> Yeah, they'll have to be done in sequence, but not ordered
04:54:46 <ziman> Hm, should we care about the operational aspect of the computation? (I'm aware that my question may be vague/stupid/too esoteric.)
04:54:50 <quicksilver> so the compiler can't re-order it just for kicks.
04:54:54 <mercury^> quicksilver: you meant "stuff <$> getRandom <*> getRandom" ?
04:55:04 <int-e> So pretty is part of the ghc package itself on Fedora.
04:55:13 <quicksilver> mercury^: no. ziman's example was not "return (stuff x y)"
04:55:17 <Jafet> GHC's Random can also be split, although you'll pretty much need a Magical Compiler Extension to get that sort of parallelism
04:55:22 <quicksilver> mercury^: maybe he meant that, but he didn't say it :)
04:56:00 <mauke> quicksilver: it still looks like a type error to me
04:56:14 <mercury^> Yeah, I don't think it works as you wrote it.
04:56:50 <mauke> ap :: f (a -> b) -> f a -> f b; stuff :: a -> b -> f c
04:57:27 <quicksilver> ah, good point.
04:57:33 <ziman> quicksilver, well, somehow I don't see the difference between stuff and return .: stuff :)
04:58:05 <mercury^> getRandom >>= stuff =<< getRandom -- should work
04:58:09 <ziman> (regarding orderability, besides the obvious type differences :) )
04:59:09 <ziman> ah, you were discussing <*> vs <$>.
05:00:30 <Jafet> Funktors in LazyTown
05:01:38 <ziman> Or, I want to generate unique identifiers for internal use in my compiler; they never make their way out. This is also when I care about the identity of the values but never about their actual values.
05:01:45 <quicksilver> yes, ziman 
05:01:47 <quicksilver> agreed
05:01:51 <ziman> *the actual values.
05:01:53 <quicksilver> "morally commutative" you might call it
05:01:59 <ziman> hehe :)
05:02:18 <quicksilver> the list monad for non-determinism is the same
05:02:30 <quicksilver> it's not actually commutative, because you get the 'possibilities' out in a different order
05:02:36 <quicksilver> but viewing as a 'set of possibilities' it's the same
05:03:25 <mercury^> Unique identifiers and pseudorandom values are something different still.
05:04:30 <mercury^> Changing the order for the unique identifiers does not change the semantics of the program; changing the random variables only statistically does not change it (and even that is somewhat debatable).
05:04:54 <ziman> yes, that was a not-so-appropriate parallel on my side. :)
05:05:24 <quicksilver> well...
05:05:39 <quicksilver> I disagree, actually.
05:05:59 <Jafet> Then deterministicness is really important and you should ignore whatever I said earlier
05:06:12 <quicksilver> if you consider part of the semantics of the RNG (and I'm changing my argument from 5 minutes ago) to included that it's seeded externally with a truly random source
05:06:28 <quicksilver> then changing the random variables does not change the only semantics you *can* give the program
05:06:36 <quicksilver> (which is, necessarily, a probabilistic semantics)
05:06:46 <mercury^> Right.
05:06:51 <quicksilver> ...and, that's probably the semantics you are thinking of/want, if you're working with random variables.
05:07:42 <quicksilver> to argue the reverse part with the unique identifiers, it might not change the "Result" of your compiler, but it might well change the actual physical manifestation of some intermediate files you write out
05:07:52 <quicksilver> GHC intermediate output files have the gensyms visible.
05:08:12 <quicksilver> so the semantics is only the same if you implicitly "quotient" your view of files being identical by the gensyms.
05:08:17 <Jafet> Surely a counter would suffice
05:08:22 <quicksilver> so, in either case, it's up to how you define your semantics.
05:08:54 <quicksilver> (you could imagine a case where it even affects which variables end up in registers, so the generated code really would be different, but hopefully you have a better register allocater than that)
05:09:25 <Jafet> (Perhaps the machine has symmetrical register semantics)
05:09:35 <mercury^> That should not happen if the only operation you perform is equality testing.
05:09:59 <quicksilver> mercury^: sure. Depends how you use the gensym-type-things.
05:10:11 <quicksilver> sometimes you really embed them into intermediate variablenames
05:10:14 <quicksilver> int$$427$$
05:10:33 <quicksilver> if you were only using them in type inference, say, then they would have no influence at all.
05:10:46 <quicksilver> (....unless you used them for some debugging output which showed type signatures!)
05:12:11 <mercury^> Hrm, I think it would be somewhat "nicer" not to write out any UIDs, instead inserting a symbol that causes the compiler to generate a UID when reading back.
05:12:16 <ziman> for such purposes I could create a simple (sequenced/ordered) number-assigning routine that deterministically assigns a visible name for each opaque ident — as a last step when printing out type terms
05:12:33 <ziman> if needed at all.
05:13:03 <Jafet> On the other hand, this guy deliberately introduced an error rate of 10% in a memoization hash table, and the algorithms still worked: http://www.cis.uab.edu/hyatt/collisions.html
05:13:03 <Jafet> (error = the program ran off into illegal moves)
05:13:03 <Jafet> So bugs can be subtle
05:14:32 <quicksilver> ziman: sure. I'm just pointing out rather laboriously that in lots of examples it's "mostly" commutative and in almost every case it depends how you define "observation" or "semantics". I'm sure you know what I mean.
05:14:58 <ziman> quicksilver, yes, definitely.
05:15:08 * ziman will think about it.
05:15:57 <mercury^> Anyone here familiar with string diagrams?
06:26:29 <wks> hello
06:27:30 <wks> Is there a standard way to do this?  http://hpaste.org/42291/comparison_cascading
06:28:18 <wks> comparing two values with different comparing functions, with different significance
06:28:54 <copumpkin> use the monoid instance on Ordering
06:29:03 <copumpkin> > EQ `mappend` GT
06:29:04 <lambdabot>   GT
06:29:07 <copumpkin> > EQ `mappend` LT
06:29:08 <lambdabot>   LT
06:29:33 <copumpkin> it'd make more sense to do it on a list than a 3-tuple though
06:29:43 <copumpkin> > mconcat [EQ, EQ, GT]
06:29:44 <lambdabot>   GT
06:31:19 <copumpkin> if you want it in that weird order though, it's not that elegant
06:31:43 <copumpkin> but you could always reorder your fields to put them in the right order before grouping them
06:33:31 <wks> thank you, copumpkin . that solved my problem.
06:35:43 <quicksilver> did it? ;)
06:35:54 <quicksilver> `mappend` is actually longer than >>>>
06:36:02 <quicksilver> all you gained was a standard name.
06:36:23 <copumpkin> and not having to repeat code ;)
06:36:35 <copumpkin> besides, people are petitioning for a shorter synonym!
06:36:39 <quicksilver> true.
06:36:46 <copumpkin> also, mappend is the worst possible name for that thing
06:36:57 <copumpkin> and I'd like to say a thing or two to whomever came up with it
06:38:38 <quicksilver> copumpkin: I think, at the time, it didn't seem useful enough to get a good name
06:38:56 <quicksilver> they didn't have enough interesting monoids and they had no interesting general combinators over monoids
06:39:11 <copumpkin> still, something like moperator if they really had to give it a shitty name
06:39:22 <quicksilver> now we have quite a few more of the former and even a couple of the latter
06:39:34 <applicative> mappend = (<>) ; mempty = o
06:39:55 * quicksilver is on record as favouring +>, but his opinion was over-ruled.
06:39:58 * applicative is suddenly taking to 'moperator'
06:40:25 <sipa> 'moperator' or `moperator` ?
06:40:54 * applicative will then shorten to 'mop'.  Maybe that will be better for 'mconcat'
06:40:55 <osfameron> as long as you don't write `moperator' I'm happy ;-)
06:41:24 <quicksilver> applicative: http://memegenerator.net/Justin-Beiber/ImageMacro/3027295/Ho-Im-Operator
06:41:26 <dcoutts_> quicksilver: that actually turned out to be a good discussion when it could have been a terrible bikeshed discussion
06:41:35 <copumpkin> `theassociativeoperatorforthismonoidwhoseunitiscalled'theelementofthemonoidthatwhencombinedwithanyotherelementreturnstheoriginal'`
06:41:37 <quicksilver> (that should put you off, applicative)
06:42:02 <quicksilver> dcoutts_: which part was a good discussion, and which part might have been a bikeshed? ;)
06:42:03 <mux> Monoid_zimappend
06:42:17 * applicative is already arranging a keybinding for `theassociativeoperatorforthismonoidwhoseunitiscalled'
06:42:45 <dcoutts_> quicksilver: e.g. people looked at evidence, like how many libs were using these ops and for what purpose, rather than just saying "<>!!" "No! >+!!"
06:42:57 * quicksilver nods
06:43:00 <quicksilver> yes, true.
06:43:25 <mux> oh, I think I meant Data_ziMonoid_zimappend
06:43:28 <sipa> copumpkin: very nice, no need for comments
06:43:55 <quicksilver> I don't care about the exact sequence of glyphs but I did think it would be nice to have something which had a 'mirror image' operator in an easy way
06:43:58 <mauke> preflex: zenc Data.Monoid.mappend
06:43:58 <preflex>  DataziMonoidzimappend
06:43:59 <quicksilver> like >> and <<
06:44:33 <copumpkin> sipa: indeed!
06:44:47 <sipa> might i suggest some word separation?
06:45:22 <mauke> preflex: zenc Data.Monoid.++
06:45:22 <preflex>  DataziMonoidzizpzp
06:45:46 <mux> mauke: I'm completely sold.
06:47:58 <applicative> dcoutts_, I think people would use monoid instances more if it werent for mempty and mappend, so I idon't know what the libraries could tell us.
06:49:30 <applicative> but the suggestion that something like do notation should be permitted for mconcat, seems kind of cool. 
06:50:34 <applicative> I don't know if it's still true but the BlazeHtml library familiarly had a fake Monad instance so it could mconcat with (>>) without list apparatus
06:51:46 <applicative> but it would presumably open a can of worms 
06:51:55 <aristid> applicative: why not just use Monoid for that?
06:52:29 <applicative> he wanted more intuitive syntax, it's in the nature of a dsl or whatever.  it's quite handsome, but at first his monoid didn't meet the monad laws
06:52:49 <aristid> (<>) = mappend
06:52:54 <aristid> there is your intuitive syntax :)
06:53:02 <mauke> ><
06:53:17 <mauke> outuitive
06:53:28 <aristid> uh
06:53:42 <aristid> mauke: (><) is the proposal? then i mistook that
06:54:02 <mauke> just making lame jokes (also programming in delphi for some reason)
06:54:29 <Nibble> (O.o) is the actual proposal
06:54:42 <dcoutts_> applicative: oh just that lots of libs already define an operator that is the mappend for that lib's type, e.g. Doc
06:55:00 <aristid> Nibble: no it's not :)
06:55:10 <mauke> `O.o`
06:55:11 <aristid> Nibble: i can tell because that is not valid syntax
06:55:17 <mauke> that is valid syntax
06:55:27 <aristid> if O is a module?
06:55:29 <mauke> yes
06:55:35 <aristid> much better.
06:55:52 <dcoutts_> applicative: yes, the do thing is interesting. I think it's hardly worse with mconcat [ ... ] but the Blaze authors did not agree.
06:56:17 <applicative> dcoutts_ yes, <> is most familiar from Doc
06:56:37 <aristid> fake monads are evil
06:56:39 <quicksilver> dcoutts_: I think it's a bit of a weird thing to worry about, but I wouldn't be opposed to some extension which let you use layout for other purposes.
06:56:43 <dcoutts_> applicative: but it turned out there were several others with the same or similar ops, I'd not realised
06:56:45 <quicksilver> dcoutts_: a fake monad is disgusting, though.
06:57:00 <applicative> aristid, yes, but suppose there was a monoidal do, "moo"
06:57:03 <mauke> fauxnad
06:57:05 <dcoutts_> quicksilver: agreed
06:57:18 <applicative> aristid, then the temptation would go away
06:57:31 <quicksilver> I consider a "," one of the most lightweight pieces of punctuation
06:57:46 <quicksilver> so I don't consider a list much heavier than just separate lines.
06:57:53 <applicative> the use of many lines for lists of complicated terms is extremely error prone, and leads to ugly [  , , , ] lineups akin to C brackets
06:58:14 <aristid> applicative: i don't really see the need for that syntax tho. putting a few <> there is hardly the end of the world
06:58:44 <quicksilver> applicative: what do you mean by error prone and ugly lineups?
06:58:48 <quicksilver> I don't understand either point.
06:59:09 <applicative> aristid, it isn't the end of the world, but not having do notation wouldnt be either.  so why not moo notation?
06:59:32 <aristid> applicative: oh, if you implement it, i won't complain
07:00:09 <applicative> the remark of yairchu here impressed me http://stackoverflow.com/questions/3336235/haskell-would-do-notation-be-useful-for-contexts-other-than-monads
07:03:16 <quicksilver> applicative: that remark doesnt' really justify itself though? :)
07:03:28 <quicksilver> I'm just not seeing how mconcat [ ,,,,, ] is that ugly
07:04:01 <aristid> applicative: what about whitespace lists?
07:04:12 <mauke> quicksilver: haskell doesn't allow trailing commas
07:04:49 <aristid> applicative: like your moo, but constructing a list first. and then you can apply any function to that list
07:04:49 <quicksilver> mauke: OK, that's mildly annoying.
07:05:10 <quicksilver> aristid: agree. If you are going to do it at all, you don't restrict it to mconcat
07:05:20 <mauke> that's why : : : [] is actually superior
07:05:21 <quicksilver> aristid: you make it an arbitrary list building
07:05:50 <aristid> mauke: it is not very common, and thus a bit harder to read
07:05:55 <quicksilver> mauke: however, I really don't find the trailing comma thing a big problem. There is always the leading comma option if you like
07:06:10 <mauke> quicksilver: that just moves the problem to the first line
07:06:10 <quicksilver> mauke: with : you have to worry about the precedence of :
07:06:45 <mauke> hmm. printf-style list builder?
07:06:50 <aristid> hmm, an extension to allow trailing commas should be possible, right?
07:16:01 <applicative> aristid, I agree maybe it is really a whitespace / newline list construction device that is more to the point in this case
07:16:42 <applicative> for example the lisp - revivalist scheme chrisdone was touting a couple of weeks ago, could be taken care of that way
07:17:47 <applicative> he wants (op t u v w x y z) where op :: foo -> foo -> foo
07:18:49 <applicative> but he was going to put t u v .. on separate lines, same indentation. 
07:19:31 <applicative> so it's really fold1 op t [u,v,w,x,y,z] 
07:20:25 <aristid> you mean fold1 op [t,u,v,w,x,y,z]?
07:20:49 <applicative> so it's really fold1 op  [t,u,v,w,x,y,z] 
07:21:01 <applicative> sorry, didn't hit return to correct myself
07:21:26 <aristid> so it could be fold1 op $ whitespace\n\tLIST HERE, INDENTATED
07:21:45 <applicative> right.  with the requirement that there be two....
07:21:50 <applicative> i guess.
07:22:16 <aristid> you wanna become a GHC hacker? :)
07:22:28 <applicative> i take that back.  i was thinking of a specifically lispy op lispdo \n \t listhere...
07:22:47 <applicative> aristid, maybe a preprocessor would be a better start.
07:23:30 <applicative> aristid, it the do -desugaring machinery simple and superficial?  I mean, not deeply embedded in the darkest strata of ghc?
07:23:41 <aristid> i have no clue
07:24:14 <applicative> in any case, like i said, i was impressed by that stack overflow bit, and other things i've seen like it. 
07:24:36 <applicative> it just makes one wonder whether the general syntactic device we have for monads, isn't kind of stuck on monads
07:25:01 <applicative> in some ways, it corresponds to an earlier period, before other great typeclasses were thought of as clearly
07:25:20 <applicative> (i am not affirming these things, but making speculative statements)
07:25:24 <aristid> we're pretty far from clarity, i think
07:27:08 <applicative> the argument for applicative do seems to turn on the question whether, there is some optimization advantage in suitable cases if you dont use the monad instance but only the applicative one.  i wonder if there are good examples of this.
07:27:39 <aristid> i want infix syntax for applicative, that's what i want :)
07:28:03 <applicative> how do you mean?
07:28:11 <aristid> liftA2 (+) x y
07:28:26 <quicksilver> he wants to be able to write "x + y" and have it mean "liftA2 (+) x y"
07:28:26 <applicative> oh like (|readFile a ++ readFile b|) 
07:28:26 <aristid> => x {+} y
07:28:31 <quicksilver> somehow
07:28:43 <quicksilver> maybe with a modifier or a bracket or whatever.
07:28:47 <applicative> as in the she version of idiom brackets
07:28:49 <aristid> yea
07:28:50 <quicksilver> it's really hard to make this work elegantly
07:29:04 <aristid> but it would be awesome :D
07:29:04 <quicksilver> you have to decide how nested subexpressions behave
07:29:18 <applicative> yes
07:29:30 <quicksilver> what does (x {+} f (y {+} z)) mean, for example
07:29:37 <applicative>  the she idiom brackets nest
07:29:37 <quicksilver> and other more painful combinations.
07:30:20 <applicative> (| x + (| f (| y + z |) |) |)
07:30:28 <Jafet> This is nothing. In common lisp, you can modify the syntax used by the reader as it reads the code subsequent to the code that modifies the syntax used by the reader...
07:30:52 <applicative> Jafet, yes, but do they have a typesystem to speak of?
07:31:13 <Jafet> They have one, but it's pretty closeted.
07:31:34 <mauke> applicative: not really
07:32:03 <yrlnry> as I recall, the original applicative functor paper suggested a way to trick Haskell so that [[ f a b .. z ]] would be sugar for f <$> a <*> b .. <*> z.
07:32:29 <quicksilver> yrlnry: yes, that's what "she" is an implementation of.
07:32:43 <yrlnry> sorry, I cam e into the conversation in the middle.
07:32:44 <quicksilver> (the author of "She" being one of the authors of that paper)
07:32:56 <Jafet> That depends on whether you consider separate preprocessing tricky
07:33:13 <quicksilver> Jafet: TeX can do that too
07:33:29 <quicksilver> it can even alter the tokeniser
07:33:38 <applicative> quicksilver what you really want is something like the mathematical device of putting a bar over relevant terms 
07:33:40 <quicksilver> which means you need to understand exactly when the tokeniser runs 
07:33:40 <Jafet> The alternative would be to have oleg actually figure out some way to make [[ .. ]] work, I guess
07:33:53 <quicksilver> applicative: yes, I think so.
07:33:56 <Jafet> quicksilver: yes, but people don't call tex the best language ever
07:34:03 <quicksilver> Jafet: I wonder why not ;)
07:34:18 <yrlnry> No, I mean that the "Applicative Programming with Effects" paper suggested a way to trick Haskell without using any separate preprocessing step or anything like that.
07:34:28 <quicksilver> yrlnry: yeah, that doesn't nest.
07:34:38 <yrlnry> Ahh.
07:34:39 <quicksilver> it's just a one-off trick with a constructor and a typeclass
07:34:40 <applicative> which trick
07:34:42 <zygoloid> Jafet: there's a trick for that already, using iI / Ii as brackets. it's somewhat horrible
07:34:50 <yrlnry> (It's on page 4 if anyone else wants to look it up.)
07:34:50 <applicative> right, doesn't nest
07:34:51 <quicksilver> iI x y z Ii
07:35:05 <applicative> but the she preprocessor brackets do, as he points out. 
07:35:28 <applicative> i tried to extract it from the rest of she, but got confused somewhere...
07:35:43 <quicksilver> applicative: I still worry that the difference between (| x + (|f (| x + z |) |) |) and (| x + f (| x + z |) |) is liable to be confusing.
07:35:54 <quicksilver> applicative: then again, I probably worry too much.
07:36:00 * hackagebot enumerator 0.4.4 - Implementation of Oleg Kiselyov's left-fold enumerators  http://hackage.haskell.org/package/enumerator-0.4.4 (JohnMillikin)
07:36:00 <quicksilver> the right thing to do is use it for a bit and see how it goes.
07:36:03 <Jafet> If I littered my code with iI..Ii's, it might resemble a transcript of Silent Hill
07:36:43 <applicative> quicksilver, yes, one would like the standard rules of precedence to somehow decide how pure (+) and so on work. not sure it would be coherent
07:37:20 <applicative> idiom brackets are of limited utility, it seems, but often they look really swank, let me find a bit of the epigram source
07:39:30 <aristid> i think for non-infix, f <$> x <*> y <*> z works quite well already
07:40:12 <applicative> here's a simple example.  clarity is  much advanced, but it's obviously a minor thing http://hpaste.org/42292/epigram_num_instance
07:41:02 <quicksilver> applicative: agreed.
07:41:19 <quicksilver> applicative: but of course, things which only work well for simple cases are annoying
07:41:26 <quicksilver> in particular, it's hard for someone to generalise them
07:41:51 <quicksilver> "Ah, so he used this (| |) thing here... how do I extend that to the following slightly more fiddly example?..."
07:42:22 <quicksilver> aristid: for infix, on an ad-hoc basis, I often do local definitions like (<++>) = liftA2 (++)
07:42:54 <applicative> here's a similar case http://hpaste.org/42293/epigram_num_instance_annotati
07:43:02 <aristid> quicksilver: if you need it a couple times, that might be a good idea
07:46:22 <quicksilver> yes, that's a better one applicative 
07:46:44 <quicksilver> I don't think this really represents a case for a substantial new syntax, though
07:46:52 <quicksilver> I think really the complaint being addressed is more
07:47:11 <quicksilver> "pure / liftA2 don't look like the same concept"
07:47:24 <quicksilver> pure / fmap / liftA2
07:47:46 <quicksilver> I think if that code was written with liftA0 / liftA1 / liftA2 it would be just as 'clear' as the (| |) code
07:47:59 <quicksilver> uglier in a very subjective "we like symbolic operators" way
07:48:14 <quicksilver> but, cleaner in a "named functions are easier to look up in the docs" way.
07:48:44 <quicksilver> what's the ~ in ~fn?
07:49:32 <applicative> in this module they are embedded, but it's hard to read since the material is opaque (the epigram mangler??)
07:49:34 <Saizan> i'd guess it's pure from the types
07:49:45 <applicative> quicksilver ~ is pure 
07:50:13 <quicksilver> ah, right.
07:50:22 <applicative> the asterisk in (| * x op y |)  applies join
07:50:43 <quicksilver> and (% %) ?
07:51:27 <applicative> so you'd write (|* readFile (| head getArgs |)|)  
07:52:18 <applicative> this of course requires a monad instance for whatever world youre in, here IO
07:52:28 <applicative> cant remember % 
07:54:05 <thoughtpolice> i remember using idiom brackets with attoparsec, I actually liked it quite a bit for describing a simple ad-hoc protocol
07:55:29 <applicative> thoughtpolice, but youre experience was that the nice cases were limited in scope?  i guess that was mine. 
07:55:59 <Nibble> I will pretend I didn't read that
07:56:03 <Nibble> youre? srsly?
07:56:14 <thoughtpolice> http://hpaste.org/42295/she__attoparsec
07:56:38 <applicative> nibble my typing is what holds me back from being the truly excellent programmer God evidently intended me to be :)
07:56:53 <Nibble> applicative: Wrong
07:57:00 <thoughtpolice> i thought it was a bit simpler than the same using <$> and <*> for the applicative stuff, but it's a pretty small example and that was the only place in the code I used she
07:57:07 <Nibble> Your're typing is what is holding you back from being understood by others :/
07:57:18 <ndrsndrs> your're..?
07:57:22 <ndrsndrs> :3
07:57:25 <Nibble> obviously intended.
07:57:41 <applicative> Nibble, maybe. It doesn't help that i'm prostrate.
07:58:23 <Saizan> thoughtpolice: so (% %) is to discard the result?
07:58:53 <thoughtpolice> yeah
07:58:57 <thoughtpolice> http://personal.cis.strath.ac.uk/~conor/pub/she/idiom.html
08:00:59 <applicative> right, the pExp :: P Char Exp  example in the middle of that page also brings out the use of (| a | b | c |)  for alternatives
08:01:25 <kmc> my first reaction is "that's nice" but not nice enough to justify going outside standard Haskell or even what's built into GHC
08:01:39 <kmc> even if this were a GHC extension i'm not sure i would use it
08:01:49 <kmc> but perhaps there's a case where it's really nice
08:01:58 <applicative> kmc, I think everyone agrees.  It seems McBride doesnt. since it permeates the Epigram source
08:02:02 <kmc> :D
08:02:18 <kmc> well, she was developed for Epigram
08:02:33 <kmc> and if you're already using some of its deeper features
08:02:35 <kmc> then why not
08:03:32 <applicative> yes, I think we agree.   We were considering other syntactic extensions earlier; other uses of something like the "do" apparatus
10:04:41 --- topic: '["GHC 7 is out: http://is.gd/hb8vE", "Haskell Platform 2010.2: http://is.gd/dCGPn", "Haskell 2010 is out: http://hackage.haskell.org/trac/haskell-prime/wiki#Status", "Haskell News: http://reddit.com/r/haskell", "The Haskell programming language http://haskell.org", "Paste: http://hpaste.org", "Logs: http://tunes.org/~nef/logs/haskell/"]'
10:04:41 --- topic: set by monochrom on [Wed Nov 24 17:56:54 2010]
10:04:55 <m_> hi
10:05:01 <m_> !list
10:05:18 <jmcarthur> haole_: if you are a competent programmer in other languages already then real world haskell is probably best
10:06:21 <haole_> amazon also recommends RWH... thanks
10:06:51 <danbrown> is there a way to hide instance declarations when importing a module?
10:07:01 <Saizan> no
10:07:04 <leimy> yes
10:07:09 <danbrown> e.g. import Data.Map, hide its Monoid instance, and define my own
10:07:23 <danbrown> so expected value: "maybe"? :P
10:07:23 <leimy> oh "instance decl's" no :-)
10:07:28 <danbrown> :(
10:07:37 <Saizan> you need to use a newtype wrapper
10:07:42 <danbrown> yeah
10:07:45 <danbrown> dislike
10:07:50 <jmcarthur> haole_: you can check out the book online before buying at http://book.realworldhaskell.org/read/
10:08:05 <danbrown> alright, thanks
10:08:11 <jmcarthur> haole_: i think the print version has some edits not found in the online version, but they are largely the same
10:08:28 <haole_> jmcarthur: that's very good to know... thanks
10:17:10 <unkanon_> I had an idea last night
10:17:14 <unkanon_> I want to share it here
10:17:18 <Nibble> suicide?
10:17:23 <Nibble> Think of Nike's logo
10:17:26 <Nibble> "Just do it"
10:17:28 <unkanon_> no, about haskell
10:17:55 <unkanon_> all it takes for a pure function to be pure is that it always returns the same output for the same input
10:18:14 <unkanon_> that doesn't mean I'm not allowed to use mutability to achieve that result
10:18:24 <mauke> see also: ST
10:18:43 <unkanon_> so there should be a way for me to have mutable arrays that I can use to compute my feedforward algorithm 
10:18:49 <kmc> that's the ST monad
10:18:51 <unkanon_> and still have that be a pure function
10:18:53 <unkanon_> oh really?
10:18:58 <kmc> ya rly
10:18:59 <unkanon_> so I can unbox values from ST ?
10:19:04 <kmc> runST :: ST a -> a
10:19:05 <kmc> except, not
10:19:08 <kmc> the actual type is more complicated
10:19:15 <kmc> because it uses the type system to enforce overall purity
10:19:17 <Nibble> @type runST
10:19:17 <mauke> :t runST
10:19:17 <lambdabot> forall a. (forall s. ST s a) -> a
10:19:18 <lambdabot> forall a. (forall s. ST s a) -> a
10:19:22 <kmc> which also means there's no runtime overhead
10:19:33 <kmc> in fact with GHC at runtime, runST is equivalent to unsafePerformIO
10:19:41 <kmc> it's just guaranteed safe at compile time 
10:19:50 <unkanon_> I've looked at ST but I don't see how that gives me mutable arrays. at most it gives me one (not really-)mutable variable
10:19:59 <kmc> no, it's really mutable
10:20:03 <mauke> unkanon_: are you thinking of Control.Monad.State?
10:20:07 <kmc> the arrays are in Data.Array.ST
10:20:13 <unkanon_> mauke: maybe I'm confusing
10:20:15 <kmc> yes, State monad /= ST monad
10:20:21 <unkanon_> we're not talking about that s -> (s,a) deal, right?
10:20:24 <kmc> we're not
10:20:25 <unkanon_> that's not mutable
10:20:29 <mauke> @src runST
10:20:29 <lambdabot> runST st = runSTRep (case st of { ST st_rep -> st_rep })
10:20:36 <mauke> @src ST
10:20:36 <lambdabot> newtype ST s a = ST (STRep s a)
10:20:39 <kmc> we're talking about Control.Monad.ST, which has an internal representation roughly as magical as IO
10:20:40 <mauke> hah
10:20:51 <kmc> and has arrays, and the 'vector' package can use it, etc
10:20:52 <mauke> @src STRep
10:20:52 <lambdabot> type STRep s a = State# s -> (# State# s, a #)
10:20:56 <c_wraith> also, roughly identical to IO
10:21:05 <kmc> ^^^ all of that @src above is GHC-internal bs which is not useful for using ST
10:21:07 <kmc> just so you know unkanon_ :)
10:21:08 * unkanon_ writes down Control.Monad.ST
10:21:13 <alpounet> @src IO
10:21:13 <lambdabot> Source not found. I feel much better now.
10:21:19 <alpounet> hey
10:21:25 <mauke> kmc: but there's that State# s -> (# State# s, a #) again!!!
10:21:30 <kmc> ;P
10:21:31 <unkanon_> kmc: lol
10:21:36 <alpounet> but IO is similar anyway yeah
10:22:03 <c_wraith> in GHC, the underlying implementation is identical.  They just are kept seperate.
10:22:10 <c_wraith> For very good reasons
10:22:16 <unkanon_> Ok so I have to read up on Data.Array.ST to use mutable arrays inside a pure function
10:22:21 <unkanon_> is that it?
10:22:23 <kmc> yes
10:22:30 <augur> hey peeps
10:22:32 <c_wraith> Really, just use the MArray interface
10:22:38 <Nibble> I still prefer suicide
10:22:40 <mauke> unkanon_: basically, you get STRefs and mutable arrays and stuff. and then you use runST
10:22:50 <unkanon_> you guys are always very helpful. and Haskell never disappoints me :)
10:22:56 <unkanon_> cool
10:23:08 <Aune> Cale: If R=G=Z2, is R[G] a commutative ring?
10:23:40 <kmc> unkanon_, once you know Haskell well enough it will start to disappoint you
10:23:51 <mauke> unkanon_: also, __attribute__((__const__)) in gcc, but that's nearly useless
10:23:54 <fryguybob> unkanon_: "...it always returns the same output for the same input" -- this just makes it a function and makes no statement about effects.
10:23:58 <unkanon_> kmc: how does it disappoint you?
10:24:04 <Cale> Aune: R[G] is a commutative ring whenever G is an Abelian group
10:24:06 <kmc> i can name about fifty gripes about Haskell; I still think it's better than most other languages for most things
10:24:12 <kmc> unkanon_, i don't have time to get into it now :)
10:24:30 <unkanon_> kmc: no worries
10:24:49 <kmc> haskell seems perfect when every other language you know is godawful terrible
10:24:53 <unkanon_> fryguybob: I was pretending that printing etc were outputs too
10:24:54 <kmc> it's not perfect, though
10:25:14 <unkanon_> kmc: well as long as it's better than the competition
10:25:57 <unkanon_> fryguybob: I don't fully know the terminology for all those things
10:26:59 <unkanon_> kmc: you should have a blog on haskell that you could link to when people ask you these questions (that I'm sure are asked many times)
10:27:49 <kmc> i have a blog on haskell
10:28:00 <kmc> but i haven't yet posted a big list of my gripes
10:28:07 <kmc> there've been reddit threads about it etc
10:28:45 <unkanon_> sequence.complete.org?
10:28:50 <kmc> no
10:29:00 <kmc> my blog is http://mainisusuallyafunction.blogspot.com/
10:29:08 <unkanon_> oh I've read that before
10:29:09 <unkanon_> nice to know
10:29:30 <mauke> heh, named after the gcc message?
10:29:36 <kmc> yep
10:30:54 <sproingie> cute blog name
10:31:10 <sproingie> i also like the python one called "from __future__ import *"
10:32:35 <sproingie> is it weird that i understand type-level fix better than the function-level fix?
10:34:04 <Aune> I hate random disconnects
10:34:24 <kmc> i love random disconnects
10:34:28 <Nibble> I hate regular disconnects
10:34:36 <mauke> I love random connects
10:34:46 <lispy> I <3 regular disconnects
10:35:09 <Aune> Nice that we all agree then ^
10:35:33 <mauke> I love corandom nnects
10:35:43 <Nibble> I love cows.
10:35:50 <sproingie> i colove ws
10:36:26 <Nibble> :sex
10:36:28 <Jafet> They don't give you lon cancer.
10:37:45 <unkanon_> is that some kind of a flash mob?
10:38:56 <Aune> Cale: Thats good to hear, because I thought i was going crazy when asked to find all the left zero divisors of the non-commutative group R[G] where R=G=Z2. And I kept getting that it should be commutative ^^
10:45:05 * edwardk waves hello.
10:45:22 <unkanon_> hi!
10:45:23 * augur waves back
10:45:47 <unkanon_> so by using ST/STU/MArray instead of a list, my code will actually be a bit faster?
10:46:03 <mauke> depends on how you use it
10:46:16 <unkanon_> I'll use it to do feedforward and backprop of a neural net
10:46:24 <Jafet> It might crash faster
10:46:24 <unkanon_> lots of mutation
10:46:44 <unkanon_> what does that mean?
10:48:09 <leimy> Well ST etc lets you write imperative algorithms that allow for in-place update.
10:48:24 <leimy> so there's certain kinds of fun mistakes you can make there that aren't otherwise possible.
10:48:47 <leimy> then again, you're probably not stuck with O(N lg N) best case runtimes either.
10:48:58 <leimy> depends on the algorithms at hand.
10:49:46 <kmc> unkanon_, there's lots of choices besides lists and mutable arrays
10:50:02 <kmc> if you need random access, you definitely shouldn't use a list, but there are other immutable structures
10:50:07 <Watermind> can you use infix notation when _defining_ a binary operator?
10:50:09 <kmc> like Data.Sequence / Data.Map / Data.IntMap
10:50:11 <kmc> yes Watermind 
10:50:14 <kmc> > let 2 + 2 = 5 in 2 + 2
10:50:15 <lambdabot>   5
10:50:17 <unkanon_> right yeah, well mainly I was asking if STArray code gets translated to something close to what one would do in C with regular arrays. You guys said it uses unsafePerformIO so I guess that's true
10:50:27 <kmc> unkanon_, it's still lazy
10:50:31 <kmc> STUArray will be closer to C
10:50:36 <leimy> unkanon_: yeah it can .
10:50:39 <kmc> you know, you can just write C and call it ;)
10:50:54 <leimy> Most people will not reach for that particular monad until it's just necessary.
10:51:01 <Watermind> kmc: good point, but that's just a local definition...
10:51:08 <kmc> you can do the same at top level
10:51:09 <unkanon_> kmc: yeah but I won't be learning anything will I :P
10:51:19 <unkanon_> I've already written a backprop in C
10:51:23 <kmc> unkanon_, so i think you should look at other immutable data structures
10:51:25 <kmc> if all you've tried is lists
10:51:33 <lispy> kmc: writing C like code in Haskell can be better due to optimization corner cases in GHC
10:51:34 <kmc> because imperative programming still has its pitfalls, even in haskell
10:51:37 <leimy> oh there's a lot of great functional data structures.
10:51:38 <Watermind> kmc:  x >>>> y = x + y
10:51:45 <Watermind> kmc: that doesn't seem to work
10:51:48 <lispy> if you want a good array us Data.Vector
10:51:50 <alpounet> unkanon_, writing some neural nets code ?
10:51:52 <lispy> see the vector package on hackage
10:51:57 <rothwell> don't suppose there's a strict, boxed array out there?
10:52:03 <rothwell> seems to be the one array type that's missing
10:52:10 <mauke> Watermind: how does it fail?
10:52:13 <unkanon_> it's really hard to reason about immutable data structures. I need a recommendation of what to use to simulate a neural network, which is kind of like a tree but some nodes point to one same node
10:52:20 <lispy> rothwell: STArray exists, IIRC
10:52:20 <Watermind> wait let me try again
10:52:28 <unkanon_> alpounet: trying hard to, yes
10:52:33 <alpounet> heh ok
10:52:58 <rothwell> lispy: STArray says it's non-strict
10:52:58 <mjrosenb> is there a library function that is similar to \f -> liftM catMaybes . mapM f?
10:53:06 <lispy> rothwell: ah
10:53:14 <alpounet> unkanon_, we're little by little working on an actually usable version of HNN with another haskeller
10:53:36 <unkanon_> why is hnn unusable? (I haven't looked at it yet, that's be cheating myself :))
10:53:47 <copumpkin> this is looking pretty good: https://github.com/yav/memory-arrays/blob/master/Memory/ArithTests.hs
10:53:53 <Watermind> nevermind it does work
10:53:54 <Watermind> weird
10:54:02 <Watermind> kmc, mauke: nevermind
10:54:38 <alpounet> unkanon_, well, it's not general enough and i'm pretty sure it has a lot of room for optimisations... i wrote it when i was learning haskell, and published it several months later
10:54:53 <alpounet> but we're working on one with a graph backend
10:55:08 <alpounet> (that could allow to define reccurent neural nets
10:55:09 <alpounet> )
10:55:50 <unkanon_> alpounet: that one you wrote when you were learning haskell, how hard would that be for a newbie to understand? I'm barely 2 weeks into Haskell :)
10:56:27 <alpounet> http://hackage.haskell.org/packages/archive/hnn/0.1/hnn-0.1.tar.gz
10:56:31 <alpounet> figure out yourself :)
10:56:35 <unkanon_> I first want to code a 3 layer backprop ann that solves XOR and then move on to having genetic algorithms evolve them and redo that mining tanks example in haskell + opengl
10:56:45 <unkanon_> oh you wrote hnn??
10:56:46 <alpounet> unkanon_, it allows for 2 layers
10:56:53 <alpounet> ('cause theoretically it's enough)
10:57:10 <alpounet> well, 3 actually, if you include the entries
10:57:26 <unkanon_> well yes, 3 layers is 2 neurons input + 2 neurons hidden + 1 neuron output
10:57:36 <alpounet> yeah
10:57:37 <unkanon_> that's the minimum I think
10:57:44 <unkanon_> did you write hnn then? that's very cool
10:57:48 <alpounet> i don't like to include the input layer
10:57:57 <alpounet> because it doesn't actually do something 
10:58:10 <alpounet> yeah i wrote hnn
10:58:14 <alpounet> but like i said
10:58:20 <alpounet> the 0.1 version sucks
10:58:47 <unkanon_> well I've glanced over the code and I could see similarities to mine
10:58:52 <unkanon_> but I guess those are the easy similarities
10:59:01 <unkanon_> like sum $ zipWith (*) inputs weights :)
10:59:02 <alpounet> if some day you're willing to participate to the next one, ping me
10:59:09 <alpounet> heh yeah
10:59:21 <alpounet> unkanon_, what do you use to store your Doubles ?
10:59:32 <unkanon_> alpounet: I will, just need to get up to speed to your knowledge. do you mind if I ask you questions to help me understand NNs?
10:59:46 <unkanon_> alpounet: a List :(
11:00:04 <alpounet> unkanon_, no problem, even if i haven't been into NNs for a while now
11:00:06 <alpounet> (studying maths)
11:00:10 <unkanon_> alpounet: my code is ugly really, my net is :: [[[Double]]] :/
11:00:29 <alpounet> that's how my very first trial looked
11:00:33 <unkanon_> alpounet: I'll show you my code when I'm home, you'll laugh (or cry)
11:00:46 <alpounet> try*
11:01:11 <alpounet> unkanon_, or maybe that will just remind me of mine heh
11:01:31 <unkanon_> right now I only have the feedfoward algorithm working and the functions to get the deltas (both outputs and regular deltas) but it's all so spread out and broken apart that it's a bit overwhelming
11:01:48 <unkanon_> alpounet: I doubt it, yours looks like what I want my code to look like at the end
11:02:08 <alpounet> unkanon_, yeah but what you see isn't what i wrote at the very beginning
11:02:38 <unkanon_> oh I can imagine what you wrote at the very beginning :) I'm dealing with a pile of muddy code now, lol
11:03:11 <alpounet> unkanon_, the motivation between HNN is to see how a pure haskell neural nets library can compete with the others (written in other languages), but trying to keep the nice composability we usually have with Haskell libraries
11:03:18 <alpounet> hnn-0.1 was only a first step
11:03:24 <alpounet> there's been a lot of discussions since then
11:03:33 <unkanon_> what was the conclusion?
11:03:45 <unkanon_> is it competitive?
11:03:59 <alpounet> not as you see it
11:04:08 <alpounet> the next one should
11:04:21 <alpounet> but we need a better graph library so the other guy is working on improving it
11:04:25 <alpounet> (FGL)
11:04:25 <unkanon_> what are the major changes from 0.1 to the one you're using?
11:04:33 <unkanon_> not counting the graph part
11:04:36 <unkanon_> I mean the implementation
11:04:49 <alpounet> well, there isn't actually "one i'm using"
11:04:53 <unkanon_> the graph part is for viewing the nn yes?
11:04:55 <alpounet> we just tried a few approaches
11:05:01 <alpounet> no unkanon_
11:05:10 <alpounet> we will use the Graph data structure
11:05:25 <alpounet> to represent the nodes and their connections
11:05:26 <unkanon_> there's such a thing, huh? maybe that's what I need
11:05:38 <unkanon_> I was looking for something like a tree but that could have several nodes pointing to the same one
11:05:45 <alpounet> http://en.wikipedia.org/wiki/Graph_(data_structure)
11:05:56 <unkanon_> no I know what a graph is
11:06:06 <unkanon_> I didn't know there was a lib for haskell. that's what your friend's working on?
11:06:11 <alpounet> yeah
11:06:15 <alpounet> he's trying to improve it
11:06:29 <unkanon_> wow that next hnn will be awesome then
11:06:31 <alpounet> http://hackage.haskell.org/package/fgl
11:06:39 <phao> a Char is compared, using the >, <, >=, etc.. operators according to what? ascii value (like in C)?
11:06:48 <phao> ops, not like in C.
11:07:06 <benmachine> unicode value
11:07:07 <phao> a Char is compared, using the >, <, >=, etc.. operators according to what? its integer value in the charset currently being used?
11:07:22 <phao> right.
11:07:25 <benmachine> whatever fromEnum returns, presumably
11:07:25 <c_wraith> phao, Char is a unicode value.  It has no charset
11:07:29 <kmc> its integer value in Unicode
11:07:36 <phao> hmm
11:07:40 <unkanon_> @src Char (==)
11:07:40 <lambdabot> (C# c1) == (C# c2) = c1 `eqChar#` c2
11:07:40 <lambdabot> (C# c1) /= (C# c2) = c1 `neChar#` c2
11:07:41 <phao> that's interesting.
11:07:41 <Zao> Codepoint.
11:07:42 <phao> =)
11:07:45 <Jafet> data CodePoint = Char
11:07:59 <Jafet> type, even
11:08:03 <unkanon_> alpounet: I'd never find that package by the name fgl
11:08:15 <kmc> phao, the encoding of a Char as a sequence of bytes is only visible when you do IO, or when you explicitly invoke some encoding/decoding function
11:08:15 <alpounet> functional graph library
11:08:18 <alpounet> heh
11:08:29 <kmc> otherwise, Char is just an integer Unicode codepoint value with an unspecified internal representation
11:08:40 <lispy> unkanon_: although, if you google "haskell graph" this is the top hit: http://web.engr.oregonstate.edu/~erwig/fgl/haskell/
11:08:44 <kmc> and IO (as of GHC 6.12) will use your system's default encoding
11:08:50 <unkanon_> alpounet: what time zone are you in? I'm EST and it's 2 PM here
11:09:00 <kmc> > foldr1 max "phao, the encoding of a Char as a sequence of bytes is only visible when you do IO, or when you explicitly invoke some encoding/decoding function"
11:09:00 <phao> kmc, right... I get all confused with this stuff. I understand practially nothing about that, and anythin related to charsets
11:09:01 <lambdabot>   'y'
11:09:17 <kmc> phao, http://www.joelonsoftware.com/articles/Unicode.html
11:09:20 <unkanon_> lispy: hmm I just never thought of using a graph for nns even though I've always known what graphs are :)
11:10:17 <alpounet> unkanon_, CET (it's 20PM)
11:10:50 <unkanon_> alpounet: hmm
11:11:18 <alpounet> but i quite frequently stay up late 
11:11:21 <alpounet> heh
11:11:31 <unkanon_> I'll only be home 5 hours from now, that's be 1 am for you
11:11:37 <unkanon_> that'll*
11:17:57 <aristid> CET is Haskell time
11:18:33 <Nibble> :t ($=)
11:18:33 <lambdabot> Not in scope: `$='
11:19:46 <alpounet> aristid, is that Glasgow's timezone ? :-P
11:20:30 <aristid> alpounet: hmm, i think Glasgow is in GMT
11:20:53 <aristid> alpounet: but Utrecht is in CET :)
11:20:56 <alpounet> haha
11:21:02 <alpounet> so you're an UHC kind of guy
11:21:18 <mjrosenb> hey, does anyone know how feasable it is to do partial garbage collection, e.g. let z = (x,y); and all references  z are (fst z)(or something equivalent), then throw away y even though it is still referenced?
11:21:24 <aristid> alpounet: i guess i gotta be
11:21:55 <alpounet> heh
11:22:07 * alpounet doesn't (yet) know about a Marseille Haskell Compiler
11:22:27 <c_wraith> mjrosenb, it would be hard to prove the GC wasn't forcing evaluation in that case, which seems like something you want to avoid
11:23:31 <mjrosenb> c_wraith: well, i was thinking of doing this statically, specifically for the case where z has aready been evaluated to a tuple of two things
11:23:57 <mjrosenb> rather, statically marking how bindings are referenced
11:24:19 <c_wraith> In that case, it's feasible, but sounds like rather a lot of work for what I'd expect to be a very tiny payoff in the best of cases
11:24:38 <c_wraith> Though it might be something supercompilation would get you for free.
11:24:55 <c_wraith> Because supercompilation would simply remove the tuple in the first place
11:25:10 <vrs> I'm looking for http://www.haskell.org/arrows/biblio.html, but it seems the urls changed
11:25:23 <vrs> where is it likely to be now?
11:30:36 <mjrosenb> <3 supercompilation
11:34:16 <edwardk> mjrosenb: ghc has (had?) some hacks in place to do just that, references to field accessors are just forwarded in certain cases to fix memory leak problems found by wadler due to just that
11:35:00 <edwardk> http://homepages.inf.ed.ac.uk/wadler/topics/garbage-collection.html
11:35:49 <mjrosenb> published in 1987.  fun.
11:36:04 <edwardk> mjrosenb: see the bits in http://hackage.haskell.org/trac/ghc/wiki/Commentary/Rts/Storage/HeapObjects?redirectedfrom=Commentary/Rts/HeapObjects regarding THUNK_SELECTORs and "Stingy evaluation"
11:36:47 <edwardk> mjrosenb: it is one reason why naive embeddings of lazy languages into things like javascript are hard, because they will leak more memory than would otherwise be optimal without implementing their own gc, etc.
11:37:15 <edlinde> is there an equally elegant implementation of the mergesort like there is for the quicksort?
11:37:22 <alpounet> how's kata going ?
11:37:29 <edwardk> heya alp
11:37:31 <edlinde> the ones I have seen for mergesort in haskell were rather ugly
11:37:37 <edlinde> can someone post a link if they know?
11:37:51 <edwardk> been getting paid to work on another language, so kata is kind of on the back burner atm
11:38:09 <monochrom> only the part about dividing the input list into two chunks is ugly. The merge is elegant.
11:38:35 <monochrom> and ugly only in "length xs `div` 2"
11:38:37 <edlinde> monochrom: yeah you could sya that... but with so many cool haskell list fns.. that should be easy
11:38:42 <Nibble> the haskell quick sort isn't a real quicksort
11:38:50 <edlinde> I would think some list comprehension
11:38:53 <alpounet> edwardk, a functional one ? 
11:39:00 <edwardk> alpounet: yeah
11:39:20 <alpounet> ok, good :P
11:40:28 <xrch> what are zygomorphisms used for?
11:40:48 <edwardk> xrch: mutual recursion
11:41:06 <edwardk> technically semi-mutual
11:41:09 <mjrosenb> edwardk: you don't actually need to divide the list in two for mergesort :-p
11:41:13 <mjrosenb> err
11:41:25 <mjrosenb> edlinde: you don't actually need to divide the list in two for mergesort :-p
11:41:36 <mjrosenb> which really saves on that "ugly step"
11:41:43 <xrch> hm ok
11:43:56 * hackagebot improve 0.2.3 - An imperative, verifiable programming language for high assurance applications.  http://hackage.haskell.org/package/improve-0.2.3 (TomHawkins)
11:44:32 <edwardk> xrch: when you have one algebra of the form f b -> b, it lets you use the answers it is giving you at each layer. the obvious choice of f b -> b is In :: f (Fix f) -> Fix f, in which case you have a paramorphism
11:46:14 <xrch> hm ok
11:46:20 <edwardk> xrch: iirc they are covered in more depth in grant malcolm's dissertation on algebraic data types and program transformation
11:46:38 <xrch> oh
11:46:44 <xrch> isn't it in german or something?
11:47:20 <edwardk> you can also just look it as all but taking the product of two algebras, and then tossing one of the results
11:47:28 <alpounet> edwardk, wasn't there something about entangling algebras together ?
11:47:36 <alpounet> oh, yeah
11:48:02 <vrs> (asking again) where did they move the old haskell.org urls like http://www.haskell.org/arrows/biblio.html?
11:48:09 <edwardk> its not quite the product, because the algebra and g-algebra are f b -> b, and f (a,b) -> a respectively
11:48:14 <xrch> hm ok
11:48:49 <Saizan> mh, dependent product?
11:49:00 <xrch> hu?
11:49:00 <edwardk> xrch: i think dutch, but that never stops one from looking at the equations and diagrams ;)
11:49:31 <xrch> oh, yes
11:52:40 <xrch> wootch, is it this paper: http://dissertations.ub.rug.nl/faculties/science/1990/g.r.malcolm/?pLanguage=en&pFullItemRecord=ON? if that's the case it's two pages long with no diagrams
11:53:37 <edwardk> that is the abstract
11:53:53 <edwardk> i believe you can even get that much of it in english
11:54:43 <xrch> ok
11:58:46 <xrch> hey
11:59:21 <xrch> I think I found a quite useful zygomorphisms
11:59:58 <xrch> http://paste.pocoo.org/show/304453/ here's all the file, if you want to test
12:00:03 <xrch> (it's the function named wut)
12:00:16 <xrch> it's basicallty the same as \n -> sum [1..n]
12:01:39 <monochrom> does WAlgebra mean "wut algebra"? :)
12:01:46 <xrch> hum
12:01:53 <xrch> nope
12:02:07 <xrch> it's the equivalent of GAlgebra, I think
12:02:14 <xrch> (in category-extras)
12:02:54 <xrch> now I think I'll try to understand this example.
12:03:20 <xrch> (oh and yes, it's actually the equivalent of GAlgabra)
12:13:04 <clklein> Can someone explain why "fst 3" type checks? 
12:13:18 <olsner> > fst 3
12:13:19 <augur> > fst 3
12:13:19 <lambdabot>   3
12:13:19 <lambdabot>   3
12:13:23 <augur> olsner: :D
12:13:28 <olsner> :D
12:13:28 <augur> :t fst
12:13:29 <lambdabot> forall a b. (a, b) -> a
12:13:38 <augur> :t 3
12:13:39 <lambdabot> forall t. (Num t) => t
12:13:40 <mjrosenb> clklein: because people have done horrible things to lambdabot
12:13:46 <mjrosenb> > (2,3) + 5
12:13:46 <augur> o_o
12:13:47 <lambdabot>   (7,8)
12:14:00 <mjrosenb> (3,4) + (5,6)
12:14:03 <mjrosenb> > (3,4) + (5,6)
12:14:04 <lambdabot>   (8,10)
12:14:04 <augur> clklein: presumably you _are_ talking about lambdabot here, yes?
12:14:21 <maurer_> fst 3 doesn't work in ghci, so I'd assume so
12:14:33 <maurer_> It appears lambdabot has an instance for Num (Int, Int)
12:14:34 <clklein> maurer_: It does for me.
12:14:50 <maurer_> clklein: Did you import some packages first?
12:14:57 <maurer_> clklein: What version?
12:15:15 <clklein> maurer_: I didn't import anything. Version 6.10.4.
12:15:31 <maurer_> Weird. Mine says there is no instance defined.
12:15:33 <clklein> which I know is not hot off the press, but it's what happened to be installed
12:15:43 <maurer_> Then again, 6.12.1 here, but still.
12:16:26 <byorgey> clklein: what does it evaluate to?
12:16:29 <olsner> > 3 :: (Int,Int)
12:16:30 <lambdabot>   (3,3)
12:16:33 <olsner> nice
12:17:00 <byorgey> clklein: oh, I can explain why it *type checks*
12:17:06 <byorgey> clklein: it's because 3 is polymorphic
12:17:13 <clklein> byorgey: It doesn't typecheck if I leave off the ":t" prefix
12:17:17 <byorgey> it can have any type which is an instance of the Num type class
12:17:24 <sproingie> is this caleskell?
12:17:47 <alpounet> @src (Int, Int) negate
12:17:47 <lambdabot> Source not found. Just try something else.
12:17:51 <kaol> > fst "hello"
12:17:52 <lambdabot>   Couldn't match expected type `(a, b)'
12:17:52 <lambdabot>         against inferred type `[GHC.T...
12:17:55 <byorgey> so if (a,b) were an instance of Num, then  fst 3  would typecheck
12:18:02 <shachaf> > 5 :: ((Int,Int),(Int,Int))
12:18:03 <lambdabot>   ((5,5),(5,5))
12:18:14 <byorgey> so GHC gives it the type   Num (a,b) => b  I suppose
12:18:22 <Saizan> Num (a,b) => a
12:18:25 <byorgey> clklein: is that the type it prints if you say  :t fst 3 ?
12:18:30 <byorgey> ah, right, a
12:18:35 <sproingie> > 5 :: [Int]
12:18:35 <lambdabot>   No instance for (GHC.Num.Num [GHC.Types.Int])
12:18:35 <lambdabot>    arising from the literal `...
12:18:35 <maurer_> :t (fst 3)
12:18:36 <lambdabot> forall a. (Num a) => a
12:19:09 <clklein> byorgey: Yes, Prelude> :t fst 3\nfst 3 :: (Num (a, b)) => a
12:19:27 <tshred> can somebody tell me what >>= does
12:19:35 <shachaf> tshred: It binds.
12:19:35 <byorgey> clklein: right.  Now, the reason it doesn't work if you try to evaluate it is because there is no instance of Num for (a,b).
12:19:36 <clklein> But "Prelude> fst 3" complains that there's no (Num (a, b)) instnace.
12:19:41 <byorgey> clklein: but it type checks because there could be.
12:19:44 <shachaf> clklein: Yep. That's defined in lambdabot.
12:19:54 <copumpkin> tshred: it's not a beginner topic, but it's similar to concatMap on lists
12:20:02 <clklein> byorgey: Isn't that kind of... insane?
12:20:08 <sproingie> \bot speaks an odd dialect
12:20:11 <tshred> context: i'm a beginner trying to implement something that is blowing my mind
12:20:27 <shachaf> clklein: When you say "5", that's implicitly "fromInteger (5::Integer)".
12:20:29 <parcs> tshred: go read "tackling the awkward squad"
12:20:37 <tshred> ok :D
12:20:47 <shachaf> clklein: Anything that's an instance of Num can define fromInteger.
12:20:52 <byorgey> clklein: well, it's a bit odd, I'll grant.  but it's a perfectly consistent outworking of the way the type system is set up.
12:20:54 <tshred> ty i'll go do that now
12:20:57 <shachaf> clklein: You can make any type an instance of Num.
12:21:09 <sm> is there any way that a ghc-compiled, statically-linked executable's behaviour (eg relating to IO encoding) could be affected by the installed ghc version at runtime ?
12:21:26 <byorgey> tshred: do you know the type of (>>=) ?
12:21:31 <clklein> shachaf: is there a way I can avoid importing `fromInteger' and still be able to type literal integers?
12:21:55 <byorgey> tshred: you are not allowed to ask what it *does* until you have stared at its type for at least ten minutes
12:21:57 <sproingie> tshred: read LYAH, it covers >>= in the section on monads
12:22:38 <monochrom> sm: statically linked? then no.
12:23:07 <byorgey> @where lyah
12:23:07 <lambdabot> http://www.learnyouahaskell.com/
12:23:08 <tac-tics> @type (>>=)
12:23:09 <lambdabot> forall (m :: * -> *) a b. (Monad m) => m a -> (a -> m b) -> m b
12:23:21 <jelly1> i must say the 99 questions of haskell are fun :)
12:23:28 <sm> thanks, that's what I thought
12:23:30 <absentia> url ?
12:23:37 <jelly1> absentia: at me?
12:24:20 <xrch> oh, I think I understand how zygo works %
12:24:27 <xrch> thaks!
12:24:30 <sm> someone is getting an encoding error when reading a utf8 file with a ghc 6.12.3-compiled app, with a valid utf-8 locale installed and configured, and I can't figure out why
12:24:33 <xrch> *thanks!
12:25:02 <tac-tics> Is there a name for parameters which are never used at runtime?
12:25:29 <Saizan> iiuc, phantom
12:25:41 <tac-tics> Like the witness parameter, Monad m in the type of (>>=)
12:25:47 <tac-tics> Thanks
12:26:07 <jmcarthur> Monad m isn't a parameter
12:26:09 <jmcarthur> it's a constraint
12:26:21 <tac-tics> it's implemented as a parameter of sorts
12:26:39 <jmcarthur> (it could be construed as a parameter if the implementation uses dictionary passing, but that isn't true in general)
12:27:00 <shachaf> tac-tics: When it's implemented as a parameter, that parameter *is* used at runtime, no?
12:27:12 <Saizan> tac-tics: then no, i was thinking of something else :)
12:27:32 <tac-tics> I'm talking about theory. I'm not worried about implementation.
12:27:42 <jmcarthur> then it's definitely not a parameter
12:28:07 <tac-tics> shachaf, I guess in the case of that, it is, isn't it?
12:29:27 <edwardk> xrch: great =)
12:30:17 <tac-tics> I was thinking the other day about proof terms, and how if you want a proof of P, and there is a proof of P sitting in the environment already, it could be passed automatically whenever it's needed
12:31:00 <tac-tics> Just as Monad m is passed automatically for you in a dictionary impl
12:31:34 <clklein> byorgey: What constraint does the REPL impose that forces the typechecker to discover that there's no instance of (Num (a, b))?
12:31:38 <clklein> i.e., when I don't use ":t"
12:33:16 <maurer_> clklein: It attempts to actually pick an instance...
12:34:18 <Saizan> clklein: when you don't use ":t" you're asking it to actually evaluate the expression, to do so it has to decide which code to run, so it has to decide which instance to use
12:35:25 <clklein> Ah, that makes sense.
12:35:35 <monochrom> indeed, the constraint of taking actions rather than just talking about it.
12:37:03 <clklein> If I write "x = fst 3" at the top-level of a module, it doesn't check for an instance because I might export x to an unknown module that does have an instance in scope?
12:38:07 <monochrom> It can check and it already checks. All import statements have already occured before any top-level definitions. There are no more imports.
12:39:08 <shachaf> Hmm, «T (fst 3)» gives an error in ghci even with instance Show (T a) where show _ = "T".
12:39:19 <shachaf> So it doesn't actually have to evaluate the expression. :-)
12:39:23 <clklein> so there's no reason it couldn't reject the definition "x = fst 3" outright?
12:39:44 <monochrom> It rejects the definition.
12:40:12 <shachaf> monochrom: It doesn't.
12:40:34 <monochrom> I just tried.
12:40:49 <shachaf> monochrom: What did you try?
12:41:08 <monochrom> ""x = fst 3" at the top-level of a module"
12:42:43 <shachaf> monochrom: Oh... It doesn't fail when you ghci the file, or a file that imports it.
12:42:53 <shachaf> monochrom: But it does fail with runghc. Never mind.
12:43:04 <monochrom> I used ghci. Rejects.
12:43:28 <shachaf> monochrom: What files did you use?
12:43:48 <monochrom> a file named "check.hs" and the only line "x = fst 3"
12:44:05 <monochrom> Then I pressed ctrl-C ctrl-L in emacs
12:44:38 <unkanon_> you gotta have more than that, fluff it up a bit and make it pretty so ghci gets distracted and doesn't typecheck that line  :P
12:44:50 <monochrom> har
12:45:17 <clklein> Ooh, I see. I had NoMonomorphismRestriction!
12:45:31 <jmcarthur> o_O
12:45:31 <kafee> hi, does anyone knows how to use qtHaskell signal  and slots ?
12:45:37 <shachaf> monochrom: http://hpaste.org/42303/loading_a_file
12:48:24 <monochrom> shachaf: http://hpaste.org/paste/42303/loading_a_file_annotation#p42304
12:50:27 <alpounet> kafee, the same way as with C++
12:50:30 <alpounet> just adapting the syntax
12:50:32 <alpounet> see the manual
12:51:03 <kafee> I saw
12:51:04 <kafee> but does not work
12:52:48 <shachaf> monochrom: That is a bit odd.
12:53:09 <kafee> alpounet: look, http://hpaste.org/42305/qthaskell
12:53:51 <jmcarthur> shachaf: what's so odd about it?
12:54:02 <shachaf> jmcarthur: That I don't get the same behavior.
12:54:23 <alpounet> kafee, then ask qthaskell's author
12:54:28 <alpounet> i haven't experienced much with it
12:54:35 <jmcarthur> shachaf: do you have a .ghci file or something that sets NoMonomorphismRestriction?
12:55:10 <shachaf> jmcarthur: Yep.
12:55:13 * shachaf just realized that.
12:56:45 <monochrom> But I was still wrong because we could functionalize it and try some other types too.
12:56:59 <monochrom> @type \y -> negate [y]
12:57:00 <lambdabot> forall t. (Num [t]) => t -> [t]
12:57:08 <shachaf> monochrom: Yes, the issue wasn't MR-related, I think.
12:57:28 <monochrom> Just in case:
12:57:38 <monochrom> > negate [3 :: Integer]
12:57:39 <lambdabot>   No instance for (GHC.Num.Num [GHC.Integer.Internals.Integer])
12:57:39 <lambdabot>    arising fr...
12:58:52 <monochrom> (\y -> negate [y]) is a rather definitive example that my opinion cannot explain.
12:59:17 <unkanon_> @src [] negate
12:59:17 <lambdabot> Source not found. BOB says:  You seem to have forgotten your passwd, enter another!
12:59:34 * unkanon_ has no idea of what it means to negate a list
12:59:34 <monochrom> or in a file you could use "x y = negate [y]" and it's accepted.
13:00:09 <monochrom> oh it's just a prelude function to bring in the Num constraint upon the list type
13:00:41 <monochrom> this is an exercise in type-checking not evaluating.
13:01:35 <unkanon_> hmm
13:05:42 <monochrom> Heh, it is indeed accepted in hope that you import it into some other module B and module B somehow also coughs up an instance for Num (a,b) or Num [t] (depending on your example)
13:20:43 <clklein> Thanks for the help, everyone!
13:22:14 <ologNation> Is it good form to nest "where" clauses in function definitions? 
13:22:23 <kmc> sometimes?
13:22:28 <kmc> you can hpaste some code and ask for an opinion
13:22:46 <sproingie> it's good form if it's more readable
13:23:18 <sproingie> personally i never see much need to nest them.  unless 'where' isn't recursive?
13:23:34 <kmc> it is
13:24:42 <sproingie> i usually use 'let' for binding variables and 'where' for defining one-off support functions
13:26:22 <ologNation> kmc, Thanks. 
13:26:27 <ologNation> I don't think a paste will be necessary. 
13:26:34 <ologNation> I think that "let" is probably the right way to go. 
13:26:48 <kmc> i like let
13:26:51 <ologNation> I am working on using ghci more. 
13:27:02 <ologNation> Is it possible to write functions in emacs and attach them one at a time? 
13:27:02 <kmc> the advantage of "where" is that it scopes over multiple guards
13:27:11 <kmc> but that's rarely used
13:27:18 <kmc> and it doesn't scope over multiple equations
13:27:20 <monochrom> I like randomly choosing between where and let.
13:27:21 <ologNation> kmc, Do you have an example? 
13:28:01 <kmc> > let f n | odd n = g "odd" | otherwise = g "even" where g x = "It's " ++ x in f 3
13:28:01 <lambdabot>   "It's odd"
13:29:31 * hackagebot hamlet 0.6.1.1 - Haml-like template files that are compile-time checked  http://hackage.haskell.org/package/hamlet-0.6.1.1 (MichaelSnoyman)
13:30:30 <ologNation> Suppose that I'm interacting with ghci and I want to add just one function. 
13:30:42 <ologNation> Is there a way to write that function in a text file and attach it to the context 
13:30:45 <ologNation> during a session?
13:30:51 <ologNation> :add? import? 
13:31:19 <ologNation> got it! (:load)
13:31:25 <ologNation> Sometimes I can't figure something out until I ask. 
13:31:32 * hackagebot mime-mail 0.0.0.4 - Compose MIME email messages.  http://hackage.haskell.org/package/mime-mail-0.0.0.4 (MichaelSnoyman)
13:31:34 * hackagebot mime-mail 0.1.0.1 - Compose MIME email messages.  http://hackage.haskell.org/package/mime-mail-0.1.0.1 (MichaelSnoyman)
13:31:36 * hackagebot yesod 0.6.7 - Creation of type-safe, RESTful web applications.  http://hackage.haskell.org/package/yesod-0.6.7 (MichaelSnoyman)
13:32:41 <ologNation> Nope.  that didn't work. 
13:32:49 <ologNation> It cleared out the other functions that I had in scope. 
13:33:03 <ologNation> Is there a way to "add" just one function while using ghci?  I wonder..
13:33:25 <monochrom> I think every way clears out other functions, especially those defined at the prompt.
13:33:42 * mjrosenb had a "use" function similar to the one from sml
13:33:49 * mjrosenb does not remember how he wrote it
13:34:09 <ologNation> monochrom, Okay.  Maybe this isn't the way to do it then. 
13:34:24 <ologNation> My usual way is to always compile and run. 
13:34:45 <monochrom> for historical reasons we seldom define things at the prompt
13:34:46 <ologNation> I've had fantasies lately at learning to use the interpreter very well for editing and coding. 
13:34:54 <merijn> If I have a tree (or some other data structure) and after a modification the original is thrown away, will GHC optimize this to in place update?
13:35:01 <ologNation> well, it's a pain to define things at the prompt. 
13:35:07 <ologNation> You want to be in an editor. 
13:36:39 <ologNation> Has anyone heard of the convention of factoring code into screenfuls? 
13:36:53 <ologNation> I had a high school programming teacher who told me that this was good practice, but I haven't heard it since then. 
13:37:11 <copumpkin> I do my best to avoid functions longer than a few lines in haskell
13:37:18 <merijn> meh, I just go by intuition of "can I write this any prettier?"
13:37:19 <copumpkin> where a few is definitely less than 10
13:37:29 <ologNation> copumpkin, Hm.. That's interesting.
13:37:35 * hackagebot authenticate 0.7.2.3 - Authentication methods for Haskell web applications.  http://hackage.haskell.org/package/authenticate-0.7.2.3 (MichaelSnoyman)
13:38:34 <teki> the earlier question about where bindings reminded me of a question about contexts i had.
13:38:34 <teki> suppose i write the following:
13:38:34 <teki> f :: ...
13:38:43 <teki> oops, hit enter not shift-enter
13:38:44 <teki> anyway
13:38:56 <teki> if we do guards in our definition of f
13:39:05 <teki> and want to where-bind two variables at the end
13:39:07 <teki> (or more)
13:39:19 <teki> how is the context of those where bindings determined
13:39:50 <teki> i was writing code yesterday, and found that if I tab over after the "where"
13:40:21 <teki> then the next definition will be weirdly aligned (under 4 spaces/tab alignment)
13:40:35 <teki> i'm trying to figure out this hpaste thing, will post there
13:40:35 <monochrom> "every procedure should be at most 60 lines" was the first factor-code-into-screenful guideline. It is 70 years old and it is based on printed pages holding 60 lines each.
13:42:18 <monochrom> My own guideline for myself is factor as much as life permits.
13:42:20 <ologNation> Modius, Cool. 
13:42:28 <ologNation> er... monochrom , cool!
13:42:57 <ologNation> Well, I'm noticing that my own code is running into screens and screens of code. 
13:43:02 <monochrom> 60 lines is really an arbitrary threshold.
13:43:29 <ologNation> It is indeed arbitrary, but it's good if it encourages you to factor intentionally. 
13:43:30 <merijn> Is the speed difference between Map and IntMap negligible or should I always prefer using IntMap where possible>
13:44:09 <copumpkin> merijn: use IntMap where possible
13:44:10 <ologNation> Another question regarding scope: Is it generally possible to collect "import" statements into one file? 
13:44:13 <monochrom> If you have a 1000-line procedure but it clearly is a sequence of 100 logical units, 10 lines each, there is nothing wrong, you just have to use blank lines and comments to section them.
13:44:15 <ologNation> Is it good form? 
13:44:46 <kmc> merijn, the speed difference is significant
13:44:56 <kmc> they're completely different datastructures; it's not just a matter of inlining polymorphism
13:45:07 <kmc> however there are pathological cases where Map performs better
13:45:38 <merijn> kmc: I saw that IntMap was O(min(n,W)), yeah. But I wasn't sure whether that'd be worth going out of my way to use Int's then say an enumeration
13:45:53 <merijn> Related question: Does GHC optimize "update & throw-away old" into in place update?
13:45:53 <kmc> there's also the EnumMap package
13:46:04 <kmc> merijn, not really
13:46:21 <kmc> however the "copy" when updating a tree-shaped structure like Map is only a copy of a small part of your data
13:46:28 <merijn> I know
13:46:32 <kmc> most of the new structure will be pointers into the old
13:46:35 <merijn> Only the nodes between update and root
13:46:47 <kmc> if you want to know what optimizations ghc is doing, you may have to look at Core and assembly
13:46:52 <kmc> (or Cmm)
13:47:29 <merijn> That's a bit premature for what I'm doing, just trying to get a rough idea of the performance characteristics of tree's and maps so I can attempt a semi-intelligent design
13:47:55 <kmc> how big is your tree?
13:48:07 <kmc> asymptotic analysis is bullshit
13:48:12 <kmc> especially for most real-world problem sizes
13:48:45 <c_wraith> eh.  Not when you're comparing O(n^2) to O(n).  doesn't take long for that difference to be a big deal
13:49:50 <c_wraith> Though it depends on if the input size is something user-defined or programmer-defined, of course.
13:50:08 <merijn> Lets take an upper limit with an unreasonably large safety margin and say in the order of 100k elements?
13:50:55 <kmc> c_wraith, if your O(n^2) has better cache behavior or something it might be faster for pretty large n
13:51:06 <C_Bomb121> Anybody looked into Snap framework?
13:51:36 <c_wraith> C_Bomb121, does helping write it count?  We're actually in a pretty active phase right now.  Join #snapframework if you have specific questions :)
13:52:20 <merijn> kmc: I'm not really interested in asymptotic analysis anyway, I just want sort of ballpark performance characteristics
13:52:25 <alpounet> is there a large website based on snap now or not yet ?
13:52:37 <teki> alright
13:52:54 <kmc> merijn, i would say use IntMap if the amount of code contortion necessary is trivial
13:52:56 <teki> can somebody familiar with where-contexts in haskell explain to me the reason for
13:53:00 <kmc> otherwise use Map and see if it matters in profiling
13:53:00 <teki> this behaviour: http://hpaste.org/42306/wherebindings
13:53:14 <c_wraith> alpounet, depends on your definition of "large"
13:53:19 <kmc> merijn, EnumMap seems like a nice package for making IntMap more easily applicable, but i had trouble building it recently :/
13:53:43 <merijn> Let's say (as before) I have 100k elements I need to keep track of, would I be better of using one big tree, multiple medium size trees or lots of small trees to spread out the updates over multiple data structures?
13:53:52 <benmachine> does anyone know how the FFI interacts with functions declared with the inline keyword?
13:54:00 <alpounet> c_wraith, well, what's the biggest one (meaning with the biggest charge) using it?
13:54:04 <benmachine> do I need wrappers?
13:54:21 <merijn> teki: I believe GHC treats tab as 8 spaces
13:54:40 <c_wraith> alpounet, I have no clue what charge means in that context
13:54:43 <teki> @merjin
13:54:43 <lambdabot> Unknown command, try @list
13:54:57 <teki> merjin - alright, the editor i'm using has tab = 4 spaces
13:54:58 <merijn> teki: In which case your tab use makes the code align wrong
13:55:10 <merijn> teki: Get an editor which is not broken
13:55:33 <teki> thanks - this was the source of several very frustrating and awkward workarounds :)
13:55:49 <alpounet> c_wraith, hm, the biggest number of visitors, pages generated, etc
13:55:56 <merijn> Tabs should be eight spaces and editor should be smart enough to replace tab presses with spaces. Which editor do you use?
13:56:08 <c_wraith> merijn, 100k elements isn't that big.  I'd just use IntMap until profiling tells you it's a problem
13:56:21 <teki> crimson editor
13:56:29 <merijn> Ah, don't know that one
13:56:43 <c_wraith> well, it requires that the keys be Ints
13:56:55 <merijn> c_wraith: That was aimed at teki :)
13:57:14 <c_wraith> merijn, heh.  misread.  But my warning remains! :)
13:57:16 <teki> it's a windows editor; i normally use programmer's notepad but that doesn't have a haskell scheme that's good
13:57:28 <teki> do you have a better recommendation for a windows editor?
13:57:42 <teki> other than installing a linux distro :)
13:57:44 <nostrand> merijn: log 100k ~ 16 så an intmap shouldn't be a problem =)
13:57:55 <merijn> I use gvim on Windows, but using vim takes effort and frustration to learn :p
13:58:07 <C_Bomb121> notepad ++
13:58:14 <nostrand> merijn: i'd follow c_wraith's advice
13:58:22 <teki> @C_Bomb121 thanks for the tip
13:58:22 <lambdabot> Unknown command, try @list
13:58:47 <dmwit> merijn: Really? What's so different between vim and gvim?
13:59:02 <dmwit> (And why not just use gvim?)
13:59:07 <fryguybob> teki: I use notepad ++  on windows, though I also have emacs with haskell-mode on windows.
13:59:13 <jelly1> dmwit: just gtk interface 
13:59:32 <teki> fryguybob : that's two recommendations for notepad++, time to try it out
13:59:32 <merijn> dmwit: Nothing, but I meant that learning (g)vim when you are a Windows user with no vim experience is likely to be frustrating
13:59:40 <jelly1> hehe 
13:59:42 <dmwit> oh, oh
13:59:58 <jelly1> but you should join the dark side of linux , we have XMonad :p
13:59:59 <teki> merijn: even worse, given that i prefer emacs to vim
14:00:05 <dmwit> jelly1: Er, right, I've turned off all the GUI elements (except the text area) in gvim, so I forgot that it existed. =P
14:00:12 <jelly1> dmwit: hahaha
14:00:15 <merijn> teki: I'm pretty sure there's an emacs for Windows...
14:00:26 <jelly1> dmwit: lol why do you use gvim then?
14:00:26 <teki> merijn: i know, but i only use it when i have to
14:00:40 <jelly1> btw are there usefull vim plugins for haskell ( now we are talking about vim)
14:00:50 <teki> oh lord i think i started an editor war
14:01:04 <armorsmith42> there is scion, which I'm working on installing right now. 
14:01:06 * jelly1 doesn't see any war
14:01:09 <merijn> jelly1: I use gvim on windows because vim requires a console and Windows' console sucks
14:01:16 <jelly1> merijn: hehe np
14:01:23 <dmwit> jelly1: More colors, plus specifying color by RGB rather than palette number.
14:01:25 <jelly1> then you need qming or stuff yeah
14:01:29 <jelly1> dmwit: aha
14:01:40 <merijn> (Actually, I mostly switched to OSX nowadays and my life is a lot happier :p)
14:01:42 <kmc> i have not once in my life met a real person who engages in editor wars
14:01:42 <dmwit> jelly1: Plus red wavy underlines on misspellings instead of obnoxious red background that makes it unreadable.
14:01:59 <dmwit> jelly1: Plus half a dozen other things that graphics can do, but text can't.
14:02:10 <jelly1> dmwit: ohh i admit i just turned spelling check off cause of the annoying spellcheck stuff
14:02:32 <armorsmith42> Speaking of which, have any vim users had any luck with scion? the installation instructions for the vim frontend seem less than clear. 
14:02:34 <ion> For a decent Windows™ terminal, there’s http://code.google.com/p/mintty/ based on PuTTY.
14:02:35 <dmwit> Well, I TeX and LaTeX a lot, and it's very helpful for actual text (though I turn it off for coding, as well).
14:02:44 <unkanon_> I use vim and I have caps lock for Ctrl. my coworker spilled gatorade over my keyboard and the stickiest key I got now is caps lock!  :((
14:03:00 <merijn> c_wraith: I'm sorta trying to figure out how to handle game state in haskell, unfortunately I find myself needing to search the data in multiple ways preferably reasonably efficient. But I guess I should just use multiple trees/IntMap's and worry about performance once profiling shows its an issue
14:03:09 <jelly1> dmwit: i use LaTeX too, then spelling check is usefull and for mail ofcourse :)
14:03:09 <dmwit> unkanon_: You can get a new keyboard for about $20 including shipping...
14:03:25 <unkanon_> dmwit: for a laptop?
14:03:29 <jelly1> 10/99 questions done :p
14:03:30 <merijn> dmwit: Not if you want a good keyboard
14:03:30 <dmwit> unkanon_: ...
14:03:49 <dmwit> merijn: If you have a good keyboard, it's easy to clean. =)
14:03:51 <unkanon_> you want me to use an external keyboard while using the laptop?
14:03:52 <c_wraith> merijn, I'd say that sounds reasonable, though you probably should abstract out adding and removing entities to all of the different indexes, so they stay consistent
14:03:54 * dmwit has a good keyboard
14:03:57 <unkanon_> there's no room
14:04:16 <dmwit> unkanon_: Yeah, sorry. For laptops you're in more trouble. =/
14:04:24 <jelly1> pfft 99 questions really give me headaches :p
14:04:36 <unkanon_> yeah. I tried learning online how to pop a keyboard key but I don't trust myself not to break it
14:06:12 <merijn> c_wraith: I guess the best approach is to have every Tree/Map just list an identifier which looks up the actual object in an IntMap so updating any objects just requires updating the IntMap and the tree/map relevant for the changed value
14:06:54 <dmwit> unkanon_: Laptop keys are especially pernicious in that regard. You probably won't break it, but with one laptop I had, it took me 90 minutes to get the key connected correctly!
14:07:13 <c_wraith> merijn, I'm not convinced that's best.  But it probably isn't bad.
14:07:36 <unkanon_> dmwit: well I'll have to give it a shot. I thought it was getting better with use but now it's worse. my pinky also hurts
14:08:00 <merijn> c_wraith: I've been wracking my brains for a day trying to think of a better way but so far I got nothing
14:08:11 <dmwit> unkanon_: Another thing you could try is just pouring a bit of water on it a few times (after turning the laptop off of course).
14:08:20 <dmwit> unkanon_: Make sure that you give it enough time to dry before turning the laptop back on!
14:09:05 <teki> unkanon_: most laptops also (with a good screwdriver, patience, and caution) can have their keyboards removed
14:09:15 <teki> unkanon_: i clean mine about once a year
14:09:52 <c_wraith> merijn, I think your best bet is to just start implementing it, and see what obstacles you run into
14:10:10 <tux_mark_5> hello
14:10:50 <tux_mark_5> is there a way to include all dependencies into single .so file when building with a shared library in linux?
14:11:13 <unkanon_> dmwit: I don't think the water is a good idea with this type of keyboard
14:11:25 <unkanon_> plus gatorade is really sugary and I doubt it would help
14:11:30 <unkanon_> I need to actually scrape it
14:11:56 <unkanon_> teki: right, the keyboard can be removed but that won't help me clean the keys
14:12:06 <unkanon_> underneath the keys
14:12:11 <teki> merijn: just installed notepad++ as per quite a few recommendations and it's also 4 spaces per tab...sighhhhhhh
14:12:31 <merijn> teki: I'm pretty sure you can configure it in notepad++
14:13:09 <merijn> teki: Search for a "soft tabs" option
14:13:10 <teki> merijn: spoke about half a minute too soon, just found it
14:13:19 <merijn> Ah, nice :)
14:13:32 <teki> thanks for the tip anyways
14:15:20 <unkanon_> notepad++ is awesome, I used it when on windows
14:17:09 <kmc> merijn, IntMap is pretty close to a flat array on lookups, and is much faster than a flat array on insert
14:17:15 <kmc> i don't think it will be a big problem
14:17:32 <kmc> if you write the code in a nice modular way, you can swap in other modules to test
14:24:16 <copumpkin> hmm
14:24:33 <copumpkin> it kind of bothers me that there's a zipWith on ByteString that returns a list, but map returns a ByteString
14:28:41 <ddarius> Simply remove one or both and you'll be set.
14:31:01 <nostrand> copumpkin: http://hackage.haskell.org/packages/archive/bytestring/0.9.1.8/doc/html/src/Data-ByteString.html#zipWith
14:31:19 <copumpkin> yes?
14:31:37 <nostrand> copumpkin: either use zipWith' or you can rely on rewrite rules =)
14:31:40 <copumpkin> I was complaining mostly in the other direction :P
14:31:47 <copumpkin> I want a map that gives me a list
14:34:09 <ddarius> zipWith (const f) (fromChunks (fix (singleton 0:)))
14:34:23 <copumpkin> lol
14:34:31 <copumpkin> it has a cycle, too
14:39:23 <dfkjjkfd>  
14:39:48 <nkpart> anyone got a minute to look over my attempt at using TypeCompose and AF composition? https://gist.github.com/739718 -- I made it work, but it's pretty ugly
14:48:33 <aristid> :t zipWith . const
14:48:34 <lambdabot> forall a b c. (b -> c) -> [a] -> [b] -> [c]
14:51:51 <aavogt> @free f :: (b -> c) -> [a] -> [b] -> [c]
14:51:51 <lambdabot> h . p = q . g => $map h . f p xs = f q ($map k xs) . $map g
14:52:39 <aavogt> hmm, was it always called $map ?
14:55:51 <JuanDaugherty> has ghc 7 been found to compile (in general) what 6 would?
14:56:06 <kmc> there's a few tricky bits
14:56:12 <benmachine> JuanDaugherty: in general
14:56:22 <JuanDaugherty> benmachine, thx
14:56:30 <kmc> the extensions GADTs and TypeFamilies now imply the extension MonoLocalBinds
14:56:41 <kmc> which could break some code, though the fix is straightforward
14:56:41 <benmachine> yeah, there are some minor differences
14:56:56 <JuanDaugherty> are they noted someplace collectively?
14:56:56 <kmc> anything that's standard Haskell should be fine
14:57:02 <kmc> release notes for GHC 7 perhaps
14:57:35 <JuanDaugherty> i c
14:57:48 <JuanDaugherty> thx
14:58:05 <Philippa> there's standard Haskell that requires annotations with MonoLocalBinds
14:58:13 <Philippa> that's kinda the whole point, really
14:58:37 <kmc> but standard haskell isn't using GADTs or TypeFamilies
14:59:19 <Philippa> sure. OTOH, code that's standard haskell that's in the same module as other code that uses them? Not so pretty
14:59:49 <Philippa> normally those kinds of things are strictly local properties, not so much here
14:59:54 <aavogt> base-3 disappeared
15:08:23 <ghulette> anyone know how can i get rid of this type error "Ambiguous type variable `m' in the constraint: `Monoid m' arising from a use of `eval'" in this code: https://gist.github.com/739723
15:08:35 <ghulette> there are actually two errors
15:08:39 <ghulette> one in test
15:08:44 <ghulette> the other in Neg
15:14:33 <Axman6> ghulette: my guess is that when you're using eval, the compiler doesn't know which monoid you mean m to be
15:15:06 <ghulette> Axman6: yes, that is true
15:16:31 <Axman6> ghulette: also, why are you using Just (_,_) and not Just _?
15:17:51 <hpc> Axman6: to cause strictness bugs, obviously :P
15:18:01 <Axman6> heh
15:18:03 <ghulette> Axman6: either way, same error.  I was using the second _ originally to try to bind the monoid's type
15:18:08 <ghulette> i want it to be strict
15:18:32 <c_wraith> That's still not fully strict.  Just strict in the tuple constructor
15:18:41 <c_wraith> (and the Just constructor, of course)
15:18:42 <ghulette> true
15:18:50 <ghulette> anyway
15:19:17 <ghulette> there must be some way to tell GHC that i want mempty to be the same type as the result from eval
15:19:18 <ghulette> right?
15:19:25 <kmc> :t asTypeOf
15:19:26 <lambdabot> forall a. a -> a -> a
15:19:34 <kmc> > 2 `asTypeOf` 3
15:19:34 <lambdabot>   2
15:19:36 <kmc> > 2 `asTypeOf` ()
15:19:37 <lambdabot>   No instance for (GHC.Num.Num ())
15:19:37 <lambdabot>    arising from the literal `2' at <intera...
15:19:42 <kmc> > 2 `asTypeOf` 7.5
15:19:43 <lambdabot>   2.0
15:19:46 <Axman6> if you wanted it to be fully strict, you should use Just (!x,!y), that's about as strict as you can make it without knowing what the monoid is
15:19:46 <ghulette> ahhh
15:19:53 <sipa> > 2 `asTypeOf` (undefined,undefined)
15:19:54 <lambdabot>   (2,2)
15:20:12 <ghulette> that looks good for Test
15:20:17 <hpc> wait, wtf?
15:20:26 <hpc> tuples are num too?
15:20:39 <sipa> > (2 `asTypeOf` (const undefined)) 3
15:20:40 <lambdabot>   2
15:20:40 <Axman6> bloody Cale
15:20:43 <benmachine> > 2 :: (Integer,Double)
15:20:43 <lambdabot>   (2,2.0)
15:20:54 <benmachine> Axman6: don't be so mean :P
15:20:55 <ghulette> but how do i use asTypeOf for the Neg case
15:21:01 <benmachine> it's lambdabot it's supposed to be a bit weird
15:21:05 <Axman6> > 2 :: (Int, Integer, Double)
15:21:05 <lambdabot>   (2,2,2.0)
15:21:11 <Axman6> >___<
15:21:25 <benmachine> it's that vector space package again
15:21:27 <sipa> > 2 :: (Int -> Int, Integer, (Double, Int))
15:21:28 <lambdabot>   (*Exception: showsPrec: No overloading for function
15:21:34 <ddarius_> Clearly we need a Num instance for ().
15:21:38 <benmachine> mmhm!
15:21:40 <hpc> hehe
15:21:48 <kmc> :t ('x',undefined)
15:21:49 <lambdabot> forall a. (Char, a)
15:21:50 <sipa> > fst (2 :: (Int -> Int, (Integer, (Double, Int)))) 5
15:21:50 <lambdabot>   2
15:21:53 <kmc> :t (undefined,'y')
15:21:54 <lambdabot> forall a. (a, Char)
15:22:02 <dmwit> > first ($3) (2 :: (Int -> Int, (Integer, Double, Int)))
15:22:02 <lambdabot>   (2,(2,2.0,2))
15:22:03 <kmc> :t ('x',undefined) `asTypeOf` (undefined,'y')
15:22:04 <lambdabot> (Char, Char)
15:27:17 <dr460neye> hi
15:30:28 <dr460neye> i got a problem in my method which should remove all chars like [] () , but everytime i get an error like> lexical error in string/character literal at character '\''     -- http://hpaste.org/42307/split_and_remove anybody online who can help me? 
15:31:36 <krey> hello, I'm trying to "fix" things, can someone take a look to help me understand why the second example (even, odd) doesn't work? http://pastebin.com/M8FXLfMu
15:33:33 <benmachine> krey: what's going wrong?
15:33:52 <krey> dr460neye: yeah, '' <- doesn't mean anything
15:34:18 <krey> benmachine: fst parity 5 never stops :(
15:34:42 <benmachine> krey: wild guess, replace your tuple pattern matches with lazy ones
15:34:46 <benmachine> ~(f,g)
15:34:59 <krey> ???
15:35:36 <benmachine> are you familiar with irrefutable patterns?
15:35:48 <krey> yes
15:35:50 <krey> it works
15:35:55 <benmachine> oh right
15:35:58 <benmachine> excellent.
15:36:05 <krey> no, I am unfamiliar with these
15:36:21 <krey> could you explain please?
15:37:51 <benmachine> oh
15:37:52 <benmachine> basically
15:37:59 <benmachine> matching a tuple is strict
15:38:02 <benmachine> observe:
15:38:19 <benmachine> case undefined of (_,_) -> ()
15:38:21 <benmachine> er
15:38:22 <benmachine> > case undefined of (_,_) -> ()
15:38:23 <lambdabot>   *Exception: Prelude.undefined
15:38:41 <krey> ok
15:38:44 <krey> I have observed
15:38:47 <krey> but why?
15:38:56 <ddarius> Matching against any constructor (other than a newtype constructor) is strict.
15:40:16 <benmachine> yeah, pattern matching is strict in general
15:40:26 <benmachine> it can be made lazy by use of irrefutable patterns
15:40:33 <copumpkin> > let (_, _) = undefined in 0
15:40:34 <lambdabot>   0
15:40:39 <ddarius> benmachine: Your statements seem contradictory.
15:40:47 <benmachine> ddarius: which ones
15:41:02 <hpc> irrefutable patterns are unsafe though
15:41:11 <krey> unsafe?
15:41:15 <benmachine> hpc: that's not entirely true
15:41:20 <copumpkin> > let ((x, y), (a, b)) = ((1, 2), undefined) in x
15:41:20 <lambdabot>   *Exception: Prelude.undefined
15:41:23 <ddarius> It's entirely false.
15:41:27 <benmachine> irrefutable patterns throw an error if they get refuted
15:41:38 <benmachine> but sometimes you know that's not possible
15:41:38 <hpc> that's what i meant
15:41:44 <benmachine> or sometimes it's ok to do that
15:42:05 <c_wraith> It's really hard to refute (,)
15:42:08 <hpc> you have to be careful though
15:42:11 <ddarius> undefined refutes it.
15:42:14 <c_wraith> The typechecker would yell at you :)
15:42:27 <c_wraith> undefined does not refute the pattern, undefined diverges.
15:42:34 <iammisc> In haskell is it possible to have a list of objects whose types are all instances of a class (even if those types are different)? For example, if I have a class C, and two instances of that class A and B, would it be possible to have a list of types of A and B?
15:42:34 <benmachine> well, the result is the same
15:42:39 <ologNation> http://hpaste.org/42308/let_and_where
15:42:40 <krey> why does pattern matching have to be strict by default?  ;(
15:42:43 <iammisc> a list of types A and B*
15:42:51 <c_wraith> krey: because pattern matching is what drives evaluation
15:42:57 <benmachine> iammisc: it is possible if you use some non-standard extensions
15:42:58 <ologNation> Can someone help me adjust the let and where in the above paste? 
15:43:15 <iammisc> benmachine: would that mean existentially quantified types?
15:43:19 <ddarius> krey: When you are pattern matching against a sum type, it necessarily needs to be strict to actually be able to make the decision.
15:43:30 <benmachine> iammisc: something like that
15:43:36 <ddarius> It's more consistent to have it also be strict in the single summand case.
15:43:51 <ologNation> I have a feeling that the "where" is special because it is paired with "instance"
15:44:02 <ddarius> (Your semantics won't suddenly change if you add or remove a constructor.)
15:44:23 <benmachine> ologNation: in a class instance you put declarations
15:44:24 <aavogt> ologNation: it's also paired with module, let, and functions!
15:44:27 <nostrand> ologNation: yep
15:44:35 <benmachine> ologNation: let ... in ... is an expression
15:44:40 <blackh> ologNation: Your problem isn't indentation.  You have to have function declarations after where
15:44:44 <tux_mobile_5> hello
15:44:56 <benmachine> ologNation: you need compare thing1 thing2 = let ... in ...
15:45:07 <benmachine> or compare thing1 thing2 = thing3 where ...
15:45:12 <tux_mobile_5> Any ideas why haskells executables are so huge?
15:45:16 <mee> iammisc: you might also make the list of some generic version of your class (g), and then build the list using like fromMyClass :: (Class k) => k -> g
15:45:26 <krey> ddarius: can you give me an example when non-strict pattern matching causes problems?
15:45:30 <benmachine> tux_mobile_5: they are statically linked by default
15:45:37 <hpc> tux_mobile_5: also the runtime system
15:45:45 <benmachine> tux_mobile_5: but ghc 6.12 and later support dynamic linking, which makes things a lot smaller
15:45:48 <tux_mobile_5> They are huge with -dynamic too
15:45:57 <benmachine> hmm
15:46:04 <c_wraith> That's all the runtime system
15:46:06 <benmachine> how huge is huge, also, which ones
15:46:25 <hpc> benmachine: hello world is about a megabyte
15:46:25 <tux_mobile_5> Bindings to gtk, pango etc take more space than the libraries themselves
15:47:25 <tux_mobile_5> Pango's source takes like 100 kb and with -dynamic it compiles into 1.8 mb so file
15:47:39 <tux_mobile_5> Which i stripped before measuring
15:48:02 * benmachine just build a 264K executable
15:48:05 <benmachine> *built
15:48:18 <benmachine> then stripped it to 180K
15:48:19 <ologNation> Thanks. 
15:48:26 <ologNation> Here's the winning code: http://hpaste.org/paste/42308/let_and_where_annotation#p42309
15:48:38 <tux_mobile_5> Executables themselves are small, but shared libs - aren't
15:48:45 <hpc> benmachine: ooh, how?
15:49:07 <benmachine> hpc: cabal configure --ghc-option=-dynamic
15:49:08 <benmachine> cabal build
15:49:15 <benmachine> ...just worked
15:49:19 <hpc> ah
15:49:32 <benmachine> if your dynamic executables are big, try using ldd to make sure they're actually dynamic?
15:49:32 <tux_mobile_5> Cairo - 1.4 mb, gio - 1.6 mb, gtk - 14 mb
15:49:53 <tux_mobile_5> Yes, i've checked ;)
15:49:53 <dcoutts_> benmachine: for an executable presumably, not a library?
15:49:58 <benmachine> dcoutts_: yes
15:50:42 <ddarius> krey: For sum types it's not a matter of "causing problems."  It just doesn't make any sense.  In if b then 1 else 2, and if is just syntactic sugar for case, how do you decide whether to return 1 or 2 without evaluating b?
15:50:46 <dcoutts_> benmachine: did we not implement --enable-shared for executables?
15:51:05 <benmachine> dcoutts_: uhm, I don't think --enable0shared builds a dynamically linked executable, no
15:51:08 <tux_mobile_5> The thing that concerns me is that bindings to c libraries take up more space than the libraries themselves - it seems even stupid
15:51:20 <dcoutts_> benmachine: mm, I guess we should fix that :-)
15:51:31 <tux_mobile_5> The big question is where these megabytes come from
15:51:52 <benmachine> dcoutts_: :)
15:52:20 <tux_mobile_5> Any ideas?
15:52:25 <benmachine> although, is wanting to build a shared library and wanting to dynamically link executables really the same thing?
15:52:31 <benmachine> *are
15:53:08 <tux_mobile_5> Btw im running on x86_64, ghc 6.12.1
15:54:33 <benmachine> tux_mobile_5: honestly I don't know enough about how dynamic linking works to help
15:54:45 <tux_mobile_5> oh
15:54:54 <benmachine> maybe someone else can
15:55:28 <tux_mobile_5> It would be nice if anyone could confirm that the same is happening on their machines
15:56:07 <tux_mobile_5> Cabal install gtk --enable-shared would do the trick i guess
15:56:27 <benmachine> it might do
15:56:37 <benmachine> I just have shared: True in ~/.cabal/config
15:56:57 <dcoutts_> benmachine: yes in principle they're orthogonal but in practice they tend to go together
15:57:17 <dcoutts_> benmachine: so long as we allow full control we should have sensible defaults
15:57:35 <benmachine> dcoutts_: mm, that makes sense I suppose
15:58:04 <dcoutts_> benmachine: there's even further combinations like building PIC but then making static libs
15:58:19 <dcoutts_> e.g. if you wanted to statically link some libs into a shared lib
15:58:46 <benmachine> eek
16:00:32 <tux_mobile_5> Btw, how one could statically link libs into one shared lib with cabal?
16:00:52 <dcoutts_> tux_mobile_5: I'm not sure that's possible with cabal
16:01:02 <tux_mobile_5> Oh
16:01:02 <blackh> dcoutts_: I met up with Finlay Thompson last week and heard all about his project.  It's a great thing.  I know you guys are supporting him, but I told him I wanted to support him in any way I can too.  I'm very keen to encourage any Haskell-related commercial activity in New Zealand.
16:01:18 <dcoutts_> blackh: oh cool :-)
16:01:48 <dcoutts_> blackh: you can poke him and tell him he's not asking us enough questions :-)
16:02:00 <dcoutts_> we have spare capacity
16:02:05 <Cale> What's Finlay Thompson's project?
16:02:22 <dcoutts_> Cale: see http://www.well-typed.com/blog/48
16:03:53 <blackh> Cale: It's a company called Dragonfly that does scientific modelling.  The project they're working on is to do with fish stocks/populations (not sure what the correct word is. :) )
16:04:48 <krey_> ddarius: so how much is evaluated? for example [a] is obviously a sum type, but patterns aren't entirely strict
16:05:26 <blackh> dcoutts_: I will give him a poke.  Next time I'm in town I'm going to visit him in his natural habitat.
16:07:37 * ezrakilty coughs
16:08:17 <ezrakilty> Is it normal to have a Haskell function exported through the FFI return IO SomeType?
16:08:38 <ezrakilty> Examples I can find don't seem to be returning data inside an IO monad.
16:08:59 <c_wraith> ezrakilty, it depends entirely on the type of the function.
16:09:00 <ddarius> krey_: Enough to make a decision.
16:09:24 <c_wraith> ezrakilty, Int -> Int is a common FFI type, if you know the function is pure.
16:09:37 <c_wraith> ezrakilty, But if you know the function isn't pure, you should have IO in the return type.
16:09:42 <benmachine> ezrakilty: just to clarify, are you talking about calling haskell from C or C from haskell?
16:10:37 <ezrakilty> Talking about calling Haskell from C
16:10:47 <ezrakilty> and I want the Haskell to do some IO
16:11:09 <ezrakilty> but the result I get back seems to be garbled--perhaps it is interpreting some part of the IO wrapper as the result value
16:11:22 <c_wraith> Oh, then ignore everything I said.
16:11:27 <krey_> ddarius: ok, so it evaluates as much as it needs to get the top level constructor, I understand. but why is this generalised to the single summand case?
16:12:41 <tux_mobile_5> I guess i'll be seeing you tomorrow about this issue, because it's late where i live
16:12:44 <tux_mobile_5> Bye
16:12:57 <ezrakilty> if I foreign export foo :: CInt -> IO CInt, for example, will the generated glue take care of running and unpacking the IO monad?
16:14:43 <benmachine> krey_: for consistency, basically
16:14:54 <benmachine> krey_: you're given the ~ construct so that you can always get the behaviour you want
16:15:30 <benmachine> krey_: suppose that pattern-matching on tuples was not strict, but you wanted to force one
16:15:49 <benmachine> krey_: suddenly you can't do that at all without seq
16:16:06 <benmachine> and actually, what does seq do on non-strict tuples?
16:16:23 <copumpkin> what's a non-strict tuple?
16:16:23 <krey_> try?
16:16:38 <benmachine> krey_: try what, what are the relevant equations
16:16:42 <copumpkin> you mean not strict in its arguments?
16:16:50 <copumpkin> it just forces the (,) constructor
16:17:01 <benmachine> copumpkin: we're tlaking about why matching against tuples is strict
16:17:11 <benmachine> why case undefined of (_,_) gives undeined
16:17:13 <benmachine> f
16:17:16 <copumpkin> oh
16:17:27 <benmachine> maybe I'm doing it badly :P
16:17:36 <benmachine> I'm going to go to bed anyway
16:17:40 * benmachine tags in copumpkin
16:17:43 <benmachine> your job now!
16:17:43 <copumpkin> lol
16:18:04 <krey_> benmachine: thanks for your help, night
16:18:15 <copumpkin> as an oversimplification: Case is what you use if you want strictness/monomorphic bindings, let is what you use if you want non-strictness/polymorphic bindings
16:18:41 <copumpkin> ! on one and ~ on the other switch it around slightly
16:18:57 <aavogt> how slightly?
16:19:04 <aavogt> why not completely?
16:19:15 <copumpkin> you don't change the polymorphism of the arguments
16:19:41 <krey_> copumpkin: sorry, what's the difference between monomorphic and polymorphic bindings? :$
16:20:01 <copumpkin> > case 5 of x -> (x :: Double, x :: Int)
16:20:02 <lambdabot>   Couldn't match expected type `GHC.Types.Int'
16:20:02 <lambdabot>         against inferred type ...
16:20:10 <copumpkin> > let x = 5 in (x :: Double, x :: Int)
16:20:10 <lambdabot>   (5.0,5)
16:20:25 * hackagebot hstyle 0.1 - Checks Haskell source code for style compliance.  http://hackage.haskell.org/package/hstyle-0.1 (DougBeardsley)
16:20:35 <mightybyte> @pl (\f -> f a b)
16:20:35 <lambdabot> flip ($ a) b
16:20:42 <Saizan> (that works only with NoMonomorphismRestriction though)
16:20:58 <copumpkin> NoMonoLocalBinds?
16:21:06 <krey_> *losing faith in Haskell*
16:21:14 <copumpkin> krey_: why?
16:22:09 <krey_> copumpkin: dunno, haskell used to be so pure, so consistent, and now I'm suddenly struggling to understand all these intricacies that have to do with the type checker and such...
16:22:23 <copumpkin> case expressions/function application pattern matching is how you "make stuff happen"
16:22:30 <copumpkin> let is how you name things
16:22:37 <copumpkin> the rest arises mostly from that distinction
16:22:55 <krey_> so, where is case?
16:23:03 <copumpkin> ?
16:23:11 <krey_> you talk about case and let
16:23:15 <aavogt> and lambda is secretly case?
16:23:18 <krey_> what about "where"?
16:23:23 <krey_> lambda is let, i think
16:23:28 <copumpkin> where is basically a let
16:23:28 <Saizan> where works like let
16:23:31 <aavogt> no lambda is mono
16:23:32 <copumpkin> and lambda is a case
16:23:50 <krey_> copumpkin: omg, got it the wrong way round
16:23:55 <copumpkin> :P
16:24:08 <krey_> time to throw my intuition out the window...
16:24:15 <aavogt> krey_: well thankfully you can produce code that seems to work without knowing these details
16:24:30 <copumpkin> > let f x = (x :: Double, x :: Int) in f 5
16:24:31 <lambdabot>   Couldn't match expected type `GHC.Types.Int'
16:24:31 <lambdabot>         against inferred type ...
16:24:51 <copumpkin> > let f :: (forall n. Num n => n) -> (Double, Int); f x = (x :: Double, x :: Int) in f 5
16:24:52 <lambdabot>   (5.0,5)
16:25:01 <krey_> aavogt: I'm supposed to be a computer scientist, not a programmer :S
16:25:02 <copumpkin> there's usually a backdoor around the various behaviors
16:25:38 <copumpkin> the issue is that it can't infer the higher-ranked type for f there
16:25:42 <copumpkin> and I can't blame it
16:25:55 <copumpkin> "gimme something that is a polymorphic value that has instances for Double and Int"
16:26:17 <aavogt> it might see Double and Int, then do a search for some instance that has both
16:26:21 <aavogt> err, class
16:26:27 <copumpkin> what if there's more than one?
16:26:34 <kafee> hi, I have that friend of mine,Paranoico, who is unsure about haskell. Could you people point him some nice things about haskell
16:26:37 <aavogt> nondeterministic choice!
16:26:42 <copumpkin> (forall c. c n => n)
16:26:48 <krey_> aavogt: lol
16:26:48 <copumpkin> universal quantification over classes!
16:26:49 <copumpkin> :P
16:26:54 <ddarius> krey_ is upset because Haskell is inconsistent for making a choice due to motivations of consistency?
16:27:27 <hpc> ddarius: don't you know? if you prove haskell is consistent, it magically becomes inconsistent!
16:27:31 <hpc> :D
16:27:39 <copumpkin> I keep thinking hpc is kmc 
16:27:45 <copumpkin> damn three-letter nicks ending in C!
16:27:49 <hpc> copumpkin: so do i :P
16:27:55 <copumpkin> doesn't help that my IRC client colors them the same
16:28:04 <krey_> ddarius: it is not clear to me yet how this whole thing works, I used to like haskell because it was simple, like a sane version of lambda calculus, friendly syntax, etc.
16:28:25 <copumpkin> krey_: it's remarkably elegant, really
16:28:36 <copumpkin> you could get more elegant if you didn't make any practical evaluation considerations
16:28:50 <krey_> by?
16:28:50 <copumpkin> but presumably we like running our programs too
16:29:13 <copumpkin> well, something like agda isn't strict or lazy because its semantics would be unchanged by the choice
16:29:17 <copumpkin> because it's total
16:29:44 <copumpkin> you might get efficiency gains by evaluating it lazily, but it doesn't make a difference from the correctness standpoint
16:29:52 <jmcarthur> except for the partial parts
16:29:56 <krey_> copumpkin: must investigate agda, thanks :)
16:29:59 <copumpkin> pff, there are none
16:30:03 <copumpkin> just don't FFI
16:30:03 <jmcarthur> runIO
16:30:05 <jmcarthur> ha
16:30:14 * copumpkin plugs his ears and says "lalalala"
16:30:22 <jmcarthur> if we accept the "standard" library then we have partial functions
16:30:51 * jmcarthur doesn't think it's so bad
16:31:17 * copumpkin runs away screaming
16:31:20 <krey_> copumpkin: sooo, back to "where let case" chaos
16:31:26 <hpc> some of the prelude functions have a rather shitty way of being "partial"
16:31:35 <jmcarthur> i was talking about agda
16:31:35 <copumpkin> krey_: deep down, there's just let and case
16:31:42 <hpc> they explicitly call error, which consumes line numbers on the exception output
16:31:47 <jmcarthur> hpc: ^^
16:31:50 <krey_> the reason I thought that where is like case
16:32:07 <krey_> is that they both allow matching multiple patterns
16:32:13 <jmcarthur> hpc: i agree that the partial functions in prelude are massive suckage
16:32:18 <krey_> whilst let and lambdas don't
16:32:24 <krey_> is this true?
16:32:39 <jmcarthur> let and lambda allow it too
16:32:47 <jmcarthur> oh
16:32:49 <jmcarthur> multiple
16:32:51 <jmcarthur> nevermind
16:33:04 <jmcarthur> where allows multiple patterns?
16:33:10 <blackh> krey_: 'where' means something different after 'instance' from what it means inside a function.
16:33:11 <krey_> yup
16:33:29 <krey_> blackh: fine, let's ignore that meaning then
16:33:33 <copumpkin> krey_: not really
16:33:42 <jmcarthur> i've never seen where with multiple patterns
16:33:50 <copumpkin> jmcarthur: he's talking about defining functions in a where clause I think
16:33:54 <jmcarthur> oh
16:33:59 <jmcarthur> in that case let is no different
16:33:59 <copumpkin> which is admittedly confusing the matter a bit
16:34:16 <krey_> * confused
16:34:23 <jmcarthur> let foo a = b; bar c = d in baz foo bar
16:34:25 <krey_> how the hell do you do that star thing???
16:34:36 <jmcarthur> baz foo bar where foo a = b; bar c = d
16:34:46 <aavogt> > let a = b where b | False = 0 | True = 1 in a -- jmcarthur?
16:34:47 <lambdabot>   1
16:34:54 * Cale uses /me to perform an action
16:35:38 * krey_ thinks this question demonstrates his lack of irc-fu, but thanks
16:35:40 <jmcarthur> aavogt: ah but that's also possible with both let and case
16:36:01 * shachaf uses \001ACTION to perform an action.
16:36:14 <sm> better answers for the (haskell-tagged) http://stackoverflow.com/questions/4430998/mathematica-what-is-symbolic-programming , anyone ?
16:37:10 <aavogt> > x ^ 5
16:37:10 <lambdabot>   x * x * (x * x) * x
16:37:32 <jmcarthur> @src (^)
16:37:32 <lambdabot> x ^ 0            =  1
16:37:32 <lambdabot> x ^ n | n > 0    =  f x (n-1) x
16:37:32 <lambdabot>   where f _ 0 y = y
16:37:32 <lambdabot>         f x n y = g x n
16:37:32 <lambdabot>           where g x n | even n  = g (x*x) (n `quot` 2)
16:37:33 <aavogt> sm: I don't think there's any more symbolic math in haskell than that
16:37:34 <lambdabot>                       | otherwise = f x (n-1) (x*y)
16:37:36 <lambdabot> _ ^ _            = error "Prelude.^: negative exponent"
16:38:34 <sm> aavogt: what is that.. something built into lambdabot ?
16:38:47 <aavogt> and there's a big difference between 'possible with syb'  and actually implemented with it
16:38:53 * monochrom uses unsafePerformIO to perform actions
16:39:04 <Cale> sm: It's a library on Hackage -- simple-reflect
16:39:05 <jmcarthur> :t x
16:39:06 <lambdabot> Expr
16:39:08 <jmcarthur> sm: ^^
16:39:17 <sm> I see
16:39:18 <monochrom> /unsafePerformIO uses unsafePerformIO to perform actions :)
16:39:28 <Cale> > foldr f z [1,2,3,4,5]
16:39:29 <lambdabot>   f 1 (f 2 (f 3 (f 4 (f 5 z))))
16:39:39 <Cale> > foldl f z [1,2,3,4,5]
16:39:40 <lambdabot>   f (f (f (f (f z 1) 2) 3) 4) 5
16:39:44 <krey_> copumpkin: soooo, should I give up on understanding this pattern matching mystery?
16:39:52 <aavogt> > iterate f x
16:39:53 <lambdabot>   [x,f x,f (f x),f (f (f x)),f (f (f (f x))),f (f (f (f (f x)))),f (f (f (f (...
16:40:08 <sm> wow that looks pretty handy for learning
16:40:12 <Cale> krey_: What was your question?
16:40:24 <copumpkin> krey_: sorry, got wrapped up in other stuff
16:40:27 <copumpkin> but Cale is to the rescue
16:40:37 <Cale> krey_: Just skimming, it seems like you want to know about the relationship between where/let/case?
16:40:56 <krey_> Cale: yep, and lambda-patterns
16:41:27 <Cale> The difference between where and let is pretty trivial. 'where' is part of the syntax of declarations, and definitions made in a where clause scope over multiple guards (and can be used inside the guards)
16:41:31 * ddarius is surprised the teachbot wasn't turned on sooner.
16:41:37 <Cale> 'let' is part of the syntax of expressions
16:42:01 <Cale> The compiler directly turns 'where' clauses into 'let' expressions pretty early on.
16:42:17 <Cale> Now, 'let' makes definitions, but it doesn't generally cause anything to be evaluated.
16:42:44 <krey_> Cale: but "where" can match multiple patterns
16:42:51 <Cale> hm?
16:43:10 <Cale> You mean how you can stick multiple declarations inside a where?
16:43:16 <Cale> You can do that with let as well.
16:43:20 <krey_> Cale: oh
16:43:30 <Cale> > let {x = 5; y = 3} in x^2 + y^2
16:43:31 <lambdabot>   34
16:43:46 <krey_> Cale: not quite
16:44:03 <krey_> just a moment, I'm terrible with inline haskell
16:45:55 <krey_> > let a = isz 5 where {isz 0 = True; isz _ = False} in a
16:45:56 <lambdabot>   False
16:46:22 <krey_> Cale: the "let" bit is unimportant here
16:46:31 <Cale> > let { isz 0 = True; isz _ = False; a = isz 5 } in a
16:46:32 <lambdabot>   False
16:46:44 <Cale> (you can also do that)
16:46:49 <krey_> I see
16:46:53 <krey_> enlightening
16:46:55 <Cale> > let { a = let { isz 0 = True; isz _ = False } in isz 5 } in a
16:46:56 <lambdabot>   False
16:47:07 <Cale> ^^ that's the actual translation
16:47:44 <krey_> Cale: but it won't be unravelled any further?
16:47:56 <Cale> Well, the compiler might do lots of things to it from there
16:48:18 <Cale> But just considering the initial desugaring of where into let, that's what would happen.
16:48:31 <krey_> Cale: ok, what about lambda-patterns?
16:48:47 <Cale> okay
16:49:02 <Cale> Those desugar into case expressions
16:49:13 <krey_> ok
16:49:36 <Cale> Operationally (and ignoring things like lazy patterns and bang patterns which confuse things a bit), 'let' is for putting new definitions in the heap
16:49:52 <Cale> while 'case' is for driving forward evaluation by pattern matching on values
16:50:29 <Cale> (and it controls the stack -- the stack consists of case expressions which are waiting for their scrutinee to be sufficiently evaluated to match a pattern)
16:51:31 <BMeph> Cale: 1) Do you ever "play" with people, by following declarations about "the stack" with "Which stack is that?";
16:51:50 <BMeph> 2) Is that a bad thing? :)
16:52:25 <Cale> BMeph: I'm not sure what you mean. :)
16:55:34 <krey_> Cale: I kind of see the difference
16:56:11 <hpc> Cale: the answers are "yes" and "no" ;)
16:56:25 <krey_> Cale: so, if I define f, a function that uses pattern matching, that will be turned into case, right?
16:56:43 <xplat> one confusing thing is that a 'let' with multiple clauses for one name, or with guards, is desugared into a let with a case on the right side
16:56:46 <dmwit> krey_: right
16:57:25 <krey_> dmwit, Cale: but this doesn't:
16:57:29 <xplat> let and case look kind of similar at a glance but once fully desugared they have nothing in common really
16:57:30 <krey_> > let { a = let { isz 0 = True; isz _ = False } in isz 5 } in a
16:57:31 <lambdabot>   False
16:57:45 <dmwit> That also uses case, yes.
16:58:04 <dmwit> let isz x = case x of { 0 -> True; _ -> False }
16:58:38 <krey_> dmwit: ok, so some of let might be desugared into case...
16:58:47 <monochrom> I do play with people who say "in theory, theory and practice are the same. in practice, they are different". I ask "which theory? whose theory?"
16:59:17 <Cale> krey_: yes, all pattern matches are turned into case expressions
16:59:17 <xplat> monochrom: in theory, it doesn't matter, but in practice it does!
16:59:20 <ddarius> monochrom: "The only one that matters.  Mine."
16:59:35 <Cale> krey_: and all evaluation is eventually due to case expressions, at some level.
16:59:53 <krey_> Cale: very interesting!
17:00:04 <monochrom> If *your* theory says "theory and practice are the same", that's really your problem. My theory says they're different at the onset.
17:00:30 <dmwit> My theory says the world would be much better if it were theoretical.
17:00:37 <ddarius> That's just your metatheory.
17:00:50 <kmc> you can't, like, own a metatheory, man.
17:00:50 <dmwit> I've never metatheory I didn't like.
17:01:03 <xplat> monochrom: you have general recursion in your theory?
17:01:11 <etpace> @src lines
17:01:11 <lambdabot> Source not found. :(
17:01:15 <dmwit> kmc: Maybe you can't, but that's because you're a penniless hippy!
17:01:24 <monochrom> I have unlimited stratification in my theory.
17:01:33 <etpace> is there a HOF that lines is made with?
17:01:40 <etpace> something like foo "\n"?
17:01:47 <krey_> Cale: thanks for helping!
17:01:47 <dmwit> etpace: Check out Data.List.Split in the split package.
17:01:52 <Cale> krey_: no problem :)
17:02:13 <krey_> Cale: also, you probably don't remember this, but you taught me monads, and I was meaning to thank you for that as well :)
17:02:39 <Cale> krey_: Lazy patterns sort of turn case into let, and bang patterns can turn let into case, so this view is sort of complicated by that.
17:03:17 <dmwit> To be honest, I don't think I really understand what lazy patterns and bang patterns do.
17:03:34 <monochrom> bang pattern is the opposite of irrefutable pattern.
17:04:10 <kmc> > case Nothing of Just _ -> ()
17:04:11 <lambdabot>   *Exception: <interactive>:1:133-160: Non-exhaustive patterns in case
17:04:15 <kmc> > case Nothing of ~(Just _) -> ()
17:04:16 <lambdabot>   ()
17:04:18 <kmc> lazy pattern
17:04:25 <kmc> > case Nothing of ~(Just x) -> x
17:04:25 <lambdabot>   *Exception: <interactive>:1:133-162: Irrefutable pattern failed for pattern...
17:04:26 <dmwit> Sure, I know what it does operationally.
17:05:43 <krey_> kmc: to me, the first behaviour makes the most sense
17:06:19 <copumpkin> why is there a Set.unions but not a Set.intersections ?
17:06:47 <aavogt> @hoogle intersect
17:06:48 <lambdabot> Data.List intersect :: Eq a => [a] -> [a] -> [a]
17:06:48 <lambdabot> Data.List intersectBy :: (a -> a -> Bool) -> [a] -> [a] -> [a]
17:06:48 <lambdabot> Data.IntMap intersection :: IntMap a -> IntMap b -> IntMap a
17:07:12 <copumpkin> I guess because there's not a unit
17:07:19 <copumpkin> we can't have a Set that contains everything
17:07:23 <copumpkin> so it'd need to be intersections1
17:07:40 <krey_> copumpkin: can you not?
17:07:50 <copumpkin> not a Data.Set that contains everything
17:07:58 <copumpkin> other representations of sets allow that
17:08:11 <aavogt>  intersections [] = S.empty; intersections xs = foldr1 intersect xs
17:08:23 <beckmx> wtf!
17:08:24 <krey_> copumpkin: oh yes, \x -> True
17:09:16 <krey_> copumpkin: I thought you were getting into set theory :D
17:09:25 <Cale> beckmx: what?
17:10:55 <aavogt> copumpkin: is there a problem that   S.intersections []  could be an empty? Is it that valuable that  S.intersections (a ++ b) == S.intersections a `S.intersect` S.intersections b ?
17:11:15 <kmc> it's valuable that intersection is a monoid, i think
17:11:27 <kmc> given that there's some infrastructure built for reducing with monoids
17:11:38 <aavogt> or you mean that to have such a function in the library will confuse people since it defies expectations
17:11:52 <ddarius> Clearly we should spurn monoids and cleave to semigroups.
17:12:28 <kmc> quasi semi-pre-magmas
17:13:02 <applicative> > let breakOn c cs = let (firstchunk, rest) = break (== c) cs in firstchunk : case rest of [] -> []; (_:more) -> breakOn c more in breakOn 'p' "etpace"
17:13:03 <lambdabot>   ["et","ace"]
17:13:07 * ddarius simply calls pre-magmas, sets.
17:14:58 <number9_> lispy: Can I ask you another qustion about unlit?
17:14:59 <ddarius> Incidentally, a -> Bool is contravariant albeit you could define a covariant functor that did the right thing but not in Haskell, at least, not for all a.
17:15:30 <Cale> pre-magmoid
17:15:40 <kmc> also if you try to do naive set theory with that type, you will send the inliner into an infinite loop
17:15:46 <kmc> did anyone check if that's fixed in GHC 7
17:15:52 <Cale> (that's just a graph?)
17:15:52 <ddarius> I'm about to.
17:18:08 <ddarius> Still there.
17:21:46 <wornof> I'm trying to install hmatrix via cabal. It depends on lapack, which I have built in ~/lapack/ (I don't have root on the server). To tell cabal, it says ' *** cabal install hmatrix --configure-option=link:lib1,lib2,lib3,etc.', but I can't find out what syntax this actually means. Any ideas?
17:23:11 <dcoutts_> try, cabal install hmatrix --configure-option=link:blah  where blah is the name of the C lapack library you want to use
17:23:25 <iammisc> Is there any way to get haskell to call a function when an object will be garbage collected (ghc specific is okay)
17:24:10 <Cale> iammisc: Well, that's sort of what ForeignPtrs are for, if you're talking about FFI
17:24:39 <Cale> (of course, you don't really just want to apply a function, you really want to execute an IO action)
17:25:05 <iammisc> Well I have a data strucure which holds a reference to a glshader object. When this shader is no longer needed, the garbage collector will delete it, but the shader object needs to be deleted too. The shader data structure isn't a foreignptr
17:25:50 <iammisc> this is for opengl obviously, where shaders are represented by unsigned integers
17:25:52 <dcoutts_> iammisc: what is the shader data structure? a pure Haskell data structure?
17:25:55 <dcoutts_> oh
17:26:01 <iammisc> Yes it is a pure haskell data structure
17:26:12 <dcoutts_> but it's really a reference to a foreign structure
17:26:19 <iammisc> no foreignptrs. But in a sense the unsigned integer representing the opengl shader object is a kind of pointer
17:26:21 <dcoutts_> it's a pointer in disguise
17:26:24 <iammisc> yes
17:26:32 <cheater99> hi
17:26:39 <cheater99> i'm having problems using the state monad
17:27:13 <cheater99> i'm using the definition of stateIntString here: http://coder.bsimmons.name/blog/2009/10/the-state-monad-a-tutorial-for-the-confused/
17:28:01 <cheater99> i expected to be able to call stateIntString over and over and have it print "bar" four times and "foo" once, in loop
17:28:07 <cheater99> can someone help me achieve this somehow?
17:28:22 <dcoutts_> iammisc: you could roll your own foreignptr
17:28:31 <iammisc> hmm? how would that work?
17:28:38 <iammisc> Cast the integer to a foreignptr or something?
17:29:38 <Cale> cheater99: runState (replicateM 5 stateIntString) 1
17:30:58 <cheater99> Cale: what if in my code i just want to do something like, say, run stateIntString in random places?
17:31:17 <Cale> stateIntString :: State Int String
17:31:30 <cheater99> yeah
17:31:31 <Cale> which means that it's secretly just a function of type  Int -> (String, Int)
17:31:38 <cheater99> yeah
17:31:41 <Cale> and you use runState to apply that function
17:31:58 <Cale> There's no magic keeping the state around everywhere.
17:32:23 <aavogt> you can put    stateIntString   on any line of a  do-block with type    State Int a
17:33:13 <cheater99> Cale: how can i pass the state around?
17:33:50 <Cale> From inside a State Int computation, we can just execute things of type State Int t, and the Int state value gets threaded through invisibly and automatically.
17:34:08 <Cale> do x <- stateIntString; y <- stateIntString; return (x,y)
17:34:26 <cheater99> ah man
17:34:27 <Cale> From outside, you have to pass the state around manually, like you normally would.
17:34:43 <cheater99> i think i have just figured out a way how to do what i wanted to do without having state, just by using tail recursion
17:34:52 <Cale> mhm
17:35:02 <cheater99> which is very cool
17:35:18 <Cale> The State monad is just a thin wrapper around the common functional programming idiom for simulating state.
17:35:52 <cheater99> aavogt: how do i make a do-block of a specific type?
17:35:57 <dcoutts_> iammisc: check out System.Mem.Weak, I think addFinalizer is what you want
17:36:35 <Cale> cheater99: You can explicitly tack a type signature on it, but usually it's unnecessary
17:36:43 <aavogt> cheater99: depends what's inside it, and what the context of it is. But it's still not different from cale's interpretation
17:36:51 <iammisc> dcoutts_: okay, I'll check it out. Thanks
17:37:11 <Cale> cheater99: The type of a do-block is the type of the last action in it, and all the actions in a single do-block are of the same monad.
17:37:22 <cheater99> i'm a bit lost right now
17:37:26 <cheater99> how would the specific code look for me to call stateIntString three times somewhere and then 
17:37:34 <cheater99> just print it out?
17:37:47 <cheater99> i'm sorry to ask about such simple stuff :x
17:38:00 <aavogt> the replicateM works like  (say for 3):
17:38:11 <Cale> main = print (runState (do x <- stateIntString; y <- stateIntString; z <- stateIntString; return (x,y,z)))
17:38:19 <Cale> er, oops
17:38:24 <Cale> main = print (runState (do x <- stateIntString; y <- stateIntString; z <- stateIntString; return (x,y,z)) 1)
17:38:29 <Cale> (need an initial state!)
17:38:36 <dcoutts_> iammisc: be very careful with your references however, note what it says about boxes
17:38:45 <Cale> that'll print out the results along with the final state
17:39:00 <cheater99> why do we return a tuple of states?
17:39:10 <Cale> Those aren't states
17:39:18 <Cale> Those are simply the results of running the action each time
17:39:22 <cheater99> what are they?
17:39:29 <cheater99> are they perhaps *changes* in state?
17:39:34 <Cale> The state is threaded through and unmentioned in that code
17:39:42 <Cale> stateIntString may update the state
17:39:50 <cheater99> what is x?
17:39:57 <Cale> and its result may depend on the state
17:40:09 * hackagebot FilePather 0.0.1 - Functions on System.FilePath  http://hackage.haskell.org/package/FilePather-0.0.1 (TonyMorris)
17:40:16 <Cale> x is the String result of running stateIntString the first time
17:40:37 <Cale> Let's take a step back, and I'll reintroduce the state monad.
17:41:36 <Cale> The basic idea here is that if we want to represent a computation which manipulates some state value of type s while producing a result of type a, then this is just a function from the initial state of type s, to a final state of type s, and a result of type a
17:41:51 <Cale> So this is where we get the newtype definition:
17:41:51 <cheater99> wait
17:42:05 <Cale> okay
17:42:21 <dcoutts_> iammisc: hmm, actually perhaps it's harder than I thought to do it safely, I'm worried about premature garbage collection
17:42:22 <cheater99> isn't x the string result of running stateIntString the first time only after we do "runState" on the tuple that contains x?
17:42:34 <dcoutts_> iammisc: I suggest you ask on the ghc users list
17:42:44 <Cale> cheater99: runState doesn't take a tuple as a parameter
17:42:46 <iammisc> Hmmm.... ok
17:43:00 <cheater99> but we do runState on it
17:43:04 <Cale> (well, it could, if the state was a tuple type, but not in this case)
17:43:06 <cheater99> don't we?
17:43:14 <dcoutts_> iammisc: in particular I'm worried that you might need a 'touch' primitive to do it safely, but there is no portable touch primitive
17:43:35 <dcoutts_> iammisc: it's what ghc uses in it's implementation of foreign ptr
17:43:45 <Cale> In this case, we're using runState at the type  runState :: State Int (String, String, String) -> Int -> ((String, String, String), Int)
17:43:52 <iammisc> Like I said, I'm okay with being GHC-specific
17:43:52 <cheater99> i see "(return Q)" and it makes me think the outer parentheses just take on the value Q.
17:43:59 <Cale> no
17:44:01 <cheater99> mhm
17:44:04 <Cale> That's not what return means
17:44:14 <cheater99> what does it mean then?
17:44:25 <Cale> Let's forget about this website's approach for a moment and go back a bit
17:44:36 <cheater99> yeah i'm totally with you. what does return mean there?
17:45:13 <Cale> I'll introduce it in a moment, I just want to go over what it is that we're really doing
17:45:18 <cheater99> ok go on
17:45:54 <Cale> So we're representing computations which manipulate a state of type s, and produce a result of type a, while doing it.
17:46:13 <ddarius> What would be the difference between return Q and Q then?
17:46:28 <cheater99> what are we representing them with?
17:46:45 <cheater99> can you point with a finger where in that code they are? 
17:46:58 <Cale> cheater99: Now, we *could* represent them as, oh, sequences of get and set operations and maybe some other stuff, but since we're in a functional programming language, the natural thing to do is to represent them with functions.
17:47:25 <Cale> A computation which depends on an initial state of type s, and produces a final state of type s, along with a result of type a
17:47:25 <cheater99> so you're talking about our x, y and z?
17:47:33 <Cale> Is the same thing as a function s -> (s,a)
17:47:43 <Cale> Ignore that example for now
17:48:01 <cheater99> ok go on
17:48:10 <Cale> Does that representation make sense?
17:48:21 <cheater99> you mean s -> (s, a)?
17:48:24 <Cale> yeah
17:48:26 <cheater99> yes
17:48:28 <Cale> okay
17:48:51 <Cale> so for the purposes of abstraction (and so that we'll be able to write instances) we're going to wrap that representation up into a newtype
17:49:03 <Cale> newtype State s a = S (s -> (s,a))
17:49:22 <cheater99> what's S?
17:49:24 <Cale> I'm choosing to call the data constructor S so as to avoid confusion while talking about it
17:49:27 <cheater99> ok
17:49:36 <Cale> S is a function  S :: (s -> (s,a)) -> State s a
17:49:45 <Cale> which makes values of our new type
17:49:53 <cheater99> ok go on
17:49:53 <Cale> and we can pattern match on it, because it's a data constructor
17:50:01 <Cale> (to extract the contained function)
17:50:25 <Cale> So, now we need a way to run one of these state machines with a given initial input
17:50:30 <Cale> and get the final state and output
17:50:40 <Cale> runState :: State s a -> s -> (s,a)
17:51:00 <Cale> this function will be super-easy to write because we chose a representation which makes it really easy
17:51:08 <Cale> runState (S f) s = f s
17:51:13 <ion> cale: You’re giving an excellent description. :-)
17:51:52 <Cale> If instead, we'd chosen to represent our State s a computations as, say, abstract syntax of a little imperative-like language, our runState would be an interpreter for that syntax
17:52:07 <Cale> But because we've chosen to use functions, there's not much work :)
17:52:36 <cheater99> ok
17:52:40 <Cale> Okay, so now we need some primitive State s a computations which we can use to build up more complicated ones later.
17:52:54 <Cale> I think I'll start with return, because you were interested in it
17:53:00 <Cale> return :: a -> State s a
17:53:04 <cheater99> ok :)
17:53:27 <cheater99> ok go on
17:53:27 <Cale> return will be a function which, given a value, gives the computation which does nothing to the state, and produces only that value every time
17:53:38 <Cale> So, we have
17:53:45 <Cale> return v = ...
17:53:59 <Cale> well, we don't have many ways to create State s a values yet
17:54:01 <Cale> just S
17:54:04 <Cale> return v = S ...
17:54:12 <Cale> and S wants a function, so it might as well be a lambda:
17:54:18 <Cale> return v = S (\s -> ...)
17:54:28 <Cale> The function needs to have type s -> (s,a)
17:54:35 <ddarius> Cale: Have you ever read about/looked at Hestenes Force Concept Inventory?
17:54:36 <Cale> v :: a here
17:54:44 <Cale> ddarius: nope
17:54:52 <Cale> return v = S (\s -> (s,a))
17:55:10 <Cale> and there's our completed return function
17:55:14 <Cale> er
17:55:15 <Cale> oops
17:55:19 <Cale> return v = S (\s -> (s,v))
17:55:20 <Cale> sorry
17:55:39 <aavogt> @unmtl State s a
17:55:39 <lambdabot> s -> (a, s)
17:55:51 <Cale> So return v takes the initial state, passes it through to the final state unchanged, and it has the result v
17:56:03 <Cale> Yeah, mtl does it backwards :P
17:56:10 <ddarius> Cale: http://modeling.asu.edu/R&E/Research.html  Whenever.  I think you'd find it interesting.
17:56:25 <cheater99> i don't think it does
17:56:38 <cheater99> because your return also takes "s" as the last parameter
17:56:41 <cheater99> right?
17:57:02 <Cale> cheater99: What don't you think it does?
17:57:16 <ion> To make it explicit, without the newtype the type of return would have been return :: a -> s -> (s,a), so ‘return’ would take a value (v) and return a function that takes a state (s) and just returns (s,v).
17:57:29 <cheater99> <Cale> Yeah, mtl does it backwards :P < <cheater99> i don't think it does
17:57:40 <Cale> cheater99: Oh, I just mean that in place of (s,a), it uses (a,s)
17:57:47 <Cale> It doesn't really make a big difference
17:57:49 <cheater99> yes, i don't think that's backwards
17:57:54 <cheater99> isn't it the only way it can work?
17:57:54 <Cale> There's a technical reason to prefer my way
17:58:01 <Cale> Either way will work
17:58:06 <cheater99> i don't see how it can work with (s, a) but then i'm not that good
17:58:07 <cheater99> :)
17:58:18 <Cale> Well, I'm showing you how it works with (s,a)
17:58:42 <cheater99> return v = S (\s -> (s,v))
17:58:52 <cheater99> but wouldn't that get called like "return v s"?
17:58:53 <Cale> Obviously whenever we build pairs, we could just write the two parts of the pair in the opposite order.
17:58:56 <Cale> no
17:59:00 <cheater99> (s being passed in by the monad)
17:59:19 <Cale> return v isn't actually a function -- it's a data structure which happens to contain only a function internally
17:59:25 <Cale> But that's the internal representation
17:59:39 <Cale> So it doesn't take another parameter
17:59:41 <cheater99> ok go on
17:59:44 <Cale> Right?
17:59:51 <Cale> return :: a -> State s a
18:00:01 <Cale> So if v :: a, then return v :: State s a
18:00:15 <Cale> and so  return v s  will be a type error
18:00:16 <ddarius> :t curry id
18:00:16 <lambdabot> forall a b. a -> b -> (a, b)
18:00:40 <Cale> It's important not to confuse the internal representation of State s a with the abstract datatype
18:01:04 <cheater99> ok
18:01:21 <Cale> Because what we're going to do is to build up a small library of primitives and combining functions which will let us write any value of type State s a without actually knowing that internally there are functions of type s -> (s,a)
18:01:48 <Cale> We'll be able to forget about the S data constructor at that point
18:01:58 <Cale> But for now, it's the only way we have to build State s a values
18:02:07 <cheater99> mhm
18:02:13 <Cale> So, to recap what we have so far:
18:02:22 <Cale> S :: (s -> (s,a)) -> State s a
18:02:31 <Cale> runState :: State s a -> s -> (s,a)
18:02:38 <Cale> return :: a -> State s a
18:02:47 <cheater99> what if i want my function to have a parameter?
18:03:10 <Cale> All functions have a parameter
18:03:41 <Cale> Do you mean, what if you want your State computation to have a parameter?
18:03:49 <Cale> You can just use a function, say  a -> State s b
18:04:00 <nostrand> cheater99: how's it going?
18:04:00 <cheater99> yes
18:04:11 <Cale> Okay, so that leads nicely to bind
18:04:12 <cheater99> nostrand: ?
18:04:14 <lars9> is data constructor function?
18:04:18 <Cale> lars9: yes
18:04:26 <ion> ‘return’ would be an example of a function that takes a parameter and returns a State computation. :-)
18:04:31 <Cale> lars9: Every data constructor is a function, not every function is a data constructor
18:04:36 <cheater99> nostrand: fine i guess
18:04:42 <nostrand> cheater99: hehe
18:04:43 <Cale> ion: good call :)
18:04:49 <lars9> Cale: so in Maybe a = Nothing | Just a, Nothing is a value, and Just is a func?
18:05:04 <Cale> lars9: Oh, good point.
18:05:10 <Cale> lars9: Nothing isn't a function :P
18:05:16 <Cale> But yeah, Just is
18:05:17 <cheater99> nostrand: :)
18:05:26 <Cale> Every data constructor with a parameter is a function :)
18:05:43 <Cale> Those with no parameter are not functions, they're just values
18:05:48 <lars9> Cale: then what type constructor is? just "type constructor"?
18:05:58 <Cale> lars9: Maybe is a type constructor here
18:06:01 <cheater99> i think i have to leave that parameter thing for another day, because i have to get some sleep
18:06:19 <Cale> lars9: Type constructors are at the type level, and they show up only on the right hand side of a ::
18:06:21 <cheater99> it's 3 am and i have work tomorrow :o
18:06:30 <Cale> cheater99: oh, all right
18:06:34 <mjrosenb> is there a generic words/lines?
18:06:41 <Cale> cheater99: See if you can catch me later :)
18:06:51 <cheater99> Cale: thanks a lot for your help, that was very well explained
18:06:54 <Cale> cheater99: and I'll explain (>>=) and get and put
18:06:55 <lars9> Cale: i mean type constructors are just "type constructors", not like data constructor is in fact values / functions?
18:07:23 <Cale> lars9: Sometimes type constructors are types, and sometimes they're type functions
18:07:36 <cheater99> great :))
18:07:37 <cheater99> thanks
18:07:47 <lars9> Cale: here it is, "type function"?
18:07:50 <Cale> Like Integer is a type constructor which is a type, and Maybe is a type constructor which is a type function
18:08:01 <Cale> Maybe Integer is a type
18:08:15 <Cale> Maybe String is another type
18:08:30 <lars9> Cale: is "type function" an official term?
18:08:33 <Cale> By type function, I mean that it takes a type and produces a different type
18:08:48 <Cale> lars9: More or less, yes. People will know what you mean if you say that.
18:08:59 <lars9> Cale: i see
18:09:06 <Cale> I'm not sure if the Report uses that terminology
18:10:06 <ion> tag:monad tag:state (for grepping the discussion from IRC logs.) :-)
18:10:29 <Cale> :)
18:10:41 <Cale> ion: If you'd like, I can continue on
18:11:25 <Cale> I've explained this enough times that I probably should write a tutorial at least. (If not a section for a future book)
18:11:26 <ion> I understand the state monad, but i don’t think i could have explained what you’ve explained so far as clearly.
18:12:15 <Cale> monochrom likes to make fun of the fact that I explain the same things over and over, but I like to do it anyway, because it lets me see where people get stuck
18:12:37 <Cale> and it also refines my own understanding of it
18:12:52 <ion> aye
18:13:47 * ddarius proposes the Monad Concept Inventory.
18:17:21 <Cale> Wat, they want me to email them for a password to open the PDF??
18:17:41 <ddarius> Cale: That's for the actual "test."
18:18:19 <ddarius> That said, it's 40-bit DES.  Your cellphone could crack it instantaneously.
18:18:19 <Cale> ah, okay
18:20:24 <lars9> in this implementation of deduplicate, http://hpaste.org/42310/dedup, does lazy evaluation work? i mean if we only want the head of the result list, how much evaluation work is required?
18:22:02 <shachaf> Cale: I haven't seen any double (double 5)s in a while.
18:22:09 <Cale> shachaf: :)
18:22:59 <Cale> lars9: that reverse is going to be painful
18:23:19 <aavogt> it reverses a list with one element as far as I can tell
18:23:32 <Cale> er...
18:23:37 <ddarius> aavogt: The best case, eh!
18:23:56 <ddarius> Way to set reverse up to succeed.
18:23:58 <aavogt> lars9: why isn't your function    \x -> [head x] instead?
18:24:14 <lars9> oh an error: dedup' (xs, []) = (xs, [])
18:24:15 <Cale> Is that really what it's doing?
18:24:35 <Cale> I didn't read it so carefully, but it looks like it's at least trying to be nub
18:24:45 <aavogt> oops,  (take 1)  is more representative
18:24:57 <Cale> oh, how is it that this doesn't need an Eq constraint? :)
18:25:00 <mdgeorge1> hello
18:25:00 <lars9> aavogt: that's a translation of an ocaml code snippt, i want to know if it works better because of laziness in haskell.
18:25:17 <Cale> oh, of course
18:25:19 <aavogt> lars9: it's probably not a correct translation then
18:25:26 <Cale> it's not recursive :P
18:25:37 <Cale> But it should still need an Eq constraint
18:25:40 <mdgeorge1> is there a way to capture a type in a nested type declaration?
18:25:43 <aavogt> @ty elem
18:25:44 <lambdabot> forall a. (Eq a) => a -> [a] -> Bool
18:26:17 <aavogt> @ty flip elem [] -- this should really be optimized to   const False?
18:26:18 <lambdabot> forall a. (Eq a) => a -> Bool
18:26:22 <mdgeorge1> I'm using a class Additive a which defines zero :: a
18:26:33 <mdgeorge1> and I'm trying to do "show zero"
18:27:01 <lars9> ehhh... yeah here is corrected version: http://hpaste.org/42312/dedup
18:27:03 <mdgeorge1> but I can't because zero has some type a, and the compiler doesn't know what a I want
18:27:06 <aavogt> mdgeorge1: you want to force some type variable to be 'a'?
18:27:13 <mdgeorge1> right.
18:27:44 <aavogt> there's  -XScopedTypeVariables,  but you can get the same using functions restricted types, like   asTypeOf
18:27:59 <lars9> when evaluating (dedup l)!!0, how much evaluation is needed?
18:28:14 <mdgeorge1> how does that work?
18:28:16 <byorgey> mdgeorge1: show (zero :: Int) ?
18:28:34 <mdgeorge1> byorgey: this is in the context of a function that is itself parameterized
18:28:34 <Cale> lars9: yeah, there you go.
18:28:44 <byorgey> oh, I see
18:28:47 <Cale> lars9: Well, one sec, I'll do some evaluation by hand to show you :)
18:28:51 <byorgey> sorry, I missed the context
18:28:51 <aavogt>  show a = show (zero `asTypeOf` a)
18:29:19 <aavogt>  show (_::a) = show (zero :: a) -- same thing with that extension I named
18:29:36 <lars9> Cale: you reminds me of something like "evaluation path visualization" :D
18:29:44 <lars9> Cale: does it exist?
18:29:58 <aavogt> deavid: but how do you get rid of the Eq?
18:30:04 <ddarius> Cale exists.  He's right there.
18:30:06 <aavogt> err, that's for lars9
18:30:44 <lars9> aavogt: yeah Eq is needed
18:31:46 <lars9> if something is not evaluated, how is it stored in mem? a (func, params) pair?
18:31:54 <mdgeorge1> aavogt: I think I get it.  Thanks
18:32:23 <aavogt> lars9: it could be lazier, for example  dedup' isn't productive with an infinite list as ys
18:32:51 <aavogt> you could instead have     filter (`elem` xs)
18:33:00 <byorgey> aavogt: no, to use ScopedTypeVariables you have to give a type signature like  :: forall a. ....
18:33:06 <aavogt> > filter (`elem` [2,3,5]) [1..]
18:33:10 <lambdabot>   mueval-core: Time limit exceeded
18:33:12 <aavogt> byorgey: not in instance decls
18:33:16 <lars9> aavogt: that is just a translation from a piece of ocaml code.
18:33:21 <aavogt> > take 3 $filter (`elem` [2,3,5]) [1..]
18:33:22 <lambdabot>   [2,3,5]
18:33:26 <byorgey> aavogt: oh, right
18:33:41 <byorgey> I'm off my game tonight, I'll just stay quiet now =)
18:33:48 <aavogt> byorgey: and probably not for patterns
18:34:08 <aavogt> byorgey: so I'm wrong about refuting you, but I'm still right :P
18:34:13 <byorgey> hehe
18:36:15 <mdgeorge1> aavogt: where do I get "a" from in your first example?
18:36:34 <aavogt> lars9: well the literal translation doesn't benefit from being lazy (though Eq could see the benefits)
18:36:41 <ddarius> Kaze Ni Naru
18:37:07 <aavogt> mdgeorge1: so no -XScopedTypeVariables?
18:37:24 <mdgeorge1> aavogt: I wanted to try it the other way first.
18:37:25 <lars9> aavogt: what do you mean by (though Eq...)?
18:38:25 <mdgeorge1> so if I'm calling "show a = show (zero `asTypeOf` a)" I need to come up with an a to pass in to show to carry the type
18:39:08 <aavogt> lars9: suppose the lists contain very large pieces of data, which can be compared without generating everything
18:39:32 <aavogt> ex. an infinite list of numbers, but the Eq instance only forces the first 5
18:40:23 <lars9> aavogt: i got it.
18:40:25 <aavogt> mdgeorge1: the 'a' you pass in can be undefined
18:41:11 <mdgeorge1> how do I create something not defined having the type of a given type variable?
18:41:14 <aavogt> or a member of      data Proxy a = Proxy    (if you have the appropriate    asTypeOfProxy :: a -> Proxy a -> a; asTypeOfProxy x _ = x
18:41:26 <mdgeorge1> I feel like I'm back to the same question
18:41:33 <aavogt> mdgeorge1: you're writing a Show instance?
18:41:41 <aavogt> I think it's the same question too
18:41:57 <mdgeorge1> aavogt: yes
18:42:13 <Cale> lars9: http://hpaste.org/paste/42312/dedup_annotation#p42313
18:42:14 <aavogt> then the definition of show  takes an argument with the right type
18:42:32 <Cale> lars9: (I hope you enjoy that ;)
18:42:41 <Cale> I left out the evaluation of elem
18:42:58 <mdgeorge1> aavogt: no.  It takes an argument of a type constructed from the right type
18:43:29 <aavogt> mdgeorge1: and that's probably as far as it goes. You can't really have     showWithZero :: String -- and have it guess the right instance of Additive
18:43:49 <aavogt> -XImplicitParams is a possibility
18:44:06 <lars9> Cale: great
18:44:11 <lars9> lemme read
18:44:14 <aavogt> mdgeorge1: so you take that parameter, and use it to make the zero have the right type
18:44:17 <Cale> lars9: I also assumed that the entire list was going to be evaluated, but you can see where the first element becomes available way near the end there
18:44:31 <Cale> lars9: This isn't a very good strategy for lazy evaluation
18:45:06 <lars9> Cale: because of that reverse?
18:45:19 <Cale> in part
18:45:35 <mjrosenb> I am trying to use ByteString.breakByte.  how do I convert a Char into a Word8?
18:45:42 <aavogt> c2w
18:45:42 <Cale> Well, the fact that it's written in such a way that the reverse is necessary
18:46:13 <mdgeorge1> aavogt: I'm not sure what you mean by "take that parameter, and use it to make z have the right type"
18:46:25 <Cale> lars9: dedup does very little but to greedily call itself without producing any data constructors until it's gone through the entire list
18:46:44 <aavogt> mdgeorge1: I mean this:   show a = show (zero `asTypeOf` a) 
18:47:07 <Cale> lars9: Or I should say, dedup'
18:47:13 <krey> is there a function of type Cont r a -> a?
18:47:20 <aavogt> show is called with the given type (any instance of Additive), which means that anybody using show will produce a value with that type
18:47:33 <ddarius> There's a Cont a a -> a
18:47:34 <Cale> krey: Is there a function of type ((a -> r) -> r) -> a?
18:47:57 <mdgeorge1> aavogt: how do I use the -XScopedVariables thing you mentioned?
18:48:12 <mdgeorge1> I think that might work better than using asTypeOf
18:48:14 <krey> Cale: true
18:48:31 <aavogt> then you discard the value, but first have a function whose only purpose is to make sure that you use the right type of zero
18:48:35 <krey> Cale: I mean, I can see why there isn't
18:48:55 <mdgeorge1> aavogt: no I understand how it works, but not how to enable it
18:49:02 <copumpkin> @djinn ((a -> r) -> r) -> a
18:49:02 <lambdabot> -- f cannot be realized.
18:49:13 <mdgeorge1> I already have a type variable in this scope which I should be able to use
18:49:15 <lars9> Cale: thanks, it is very clear, everything is evaluated to evaluate the head, execept last two lines but they are minor.
18:49:26 <aavogt> mdgeorge1: {-# LANGUAGE ScopedTypeVariables #-} -- at the top of your file
18:49:34 <mdgeorge1> great, let me try that.
18:49:48 <krey> copumpkin: yeah, i've got my on home-brew djinn...
18:49:50 <Cale> lars9: right, you'll get a long wait for the first element, and then everything will come through at once
18:50:03 <copumpkin> :t \f -> f id
18:50:04 <aavogt> but really, asTypeOf should be easy enough to use. I'm not sure what the problem you have with it is
18:50:04 <lambdabot> forall a t. ((a -> a) -> t) -> t
18:50:44 <aavogt> mdgeorge1: maybe the situation isn't exactly as I think it is? (that is, there should be more code you've tried?)
18:51:05 <mdgeorge1> aavogt: here's roughly what I'm doing
18:51:12 * ddarius would assume there's a decent amount of collaboration between the inductive logic programming people and the proof assistant people.
18:51:18 <mdgeorge1> aavogt: I have a type T a
18:51:21 <krey> this is the actual problem I'm facing: type Pair r a b = (a -> b -> r) -> r, write fst and snd
18:51:47 <copumpkin> krey: mmm :)
18:51:51 <mdgeorge1> aavogt: I want to write a function "prettyPrint :: (Show a) => T a -> String"
18:52:07 <copumpkin> krey: not sure what kinds of hints I could give without giving it away
18:52:28 <byorgey> you can start by writing the appropriate types for fst and snd
18:52:29 <lispy> krey: start with the type
18:52:36 <lispy> krey: what does r have to be come to write fst?
18:52:36 * byorgey high-fives lispy
18:52:39 <aavogt> mdgeorge1: T is  data?
18:52:43 <mdgeorge1> aavogt: somewhere inside the definition I have a helper function "helper :: [a] -> String"
18:52:45 <krey> fst :: Pair r a b -> a ?
18:52:47 <mdgeorge1> aavogt: yes, sorry
18:52:52 <ddarius> byorgey: That just the Haskellers response to everything.
18:53:09 <mdgeorge1> and I want to show zero for an empty [a]
18:53:10 <lispy> byorgey:  :)
18:53:27 <copumpkin> why are you infecting the Pair type with r?
18:53:37 <byorgey> helpSomeoneInIRC :: a -> Msg TryWritingTheType
18:53:42 <aavogt> mdgeorge1:  data T a = T [a] -- roughly?
18:53:48 <mdgeorge1> aavogt: yes
18:53:49 <Cale> lars9: A drastic improvement would be http://hpaste.org/paste/42312/drastic_improvement#p42315
18:53:52 <ddarius> copumpkin: He be scared of the higher rankin' types.
18:53:57 <lispy> krey: well, r may have to change.  What does Pair Int Int String mean?
18:54:18 <Cale> lars9: a smaller improvement would be to stop using extraneous pairs :)
18:54:26 <krey> lispy, copumpkin: Pair is almost like Cont
18:54:38 <lispy> krey: I know :)
18:54:42 <Cale> lars9: Note that this new dedup will work on [1..]
18:54:44 <lispy> krey: I can tell from the type actually
18:54:59 <ddarius> Indeed if we ignored meaning, it would be isomorphic to Cont r (a,b).
18:55:05 <Cale> lars9: and other infinite lists
18:55:12 <ologNation> Can someone suggest the right way to do this? 
18:55:13 <ologNation> http://hpaste.org/42314/reading_a_float
18:55:18 <krey> lispy: cool, so your question is one for which you know the answer, and I should think about
18:55:22 <Cale> (of course, it might hang if it stops finding new unique elements)
18:55:25 <lispy> krey: right!
18:55:27 <ologNation> I have a string of comma separated values
18:55:34 <mdgeorge1> aavogt: you see the problem?
18:55:35 <ologNation> that I want to turn into a string and some floats 
18:55:45 <aavogt> mdgeorge1: you can pattern match on the [] ( x @ [] ) , then get a type of what should have been in the list with    head x
18:56:02 <Cale> ologNation: Use pattern matching
18:56:03 <aavogt> I mean that   x@[]  is a pattern
18:56:10 <lars9> Cale: yeah, that's the same method as in Data.List.nub. 
18:56:19 <Cale> lars9: more or less
18:56:37 <Cale> lars9: The Report nub just uses filter
18:56:57 <lars9> Cale: I guess the difference in ocaml is minor, but huge in haskell
18:56:58 <aavogt> mdgeorge1: then so long as you don't do anything with   (head x), nothing will happen. And that includes passing it as the second argument to asTypeOf
18:57:05 <Cale> nub [] = []; nub (x:xs) = x : nub (filter (/= x) xs)
18:57:15 <mdgeorge1> aavogt: clever
18:57:51 <mdgeorge1> so
18:57:51 <mdgeorge1> helper x@[] = show (zero asTypeOf (head x))
18:57:59 <Cale> But expressed in this new version of dedup way, it makes it easier to specialise to a more effecient algorithm depending on Ord
18:58:07 <aavogt> mdgeorge1: needs ``
18:58:11 <mdgeorge1> right
18:58:15 <ologNation> Cale, Can you give me a hint? 
18:58:21 <mdgeorge1> that seems slightly too devious, but let me try it :)
18:58:23 <krey> lispy: well, I mean: (Int -> String -> Int) -> Int...
18:58:26 <Cale> You can replace the accumulated list with a Set, and the list elem function with a set membership test
18:58:30 <Cale> and it'll still be lazy
18:58:34 <krey> lispy: other than that...
18:59:01 <Cale> ologNation: let [name, midterm1, midterm2, homework, final] = splitOn "," l
18:59:15 <lars9> Cale: yeah, that is
18:59:20 <Cale> ologNation: in Student name (read midterm1) ...
18:59:43 <aavogt> mdgeorge1: if you find yourself trying to get type variables out of more complicated types, you can just write (ex. in a where clause)     getMyTyVar :: Either [(a,b)] c -> a; getMyTyVar = undefined
18:59:50 <ologNation> Hm... I'll try it. 
19:00:05 <ologNation> It's just saying "Prelude.read: no parse" 
19:00:10 <Cale> ologNation: oh
19:00:11 <aavogt> then use such a function in place of head (which is conveniently in the Prelude here, so you don't need it)
19:00:11 <ologNation> So it's hard for me to tell where the problem is. 
19:00:12 <mdgeorge1> aavogt: yes, there's nothing special about head in your solution
19:00:17 <lars9> Cale: thank you very much:) 
19:00:20 <Cale> ologNation: Then there's something wrong with your input, perhaps
19:00:27 <Cale> lars9: no problem :)
19:00:37 <mdgeorge1> aavogt: in fact, I think I'm going to do that anyway because it seems less evil to me
19:01:24 <aavogt> is it really evil, or taking advantage of laziness and how these type classes work :p
19:01:37 <ologNation> Hm.. There should be a way to get it to say _what_ it couldn't parse
19:01:40 <ologNation> That would be helpful. 
19:01:43 <mdgeorge1> aavogt: yes. :)
19:01:58 <mdgeorge1> aavogt: perhaps not evil, but confusing
19:02:08 <mdgeorge1> aavogt: a new function would make things clearer
19:02:12 <mdgeorge1> aavogt: IMHO
19:02:40 <mdgeorge1> of course, if I want to be avoiding trickyness in my code, god only knows  why I'm programming in haskell ;-)
19:03:25 <Cale> Haskell has *way* less trickiness per line than code in most imperative languages, and usually does more.
19:03:27 <lispy> krey: so if I told you, that I need a function of type (Int -> String -> Int) and that once you give it to me I'll give you back an Int, what function would you hand me?
19:04:05 <djahandarie> Cale, I'd say it can get pretty tricky with the various performance things that have been done
19:04:11 <krey> lispy: probably const
19:04:14 <djahandarie> Perhaps not sementically tricky, but operationally it is
19:04:18 <Cale> Well, understanding code operationally sure.
19:04:25 <lispy> :t const
19:04:26 <lambdabot> forall a b. a -> b -> a
19:04:34 <lispy> > const 5
19:04:35 <lambdabot>   Overlapping instances for GHC.Show.Show (b -> t)
19:04:35 <lambdabot>    arising from a use of `...
19:04:47 <aavogt> it could add the length of the String... anything really
19:04:51 <lispy> overlapping instances??
19:04:58 <lispy> that's really broken
19:05:02 <krey> ?
19:05:06 <copumpkin> lispy: it takes two parameters :P
19:05:10 <krey> const 5 is (a -> Int)
19:05:11 <lispy> oh, I'm just stupid
19:05:20 <djahandarie> lispy, functions have more than one show instance
19:05:22 <lispy> > const 5 "krey"
19:05:22 <aavogt> from the  Num n => Num (a -> n)  and the smallcheck instances for functions
19:05:22 <mdgeorge1> Cale: depends on your perspective
19:05:22 <lambdabot>   5
19:05:36 <lispy> krey: Yay, so that const you gave me, worked out perfectly
19:05:37 <Cale> Though, even the operational semantics consists of rewriting expressions into equivalent expressions, and you have equational reasoning to help you skip over any bit that you're not interested in.
19:05:42 * ddarius just thought that lispy was surprised that there were -two- (or more) instances of Show for functions in scope.
19:05:57 <ologNation> Cale, Thanks.  I think you're right. 
19:06:00 * lispy only slept 4 hours of the last 24....so is not all here
19:06:07 <copumpkin> edwardk: what was that funky pair of unital operations => one property you were telling me about today?
19:06:22 <Cale> With understanding imperative programs, you skip over a procedure call at your own peril. It might or might not do things other than producing a result.
19:06:26 <copumpkin> ddarius: that is also rather odd :)
19:06:39 <Cale> that might or might not affect the remainder of how your program executes
19:06:40 <aavogt> imperative programs exist in haskell though
19:06:41 <ddarius> copumpkin: Eckmann-Hilton ?
19:06:47 <mdgeorge1> Cale: I agree with you.  I was being somewhat facetious
19:06:48 <lispy> krey: So do you see how that worked?  I was holding on the pair (in my brain) of 5 and "krey".  When you gave me const, I gave both of those to const and you got 5.
19:06:48 <ddarius> edwardk: Poke.
19:06:50 <copumpkin> ddarius: that's the one!
19:06:52 <Cale> and so you're forced to keep a lot of things in your head
19:06:57 <copumpkin> ddarius: thanks
19:07:06 <mdgeorge1> Cale: but with too many uses of higher order functions in one line, things can get pretty hairy
19:07:37 <aavogt> excessive @pl, not knowing how many arguments are involved and so on
19:07:41 <krey> lispy: but the types... ???
19:07:58 <lispy> krey: I know right?  They're nice. ;)
19:08:53 <lispy> krey: Pair Int Int String = (Int -> String -> Int) -> Int, right?  So you gave me const, which we applied at the type const :: Int -> String -> Int.
19:09:06 <mdgeorge1> I just have to resist the temptation to keep jumping to higher and higher levels of generality
19:09:08 <ddarius> Haskell programmer sees releasePressure :: () -> () and omits it.
19:09:17 <Mathnerd314> aavogt: but you can always use GHCi to get the type
19:09:18 <djahandarie> Hmm... would it be possible to somehow pass around type-classes on the value level then reflect them back into the type level? Maybe using something like edwardk's trick where he builds everything off of pointers
19:09:34 <Mathnerd314> mdgeorge1: why?
19:09:39 <mdgeorge1> or I'll hit the old maxim - any sufficiently general program contains a lisp interpreter :)
19:09:48 <aavogt> Mathnerd314: or this http://github.com/sebastiaanvisser/ghc-goals
19:09:55 <krey> lispy: I can see how I can turn Pair Int Int String into Int, yes
19:10:00 <mdgeorge1> Mathnerd314: because at the end of the day I actually want to compute something
19:10:28 <ddarius> Computing sounds like a boring thing to do at the end of the day.
19:10:29 <krey> lispy: but it doesn't work...
19:10:38 <mdgeorge1> ddarius: depends what you're computing :)
19:10:41 <applicative> olegNation, like this? But I take it you are going to calculate the total?, or do you already have it. 
19:10:51 <mdgeorge1> ** computed ddarius's mom's phone number ;-)
19:10:58 <lispy> krey: what is it and what about it doesn't work?
19:11:02 <mdgeorge1> s/computed/computes/
19:11:19 * aavogt uses /me
19:11:31 * mdgeorge1 always wondered how to do that
19:11:46 * krey I just found out today!
19:11:53 * krey oh crap
19:11:54 <Mathnerd314> mdgeorge1: I see, we're from different disciplines.
19:12:03 <mdgeorge1> actually not really
19:12:30 <mdgeorge1> I mean, I am trying to decide whether to implement 6th roots of unity or nth roots of unity :)
19:12:43 <mdgeorge1> to stick into my quaternions...
19:12:52 <ddarius> Quaternions, blech.
19:13:04 <krey> lispy: so, how do you turn Pair r into (,) ?
19:13:24 <krey> lispy: I mean: Pair r a b into (a,b)
19:13:25 <ddarius> krey: Where is the r supposed to go?
19:13:45 <krey> ddarius: possibly nowhere
19:14:01 <lispy> krey: what would r have to be equal to for Pair r a b to give back (a,b)?
19:14:01 <krey> ddarius: I'm trying to do an alternative representation of pairs here
19:15:03 <krey> lispy: could it be Pair (a,b) a b?
19:15:10 <lispy> krey: yes!
19:15:33 <lispy> at this point, the solution is probably obvious to you
19:15:34 <ddarius> @let i = 0 :+ 1
19:15:35 <lambdabot>  Defined.
19:15:48 <ddarius> > exp (i/3) :: Complex (Complex Double)
19:15:49 <lambdabot>   Ambiguous occurrence `i'
19:15:49 <lambdabot>  It could refer to either `L.i', defined at <local...
19:15:54 <ddarius> Curses.
19:15:54 <krey> lispy:so, could you do it as \pair -> (fst pair, snd pair)
19:15:58 <krey> I guess not...
19:16:05 <lispy> krey: you're on the right track
19:16:06 <ddarius> @undefine
19:16:20 <lispy> krey: expand Pair (a,b) a b all the way out
19:16:26 <lispy> krey: what is that equal to as a type?
19:16:51 <ddarius> If Haskell had eta rules, \pair -> (fst pair, snd pair) would be equivalent to id.
19:17:16 <ddarius> > exp (0 :+ 1/3) :: Complex (Complex Double)
19:17:17 <lambdabot>   No instance for (GHC.Float.RealFloat
19:17:17 <lambdabot>                     (Data.Complex.Comp...
19:17:33 <krey> ddarius: sorry, avoiding confusion: \pair -> (first pair, second pair)
19:17:46 <krey> where first and second are the functions I'm trying to figure out
19:18:21 <parcs> fst and snd are the actual function names of what you're trying to figure out
19:18:22 <mdgeorge1> ddarius: are you talking to me?
19:18:53 <lispy> krey: just go one step at a time.  If Pair r a b = (a -> b -> r) -> r, then what does Pair (a,b) a b = ?
19:19:07 <krey> lispy: (a -> b -> (a,b)) -> (a,b)?
19:19:13 <lispy> yup
19:19:22 <krey> but this is the thing
19:19:28 <krey> I've got Pair r a b
19:19:29 <krey> :D
19:19:47 <lispy> Right, so let r = (a, b)
19:20:05 <lispy> I can't see any reason why that would be illega
19:20:07 <lispy> illegal*
19:20:14 <krey> but I'm trying to write a fully polymorphic function here
19:20:21 * lispy nods
19:20:27 <krey> fine...
19:20:33 <lispy> But, you wanted to go from Pair to (,) right?
19:20:40 <krey> yep
19:20:48 <krey> Pair r actually
19:20:57 <lispy> To do that, you have to fix a specific r
19:21:04 <krey> how do I do that?
19:21:17 <krey> oh, I don't..
19:21:41 <lispy> You pretty much had it when you said: <krey> lispy: (a -> b -> (a,b)) -> (a,b)
19:21:53 <lispy> I can't think of very many fuctions to satisfy that
19:22:02 <lispy> ?djinn a -> b -> (a, b)
19:22:02 <lambdabot> f a b = (a, b)
19:22:04 <krey> yeah, I can write the function
19:22:14 <krey> and so can djinn :)
19:22:23 <djahandarie> @djinn-more
19:22:23 <lambdabot> Unknown command, try @list
19:22:27 <djahandarie> Gah
19:23:12 <djahandarie> Hm, apparently no way to do that via the \bot plugin
19:23:30 <ddarius> djahandarie: You could always hack lambdabot.
19:23:46 <lispy> ?version
19:23:46 <lambdabot> lambdabot 4.2.2.1
19:23:46 <lambdabot> darcs get http://code.haskell.org/lambdabot
19:23:49 <djahandarie> My policy is to not compound hacks on each other
19:23:50 <ddarius> djahandarie: Of course your changes won't matter 'cause Cale never rebuilds lambdabot.
19:24:04 <ddarius> djahandarie: That's all lambdabot is.
19:24:10 <djahandarie> Exactly.
19:24:14 <krey> lispy: omg, I see the light
19:24:24 * ddarius doubts that.
19:24:26 <lispy> krey: \0/
19:24:49 <krey> ddarius: come on, I ain't that terrible at this
19:24:50 <mdgeorge1> is djinn a theorem prover then?
19:24:51 <ddarius> It's an ecstatic lightbulb head!
19:24:55 <mdgeorge1> that's pretty cool
19:25:45 <Cale> I plan to rebuild lambdabot at some point in the future when I can :)
19:25:57 <lispy> Cale: remind me why you can't?
19:26:08 <lispy> I've basically given you root on that machine, BTW :)
19:26:27 <Cale> oh
19:26:28 <Cale> ?
19:26:43 <lispy> Yeah, you should try typing sudo :)
19:26:53 * lispy wonders if he shouldn't have told Cale ;)
19:26:57 <Cale> It asks for a password
19:27:06 <lispy> Yeah, sudo needs your user password
19:27:20 <Cale> Yeah, I don't think my user password is set. I just have an ssh key.
19:27:21 <lispy> Then it tells you not to drink and drive
19:27:27 <lispy> hah, okay
19:27:53 <Cale> and of course, passwd asks for it as well :)
19:27:58 <lispy> Not sure how to make sudo take an ssh key
19:28:24 <ddarius> Why would it want a strong encryption key when it could take a weak password?
19:28:28 <krey> lispy: thanks for your help, I see that my problem was that I was hoping that I could turn Pair monkeys a b into a or b or anything... done too much untyped stuff recently!
19:29:01 <lispy> krey: yeah, the trick is the r.  Continuation types are a bit much to grok at any rate.
19:30:26 <Cale> lispy: But basically, right now there's a problem because of no libreadline, and I seem to recall just needing a newer ghc at one point.
19:30:32 <Cale> But maybe that's been solved
19:30:47 <aavogt> Cale: you can't compile elsewhere?
19:30:55 <djahandarie> Even the lambdabot maintainer is too scared to compile lambdabot
19:30:57 <lispy> oh, 6.10.4 is kind of od
19:31:03 <lispy> old*
19:31:08 <Cale> I'm not the lambdabot maintainer
19:31:19 <ddarius> djahandarie: I had no trouble building lambdabot.
19:31:22 <Cale> The lambdabot package on hackage lists me as the maintainer, but I didn't make it!
19:31:40 <ddarius> Everyone disowns lambdabot before long.
19:32:03 <djahandarie> Well, finals and such tomorrow, night
19:32:11 <number9> lispy: do you have a minute again? Sorry to pester you with so many questions.
19:32:12 <lispy> libreadline5-dev installed now
19:32:17 <lispy> number9: what's up?
19:32:30 <number9> So the cabal version of unlit doesn
19:32:36 <number9> t work either
19:32:57 <dcoutts_> how so?
19:32:58 <number9> I am going to try to fix it as well as make it a program that can be a drop in replacement for the c program that is currently used
19:33:14 <lispy> number9: that's great
19:33:55 <lispy> dcoutts_: number9 is trying to use markdown with bird style lhs and unlit doesn't understand that # outside of a code block should not appear in the unliterate version
19:34:07 <number9> lispy: My question is this: What do cpp lines look like and why would you want them to appear outside of code-blocks?
19:34:11 <dcoutts_> number9: note that the Cabal unlit does something different to GHC unlit, it's designed to work properly with haddock
19:34:55 <number9> dcoutts: understood, I know that it trys to make all the comments etc. work whereas the c version just strips lines that are irrelevant to the code
19:35:01 <dcoutts_> lispy: you mean lines beginning with '#', as in cpp directives?
19:35:15 <lispy> number9: they look like, #word, where word is if, ifdef, define, endif, include, etc. and sometimes they take args like, #ifdef FOO
19:35:21 <lispy> dcoutts_: correct
19:35:27 <dcoutts_> why do you think that is wrong?
19:35:37 <kmc> lispy, number9, dcoutts_ maybe you all know this already, but pandoc already knows about lhs
19:35:37 <dcoutts_> from my recollection of the unlit spec, that's right
19:35:43 <number9> dcoutts: yes: http://hackage.haskell.org/trac/ghc/ticket/4836
19:35:45 <kmc> you don't need to run unlit
19:36:01 <kmc> pandoc -o foo.html -i foo.text -f markdown+lhs
19:36:11 <lispy> dcoutts_: then the spec has a bug :)
19:36:42 <dcoutts_> you want it to do something special for # lines, even though # is nothing to do with Haskell or the unlit format?
19:36:45 <number9> kmc: it is an error in trying to run a .lhs file
19:36:53 <dcoutts_> sounds to me like you want to pre-process before unlitting
19:36:56 <dcoutts_> or after
19:37:00 <kmc> ah
19:37:12 <number9> dcoutts: no I want it to comment them out or remove them, like I am fairly sure that it should
19:37:18 <dcoutts_> but before passing to you other tool
19:37:34 <dcoutts_> I'd consider it a layering violation for unlit to know about cpp
19:37:49 <lispy> dcoutts_: it currently knows about cpp
19:37:59 * krey has finally overcome the strictness of pattern-matching using continuations!!!
19:38:06 <dcoutts_> surely the file is either a .lhs.pp or it is a .pp.lhs, in which case one ordering or the other should work
19:38:32 <dcoutts_> lispy: does it, or is that just to remove #!/bin/blah lines as a unix hack?
19:38:54 <lispy> dcoutts_: take a look at the source in the ghc repo at utils/unlit/unlit.c
19:38:56 <hengxin> how to implement eight queens problem in haskell?
19:39:05 <number9> can someone please explain why a message to the cpp would appear outside of a code-block in a literate haskell file? I am assuming that cpp messages are something that appear in haskell code, as in .hs, so why wouldn't they too be delimited by > or in /begin{code} /end{code} block?
19:39:10 <lispy> dcoutts_: it leaves in any line that starts with #
19:39:44 <number9> dcoutts: and the problem is that # is used in markdown for gitit for headings
19:39:59 <number9> * or my problem really. 
19:40:05 <aavogt> can't you use == heading == ?
19:40:31 * aavogt doesn't remeber gitit very well
19:40:43 <number9> aavogt, you can underline with == or -- for a h1 or h2 heading respectively but for h3-h6 it is determined by the number of #
19:40:57 <number9> ex h3 heading: ### This is an h3 heading
19:42:03 <dcoutts_> lispy: ah, but hold on, you're using ghc's unlit which removes all comments, which isn't what you want at all
19:42:23 <xplat> people actually use h3 headings?  :)
19:42:27 <dcoutts_> the fact that it passes through # lines is just a distraction isn't it?
19:42:57 <dcoutts_> it's going to remove all normal text
19:42:58 <lispy> dcoutts_: did you look at the ticket that number9 linked?  There is a test case so you can see how it fails first hand
19:43:28 <Saizan> dcoutts_: here they are trying to run the code afaiu, so removing the comments is fine, but leaving in lines starting with # isn't, unless they are actually CPP directives, which they aren't in this case
19:43:33 <lispy> (and I pasted in the output of the unlit step so you can just read the ticket if you trust me :)
19:43:34 <xplat> i always thought that html should just have an optional heading for each block-style element and the heading level/importance would be inherited from the block position, or later once css came around, the block class would be possible as well
19:43:50 <krey> how is fst defined in the prelude?
19:43:58 <dcoutts_> Saizan: ahh, they're trying to run it, sorry, was not clear
19:43:58 <aavogt> @src fst
19:43:58 <lambdabot> fst (x,_) =  x
19:44:08 <Saizan> dcoutts_: the problem is that # lines in comments get promoted to # lines in code
19:45:26 <number9> dcoutts_: Saizan lispy: Why wouldn't cpp directives appear in code blocks with the rest of the "code"?
19:45:41 <Saizan> you could argue that if you want to use CPP in a pp.lhs file you should use "> #ifdef .."
19:45:41 <dcoutts_> ok, so I wonder what order ghc runs unlit vs cpp
19:46:01 <lispy> dcoutts_: unlit first
19:46:04 <number9> because it would be almost trivial to remove/comment out all lines that weren't in code blocks or after >
19:46:09 <kmc> Saizan, that depends on whether you want to conditionalize the documentation too! :D
19:46:29 <lispy> dcoutts_: it passes a flag to unlit that actually tells it to insert a # directive
19:46:34 <kmc> alias unlit='grep ^\>'
19:46:36 <kmc> ;)
19:46:38 <dcoutts_> lispy: so perhaps that's the real problem then, if it did cpp first then it would not need a special mode to pass # through
19:46:40 <number9> kmc: but this is just to produce an executable
19:46:53 <Saizan> kmc: but then it's a .lhs.pp (iyswim) and you run cpp before unlit
19:47:05 <lispy> dcoutts_: then you couldn't use hashes in your comments
19:47:36 <dcoutts_> lispy: then don't declare that your file use CPP
19:48:44 <lispy> I don't understand why that would be a desirable restriction
19:49:09 <xplat> i'm sure it would be great if all the file formats we used every day were designed to respect layering, but guess what?
19:49:44 <ddarius> xplat: The solution is clearly XML and XML namespaces.
19:50:02 <dcoutts_> lispy: if your module uses CPP then you can use # directives, but that steals # from comments. If you don't use CPP then you can use # in comments. I don't get the difficulty.
19:50:44 <lispy> dcoutts_: why not just layer things correctly and be done with it?
19:50:45 <number9> correct me if I'm wrong but if we saved cpp directives for in the code blocks wouldn't that make 99.99% of people happy and be simple to implement? If you wanted conditional documentation, than that could be accomplished with haddock or the like right?
19:50:55 <dcoutts_> lispy: that is a correct layering, it's .lhs.pp
19:51:11 <krey> can someone explain why line 33 works and 32 doesn't? (my pattern-matching misery continues) http://pastebin.com/Qtd5U760
19:51:12 <kmc> the solution is clearly IFF / sexprs / ASN1 / XML / Agile JSON on Rails
19:51:41 <xplat> heh.  the thing that actually bugs me the most is how mime types can't even name a multilayered format, except for a few special-purpose hacks.  even extensions coudl handle that.
19:51:47 <kmc> what do you mean by "works" krey?
19:52:24 <krey> kmc: with 32, fst parity x doesn't terminate for any x
19:52:27 <dcoutts_> lispy: if we want to support both orderings then it needs to be specified more explicitly, e.g. cabal could do either based on extension, but ghc's builtin cpp has to just pick one
19:52:42 <kmc> krey, because it has to pattern-match the (,) constructor before proceeding
19:52:47 <kmc> tuples are a common use case for irrefutable patterns
19:52:52 <kmc> parityRec ~(f,g) = ...
19:53:12 <dcoutts_> lispy, number9: so I see the cabal unlit code does the same thing, which I think is wrong.
19:53:18 <krey> kmc: so I've been told
19:53:27 <dcoutts_> I don't think it should treat # directives specially
19:53:45 <krey> kmc: ut why does it get _|_ rather than (_|_, _|_) ?
19:53:55 <number9> dcouts: so you think it should be only in code blocks or after > that get passed down?
19:54:25 <xplat> ghc's builtin cpp has to pick 'unlit goes first' because ghc can't tell whether cpp should be run until it reads the unlitted code to check for inline compiler options, right?
19:55:05 <kmc> krey, if f is defined by «f (C x y) = ...», then f is strict
19:55:08 <dcoutts_> xplat: I guess that's right
19:55:28 <dcoutts_> number9: that get turned into code, yes.
19:55:38 <krey> kmc: thanks
19:55:39 <kmc> krey, the definition of "f is strict" is "f ⊥ = ⊥"
19:55:54 <dcoutts_> number9: and everything else should be removed (or turned into a comment)
19:55:57 <kmc> and any function which has to pattern-match immediately is going to satisfy f ⊥ = ⊥
19:56:18 <krey> kmc: I think I understand it now
19:56:19 <kmc> and if f is strict, then «fix f = ⊥»
19:56:26 <kmc> because ⊥ is a fixed point of f
19:56:32 <krey> oh wait what?
19:56:32 <krey> :D
19:56:33 <kmc> (as we just noted)
19:56:48 <krey> yep
19:56:48 <kmc> "f is strict" ⇒ "f ⊥ = ⊥" ⇒ "⊥ is a fixed point of f"
19:56:51 <krey> true
19:56:55 <kmc> and 'fix' finds the least fixed point
19:56:56 <number9> dcouts_: well that should be much easier to code. I am going to work on it and get back to you but it might take a couple of days
19:57:03 <dcoutts_> number9: but xplat makes a good point, that since {-# LANGUAGE CPP #-} is specified inside the .lhs file inside a code block, then it only makes sense to unlit first
19:57:25 <dcoutts_> number9: but for ghc's unlit it's easy, it seems to have a command line flag for this mode
19:57:27 <kmc> where 'least' is defined in the lattice of partially-defined values ordered by the more-information / less-information relation
19:57:40 <kmc> so ⊥ < (⊥,⊥) < (3,⊥) < (3,4)
19:57:46 <number9> dcouts_: one other thing of note is that the original unlit.c has a setting where it can leave in or drop cpp lines
19:57:54 <dcoutts_> number9: right
19:58:00 <kmc> and also (⊥,⊥) < (⊥,4) but (3,⊥) and (⊥,4) are not related by ≤
19:58:06 <number9> dcouts_: That shouldn
19:58:08 <krey> kmc: yeah
19:58:11 <krey> I get that part
19:58:14 <kmc> conal's blog has some good posts about this, and ezyang did a few recently
19:58:16 <dcoutts_> number9: so why do you need to rewrite anything?
19:58:16 <kmc> cool
19:58:17 <lispy> the leavecpp thing is a compile time constant though, nothing sets it
19:58:20 <number9> t be to hard to reimplement if you think it is worth it
19:58:31 <kmc> so yeah, «fix f = ⊥» if f is strict
19:58:36 <number9> dcoutts_: because right now it doesn't work?
19:58:36 <dcoutts_> lispy: oh, I assumed it was connected to the -# flag
19:58:38 <krey> kmc: the pattern matching strictness is the thing that's new to me
19:58:57 <krey> kmc: you could still link that blog, i'll read it
19:59:11 <number9> dcouts_: both the unlit.hs from Distribution.Simple.PreProcess.Unlit and the unlit.c that comes with ghc don't work
19:59:20 <lispy> dcoutts_: if (strcmp(*argv,"-#")==0)
19:59:20 <lispy>             ignore_shebang = 0;
19:59:27 <dcoutts_> oh
19:59:32 * dcoutts_ should just read the code
19:59:37 <lispy> :)
19:59:49 <number9> or go to the bug page, it is almost all there
20:00:16 <dcoutts_> what about the -r flag?
20:00:25 <dcoutts_> "-r  remove cpp droppings in output."
20:00:44 <lispy> dcoutts_: I don't think it's real
20:00:59 <number9> dcoutts_: I will try again, but I don't think so either
20:01:29 <Axman6> @hoogle numCapabilities
20:01:29 <lambdabot> No results found
20:01:31 <Axman6> :(
20:01:36 <number9> when I tried setting the "LEAVECPPLINES" to 0 (from 1) it got rid of everything after the # but not the # itself which caused problems
20:01:50 <lispy> dcoutts_: maybe it used to support that via -r, but there is no code to handle -r in the command line args parser
20:01:57 <dcoutts_> mm
20:01:57 <number9> dcouts_: which is what I think the -r flag does, but not sure
20:02:30 * lispy thinks that when the code and the comments disagree, both are wrong :)
20:02:53 <dcoutts_> have you tried connecting up the -r flag to the leavecpp variable?
20:02:59 <xplat> maybe it was one of those 'removed because if anyone was using it there would have been a bug report that it didn't work at all' things
20:03:43 <lispy> dcoutts_: No, I dug deep enough to write the bug report and left it that.  I think number9 is more motivated to actually fix it :)
20:03:53 <number9> dcouts_: now, and I think that I agree with lispy in thinking that I would be happy to see this old chunck of c code go either way
20:04:13 <number9> dcouts_: I will try that first though because it would be nice to know
20:04:38 <stephane_> hi guys
20:04:51 <lispy> it would be nice to have someone (number9?) look at the spec for unlit and write some test cases and add them to ghc.
20:05:13 <lispy> I think that's part of why we're even having this discussion
20:05:35 <lispy> and if the spec isn't clear, we have processes these days to get clarification with community input
20:06:53 <deibit> hi there, (haskell platform on OSX) I'm trying to update Network package to 2.3 but when doing "cabal upgrade Network" it says that package "bytestring" failed during building phase. How can I update the libraries the right way?
20:07:12 <dcoutts_> deibit: don't use upgrade, use install
20:07:23 <c_wraith> dcoutts_, why hasn't upgrade been removed yet?
20:07:23 <dcoutts_> upgrade does not do what you want
20:07:30 <c_wraith> It never does what anyone wants :)
20:07:32 <dcoutts_> c_wraith: it has in the dev versions
20:07:42 <dcoutts_> and thus in the next release
20:07:47 <c_wraith> ah, ok
20:07:54 <deibit> :D
20:07:58 <deibit> it worked
20:08:48 <number9> lispy, dcouts_: thanks again for your time and help, i'll be back tomorrow, I'm sure with more questions...
20:08:53 <deibit> it is dangerous to do the same with base package?
20:09:02 <lispy> number9: cheers
20:09:14 <c_wraith> deibit, it won't even let you install a new version of base.  those are part of ghc
20:10:05 <deibit> thanks c_wraith, I find more hard to manage the Haskell ecosystem than the language itself
20:11:27 <xplat> right now it seems like hackage/cabal is in that not-so-sweet spot halfway between cpan and debian
20:11:56 <xplat> as opposed to redhat's old not-so-sweet spot halfway between debian and cpan
20:12:27 <dcoutts_> xplat: I'm not sure I follow
20:12:31 <tg_> me neither
20:12:52 <tg_> though I admit there are not-so-sweet spots in base-rpm mgt, deb(file) mgt, and cpan (which is great sort of)
20:14:19 <c_wraith> also, note that the way debian does ghc packages is horribly incompatible with anything else that uses GHC.  just for fun :)
20:14:32 <xplat> well, cpan works pretty well for what it is, which is basically pretty anarchic.  you build everything on your own system, the dependency management is sort of loose, if something breaks, well, it probably just wasn't good enough to use anyway.
20:14:34 * ddarius suspects that Hackage/Cabal is in the not-so-sweet spot of not being 20 years old.
20:15:24 <dcoutts_> xplat: so does that mean they use a lot more people to curate the collection?
20:15:46 <dcoutts_> ie more like a distro, doing QA of the collection
20:16:35 <xplat> whereas debian is more, everything is managed, there is a process for every contingency, including updating things taht are without maintainer, deprecating and removing packages taht can't keep up, etc
20:17:33 <xplat> hackage is sort of in the middle, it tries to have all the automatic dependency stuff and autobuilds and things but the social process is still anarchic
20:18:26 <dcoutts_> aye, my plan in that regard is more tools on hackage to help track what works and point out what doesn't
20:18:48 <dcoutts_> and add a role for people to help curate the collection e.g. by tweaking deps or pestering maintainers
20:19:49 <xplat> the main thing that allows debian to do aggressive QA is that they separate the roles of package authorship and package maintenance
20:20:48 <xplat> not just in practice but in principle, as it were
20:21:37 <dcoutts_> and the fact that it is deliberately not a release point
20:21:50 <dcoutts_> so authors cannot release when they like
20:22:03 <xplat> yes
20:22:33 <dcoutts_> and they (mostly) pick one version of each package and make sure that set has consistent deps
20:22:50 <dcoutts_> if necessary by patching
20:23:00 <xplat> debian makes itself all about integration and not at all about all the other stuff that goes into software
20:23:15 <Axman6> anyone have any experience with a System.Event based server? I'm wondering how i'd send information to a (bunch of) callbacks to get them to do something. everything's file descriptor based, so i'd need to register the callback with an fd that's used to tell the callback i want it to do something
20:23:43 <Axman6> basically i need an fd internal to the program, that i can use to poke a bunch of callbacks into doing something
20:23:43 <xplat> it would be hard for a language-centric repo to do that, unless maybe it had a community the size of java.  really, java doesn't even manage to do it.
20:24:04 <dcoutts_> Axman6: are you sure you need to use events and not just threads?
20:24:28 <Axman6> designing this server i think may be easier than the current threaded one i'm working on
20:25:15 <dcoutts_> Axman6: if it's only for structure and not performance, then implement your event API in terms of threads and ordinary IO
20:26:02 <dcoutts_> xplat: I think something similar is possible by defining subsets of hackage, authors can still release whenever they like but their releases do not necessarily immediately get included into the managed subset 
20:27:21 <Axman6> dcoutts_: well, the way i can see to do it with threads ends up with three threads per client, plus at least one main thread that keeps track of everything
20:27:39 <dcoutts_> Axman6: sounds pretty reasonable
20:27:53 <xplat> dcoutts_: ah, like debian unstable/testing/release only with an extra stage like a more populated 'experimental' that is managed directly by authors?
20:27:57 <dcoutts_> Axman6: or do you mean that's annoying from a code cleanlyness pov?
20:27:58 <Axman6> heh, if you say so
20:28:06 <Axman6> well, that too
20:28:18 <dcoutts_> Axman6: in principle it's fine from a performance pov
20:28:26 <Axman6> i know threads ar cheap in haskell, that still sounds like a hell of a lot of threads
20:28:37 <dcoutts_> 10,000,  100,000 ?
20:28:39 <Axman6> it feels ugly
20:28:45 <Axman6> more like hundreds
20:28:51 <dcoutts_> oh, that's nothing
20:28:58 <Axman6> yeah i guess
20:28:59 <dcoutts_> I suggest you relax
20:29:02 <dcoutts_> :-)
20:29:04 <Axman6> :)
20:29:04 <xplat> Axman6: ghc threads aren't quite as cheap as erlang threads, but they're a lot closer to that than threads in any other language you've heard of
20:29:22 <dcoutts_> xplat: I'm not so sure, I was under the impression they were cheaper
20:29:27 <Axman6> xplat: my experience is that they're even cheaper than erlang threads (in terms of performance)
20:29:44 <dcoutts_> they have less per-process/thread state
20:29:59 <xplat> hm, maybe i'm thinking relative to unthreaded code
20:30:06 <Axman6> http://shootout.alioth.debian.org/u32/benchmark.php?test=threadring&lang=all
20:30:33 <Axman6> nice to see Erlang's #2 there, it was slower than C and Java a while ago i believe
20:31:27 <Axman6> ah, reading the erlang entry has made me want to play with erlang again@
20:31:29 <Axman6> !*
20:31:31 <xplat> when erlang was slower it was probably because in the old days you needed IPC to use multiple cores.  GHC used to have the same problem iirc.
20:31:40 <xplat> only without the workaround
20:34:01 <kmc> you needed IPC to use multiple cores?
20:34:11 <xplat> anyway, it sure is nice to have languages where you can use threads without counting them
20:34:29 <Axman6> aye
20:35:18 <xplat> kmc: erlang was designed in the case of single-core machines.  it used marshalled interprocess message passing to run on clusters.
20:35:29 <qfr> What might I want to use for embedded scripting in a Haskell application? Does the standard library provide some facilities to compile Haskell at runtime?
20:36:32 <xplat> because that mechanism existed, you could run one process for each core and use them like a machine cluster, so it took some pressure off the language maintainers to support multicore directly and so it took longer than you'd expect for a language where massive multithreading is the advertised killer feature
20:37:38 <kmc> qfr, the standard lib doesn't, but see packages: plugins direct-plugins hint mueval metaplug
20:37:40 <aavogt> qfr: I've heard it's possible to embed hugs. Otherwise check out the  ghc-api (or a nicer wrapper  hint). There's also this 'plugins' thing
20:37:43 <kmc> and the GHC API itself
20:38:06 <kmc> there's also 'dyre', which is not about runtime code loading, but rather about recompiling and re-executing your program when its config changes
20:38:14 <kmc> with saved state
20:38:21 <kmc> basically how xmonad works
20:38:46 <aavogt> metaplug doesn't look like it would ever work
20:38:49 <qfr> Doesn't have to be Haskell anyways
20:38:59 <qfr> I just thought it might be the easiest to integrate
20:40:18 <xplat> you can also embed perl5 and perl6, if you're the type who likes intriguing contrasts :)
20:40:26 <qfr> Hahaha
20:40:30 <qfr> I'm afraid I do not like Perl
20:40:43 <qfr> But I'd consider it if the interface was good
20:40:56 <aavogt> pugs isn't very cutting edge anymore, is it
20:41:37 <xplat> i know why the perl6-in-haskell embedding came to be, but the perl5 one kind of mystifies me
20:42:15 <xplat> aavogt: yeah, i think pugs has fallen pretty far behind the bleeding edge of figuring-out-what's-actually-supposed-to-be-inperl6
20:42:23 <kmc> there's libs to embed python too
20:45:08 <aavogt> qfr: what kind of interface are you expecting to provide?
20:45:45 <qfr> aavogt none really, I have never written any Haskell of significance, I'm a total newbie at FP in general, I was just wondering about it anyways because I was recently dealing with that in C++ and C# etc
20:53:31 <int80_h> I'm using a module that is in mumtiple packages. How can I address this problem?
20:53:37 <int80_h> multiple, too
20:55:29 <int80_h> I'm using a module that is in multiple packages. How can I address this problem?
20:56:46 <xplat> 598 gone.  i guess that makes this the small side of the netsplit.
20:56:46 <Axman6> ouch
21:03:52 <int80_h> how can I deal with the ambiguous module name error?
21:04:00 <kmc> build your project with cabal
21:04:05 <kmc> specify the package dependencies in your cabal file
21:04:13 <xplat> int80h: or you can use -hide-package to hide one of the packages, or use the module name in double quotes like this: import "mtl" Control.Monad.State
21:04:40 <Saizan> the latter requires {-# LANGUAGE PackageImports #-}
21:04:43 <int80_h> so ghc -hide-package foo ... would work?
21:04:56 <kmc> i think Cabal is the Right Way to do this; in particular PackageImports is really not meant for end-user use
21:05:03 <xplat> my methods are better for one-off hacks, though, cabal is better for a real project
21:05:09 <kmc> the manual says as much
21:05:25 <kmc> specifying -package and -hide-package yourself would make sense if you're just screwing around in ghci
21:05:33 <int80_h> kmc, agreed but I am very beta right now. I need to solve this bizarre problem, then I can assert some normality.
21:05:44 <kmc> ok i'm not sure what that means
21:05:53 <kmc> but i bet you can get it working with some combo of cabal and -package and -hide-package
21:06:22 <int80_h> kmc, don't want to add another layer of complexity until I solve a particluar problem. When I do, I'll build the right way.
21:06:37 <aavogt> xplat: an easier hack is to   ghc-pkg hide something
21:06:38 <int80_h> kmc, hmm you're right
21:07:12 <xplat> aavogt: i hate introducing state for quick hacks, though, or i wouldn't be using haskell :)
21:08:06 <xplat> in this case, hidden state
21:08:28 <Saizan> do it the cabal way: -hide-all-packages -package foo -package bar ...
21:08:53 <Saizan> (not actually recommended for human users)
21:09:02 <int80_h> Saizan has spoken. Very well, I'll do it the right way from the begining.
21:09:40 <aavogt> xplat: I think it's no different that not specifying package versions normally
21:10:50 <interferon> quickcheck is amazing
21:10:53 <xplat> aavogt: but now there are two sets, your installed packages and your default-visible packages, and neither is a function of the other
21:27:43 <ian_mi> How efficient are incremental array updates compared to mutable arrays? If the original array is no longer needed is a copy avoided?
21:27:56 <kmc> not efficient, and probably not
21:28:10 <kmc> the DiffArray package is supposed to do this better, but has performance issues
21:28:26 <kmc> i recommend using a tree-shaped structure like Data.IntMap rather than flat arrays, if you need updates
21:28:41 <kmc> because with a tree your "copy" is only a small amount of data
21:28:47 <kmc> most of the new tree is pointers into the old tree
21:29:09 <kmc> and, this isn't some fancy GHC optimization; it's a natural consequence of the straightforward way of implementing immutable algebraic data
21:29:13 <kmc> which is pretty cool :)
21:29:38 <ian_mi> I was going to try using IntSet next, but it bothers me that arrays are infeasible
21:29:44 <kmc> why does it bother you?
21:29:57 <kmc> they're not infeasible, you can try both easily
21:30:26 <ian_mi> because modifying IntSets aren't O(1)
21:30:42 <kmc> yeah it is
21:30:52 <kmc> the depth of the IntSet tree is bounded by the number of bits in an Int
21:30:54 <kmc> i.e. 32 or 64
21:31:10 <kmc> this is kind of a smartass answer; my real point is that asymptotic complexity analysis is not relevant
21:31:40 <kmc> just profile it and see which is faster
21:32:01 <ian_mi> it won't be as fast in any case as a proper implementation using an array
21:32:09 <kmc> you say that, but you haven't profiled it
21:32:09 <xplat> if you want O(1) modification that badly there are always finger trees, as long as you modify sequentially or otherwise at bounded distance from your last mod you get O(1) average
21:32:16 <aavogt> then use a mutable array
21:32:32 <kmc> if you want the best performance then write hand-unrolled cache-aware assembly code
21:32:34 <kmc> and call it via FFI
21:32:52 <kmc> (what?  CS 101 asymptotic analysis didn't mention caches?)
21:33:03 <xplat> if an array is really the best data structure use an STArray, they work well for the cases where an array is actually good
21:33:44 <kmc> this topic comes up every day and nobody has any numbers to back up the contention that IntMap is *obviously* too slow
21:33:57 <kmc> (the papers which actually profiled it found it fairly competitive, iirc)
21:35:03 <ian_mi> I will try IntSet but the algorithm I have in mind doesn't need to be modified to make ideal use of cache
21:35:23 <ian_mi> so I don't see how that is relevant
21:35:25 <xplat> what is the algorithm you have in mind?
21:35:38 <ian_mi> sieve of eratosthenes
21:36:38 <xplat> that doesn't make ideal use of cache in an array-based implementation.  also, zomg, you can get O(1) dropping those composites with a linked list implementation
21:37:32 <kmc> i'd like to point out that even without being a prefix trie, operations on a balanced binary tree are O(64) = O(1), if your data fits in memory
21:37:40 <xplat> the real optimization challenge for sieve is not screwing it up so it processes the same numbers over and over again
21:38:56 <xplat> kmc: it's possible to push that point too far, but you are correct insofar as you stop seeing a log factor pretty quickly at realistic ranges of problem size
21:39:07 <kmc> yeah
21:39:25 <kmc> i'm mostly trying to point out the limits of asymptotic analysis as a way to make real-world data structure decisions
21:39:35 <kmc> caching is another big reason for that
21:40:09 <kmc> basically i think the area between "IntMap is too slow" and "STArray is too much work" is very small
21:40:20 <kmc> except for essentially read-only data, where you can use plain old Array just fine
21:41:01 <xplat> i don't get 'STArray is too much work'
21:41:05 <aavogt> which Array?
21:41:13 <kmc> well imperative programming is a pain xplat
21:41:32 <kmc> ian_mi, but you were asking about operations which can remove intermediate copies, and i believe the 'vector' package does some of that, through its stream-fusion framework
21:41:56 <kmc> Data.Array.Array
21:41:58 <kmc> aavogt,
21:41:59 <xplat> well, yeah, except you can't get O(1) writes from an array outside imperative programming anyway
21:42:27 <kmc> you could if your declarative program happens to have the right structure to be linearity-analyzed
21:42:40 <kmc> but the limits on this technique are pretty severe right now
21:43:03 <xplat> kmc: luckily, you can ensure that by writing it in an appropriate state monad
21:43:25 <Boxo> > sequence [not, not]
21:43:26 <kmc> no, you can't
21:43:26 <lambdabot>   Overlapping instances for GHC.Show.Show
21:43:26 <lambdabot>                              (GHC.B...
21:43:41 <kmc> that removes the ability to treat arrays as first-class values
21:43:49 <aavogt> > sequence [not, not] True
21:43:50 <lambdabot>   [False,False]
21:43:51 <Boxo> I was wondering if there was a handy way to compose a list of functions...
21:43:54 <kmc> replacing it with an imperative concept of in-place update, regardless of how it's implemented
21:44:02 <kmc> :t foldr (.) id
21:44:02 <lambdabot> forall b. [b -> b] -> b -> b
21:44:18 <aavogt> mconcat
21:44:23 <Boxo> mconcat!
21:44:30 <kmc> appEndo . mconcat . map Endo ?
21:44:36 <Boxo> > mconcat [not,not,not] True
21:44:37 <lambdabot>   No instance for (Data.Monoid.Monoid GHC.Bool.Bool)
21:44:37 <lambdabot>    arising from a use of...
21:44:38 <aavogt> that should do it
21:44:52 <Cale> Yeah, I usually just use foldr (.) id
21:45:07 <xplat> kmc: it doesn't capture the whole space of declarative programs that can be analyzed and then transformed into stateful array programs, sure
21:45:07 <kmc> :t appEndo . mconcat . map Endo
21:45:08 <lambdabot> forall a. [a -> a] -> a -> a
21:45:11 <Cale> It's a bit odd that this isn't defined as  compose  in the Prelude or something.
21:45:11 <aavogt> Boxo: the right Monoid instance is with a newtype unfortunately
21:45:17 <ian_mi> xplat: I don't see how sieve would be efficient using lists. The entire list would need to be traversed when we really just need to modify the indices p, 2p ..
21:45:42 <aavogt> but the instance that exists      Monoid r => Monoid (a -> r)  -- is probably more useful
21:46:32 <Boxo> looks like foldr is the handiest
21:47:05 <xplat> ian_mi: you don't keep little flags on all the list elements, you remove the ones you won't be using.  there's a whole paper on implementing efficient list-based sieve of erasothenes ...
21:47:28 <ian_mi> kmc: it's my belief that every algorithm is O(1) because they cannot take more than 2^(2^64) operations
21:47:46 <kmc> yeah
21:48:23 <kmc> i mean the runtime of my computer is bounded by the busy beaver sequence at index 2^64 and also by the total amount of energy in the universe so, problem solved
21:48:59 <xplat> either we live in a finite universe and every algorithm is O(1) because you can only do so many operations, or the universe is infinite and every algorithm is O(1) by memoization
21:49:35 <aavogt> this seems to be missing the point of complexity
21:50:24 <xplat> yes.  kmc's usual point works because 64 is not that big a number compared to constant factors that regularly come up in applied algorithmics.  the other versions don't work.
21:52:18 <kmc> in particular i think it's worth worrying about O(n) versus O(log n) but not O(log n) versus O(1)
21:52:24 <kmc> and yes this is even more arbitrary
21:52:41 <kmc> but once you get down to the point where a vanilla binary tree isn't fast enough, you have all kinds of non-asymptotic stuff to think about
21:52:59 <xplat> kmc: the runtime of your computer by itself is bounded at 2^(2^64), or a few powers of two more or less to account for things like registers.  BB(2^64) is your computer with an unbounded amount of passive external storage.
21:53:01 <kmc> (for example, binary trees are woefully cache-inefficient compared to higher-order trees)
21:53:56 <xplat> the best way to be cache-efficient if you don't know the operational characteristics of the cache turns out to be variadic trees like van emde boas trees
21:54:28 <xplat> however you still lose some compared to tuning for a specific cache
21:55:21 <kmc> interesting
21:55:35 <xplat> in practice they can be handy, though, because you don't usually have just one cache to tune for these days and it usually gets impractical dealing with the multiple levels explicitly
21:55:44 <kmc> yeah
21:56:00 <kmc> judy arrays are full of cache microoptimization and even there i think they have essentially a one-level cache model
21:56:06 <xplat> although people have done some really mad cache tuning for sorting, they even take into account things like TLBs
21:56:15 <kmc> just use huge pages :D
21:56:48 <kmc> POWER supports 16GB pages, that should be enough for anyone ;)
21:57:36 <xplat> yeah, probably
21:58:01 <xplat> although i have a problem involving suffix sorting strings bigger than that
22:00:59 <sm> qfr: there's also a couple of schemes on hackage, particularly http://hackage.haskell.org/package/husk-scheme
22:30:46 <Chaze> @pl  flip ((flip (:)) . (:[]))
22:30:46 <lambdabot> (. return) . (:)
22:35:36 <Chaze> @pl curry (concat . transpose . (uncurry ((. return) . (:))))
22:35:36 <lambdabot> curry (join . transpose . uncurry ((. return) . (:)))
22:36:44 <Chaze> is there no better way to achieve curry (f . (uncurry g)) ?
22:37:24 <aavogt> @ty \f g -> curry (f . (uncurry g))
22:37:24 <lambdabot> forall c a b c1. (c1 -> c) -> (a -> b -> c1) -> a -> b -> c
22:37:42 <xplat> :t (.:)
22:37:43 <lambdabot> forall a b (f :: * -> *) (g :: * -> *). (Functor f, Functor g) => (a -> b) -> f (g a) -> f (g b)
22:38:02 <aavogt> @ty \g f -> (f .) . g
22:38:03 <lambdabot> forall a b (f :: * -> *) (f1 :: * -> *). (Functor f, Functor f1) => f1 (f a) -> (a -> b) -> f1 (f b)
22:38:18 <aavogt> @ty \g f -> (f Prelude..) Prelude.. g
22:38:18 <lambdabot> forall b c a a1. (a1 -> a -> b) -> (b -> c) -> a1 -> a -> c
22:38:46 <aavogt> @pl \g f -> (f Prelude..) Prelude.. g
22:38:46 <lambdabot> flip (flip ($ Prelude..) Prelude..)
22:38:51 <xplat> :t (Prelude..) . (Prelude..)
22:38:51 <lambdabot> forall a b c a1. (b -> c) -> (a -> a1 -> b) -> a -> a1 -> c
22:39:18 <Chaze> :t Prelude
22:39:18 <lambdabot> Not in scope: data constructor `Prelude'
22:39:23 <Chaze> :t Prelude..
22:39:24 <lambdabot> parse error on input `Prelude..'
22:39:28 <Chaze> what is that?
22:39:34 <xplat> :t (Prelude..)
22:39:35 <lambdabot> forall b c a. (b -> c) -> (a -> b) -> a -> c
22:39:44 <aavogt> @src (.)
22:39:44 <lambdabot> (f . g) x = f (g x)
22:39:44 <lambdabot> NB: In lambdabot,  (.) = fmap
22:40:26 <xplat> normally you call it (.), but lambdabot overrides (.) with something more general so we qualified it
22:41:53 <Chaze> thanks
22:42:00 <Chaze> looks nice and cryptic now :)
22:42:07 <Chaze> > let merge = ((concat . transpose) . ) . ((. return) . (:))
22:42:08 <lambdabot>   not an expression: `let merge = ((concat . transpose) . ) . ((. return) . (...
22:42:25 <Chaze> > let merge = ((concat . transpose) . ) . ((. return) . (:)) in merge "abc" "123"
22:42:26 <lambdabot>   "a1b2c3"
22:43:33 <aavogt> you could just accept input like   ["abc","123"], which makes merge much prettier
22:45:44 <Chaze> aavogt: or simply use arguments. but i think this point-free style is good practice for understanding haskell
22:51:18 <eee> wow
22:51:30 <eee> haven't been to haskell.org in a while
22:51:38 <eee> got this cool web interface
22:51:53 <kmc> yep, it looks like a Real Language suitable for Serious Professional Enterprise
22:52:11 <eee> you guys are dedicated
22:52:13 <kmc> not a Master's Thesis written by Greasy Hippies
22:52:23 <kfish> greasy!
22:53:08 <eee> i'd take a job programming in this stuff, but if there aint any "enterprise" jobs out there for it, maybe we need to create the jobs
22:53:35 <eee> i revisit things like this when i get tired of java
22:54:38 <kmc> there's more than a few companies using Haskell now
22:54:45 <eee> wow. up arrow works in the repl
22:54:55 <eee> maybe in boston kmc
22:54:58 <kmc> but jobs will still be very hard to come by
22:55:20 <eee> that's what makes y'all so dedicated
22:57:27 <eee> 2 years ago i fought with installing haskell on my out of date os.  finally got it after days before i could do hello world.  now you have a repl at the website
22:57:31 <eee> awesome
22:57:45 <kmc> yeah, a lot has changed in 2 years
22:57:49 <kmc> Haskell Platform is a thing now
22:58:02 <kmc> and installs pretty well on a modern OS
22:58:06 <aavogt> you might still have issues on your out of date os though
22:58:14 <kmc> it's still a fight to get it working on 6-year-old Red Hat Enterprise Linux
22:58:18 <eee> got a new machine
22:58:20 <eee> snow lep
22:59:37 <kmc> i had about as much trouble building ocaml and ocaml libs on RHEL4
23:00:01 <kmc> coqide segfaulted constantly
23:00:30 <kmc> to anyone employed as a software dev
23:00:49 <kmc> do not put up with admins who will not let you configure your own work machine the way you need it to be productive
23:00:59 <eee> i think where I petered out wasn't compiling though, it was not being on a project doing something
23:01:00 <kmc> don't put up with it for even one day
23:01:13 <kmc> it seems like such a small thing but is a big fucking symptom
23:01:17 <eee> just reading about mondas endlessly was somewhat dreary
23:01:25 <kmc> then don't worry about monads...
23:01:29 <kmc> they're not a big deal
23:01:41 <eee> i just needed a project
23:02:09 <kmc> by which i don't mean "monads are easy" but rather "don't focus on the interface, focus on its instances"
23:02:16 <eee> oh, and i saw an empirical version of quicksort in haskel; that does it in place and my head about exploded
23:03:31 <lars9> omg, check this haskeller out: http://ezyang.com/ezyang-resume.pdf  MIT, GPA5.0/5.0 ...
23:03:33 <eee> but i still wanna know it.  I think of haskell sort of as the real python.  that may be the wrong way to look at it ... but it's where i learned about list comprehensions
23:03:50 <kmc> Haskell and Python are pretty different
23:03:55 <kmc> that's one thing they have in common, true
23:04:10 <kmc> lars9, he has a cool blog too
23:04:48 <lars9> kmc yeah that's from his blog site
23:05:00 <eee> i understand some other differences
23:05:06 <eee> persistent data structures
23:05:10 <eee> lazy
23:05:15 <eee> purely functional
23:05:21 <kmc> static types, algebraic data with pattern matching
23:05:25 <eee> not just ad hoc and hap hazzard
23:05:44 <kmc> i think the main thing Haskell and Python have in common is that they're not Java
23:05:50 <eee> is it starting to have all the same libs?
23:05:51 <lars9> kmc: his name is 99% chinese though, but should be ABC
23:06:19 <eee> by that I mean that people find python to be a great multitool
23:06:33 <kmc> there's a lot of haskell libs: http://hackage.haskell.org/packages/archive/pkg-list.html
23:06:36 <eee> like one time i needed to untar unzip on windows and all it had was python
23:06:39 <kmc> quality is variable, as it is with python
23:07:14 <eee> i think java's success correlates with swing
23:07:25 <lars9> eee it's normal to feel excited about python, but it IS a glue language
23:07:27 <eee> but now with the web being prominent
23:07:39 <eee> javascript makes functional interesting to peeps
23:08:18 <eee> i get burned when i use python
23:08:24 <eee> it works nice until it doesn't
23:08:55 <lars9> python is great. but haskell is cooler, and blazing fast comparing with python.
23:09:03 <kmc> languages don't have speeds
23:09:17 <lars9> s/haskell/ghc/
23:09:26 <kmc> there's also many python implementations
23:09:53 <kmc> the CPython interpreter most people use is particularly terrible
23:10:23 <kmc> but there are a few credible alternatives now
23:10:23 <eee> it's an error to do this in python: "abc" + 5 ...... but it is just arbitrarily defined to do this: "abc" < 5
23:10:32 <eee> or it was as of some version i was in
23:10:43 <eee> it's deterministic but arbitrary
23:10:44 <kmc> eee, you'll find the same in Haskell if you derive Ord for a data type with more than one constructor
23:11:08 <kmc> a lot of the supposed flexibility of dynamic typing comes from using the type system as a single, global, ad-hoc variant/sum type
23:11:10 <eee> Ord?
23:11:14 <kmc> yes
23:11:22 <shachaf> kmc: "arbitrary"? I think it makes sense.
23:11:38 <kmc> Ord is the interface (type class) implemented by types which have comparison
23:12:01 <kmc> and it's one of the magical built-in type classes for which the compiler can generate instances automatically ("deriving")
23:12:06 <eee> so are you guys mostly professors?
23:12:12 <kmc> haha
23:12:17 <eee> :)
23:13:18 <lars9> eee: professors are mostly Cale
23:13:41 <ddarius> Cale writes games for phones.
23:14:20 <kmc> i think real professors are too busy to hang out on IRC
23:14:35 <lars9> he can be a great instructor though
23:14:53 <eee> i am probably too busy too
23:14:57 <eee> 2:!5am
23:15:04 <eee> should be writing a paper
23:15:07 <Cale> Yeah, I might like to be a professor at some point, but I'm sort of meandering about in my life :)
23:15:26 <eee> let's start a company Cale
23:15:45 <eee> ah, but
23:15:52 <eee> then you wouldn't be meandering
23:15:56 <Cale> I'd need to go back to get a master's degree and then a Ph.D. if I wanted to become a prof.
23:15:58 <eee> you'd be overly committed
23:16:13 <eee> it's great to work for people
23:16:18 <shachaf> Cale writes games for phones?
23:16:20 <Cale> yep
23:16:22 <eee> cause you can reinvent yourself much faster
23:16:24 <Cale> I'm working for iPwn
23:16:33 <eee> oh
23:16:36 <Cale> (on BloodKnight)
23:16:55 <shachaf> Cale: Oh, really? Is that recent?
23:17:16 <Cale> Less than a year old :)
23:17:40 <Cale> I forget what was my first month, I'd have to look at the contract :)
23:17:48 <eee> hmmmm the closest i can come to a meetup.com group in my area is a clojure group
23:18:07 <Cale> Clojure's kinda cool.
23:18:24 <eee> java is in an odd state with oracle
23:18:32 <eee> and your user needs a jvm
23:18:42 <eee> i would love if clojure just ran
23:18:50 <eee> and no java interrip
23:18:54 <eee> interop
23:19:17 <eee> else you gotta learn java and clojure and deal with classpaths and stuff
23:19:37 <Cale> Yeah, though it's nice that there are options if you ever have to do anything on the JVM
23:20:45 <lars9> eee: if you know anyone of c++, java and c#, you can learn the rest two in 1 day.
23:21:16 <eee> well, not the idioms
23:21:26 <eee> that takes a lifetime to master
23:21:31 <lars9> eee: start with existing code i mean
23:21:32 <eee>  c++ and java/c#
23:21:40 <Cale> Also, if you want to know C++ well, it's *really* complicated.
23:22:07 <Cale> I doubt that anyone really *knows* C++, even not counting libraries.
23:22:32 <eee> java is more about libs, too
23:22:40 <lars9> Cale: usually companies only use tailored c++
23:22:49 <eee> "knowing java" is knowing how to handle xml
23:22:53 <eee> and jdbc
23:23:01 <eee> and all kinds of other banal things
23:23:30 <eee> C++ there's some neat things like knowing the algorithms stl stuff
23:23:39 <lars9> eee: it's even easy to read java byte code... quite neat.
23:23:58 <eee> someone did a cool trick with byte code
23:24:10 <eee> they compiled their c code to assembler
23:24:26 <eee> then ran it in an interpreter in jvm
23:24:38 <eee> because they could map from risk to byte code
23:25:01 <eee> http://nestedvm.ibex.org/
23:25:24 <eee> neat, right?
23:29:57 <eee> so, how's ghc doing performance wise these days?
23:30:13 <eee> i mean, fortran or c++ are still thought to be the fastest
23:30:31 <lars9> eee: google programming language shootout
23:30:56 <lars9> @google programming language shootout
23:30:57 <lambdabot> http://shootout.alioth.debian.org/
23:30:57 <lambdabot> Title: Computer Language Benchmarks Game
23:34:31 <Axman6> eee: very well
23:36:47 <eee> pretty impressive.  surprised java-server showing faster mostly
23:36:58 <eee> larger community i suppose
23:37:06 <eee> corporate backing
23:37:21 <Axman6> and the jvm is an very impressive piece of code
23:37:27 <eee> but probab;ly also silly comparison
23:37:43 <eee> like whose haskell
23:38:16 <lars9> that site shows java consumes more mem than ghc
23:38:19 <lars9> much more
23:38:33 <qfr> I have huge problems with Java memory consumption
23:38:53 <qfr> I have this app which performs some TLS IO and displays 200 KiB of text data in a JTable
23:38:59 <eee> depends on the comparison
23:39:02 <eee> machine type etc
23:39:05 <eee> java-server
23:39:18 <qfr> http://siyobik.info/misc/warehouse-client-juarez.png guess how much memory this uses on launch
23:39:35 <eee> http://shootout.alioth.debian.org/u64q/which-programming-languages-are-fastest.php
23:39:45 <qfr> 220 MiB :/ all non shared
23:39:52 <Axman6> o.O
23:39:58 <Axman6> i was going to guess 20MB
23:40:01 <qfr> Hahaha
23:40:04 <Axman6> which is still a lot
23:40:07 <eee> guess i'm gonna try to sleep
23:40:12 <eee> nice chatting
23:40:17 <Axman6> o/
23:40:21 <qfr> Night
23:40:27 <eee> i'll try to visit again. yer cool
23:43:01 <dibblego> does there exist an encoding of a cons list using a tagged union somewhere handy?
23:45:04 <Cale> dibblego: Doesn't the standard list type satisfy that?
23:45:28 <dibblego> Cale, yeah sorry I was thinking of a specific encoding -- I'm trying to read some javascript
23:46:36 <dibblego> I think it is: data List a = List (Bool, Maybe (a, List a))
23:54:57 <lars9> why doesn't list use double-linked list? a little bit more mem, but supports operation at each end equally effiicent
23:58:53 <dibblego> lars9, try writing one in haskell
