00:15:50 <ertes-w> helo
00:19:52 <[exa]> hi!
00:38:24 <fabien> hi , can someone help me how do i decrement a constant in a foldl ?
00:38:24 <fabien> <fabien> thank you
00:40:51 <Tehnix> fabien: we'll probably need a bit more to go one than that :confused: Do you have any code examples?
00:41:12 <fabien> yea yea sorry
00:41:14 <fabien> compute lst = do
00:41:14 <fabien>     let x = 11
00:41:14 <fabien>     foldl(\a b -> a * (x - 1)) 1 lst
00:41:31 <fabien> i want my x - 1 to decrement at every loop of foldl
00:41:37 <fabien> ..
00:51:29 <ertes-w> fabien: make it part of the fold state
00:52:03 <ertes-w> foldl (\(a, x) b -> (a * (x - 1), x - 1)) (1, 11)
00:52:16 <fabien> oh
00:52:17 <fabien> ok
00:52:56 <ertes-w> BTW, you're not using 'b'…  why are you folding a list?
00:53:35 <fabien> .. i don't knowww
00:53:54 <fabien> a and b are the head and the tail right ?
00:54:27 <ertes-w> no…  'a' is the current state and 'b' is the next list element…  your function should return the new, updated state
00:55:00 <fabien> a is the whole list now and b is the tl ?
00:55:09 <ertes-w> no
00:55:10 <fabien> or the head
00:55:39 <ertes-w> > foldl (\x dx -> x + dx) x0  -- x0 is the initial state, x is the current state, dx is the next list item, x + dx is the new state
00:55:41 <lambdabot>  error:
00:55:41 <lambdabot>      • Variable not in scope: x0
00:55:41 <lambdabot>      • Perhaps you meant ‘x’ (imported from Debug.SimpleReflect)
00:56:20 <ertes-w> foldl traverses a list while updating a state value and returns the final state at the end
00:57:11 <fabien> i want to go through a list and multiply each element by the x im decrementing by one each time
00:57:27 <fabien> and at the end return the result
00:57:49 <ventonegro> fabien: What if the length of the list is bigger than `x`?
00:58:21 <ertes-w> fabien: foldl does not map or zip, it *reduces* a list to a final state value
00:58:26 <ertes-w> > zipWith (*) [10,20,30,40] [11, 10 ..]
00:58:28 <lambdabot>  [110,200,270,320]
00:58:38 <ertes-w> perhaps this is more like what you want
00:59:19 <ertes-w> > zipWith replicate [1..] "hello"
00:59:21 <lambdabot>  ["h","ee","lll","llll","ooooo"]
00:59:33 <fabien> > map (\a b -> a + b) [110,200,270,320]
00:59:36 <lambdabot>  [<Integer -> Integer>,<Integer -> Integer>,<Integer -> Integer>,<Integer -> ...
00:59:47 <fabien> =(
01:00:01 <ertes-w> fabien: what's the type of 'map'?
01:00:05 <fabien> map (+) [110,200,270,320]
01:00:14 <fabien> > map (+) [110,200,270,320]
01:00:16 <lambdabot>  [<Integer -> Integer>,<Integer -> Integer>,<Integer -> Integer>,<Integer -> ...
01:00:18 <fabien> the type of map 
01:00:19 <ertes-w> fabien: stop guessing
01:00:23 <fabien> aha
01:00:31 <ertes-w> what's the type of 'map'?
01:00:41 <fabien> its a function that take a list and a function to apply for each element
01:00:42 <fabien> no ?
01:00:47 <fabien> so its like
01:00:52 <fabien> i dont know 
01:01:13 <ertes-w> it is, but you need more information…  you need the actual type
01:01:26 <ertes-w> do you know how to figure it out?
01:01:30 <fabien> mmh
01:01:36 <fabien> the actual type 
01:01:47 <fabien> like the type for starting ?
01:02:02 <ertes-w> no, 'map' is a value…  every value in haskell has a type
01:02:19 <fabien> ok ok  but i dont know
01:02:43 <ertes-w> so let me introduce you to the most important command you will need for the next six months of haskell =)
01:02:52 <ertes-w> fire up GHCi and type:   :t map
01:03:09 <fabien> map :: (a -> b) -> [a] -> [b]
01:03:16 <fabien> if i didnt give up = O
01:03:20 <fabien> dont
01:03:30 <ertes-w> yeah…  do you know how to read that?
01:03:35 <fabien> y
01:03:53 <fabien> take a func a to b and [a] (list) and return a [b]
01:04:01 <fabien> return [b]
01:04:05 <ertes-w> yeah, there you go
01:04:11 <ertes-w> now what's the type of (+)?
01:04:37 <fabien> take a and b and return 
01:04:40 <fabien> the sum
01:04:51 <ertes-w> that's semantics, not types
01:05:04 <ertes-w> you're explaining what (+) *does*
01:05:09 <fabien> oh 
01:05:23 <fabien> hmm
01:05:25 <ertes-w> use The Command
01:05:40 <fabien> (+) :: Num a => a -> a -> a
01:05:46 <ertes-w> there you go =)
01:06:14 <ertes-w> so variable names don't really matter, so we could say that:  (+) :: (Num z) => z -> z -> z
01:06:15 <ertes-w> right?
01:06:22 <fabien> of course
01:06:31 <fabien> why three a ? why not 2 
01:06:40 <fabien> oh
01:06:45 <fabien> Num a isnt an argument
01:06:48 <fabien> its just the type
01:06:59 <ertes-w> (Num z) is a constraint on the type 'z'
01:07:03 <fabien> ok
01:07:06 <ertes-w> but that's not really important right now
01:07:06 <fabien> i understand
01:07:14 <ertes-w> ok, you're using (+) as the argument to 'map'
01:07:16 <fabien> it forced the function to take only numbers
01:07:24 <ertes-w> map's argument type is:  a -> b
01:07:40 <ertes-w> (+) has:  z -> z -> z  -- (with a constraint that we'll ignore here)
01:07:49 <ertes-w> do these types match?
01:08:03 <fabien> no
01:08:16 <fabien> we need to take only one arg for (+) so
01:08:36 <ski> `z -> z -> z' means `z -> (z -> z)'
01:09:08 <fabien> yea partial function
01:09:12 <fabien> ok
01:09:47 <ertes-w> fabien: so you're using 'map' like this:
01:09:50 <ski> (not a partial function, no. perhaps you were thinking of "partial application" (which is something different") ?)
01:09:58 <ertes-w> map :: (z -> (z -> z)) -> [z] -> [z -> z]
01:10:24 <fabien> yea application
01:10:31 <ski> matching `a -> b' to `z -> (z -> z)', `a' becomes `z' and `b' becomes `z -> z'
01:10:34 <ertes-w> notice how you used a = z
01:10:36 <ertes-w> and b = z -> z
01:10:49 <ertes-w> because that's how (a -> b) matches (z -> (z -> z))
01:12:02 <fabien> ok
01:12:20 <ski> (two function types match with each other in case the corresponding argument types match, and also the corresponding result types match. the result type of the function type `z -> z -> z' is, strictly speaking, `z -> z', since `z -> (z -> z)' is the type of functions which take a `z' as argument and return a function from `z' to `z' as result)
01:12:53 <fabien> ouch
01:13:12 <fabien> ok so if defintions are the same its working
01:13:12 <fabien> aha
01:15:31 <ocramz> hullo!
01:16:33 <ertes-w> fabien: so what you did above is not strictly a type error…  but it's probably not what you intended either…  on the other hand i don't really know what you intended when you wrote (map (+) [110,200,270,320])
01:17:02 <fabien> i wanted to do that
01:17:12 <fabien> > foldl (+) [10,9..]
01:17:15 <ski> you wanted to get a list of functions ?
01:17:15 <lambdabot>  error:
01:17:15 <lambdabot>      • No instance for (Num [Integer]) arising from a use of ‘e_1109’
01:17:15 <lambdabot>      • In the expression: e_1109
01:17:19 <fabien> what
01:17:22 <fabien> oh
01:17:30 <fabien> > foldl (+) 0 [10,9..]
01:17:36 <lambdabot>  mueval-core: Time limit exceeded
01:17:37 <ski> > [10,9 ..]
01:17:39 <lambdabot>  [10,9,8,7,6,5,4,3,2,1,0,-1,-2,-3,-4,-5,-6,-7,-8,-9,-10,-11,-12,-13,-14,-15,-...
01:17:47 <fabien> > foldl 0 (+) [10,9,8]
01:17:49 <lambdabot>  error:
01:17:49 <lambdabot>      • Could not deduce (Num a0)
01:17:49 <lambdabot>        from the context: (Num ((a -> a -> a) -> a1 -> a -> a -> a),
01:17:50 <ski> (that's an infinite list. can't sum that)
01:17:53 <fabien> ok i stop
01:17:56 <fabien> ok ok
01:17:58 <ertes-w> fabien: continue
01:18:07 <ocramz> can anybody suggest a good queue library?
01:18:11 <fabien> > foldl 0 (+) [10..0]
01:18:13 <lambdabot>  error:
01:18:13 <lambdabot>      • Could not deduce (Enum a0)
01:18:13 <lambdabot>        from the context: (Enum a1,
01:18:17 <ski> wrong argument order
01:18:21 <fabien> > foldl (+) 0 [10..0]
01:18:23 <lambdabot>  0
01:18:29 <ertes-w> ocramz: Data.Sequence?
01:18:41 <ski> > [10 .. 0]  -- is the empty list
01:18:42 <fabien> > foldl (+) 0 [1..10]
01:18:42 <ertes-w> ocramz: Seq is a deque
01:18:43 <lambdabot>  []
01:18:45 <lambdabot>  55
01:18:47 <fabien> > foldl (+) 0 [1..10]
01:18:48 <ocramz> ertes-w in containers?
01:18:48 <fabien> ok
01:18:50 <lambdabot>  55
01:18:51 <ertes-w> ocramz: yeah
01:19:00 <ski> > [10,9 .. 0]
01:19:02 <lambdabot>  [10,9,8,7,6,5,4,3,2,1,0]
01:19:19 <fabien> so i can do that with map ? foldl (+) 0 [1..10]
01:19:26 <ski> no
01:19:27 <fabien> just to understand
01:19:32 <ski> `map' produces a new list
01:19:37 <ertes-w> fabien: why can't you do that with 'map'?
01:19:47 <fabien> ok because foldl keep an acc
01:19:59 <fabien> i can but i mean its not suitabl
01:20:00 <fabien> i can but i mean its not suitable
01:20:01 <ertes-w> fabien: reason using only types
01:20:16 <howdoi> trying to implement foldMap in JS, any suggestions?
01:20:29 <ski> of course, if you do some further processing with that list, then it could be that you could achieve summing of the initial list, when using `map' on that to get the intermediate list
01:20:37 <howdoi> foldl (+) 0 [1..10]; is a monoid?
01:21:06 <ski> howdoi : ill-typed question, sorry
01:21:43 <ski> `Integer', together with `0' and `(+)', forms a monoid
01:21:54 <ertes-w> howdoi: foldMap is a member of Foldable, so you can't "implement foldMap"
01:22:04 <ertes-w> you can "implement foldMap for []"
01:22:25 <howdoi> ski: semi-group is a one which would fail to return is the initial value is not provided to the accumulator right?
01:23:00 <ski> is there one too many "is" in that sentence ?
01:23:02 <howdoi> ertes-w: yes for a `[]`, that's like reduce-map?
01:23:05 * ski has trouble parsing it
01:23:17 <ertes-w> and in JS due to the lack of static typing you're probably doing yourself a favour by just abstracting over it: take foldMap as a function
01:23:27 <howdoi> *return if (sleepy me)
01:23:28 <ertes-w> s/as a function/as an argument/
01:24:04 <ertes-w> i think trying to mimick type classes in JS only leads to sadness
01:24:16 <howdoi> basically trying to understand semi-group vs monoid 
01:24:26 <ertes-w> holy crap, i suck at typing to day
01:24:33 <ertes-w> ^ case in point
01:24:35 <ski> with a semi-group, you can reduce a non-empty list, using an associative binary operation
01:24:36 <howdoi> ertes-w: true, but just an attempt 
01:24:50 <ski> if you have a monoid, then you can reduce any (finite) list
01:25:46 <howdoi> [].reduce((acc,x) => acc +x, 0) is a ?
01:25:52 <ski> one could say that this is the point of semi-groups, respectively monoids
01:26:49 <howdoi> [].reduce((acc,x) => acc +x, 0) -> monoid
01:26:49 <howdoi> [].reduce((acc,x) => acc +x) -> semi-group 
01:26:52 <jchia> When I have a very long type signature line like this "x1, x2, x3, ..., xn :: LongType", is it possible to break up the line into multiple lines? I'm getting syntax error from breaking.
01:26:58 <howdoi> Sorry for the JS syntax here
01:27:11 <ski> howdoi : no. that `reduce' operation brackets/groups the reduction in a certain way. with semi-groups, as with monoids, the point is that the bracketing should not matter
01:27:49 <howdoi> associative!
01:27:53 <ertes-w> howdoi: a semigroup is a function that is associative and closed (gotta mention this explicitly in JS land)
01:28:06 <ertes-w> howdoi: just a function…  any function that is associative and closed is a semigroup
01:28:16 <howdoi> "closed"?
01:28:20 <ertes-w> if that function also has an identity, then that function is a monoid (and a semigroup)
01:28:27 <ski> howdoi : you can *implement* monoid, as well as semi-group, reduction / "fold", using that `reduce' operation. but i wouldn't say that that operation (called in one of those two ways) are *about* monoids or semi-groups, per se
01:28:48 <ertes-w> "closed" means: given two arguments of type A, returns a value of type A
01:28:57 <howdoi> interesting insights! ski 
01:28:58 <ertes-w> you stay in a certain carrier set
01:29:23 <ertes-w> for example: (+) on Integer will never return a result that isn't an Integer
01:29:29 <howdoi> ertes-w: that's so uncommon is JS, unless we explicitly handle it to be closed! 
01:30:07 <ertes-w> howdoi: an equivalent formulation of "closed" is:  the function needs to be prepared to handle any of its own results as an argument
01:30:15 <ski> howdoi : *if* you have a monoid with neutral element `z' and binary operation `f', then `foldr f z xs' and `foldl f z xs' will be equal, for finite lists `xs' (though one may be more efficient than the other)
01:31:13 <ski> but `foldr' and `foldl' can be used even when the binary operation `f' isn't associative, and even when the element `z' isn't a (left, nor right) neutral element wrt `f'
01:31:23 <howdoi> So, a semigroup is a function that is associative and closed and a monoid is a semi-group with identity.
01:31:29 <ski> (and their behaviour in such cases are clearly defined, and useful)
01:31:33 <ertes-w> howdoi: correct
01:31:58 <howdoi> if a monoid is a semi-group with identity, what's a monad?
01:31:59 <ski> one can think of the neutral element / identity as a nullary operation
01:32:16 <ertes-w> howdoi: that's quite a jump =)
01:32:35 <ertes-w> howdoi: honestly i don't know how to really explain monads without types
01:32:50 <ski> given an associative binary operation, and a nullary operation that is the neutral element for the former, one can "extend" these to an `n'-ary operation, for any natural number `n' (from zero, upwards)
01:33:41 <ski> iow, one can extend those into an operation that can handle any finite (possibly empty) list. and we don't care about how we think of the elements in the list as being bracketed
01:34:05 <ertes-w> howdoi: much like a monoid a (haskell) monad is a function with certain properties
01:34:24 <ski> the difference with a semi-group is that it only extends to an operation taking a non-empty list (iow to an `n'-ary operation, for any *positive* integer `n')
01:34:44 <aplainzetakind> A semigroup is a set A together with a binary operation f on A (this means f takes two arguments from A and produces an element of A) with a left and right identity in A.
01:35:40 <ski> (this is related to that (possibly empty) lists being the "free monoid". and, correspondingly, non-empty lists are the "free semi-group". the binary operation on the list monoid is `(++)', and its neutral element is `[]', the empty list)
01:35:48 <howdoi> phew! Too much knowledge in this channel! 
01:35:58 <ertes-w> howdoi: if you want to learn about haskell monads, start with haskell functors…  however, i strongly suggest that you play around with semigroups and monoids first
01:36:21 <aplainzetakind> Jargon makes it sound more complicated than it is.
01:36:21 <ski> aplainzetakind : nope
01:36:22 <howdoi> functors are just mapables :D ?
01:36:38 <ertes-w> howdoi: with laws
01:36:39 <howdoi> any [] is a functor as it has `map` implemented in it
01:37:07 <ski> aplainzetakind : a semi-group doesn't need to have any identity element. `max' on the integers forms a semi-group, e.g.
01:37:07 <aplainzetakind> ski: Which part is nope?
01:37:16 <howdoi> aplainzetakind: true, so was looking into http://git.io/fp-jargons
01:37:20 <aplainzetakind> Sorry I meant monoid.
01:37:40 <ertes-w> howdoi: again to explain functors it's necessary to talk about types, and if you want to use functors in JS you need to be *very* careful to keep track of the types in your head
01:37:41 <ski> with that correction, what you said it ok
01:38:33 <ertes-w> :t map
01:38:34 <lambdabot> (a -> b) -> [a] -> [b]
01:38:37 <ertes-w> this is a functor
01:38:59 <aplainzetakind> No, [] is the functor.
01:39:06 <ertes-w> the first aspect is that it takes a function (a -> b) and "lifts" it to a function of type ([a] -> [b])
01:39:12 <aplainzetakind> Fuctor is the mapping between types.
01:39:20 <aplainzetakind> [] :: a -> [a]
01:39:38 <aplainzetakind> Takes type a and returns type [a]
01:39:45 <ski> the functor is really `[]', together with the mapping function on it, here `map'. but we often say `[]' is a functor, by virtue of coming equipped with a mapping function
01:39:54 <ertes-w> aplainzetakind: a functor is fully determined by its fmap function…  the fact that [] is a functor *in haskell* is really a "haskellism"
01:40:46 <ertes-w> howdoi: the second aspect is that it follows a pair of laws:  map f (map g xs) = map (\x -> f (g x)) xs  -- you need to translate this to JS in your mind
01:40:57 <aplainzetakind> Still, the mapping is what you call the functor provided that it preserves the arrows too.
01:40:59 <ertes-w> and:  map (\x -> x) xs = xs
01:41:44 <aplainzetakind> But yeah, the arrows part has to be expressed separately in haskell.
01:41:47 <aplainzetakind> Fair.
01:42:08 * howdoi shudders
01:42:35 <howdoi> so if I were to implement a foldMap on JS [], it would be like?
01:43:24 <ski> (similarly, `Integer', together with `0' and `(+)' (satisfying the monoid laws) is the monoid. but if one would speak of the monoid `Integer', without mentioning the operations, then usually one would mean these operations)
01:43:43 <ertes-w> loop over the elements while accumulating them using a certain monoid
01:44:08 <ski> s/certain/given/
01:44:11 <ertes-w> > foldMap (Sum . length) ["hello", "world!"]
01:44:14 <lambdabot>  Sum {getSum = 11}
01:44:46 <ertes-w> the way i would see this working in JS is:  foldMapList = (monoid, f, xs) => …
01:45:03 <howdoi> but what would `monoid` be in JS?
01:45:11 <ertes-w> a function together with its identity
01:45:34 <ski> (in Haskell, `Integer' itself is not an instance of the type class `Monoid'. to access the above monoid, one would use `Sum Integer', as ertes-w just showed)
01:46:35 <ertes-w> :t foldMap
01:46:36 <lambdabot> (Monoid m, Foldable t) => (a -> m) -> t a -> m
01:46:51 <howdoi> [1,2,3].map(x => x+1).reduce((acc,x) => acc + x,0); in foldMap -> foldMap({}, x => x + 1, [1,2,3]) sounds good?
01:47:01 <ertes-w> see how foldMap abstracts over a monoid…  the way we communicate that monoid to foldMap in haskell is really very haskell-specific (we use type classes)
01:47:02 <ski> it's a precondition of `foldMapList' that that binary function, together with its supposed neutral element, actually forms a monoid, iow actually satisfies the monoid laws
01:47:13 <ertes-w> in JS you need to take the monoid as an argument
01:47:53 <ski> what's `{}' there ?
01:48:12 <howdoi> a semi-group with identity function 
01:49:03 <howdoi> identity function differs so much for each data type, like True, 0, [], ''
01:49:23 <ertes-w> the identity is not a function, but just a value
01:49:33 <ertes-w> (not *necessarily* a function)
01:49:52 <ertes-w> and it is per *monoid*, not per type
01:50:16 <howdoi> yeah, identity values: True, 0, [], ''
01:50:41 <howdoi> I think JS wouldn't gain much with foldMapList 
01:50:54 <howdoi> maybe, a flatMap would be useful 
01:51:16 <howdoi> I like FP, but can't do haskell and stuck with JS
01:51:25 <howdoi> maybe I should switch to elm :D
01:51:33 <tdammers> stuck with JS why
01:51:35 <tdammers> job?
01:52:59 <ertes-w> howdoi: or compile haskell to JS ;)
01:53:12 <howdoi> tdammers: true that, for bread :/
01:53:17 <howdoi> ertes-w: heh heh
01:53:30 <RedNifre> Hi.
01:53:34 <tdammers> howdoi: nothing stopping you from studying Haskell or PureScript on the side though, and then applying your insights to JS
01:54:02 <ertes-w> howdoi: well, JS lives in a world where you can benefit from *some* FP patterns…  don't underestimate the power of abstracting over monoids
01:54:03 <RedNifre> Is there a term for side effects that are somewhat harmless because they leave the system, never to enter it again, so that for the program itself they don't exist and you won't have to reason about them?
01:54:06 <tdammers> howdoi: if you do that, you might enjoy ramda.js for your JS work; it borrows quite a few concepts from typed FP
01:54:14 <RedNifre> E.g. writing to a log file that is not read by the program.
01:54:29 <howdoi> tdammers: trying the same, sometimes it feels like a overkill or I fail to see the real benefit 
01:54:57 <howdoi> tdammers: I am using it in our current project :)
01:55:02 <tdammers> howdoi: the benefits are not very quantifyable and may take a long way to pay off
01:55:08 <RedNifre> Fun fact: I ported ramda.js to Java, complete with currying, the __ parameter placeholder etc. so if you want to write haskelly code in javascript and java you can now do it the same way: https://github.com/RedNifre/ravr
01:55:33 <howdoi> interesting 
01:56:13 <ertes-w> howdoi: the benefit is a high level of code reuse, much more domain-specific code (easier to read and refactor), easier reasoning about code, and better safety (because you're using algebraic constructions with well understood rules, there is little room for error)
01:56:55 <ertes-w> howdoi: for example if you use foldr in haskell, you can't accidentally skip an element or process it twice…  you can't accidentally switch semantics in the middle of the list either
01:57:13 <howdoi> ertes-w: I was looking into https://twitter.com/GNUmanth/status/923169797319180289
01:57:38 <howdoi> ertes-w: planning to re-write few stuff 
02:01:27 <howdoi> would you guys suggest to read the The Little Schemer? 
02:03:07 <ertes-w> honestly i would just learn haskell
02:05:31 <ertes-w> there is this weird trend of trying to cover everything *around* haskell without ever touching haskell itself
02:12:11 <ertes-w> howdoi: BTW, i just looked at the tweet and it rings my bullshit alarm
02:12:27 <ertes-w> it's either bullshit or satire
02:17:25 <raek> to me it seems like he means: favor pure functions and composition
02:20:18 <tdammers> ertes-w: if it's satire, it's not very good. and if it's bullshit, then also meh.
02:27:13 <ertes-w> raek: to me it seems like "i haven't ever done FP myself, but if *you* want to do it, i know everything about it there is to know!  just commit to this list of arbitrary restrictions, and you will be doing FP!"
02:28:01 <erjgfoja> Why:   foldMap (\_ -> All False) []        returns: All {getAll = True} ?
02:28:34 <erjgfoja> there is no element, so shouldn't it return False?
02:29:05 <ertes-w> erjgfoja: a property P should be true for all elements of a list
02:29:12 <ertes-w> erjgfoja: is P true for all elements of the list?
02:29:57 <erjgfoja> no
02:30:06 <ertes-w> erjgfoja: yes, iti s
02:30:09 <ertes-w> it is
02:30:12 <ahihi> which element is it not true for?
02:30:24 <drdo> the empty set strikes again
02:30:40 <ahihi> https://en.wikipedia.org/wiki/Vacuous_truth
02:30:47 <RedNifre> it's also false for all elements.
02:31:07 <ahihi> which is why the result will be True regardless of the predicate
02:31:08 <Guest80089> When you execute code inside repl(stack repl), does it by default execute code in parallel? Or do you have to give it the flag -threaded?
02:31:29 <ertes-w> erjgfoja: assuming that there aren't any blue elefants, are all blue elefants green?
02:31:41 <ertes-w> erjgfoja: if you say no, which one isn't green?
02:32:26 <erjgfoja> don't no
02:32:47 <tdammers> reasoning about this in terms of everyday logic isn't very productive
02:33:03 <tdammers> instead, try to formulate a formal definition of the thing you're asking for
02:33:11 <ertes-w> erjgfoja: the more technical reason is that True is the identity of the All monoid
02:33:21 <drdo> tdammers: No, let's use unhelpful analogies
02:33:39 <tdammers> drdo: they are helpful alright, just not to figure out edge cases
02:33:42 <ertes-w> tdammers: i disagree with the former, but agree with the latter
02:34:46 <tdammers> ertes-w: analogies and metaphors are useful for exploring possible intuitions; as long as you keep in mind that they are just that, metaphors and analogies, and keep an open and critical mind, I don't really see a problem
02:34:51 <drdo> RedNifre: Just sit down and think about what it means for some property to hold for all elements of a set
02:35:02 <ertes-w> in logic real-world examples can be very helpful
02:35:21 <tdammers> so back to the problem at hand
02:36:05 <tdammers> it's pretty obvious what All should do for non-empty lists: if all the elements are True, then it should evaluate to True; but if there are one or more elements that are False, then it should evaluate to False
02:36:21 <tdammers> now you can argue along two paths, and both lead to the same result
02:36:44 <tdammers> one path is that adding additional True elements to the list should clearly not change the result
02:37:02 <tdammers> if all elements are True, and you add another True element, all elements are still True
02:37:20 <erjgfoja> ok, so [] is True by default?
02:37:30 <tdammers> so it makes sense to make it such that the reverse also holds: removing a True element from an all-true list should yield an all-true list
02:37:37 <ertes-w> > mempty :: All
02:37:39 <lambdabot>  All {getAll = True}
02:37:48 <erjgfoja> Right..
02:37:55 <ertes-w> foldMap f [] = mempty
02:37:58 <tdammers> this is the monoid argument - True is the empty element for the All monoid
02:38:13 <erjgfoja> Got it. Many thanks all.
02:38:19 <tdammers> the other argument is simply that if one or more elements are False, All should also evaluate to False
02:38:36 <tdammers> but the empty list contains zero False elements, so it should not evaluate to False
02:38:40 <ski> more generally, `all p (xs ++ ys) = all p xs && all p ys'
02:38:53 * ski . o O ( "Empty Sum, Product, Forall, Exists" by monochrom at <http://www.vex.net/~trebla/homework/empty.html> )
02:39:10 <ski> erjgfoja : perhaps that ^ can help a bit with stuff like this
02:39:29 <erjgfoja> Great. Thanks too.
02:39:50 <tdammers> speaking in terms of metaphors, having all [] == False would be like saying "I have no pets, and some of them are not dogs"
02:40:38 <mniip> huh
02:40:47 <erjgfoja> true :)
02:40:59 <mniip> monochrom, why do you type \forall v \cdot?
02:41:12 <mniip> isn't '\forall v.' the conventional notation
02:42:15 <ertes-w> mniip: or ',' or ':' or …
02:42:29 <mniip> well yes
02:42:32 <mniip> but not \cdot
02:43:52 <ertes-w> i like "(x : A) →" =)
02:49:48 <__Lorn__> 4Hello
02:50:19 <APic> llol
02:56:13 <AWizzArd> ertes-w: ping
03:00:51 <fendor> is there a stack or haskell oh-my-zsh theme?
03:03:46 <ertes-w> AWizzArd: pong
03:12:51 <osa1> anyone know what SockAddrCan is here http://hackage.haskell.org/package/network-2.6.3.2/docs/Network-Socket.html#t:SockAddr ?
03:13:46 <mniip> osa1, PF_CAN
03:14:01 <osa1> mniip: which man page?
03:15:12 <osa1> I can't see it in man 7 socket or man socket
03:15:38 <osa1> nvm nothing to worry about I guess
03:16:50 <mniip> well it's an obscure socket interface
03:17:54 <mniip> my linux-manpages don't seem to even mention it
03:19:12 <kahlil29> is there an IRC channel for opaleye ? 
03:28:23 <quchen> Yes! #haskell.
03:35:38 <haskell873> hello all
03:37:07 <haskell873> I am using Mutable vector and I am facing difficulties with its Binary instance particularly with Data.Vector.Unboxed.Mutable MVector Realworld Int64
03:37:37 <haskell873> i need to serialize this .... is that possible ... please help its urgent for me
03:38:26 <haskell873> ertes-w: are you there ...Q
03:39:03 <haskell873> I am using Mutable vector and I am facing difficulties with its Binary instance particularly with Data.Vector.Unboxed.Mutable MVector Realworld Int64
03:39:06 <haskell873> i need to serialize this .... is that possible ... please help its urgent for me
03:40:33 <ertes-w> it's always urgent for you, isn't it =)
03:43:07 <haskell873> ertes-w: Yes actually the project I am working right now is very important for my life :)
03:44:34 <haskell873> ertes-w: I am using distributed-process ... in my project and I have to pass that state a mutable unboxed vector Realworld Int64 to some other node
03:44:52 <RedNifre> haskell873 is it the software running your life prolonging machinery?
03:44:55 <ertes-w> in e-mails i have a habit of dealing with the "urgent" ones last, except in the rare case where i know that the sender actually uses the word "urgent" meaningfully =)
03:45:14 <RedNifre> "If I can't solve this problem quickly my heart will stop when unix time rolls over" ;)
03:45:47 <haskell873> ertes-w: for that It should be serializable i.e Binary and Typeable
03:47:00 <haskell873> RedNifre: Its more than that for me .... as If I can perform well in this Project then I can get a Haskell Job .... which is more than anything for me
03:47:55 <ertes-w> haskell873: in that case i'll do you a favour and allow you to figure this out on your own, because that's exactly what you're gonna have to do in your job
03:48:48 <ertes-w> learning to learn is perhaps the most valuable skill in practice
03:48:52 <haskell873> ertes-w: haha .... I have less time I can figure it out but I asked here to save my time
03:49:43 <haskell873> ertes-w: I am learning only ..... please give me relative ans if you have
03:50:14 <ertes-w> haskell873: here is a relative answer: a mutable vector can't be serialised using a pure function…  'binary' serialises using a pure function
03:51:30 <haskell873> ertes-w: so I can'thave a binary instance of a Mutable vector ... Q
03:51:44 <mniip> no
03:51:54 <JuanDaugherty> RedNifre, you know about #haskell-distributed? There are 12 years before unix time turns over.
03:52:36 <haskell873> okay thanks for the support community :)
03:52:57 <mniip> you'll have to serialize it into a pure structure first
03:53:10 <mniip> vector probably has a function for this
03:53:13 <mniip> a freeze of some sort
03:53:49 <mniip> Data.Vector.freeze
03:54:00 <haskell873> mniip: yes there is one ... and also vector-binary-instances
03:54:26 <haskell873> mniip: which have several instances of binary for vectors
03:55:48 <mniip> sounds like you found your answer then
03:56:24 <haskell873> mniip: I was using that but I need for mutable vectors
03:56:40 <mniip> you'll need to freeze your vector first
03:56:57 <mniip> make an immutable copy of it
03:57:01 <mniip> that can be passed to pure functions
03:57:13 <haskell873> If I will freeze then can I mutate it again
03:57:20 <bigos> Hello, does anyone do practical Haskell development on Windows?
03:57:37 <mniip> haskell873, you can continue to mutate the original MVector
03:57:46 <mniip> but the Vector you got back obviously wouldn't be updated
03:57:59 <haskell873> yes Thats the problem
03:58:05 <mniip> is it a problem
03:59:00 <haskell873> hmm.... actually I want to start two process with same mutable vector ... such that one will read it and other will update it
04:00:12 <haskell873> I can pass to process only serializable objects .... one process will receive message and update it while other will just read data and send message 
04:00:48 <haskell873> I am using cloud-haskell for distributed process
04:02:45 <tdammers> haskell873: then you will have to thaw / freeze / copy as needed between destructive processing and serialization / deserialization
04:04:17 <tdammers> haskell873: also, if they're actually separate OS processes, then you can't really share mutable vectors between them; you have to send messages between the processes to keep their copies of the data synchronized. That's kind of the essence of process isolation, really.
04:04:54 <bigos> does anyone have idea how to get development files for MSYS2 distributed for Haskell platform on windows?
04:05:37 <haskell873> yes I got it
04:06:03 <haskell873> Thanks to all
04:06:06 <haskell873> byee
04:15:57 <mniip> ooh I just recalled a thing
04:36:20 <__Lorn__> 7Hi everyone :)
04:36:32 <mniip> no
05:06:02 <SrPx> Suppose we take the STLC and add a restricted form of Fix where fixed variables are only admissible on non-applying positions (i.e., as functions arguments). Would such language be non-terminating and cause all types to be inhabited? 
05:06:30 <SrPx> Is there any obvious reason to assume so, or instead a trivial counter example?
05:07:38 <mniip> SrPx, so you mean like in 'f x' f is in applying position and x is not?
05:07:40 <ertes-w> SrPx: you mean like (fix id) would be allowed?
05:08:12 <SrPx> ertes-w: (fix id)? More like `fix u . λ x . x` if that's what you mean
05:08:13 <ertes-w> ah, you mean they can *only* be function arguments
05:08:14 <SrPx> mniip: yep
05:08:21 <mniip> SrPx, f x = id f x
05:08:30 <ertes-w> so like this:  fix (\x -> id x)
05:08:32 <SrPx> mniip: fair enough
05:08:54 <SrPx> Is there any known "weak" way to add fix that allows things such as infinite lists and scott encodings but not anything harmful?
05:09:20 <SrPx> Infinite lists and scott encodings are much simpler than recursive functions, that's why I wonder that'd be the case
05:09:44 <ertes-w> SrPx: i think i see what you're getting at (enforce wrapping), but it doesn't work without somehow encoding it on the type level
05:10:15 <SrPx> but how could I encode it on the type level? what I'm trying to do specifically is
05:11:01 <SrPx> take the calculus of constructions, remove exponential functions (only allow linear functions), add restricted duplications (like those of linear logic) and then add a weaker form of fixed points which would allow one to express inductive types
05:13:03 <ski> `exists s. s * (s -> f s)' expresses `nu s. f s', for `f' a functor
05:14:20 <ski> (similar to how `forall r. (f r -> r) -> r' expresses `mu r. f r', `f' a functor)
05:30:37 <Itkovian> what is the preferred regex package these days?
05:31:40 <SrPx> ski: was that addressed to me?
05:34:59 <ski> yes
05:35:23 <ski> (not specifically, but towards the discussion you and others were having)
05:49:56 <infinisil> I'm using ghc-mod and company-ghc with emacs, and I'm getting "EXCEPTION: browse: module ‘Data.Aeson.Types’ is a package module", any idea what that's about?
05:50:08 <infinisil> It occurs with all external modules
06:05:35 <howdoi> ertes: lol, just read the bullshit alram :D 
06:26:35 <fakenullie> am I right that function that can go into infinite recursion is partial, even if doesn't raise exceptions?
06:28:10 <kadoban> fakenullie: I'd think so, as long as it's not productive infinite recursion. (like giving an infinite list to 'filter')
06:28:13 <mnoonan> fakenullie: I wouldn't call it partial if the recursion is productive (e.g. map (+1) [1..])
06:29:35 <fakenullie> Well, not accounting for laziness
06:30:02 <mnoonan> kadoban: actually, filter is an interesting case.. do we want to call "filter even" non-partial and "filter (const False)" partial?
06:31:27 <kadoban> mnoonan: I'm not sure. I guess technically you'd want to, but it's not like its filter's fault if you do that ... so it's to me a little bit like saying, I dunno,  (+) is partial because if you pass it two ⊥ it'll be ⊥. It's not quite the same, but feels like it.
06:31:48 <mnoonan> i guess even "filter even" should be partial (e.g. if you hand it [1,3,5,..]). but maybe "filter (const True)" vs "filter (const False)"
06:34:04 <kadoban> I don't think I'd actually call filter partial just because of that honestly, though I'm not sure if that's correct or helpful.
06:35:44 <mnoonan> ah, this exact point is discussed on one of adam chlipala's pages: http://adam.chlipala.net/cpdt/html/Coinductive.html
06:36:16 <kadoban> I'll have to check that out
06:36:39 <mnoonan> "The rule we have run afoul of here... every co-recursive call must be a direct argument to a constructor of the co-inductive type we are generating" (the "guardedness condition")
06:37:00 <mnoonan> "Since the predicate passed to 'filter' may reject every element of the stream, we cannot satisfy the guardedness condition."
06:37:02 <ski> hm, but `const False' isn't `const ⊥'
07:17:50 <infandum> What is the best function to use for concurrently executing a list of IO actions such that with +RTS -NX, there are X threads being used on that list?
07:21:03 <ertes-w> infandum: i wouldn't do that
07:21:19 <ertes-w> infandum: unless each action actually runs at 100% CPU load most of the time
07:21:32 <tdammers> infandum: wouldn't just kicking off M green threads (where M is the length of the list) simultaneously achieve that already?
07:21:50 <infandum> ertes-w: Then what do you do?
07:22:28 <infandum> tdammers: No, because if I have 10000 elements in the list, I wouldn't want 10000 threads.
07:22:34 <ertes-w> infandum: write a CLI, where the number of concurrent actions is specified through an extra parameter
07:22:47 <tdammers> ^ this, pretty much
07:23:00 <infandum> ertes-w: Then the user would need to do "program -j6 +RTS -N6"?
07:23:07 <JuanDaugherty> are these supposed to be ghc parms? what is -NX? 
07:23:08 <ertes-w> infandum: why not?
07:23:15 <merijn> infandum: You can programmatically set the RTS flags
07:23:23 <JuanDaugherty> i only see a related compiler control
07:23:30 <merijn> eh, well not flags I guess, but parallelism
07:23:33 <tdammers> JuanDaugherty: I think that is supposed to be program +RTS -j6 -RTS -N6
07:23:54 <merijn> tdammers: That looks wrong?
07:23:57 <infandum> tdammers: I thought threads were controlled with -N
07:24:05 <ertes-w> infandum: the point is that something like "-j10000 +RTS -N6" is entirely reasonable
07:24:13 <tdammers> oh right, nm
07:24:18 <tdammers> brainfart
07:24:22 <infandum> ertes-w: What does that mean though?
07:24:29 <ertes-w> infandum: don't confuse OS threads with haskell threads
07:24:31 <infandum> 10000 threads using...6 threads?
07:24:32 <merijn> infandum: In GHC there's two notions of threads
07:24:41 <infandum> ertes-w: That's exactly what I'm confusing haha
07:24:46 <merijn> infandum: It means 10000 *forkIO* lightweight threads
07:25:00 <merijn> infandum: And 6 capabilities (i.e. OS level threads running haskell code)
07:25:23 <merijn> infandum: You can use GHC's lightweight threads even with the unthreaded runtime, they just won't be running in parallel
07:25:34 <ertes-w> infandum: a haskell thread is an abstraction that is compiled into whatever selection mechanism your OS supports
07:25:43 <merijn> infandum: This is useful for blocking IO code, since the runtime will block/unblock threads and schedule when IO is done
07:25:46 <infandum> What does that mean exactly, the threads wouldn't be running in parallel?
07:25:47 <tdammers> "green threads"
07:25:51 <ertes-w> infandum: under linux it would compile to something like epoll
07:26:05 <merijn> infandum: It means *1* forkIO thread will run at a time, despite having 10k of them
07:26:34 <ertes-w> infandum: in other words a server written in haskell uses one or more haskell threads for each client, and the RTS groups the threads into epoll groups and distributes them over a certain number of OS threads (given by -N)
07:27:00 <merijn> infandum: TO avoid confusion it's best to refer to capabilities. Capabilities are the things that execute haskell code in the GHC runtime. And each capability has a dedicated OS thread to work it's computing on
07:27:13 <infandum> But which is the "real" parallelism? Which threads are running at the SAME time?
07:27:27 <merijn> infandum: You can have any number of threads (as in forkIO) running on any number of capabilities
07:27:40 <tdammers> capabilities are truly parallel, at least to the degree the OS and hardware can actually run them in parallel
07:27:42 <merijn> infandum: If you want real parallelism you need BOTH threads AND multiple capabilities
07:27:58 <tdammers> threads are dispatched onto available capabilities
07:28:18 <merijn> infandum: Each capability only runs a single thread at any time. So if you have more threads than capabilities, each capability will be running a thread and all remaining threads are blocked/waiting
07:28:26 <infandum> So with a list of 100 IO actions, I can put them into 10 thread chunks, each run by one capability (for 10 cores)?
07:28:32 <merijn> infandum: If you have less threads than capabilities, some capabilities will be idle
07:28:39 * JuanDaugherty smells a rat
07:28:45 <merijn> infandum: You can also just make 100 threads and run on 10 capabilities
07:28:57 <tdammers> infandum: you can just spawn 100 Haskell threads, and run the thing with 10 capabilities, and the runtime will take care of it
07:29:00 <merijn> infandum: That has the same effect, but you don't have to group the IO actions
07:29:12 <ertes-w> infandum: this sounds like an XY question
07:29:13 <JuanDaugherty> don't see no damn -N in 8.0 or 8.2.1
07:29:38 <merijn> JuanDaugherty: -N isn't a GHC flag, it's a runtime flag
07:29:47 <ertes-w> @where parconc
07:29:47 <lambdabot> http://chimera.labs.oreilly.com/books/1230000000929
07:30:04 <infandum> tdammers: So I can completely ignore chunking and make 1000000 threads and assign 10 cores to use them?
07:30:11 <merijn> infandum: Yes
07:30:39 <merijn> infandum: And then the runtime will run those threads 10 at a time
07:30:44 <infandum> So mapConcurrently f [1,2,3,...100000000] with +RTS -N5 will just magically work?
07:30:53 <tdammers> in fact, leaving it to the runtime is probably more efficient usually, because not all threads will have the same amount of work to do
07:30:54 <merijn> infandum: 
07:30:57 <merijn> infandum: Yes
07:31:00 <infandum> huh
07:31:13 <infandum> Then how does one control for course / fine grain parallelism?
07:31:18 <tdammers> oh, but, note that "runs them 10 at a time" doesn't mean they're not interleaved
07:31:18 <infandum> coarse
07:31:36 <merijn> infandum: In what sense? If you wanna limit concurrency you mean?
07:31:40 <ertes-w> infandum: more or less…  haskell threads do use some memory
07:31:54 <merijn> JuanDaugherty: You might wanna look at the GHC rtsopts section instead
07:32:16 <ertes-w> infandum: also even with 10 capabilities you might actually see thousands of them running almost simultaneously
07:32:24 <JuanDaugherty> i c, I wouldn't interpret that as a programmatic user control as such
07:32:24 <infandum> Okay, I think I understand, so I don't need to use chunking at all, haskell does it for me
07:32:25 <merijn> infandum: Haskell threads are more lightweight than goroutine/erlang processes, so a server should easily be able to handle more than 100k of them
07:32:34 <ertes-w> it depends a lot on what kind of I/O you're doing
07:32:44 <tdammers> e.g. if you do { forkIO $ forever $ do { putStr "a"; threadDelay 1000000 }; forkIO $ forever $ do { putStr "b"; threadDelay 1100000 } }, and run it on a single capability, you'll still see a's and b's being printed in an interleaved fashion
07:32:52 <ertes-w> whenever a thread has to wait for something to happen (like data arriving to a Handle), it goes to sleep
07:33:10 <ertes-w> also there is an actual scheduler in the RTS that preempts threads
07:33:18 <infandum> that would explain why when my f was threadDelay, it was still simultaneous
07:33:25 <infandum> with -N1
07:33:32 <merijn> infandum: Yes
07:33:37 <infandum> that confused me, I was expected 0 parallelism with -N1
07:33:38 <JuanDaugherty> but rather as a general operational control on the ghc runtime
07:33:42 <infandum> weird
07:33:58 <merijn> infandum: Basically the RTS will block threads on IO and run other stuff until it's ready
07:34:07 <tdammers> it's best to think of Haskell threads as running concurrently; inhowfar they are actually running in parallel is really more like an implementation detail, and you shouldn't normally worry about it much
07:34:21 <infandum> So instead of being careful with -NX (the number of X for grain), I should be careful where I put mapConcurrently
07:34:35 <merijn> infandum: So if you simply write "dumb" blocking IO code, the threads + runtime will automagically turn it into an event loop implementation
07:34:36 <ertes-w> threadDelay sends a thread sleeping, and the RTS immediately hands control to another thread
07:34:46 <merijn> infandum: Yes, -N bigger than the number of cores would be pointless
07:34:49 <infandum> So what's preventing me using parMap and mapConcurrently EVERYWHERE?
07:35:00 <tdammers> thread switching overhead
07:35:01 <ertes-w> infandum: even if you had no blocking operation at all, you would still observe concurrency
07:35:06 <merijn> infandum: Also, if you wanna rate limit mapConcurrently, have a look at this: https://stackoverflow.com/questions/18896103/can-haskells-control-concurrent-async-mapconcurrently-have-a-limit
07:35:47 <infandum> merijn: I saw that, but I wanted to know if there was a better function that what they propose. However, it seems like I might not need that after all
07:35:51 <merijn> Yeah, thread switching overhead/memory overhead means you have a little bit careful. But often it should "just" work
07:36:04 <tdammers> that, and the fact that in most practical applications, you benefit more from parallelism at a larger level
07:36:25 <tdammers> e.g., in a web server, rendering an individual HTML page in parallel won't be as efficient as just serving multiple requests in parallel
07:36:37 <ertes-w> infandum: threads are cheap, but not free…  you need to benchmark each case to see how fine you should parallelise
07:36:37 <ertes-w> or whether you should at all
07:36:39 <infandum> ok, I'm starting to understand. I truly thought -N controlled ALL concurrency, but I was wrong
07:37:10 <infandum> one last question: is the only difference between mapConcurrently and parMap the IO monad?
07:37:33 <infandum> Or is there a case where I should use mapConcurrently even with pure computations?
07:37:38 <ertes-w> that's also why GHC doesn't just auto-parallelise
07:38:15 <ertes-w> infandum: mapConcurrently is an abstraction, parMap is about speed
07:38:52 <ertes-w> example: you have a list of hostnames and ports, then mapConcurrently makes it easy to just contact all of them concurrently
07:39:37 <ertes-w> in particular: mapConcurrently changes semantics, while parMap behaves like a faster 'map' (ideally)
07:39:57 <merijn> infandum: Yeah, the rate limiting in there would mostly be useful for things like: avoid running out of file descriptor/sockets, avoid trampling a remote server, etc.
07:39:57 <merijn> infandum: BEcause 100k threads each opening their own socket will work fine, but you will run out of file descriptors, so that's when you might wanna rate-limit like the SO question
07:41:10 <infandum> merijn: I see. What do you mean by trampling a remote server though, that's only if the IO actions use sockets and open files etc., they shouldn't use up all the cores if -N2 is provided, right?
07:43:13 <infandum> Also, when you say using parallelism higher up, do you mean that a list of 10000000000 numbers and parMap (+ 1) or whatever wouldn't be faster than fmap (+ 1) because of the overhead?
07:43:46 <merijn> infandum: Well, I was scraping a webcomic at some point and doing the scraping with a 1000 threads in parallel it trampled the server >.>
07:44:02 <merijn> infandum: So I had to limit it to about 5 connections at a time to avoid causing it to 404 :p
07:44:38 <tdammers> 404, how lame
07:44:57 <infandum> merijn: OK, phew, so that's fine in my case
07:44:58 <tdammers> should have used 429
07:45:07 <tdammers> or 420, for giggles
07:45:28 <howdoi> :t flatMap
07:45:28 <merijn> tdammers: Not sure whether it actually 404, I forgot what happened. It just buckled and errored, etc.
07:45:30 <lambdabot> error: Variable not in scope: flatMap
07:45:31 <ertes-w> infandum: parMap would almost certainly be terrible for your example
07:45:42 <ertes-w> but the reason is not parallelism, but the way parMap itself works
07:45:52 <infandum> ertes-w: What would be good for that example then?
07:45:54 <howdoi> isn't that :t ?
07:46:08 <howdoi> |:t flatMap
07:46:12 <howdoi> :/
07:46:12 <merijn> howdoi: What is flatMap?
07:46:13 <infandum> > :t flatMap
07:46:15 <lambdabot>  <hint>:1:1: error: parse error on input ‘:’
07:46:15 <ertes-w> infandum: parMap works best for expensive functions on smaller lists
07:46:38 <ertes-w> infandum: the optimal length varies with the cost of the function
07:46:47 <ertes-w> infandum: something like parBuffer is much more versatile
07:46:49 <infandum> ertes-w: Okay, that's what I originally thought.
07:46:55 <ertes-w> and also works on long/infinite lists
07:47:32 <infandum> ertes-w: Never heard of it, I'll look it up. I always have lots of small operations on giant lists and I always feel like it should be parallelized.
07:47:32 <howdoi> > :t foldMap
07:47:34 <lambdabot>  <hint>:1:1: error: parse error on input ‘:’
07:48:13 <ertes-w> infandum: parBuffer still does fine parallelism, so it will allocate a spark for each individual item
07:48:14 <merijn> :t foldMap
07:48:16 <lambdabot> (Monoid m, Foldable t) => (a -> m) -> t a -> m
07:48:22 <ertes-w> infandum: if the operation is very cheap there are a bunch of options
07:48:55 <ertes-w> infandum: one is to split the list into sublists of a certain length and then use parBuffer to handle whole sublists
07:49:16 <ertes-w> infandum: another option is to use something like repa
07:49:31 <infandum> ertes-w: repa is for vectors though, right?
07:49:34 <ertes-w> yes
07:49:59 <ertes-w> if you have small operations on large sequences, you should probably use vectors anyway
07:50:07 <ertes-w> probably – it depends
07:51:41 <infandum> I very rarely use vectors: I only use them when I needs lots of random access
07:51:52 <infandum> for joining a lot I use sequences, for everything else I use lists
07:53:42 <ertes-w> infandum: it's really difficult to give a general answer here
07:54:36 <infandum> ertes-w: Unless it's with <>, because I find CLEAR differences with many mconcats with these traversables 
07:54:58 <infandum> Like, orders of magnitude, like five minutes versus three hours
07:58:50 <infandum> I think I understand parallelism and concurrency a little better now, thank you everyone!
07:59:38 <merijn> infandum: Also, Simon Marlow's book Parallel & Concurrent Haskell is excellent and available for free online
08:01:10 <cement> though it can take a few reads to really "get it"
08:04:25 <ski> @where PCPH
08:04:25 <lambdabot> "Parallel and Concurrent Programming in Haskell" by Simon Marlow in 2013 at <http://community.haskell.org/~simonmar/pcph/>,<http://chimera.labs.oreilly.com/books/1230000000929/>
08:05:51 <mud> That book is so good
08:08:52 * JuanDaugherty has strong sense of deja vu about same thing on that parm in this channel within last 5 y
08:20:26 <dminuoso> In the monoidal category of endofunctors (that gives rise to monads), what is the binary operation of its bifunctor? What is that "TxT" in TxT -> T?
08:20:52 <dminuoso> Based on various things it appears to be nesting a monad inside a monad m (m a)
08:21:01 <dminuoso> (Since the bifunctor would appear to be `join`)
08:21:35 <mnoonan> it should be a natural transformation from the functor T o T to the functor T.
08:21:48 <mnoonan> so a polymorphic function of type "forall a. m (m a) -> m a"
08:21:57 <dminuoso> mnoonan, right. What exactly is that o in the context of haskell?
08:22:15 <dminuoso> mnoonan, m (m a) does not look like T o T visually
08:22:19 <mnoonan> composition of functors
08:22:54 <dminuoso> mnoonan, oh... so it would mean fmap . fmap for morphisms, and nesting the type constructors like [[a]] ?
08:23:03 <mnoonan> m :: * -> *, so you can compose it with itself to get another * -> *
08:23:27 <dminuoso> Interesting, I never realized that m (m a) was just two composed functors. :o
08:23:45 <dminuoso> mnoonan, alrighty great! 
08:24:14 <dminuoso> mnoonan, oh and that composition forms a monoid indeed.
08:25:03 * mnoonan nods
08:25:41 <dminuoso> This is so strangely satisfying.
08:26:01 <mnoonan> one of us.. one of us.. one of us..
08:29:45 <ski> dminuoso : we have `eta : Id >---> T', `mu : T . T >---> T'. `eta' is `return', and `mu' is `join'
08:31:15 <ski> the composition
08:31:17 <ski>   T  ~~~  Id . T  >-{ eta . T }->  T . T  >-{ mu }->  T
08:31:51 <ski> should be identity. in Haskell terms, this says `join . return = id'
08:31:55 <ski> the composition
08:32:09 <ski>   T  ~~~  T . Id  >-{ T . eta }->  T . T  >-{ mu }->  T
08:32:15 <ski> should be identity. in Haskell terms, this says `join . fmap return = id'
08:32:26 <ski> and the composition
08:32:56 <ski>   T . (T . T)   ~~~  (T . T) . T  >-{ mu . T }->  T . T  >-{ mu }->  T
08:33:04 <ski> should be equal to the composition
08:33:14 <ski>   T . (T . T)   >-{ T . mu }->  T . T  >-{ mu }->  T
08:33:36 <ski> in Haskell terms, this says `join . join = join . fmap join'
08:34:35 <ski> these are the three "monoid laws" for a monoid object (here `T') in a monoidal category (the monoidal category of endofunctors, with composition and identity of them as the monoidal structure)
08:37:17 <ski> (some clarifying signatures : `eta . T : Id . T >---> T . T', `T . eta : T . Id >---> T . T', `mu . T : (T . T) . T >---> T . T', `T . mu : T . (T . T) >--> T . T')
08:38:55 <ski> (in Haskell terms, those become `return :: forall a. T a -> T (T a)', `fmap return :: forall a. T a -> T (T a)', `join :: forall a. T (T (T a)) -> T (T a)', `fmap join :: forall a. T (T (T a)) -> T (T a)')
08:40:01 <benpr> Hey, Im having trouble with creating a functional parser using readP, ie turn "3 * (4 + 2)" into Mult (Number 3) (Plus Number 4 Number 2) I am using chainr1 to chain together orders of precedence yet I dont know how to handle the unary operator of negation
08:41:06 <benpr> as chainr only takes either ReadP (a) or ReadP (a -> a -> a) yet Negation is a unary operator
08:41:19 <ski> conceptually, `return' adds a "trivial" `T' layer, and `join' "smaches/collapses" together two `T' layers into one. the first law says that intoducing an *outer* trivial `T' level ("over" an inner one, that may not be trivial), and then combining them together again, amounts to doing nothing
08:42:22 <ski> the second laws says that introducing an *inner* trivial `T' layer ("wrapping the results/elements"), inside an existing outer `T' layer, and then collapsing them together again, amounts to doing nothing
08:43:42 <ski> the third laws says that if you have three `T' layers, it doesn't matter if you first smash the outer two together, and then smash that with the innermost. or else smash together the two inner, and then smash that together with the outermost. both these processes will end up with the same result. the same way of going from `T (T (T a))' to `T a'
08:44:55 <ski> the former two laws are the neutral element laws, corresponding to introducing trivial levels, and then combining them back again with an existing one. while the last law is an associativity law of the level nestings, claiming that only the nesting order (may) matter, not the "grouping of layers" (however you'd express that ..)
08:45:40 <mnoonan> benpr: can you build it into whatever you're using to parse numbers?
08:45:42 <ski> (instead of `T (T a)' you can define a `Compose' so that you say `Compose T T a'. then the two "groupings of nested layers" would be `Compose (Compose T T) T a' and `Compose T (Compose T T) a')
08:46:41 <paolino_> phadej: thank you for treediff lib :-)
08:47:13 <ski> benpr : decide the relative precedence of unary negation, wrt the infix operators
08:48:05 <ski> benpr : iow, should `- 2 + 3' be parsed as `(- 2) + 3' or `- (2 + 3)'. should `- 2 * 3' be parsed as `(- 2) * 3' or `- (2 * 3)', &c.
08:48:43 <ski> hm, they left
08:49:33 <elphe> join #qgis
08:54:56 <eman0n> hi, anyone knows why haskell gives me this error in this section of code?
08:55:01 <eman0n> Could not deduce (ColorComponent Float)
08:55:01 <eman0n>   arising from a use of ‘color’
08:55:01 <eman0n> from the context (RealFloat a, VertexComponent a)
08:55:01 <eman0n>   bound by the inferred type of
08:55:01 <eman0n>            drawVert :: (RealFloat a, VertexComponent a) => Vertex2 a -> IO ()
08:55:10 <eman0n> my code:
08:55:12 <eman0n> drawVert v = do color . getcolor $ mandel v
08:55:12 <eman0n>                 vertex v
08:57:08 <fendor> eman0n, in general is hpaste or pastebin better to show some code than several lines in irc
08:58:09 <eman0n> thanks
09:00:18 <mud> lpaste.net is the recommended one. Actual pastebin is quite awful, for haskell in particular.
09:00:33 <mud> The syntax highlighting is a travesty.
09:01:17 <eman0n> http://lpaste.net/360046
09:03:10 <mud> http://hackage.haskell.org/package/OpenGL-3.0.2.0/docs/Graphics-Rendering-OpenGL-GL-VertexSpec.html#t:ColorComponent  Doesn't look like Float is an instance of that
09:03:19 <sternmull> i made a modification to the process package. How do i run its tests? If i run "stack test" then some stuff runs. But there is a "tests" subdirectory with an all.T that lists more tests... and i don't think it gets executed.
09:03:25 <mud> You probably have to use those special GL things, GLfloat? I don't know what that is.
09:03:46 <mud> sternmull: What's in the .cabal file? Are there multiple test suites?
09:05:00 <sternmull> no, there is only a single "test-sute test": https://github.com/haskell/process/blob/master/process.cabal
09:06:25 <eman0n> It seems like that I want to use color in GLUT but haskell recognize it as color in OpenGL
09:06:45 <mud> sternmull: Looks like they have some weird custom thing in tests/  It appears that you're supposed to use 'make'
09:07:17 <sternmull> mud: The comment in the makefile says it expects to be in the GHC source tree... i think i will give up.
09:08:05 <mud> Sounds like a reasonable solution, yeah. That seems like a pain in the ass to me. Maybe there's a doc that describes how to set it up correctly
09:09:12 <eman0n> what should I write for import the color in GLUT instead of the color in OpenGL?
09:14:01 <dminuoso> ski, okay let me process that notation for a second.
09:16:17 <dminuoso> <ski18>   T  ~~~  Id . T  >-{ eta . T }->  T . T  >-{ mu }->  T
09:16:22 <dminuoso> That notation, how do I read this?
09:17:04 <ski> `T' is isomorphic to `Id . T'. we have a morphism `eta . T' from `Id .T' to `T . T', and a morphism from `mu' from `T . T' to `T'
09:17:05 <dminuoso> And is Id just the Identity functor?
09:17:40 <ski> (these morphisms happen to be natural transformations between the objects, which here are functors : `T',`Id . T'.`T . T',`T')
09:18:00 <ski> `Id' is the identity functor, yes. and `F . G' is the functor composition of functors `F' and `G'
09:18:43 <ski> anyway, this (natural) isomorphism and these two morphisms (natural transformations) are to be composed in the way indicated, yielding a morphism (natural transformation) from `T' to `T'
09:19:09 <ski> and the law was claiming that this composition is the same as the identity morphism (natural transformation) from `T' to `T'
09:20:22 <ski> `.' is a bifunctor. `eta . T' is the functorial map (`bimap', in Haskell terms, in the general case), using the morphism `eta' on the left of `.', and the identity morphism on `T' on the right
09:20:58 <ski> iow, it corresponds to
09:20:59 <ski> @type Data.Bifunctor.first
09:21:01 <lambdabot> Bifunctor p => (a -> b) -> p a c -> p b c
09:21:07 <ski>   first f = bimap f id
09:21:57 <dminuoso> ski, okay those almost read almost exactly like the monad laws using kleisli arrows. :)
09:23:14 <ski> this is similar to how for the cartesian product `*', in the `bimap' case `f * g : A * B >---> C * D', with `f : A >---> C' and `g : B >---> D'. here, if `D' is `B' and `g' is `id_B', then instead of `f * id_B', people often just write `f * B', treating `(* B)' as a plain functor (parameterized on the object `B')
09:23:51 <ski> dminuoso : well, it's not the same thing. monad laws in terms of kleisli is the laws for identity morphism and composition for the kleisli category
09:24:06 <dminuoso> ski, yeah. I was just making a remark about the stunning similarity.
09:24:28 <ski> dminuoso : but here we're not composing kleisli morphisms, but natural transformations
09:24:36 <ski> yeah, both look "monoidy"
09:25:27 <ski> in Haskell terms, one set of laws are in terms of `return' and `(>=>)'. passing the former to the latter, and associating the latter
09:25:53 <dminuoso> That didn't compute
09:26:03 <ski>   return >=> g  =  g
09:26:13 <ski>   f  =  f >=> return
09:26:26 <ski>   (f >=> g) >=> h  =  f >=> (g >=> h)
09:26:43 <dminuoso> Yeah those were the laws I was referring to, when noticing the resemblence of laws.
09:27:02 <ski> (note that we don't use `return' as a natural transformation here. we use a particular instance of it, iow just a (kleisli) morphism)
09:27:26 <dminuoso> Indeed, I noticed immediately.
09:28:52 <ski> while the other set of laws are in terms of `return' and `join' (as natural transformations). we don't pass the former to the latter. rather we either apply `return' on the "outer" of an existing "inner" layer (that's `return :: forall a. T a -> T (T a)'), or apply `return' (using `fmap') on the "inner" layer inside out an existing outer layer (that's `fmap return :: forall a. T a -> T (T a)')
09:28:54 <dminuoso> ski, does `return` somehow imply that a value could be trivially wrapped in Identity, and then transformed to a monad?
09:29:20 <ski> and then we compose both of these with `join :: forall a. T (T a) -> T a', to get back to `T a' from the input `T a'
09:29:41 <dminuoso> (Because just looking at the type signature makes it obvious that `return` is not exactly a natural transformation.
09:29:47 <dminuoso> :t return
09:29:49 <lambdabot> Monad m => a -> m a
09:30:40 <ski> and for the last law, we either use `join :: forall a. T (T (T a)) -> T (T a)' to "smash" two outer level, not touching the innermost level, or we use `fmap join :: forall a. T (T (T a)) -> T (T a)' to "smash" the two inner levels (inside `fmap'), not touching the outermost level
09:30:56 <dminuoso> Okay give me a moment to process this.
09:31:12 <ski> and then, we compose both of these with a `join :: forall a. T (T a) -> T a' to get back to `T a' from the input `T a'
09:31:24 <ski> in Haskell code
09:32:05 <ski>   join . eta  =  id
09:32:33 <ski> er, i should be consistent, either using `return' and `join', or `eta' and `mu'
09:32:40 <ski>   join . return  =  id
09:32:47 <ski>   id  =  join . fmap return
09:33:05 <ski>   join . fmap join  =  join . join
09:33:10 <ttrx> Can someone explain nomads to me?
09:33:35 <ski> if we reformulate these using categorical notation instead, `eta' (`return') and `mu' (`join') have signatures
09:33:37 <dminuoso> ttrx, if we're evil we'd just say read up! :-P
09:33:46 <ski>   eta : Id >---> T
09:33:51 <ski>   mu : T . T >---> T
09:34:20 <ski> using "components" notation (corresponding to instantiation of polymorphic operation in Haskell), we say that for any object `A', we have
09:34:30 <ski>   eta_A : Id A >---> T A
09:34:31 <ski> iow
09:34:35 <ski>   eta_A : A >---> T A
09:34:36 <ski> and
09:34:45 <ski>   mu : (T . T) A >---> T A
09:34:46 <ski> iow
09:34:54 <ski>   mu : T (T A) >---> T a
09:34:57 <dminuoso> Wew you're bombaring me with information way faster than I can process.
09:34:58 <ski> @type return
09:34:59 <lambdabot> Monad m => a -> m a
09:35:00 <ski> @type join
09:35:01 <lambdabot> Monad m => m (m a) -> m a
09:35:55 <ski> `eta' being the polymorphic `return :: forall a. a -> T a'. while `eta_A' being a monomorphic instantiation of this to some concrete type `A' : `eta_A : A -> T A'
09:36:03 <ski> (and similarly for `mu_T')
09:36:14 <dminuoso> You are on the brink of confusing me.
09:36:24 <ski> Haskell isn't explicit about this
09:36:35 <ski> Category theory is. and i think it can help in this case
09:36:37 <dminuoso> ski, Im still on what you said 8 minutes ago.
09:37:53 <ski> hm .. i need to leave in a couple of minutes .. i was attempting to get some kind of exposition of the laws here out, before then
09:38:26 <dminuoso> ski, if in doubt we continue this tomorrow. I got some processing to do.
09:38:32 <ski> in Haskell, given `fmap :: forall a b. (a -> b) -> (T a -> T b)' for a functor `T', we write `fmap f :: T A -> T B', given `f :: A -> B'
09:38:53 <ski> in categorical notation, given `f : A >---> B', we simply write `F f : F A >---> F B'
09:38:53 <dminuoso> Right, easy peasy.
09:39:27 <ski> now, i can restate the three laws from above (the `join' and `return' ones, also using `fmap'), using this categorical notation (with `eta' and `mu')
09:39:33 <ski> they now become
09:40:15 <ski>   mu_(T A) . eta_(T A)  =  T A
09:40:39 <ski> er, no, sorry, that should be just
09:40:43 <ski>   mu_A . eta_(T A)  =  T A
09:40:56 <ski>   T A  =  mu_A . T (eta_A)
09:41:16 <ski>   mu_A . T (mu_A)  =  mu_A . mu_(T A)
09:41:52 <ski> (and when i say `T A' here, i mean `id_(T A)', the identity on `T A', i.e. `id_(T A) : T A >---> T A')
09:42:23 <phadej> paolino_: you're welcome, it's nice to hear someone uses something one made :)
09:42:53 <ski> you may note that before `join . return' looked assymetrical to `join . fmap return', in the first two laws. well, now we instead have `eta_(T A)' vs. `T (eta_A)'. the `T' in the latter is `fmap'. the `T' in the former is that we're instantiating `eta' at `T A' rather than `A'
09:43:41 <ski> similarly, for the last law, we had only `join' on one side (to the right of the composition), but `fmap join' on the other side (still to the right of the composition)
09:44:07 <dminuoso> ski, I need to translate this into diagrams to make sense of this I think.
09:44:14 <ski> well, now in the categorical formulation, we have `T (mu_A)' vs. `mu_(T A)', with the same difference in placement of `T', as just above
09:44:18 <ski> ok, that's it
09:44:27 <dminuoso> Alright, thanks a lot. Processing and drawing time.
09:44:55 <ski> please ask me later (probably not today, but perhaps tomorrow or later in the week) to clarify and elaborate more
09:46:17 <wroathe> Why is cancelling common factors between the numerator and demonator in a rational expression allowed if we lose information about the values of x that make the denominator 0? Doesn't cancelling common factors, in this case, make the resulting rational expression completely different than the original? To me this seems like just arbitrarily adding a constant to one side of an equation and not the other...
09:46:50 <inhortte> yeah.
09:47:04 <wroathe> For example, simplifying 2(x + 3) / (x + 3) to 2/1
09:47:39 <wroathe> Whoops
09:47:41 <wroathe> Wrong channel
09:47:46 <wroathe> Ignore me
09:48:06 <ski> wroathe : i could explain the issues here .. but not now. perhaps tomorrow ?
09:48:09 <dminuoso> ttrx, was this an honest question by the way? (Based on the fact that we were just discussing this in a mathematical sense, I couldnt tell)
09:49:18 <wroathe> ski: Just ignore that. That was meant for #math.
09:50:08 <ski> i'll try to remember to take it up with you tomorrow, then
09:50:14 <ski> (not going to ignore)
09:52:43 <namosca> Hi all
09:54:38 <cocreature> hey namosca 
09:54:44 <namosca> cocreature: Hi
09:56:07 <ttrx> +dminuoso well I got confused so maybe not
09:56:52 <namosca> I need to write a string or number to a file, but respecting some padding, so as part of this I use the printf function. A problem is that the printf for doubles is works as: printf "%8d" works for integers, and for doubles I need printf "8%.3e"
09:56:54 <paolino_> phadej: it's just a huge jump in the quality of failure reporting in tests, especially for matching expected json which is usually fat  
09:57:47 <namosca> I wanna function that takes a number (double or int) or a string and applies the corresponding printf... but I tried to use the case (typeOf input) of Int -> bla bla | String ->  Bla bla, but it doesnt work
09:57:51 <dminuoso> ttrx, feel free to ignore our discussion. It's just of academic nature out of curiosity. It has no relevance on ones ability to understand and utilize monads.
09:58:19 <namosca> I made three functions for each type of input, but its so annoying.. how can I put everything into only one function?
09:58:22 <paolino_> namosca: should be %8.3f btw
09:59:12 <dminuoso> ttrx, for users a monad is just a 'lawful programming pattern'
10:03:07 <phadej> paolino_: that [better tests reporting] was my primary motivation indeed
10:04:04 <namosca_test> MYTEXTOUT
10:04:05 <namosca_test> MYTEXTOUT
10:04:06 <namosca_test> MYTEXTOUT
10:04:06 <namosca_test> MYTEXTOUT
10:05:39 <lyxia> Spamming is bad.
10:07:10 <paolino_> phadej: we've been very lucky, your library came right before we looked for it
10:09:07 <phadej> paolino_: as I didn't do any SEO, how did you found it?
10:09:42 <paolino_> well diff generic haskell values
10:11:07 <paolino_> phadej: We just missed the "tree" word
10:11:18 <phadej> :)
10:11:58 <phadej> there's gdiff, but it has different focus
10:12:39 <paolino_> and with the colored pretty printing is just unresistable, it's a pity when there are no failures
10:14:07 <paolino_> it should just be part of HUnit.@?=
10:14:59 <EvanR> why is the total amount of code supporting a given program so huge
10:16:00 <EvanR> when any given paper will have a non trivial algorithm or fancy implementation as less than a page of haskell code
10:16:04 <phadej> paolino_: :)
10:16:49 <phadej> EvanR: any example?
10:18:06 <EvanR> print out the fib sequence... which relies on the ghc library, ghc runtime, the operating system, the terminal emulator, the graphics system, font engine...
10:18:18 <EvanR> not to mention the compiler
10:18:49 <EvanR> you could explain how to evaluate the fib sequence to a kindergartener
10:19:57 <EvanR> is the state of software necessary
10:23:00 <EvanR> FP is so great for distilling the core aspects of what youre doing. But if you dont want to use any one piece of that giant tower of software platform, you will be suddenly be spending 80% of the work on implementing that
10:23:07 <EvanR> it feels wrong
10:23:55 <cocreature> being able to reuse the work other people have done feels pretty right to me :)
10:24:09 <EvanR> but why is that work so huge?
10:24:09 <phadej> does it? You can make Commondore64 spit out fib-sequence, but we built all abstractions you mention since then, to build better software faster 
10:24:35 <EvanR> why do better abstractions have exponentially huger implementation effort
10:25:48 <phadej> philophical questions, if you ask one, you have to try to answer it yourself ;)
10:27:13 <cocreature> phadej: btw how stable is tree-diff? last time I looked at it it was at version 0 which resulted in me not wanting to depend on it, it’s at 0.0.0.1 now which is slightly better but still seems very alpha :)
10:28:41 <EvanR> a lesson from turing tarpits is... almost all implementations of anything are huge. maybe thats just a fact of the universe
10:28:51 <EvanR> and we are lucky to have what we have
10:28:52 <phadej> cocreature: I don't have any plans to make any changes
10:28:56 <namosca> Hi all.. I am making a small IRC client and I need to get input from the user to send to the chat program. So I made a non-pure function called speak which takes a handle and string and makes an IO. How can I make it run in paralel to my main code (concurrently)... I only find complicated or outdated concurrency stuff about Haskell on the internet
10:29:14 <phadej> cocreature: but I'm sure it's not perfect, so started with 0, and not 1 (like e.g. microstache!)
10:29:55 <EvanR> namosca: the library async is pretty good for this. its a nicer wrapper around the, still pretty easy to understand from the docs, Control.Concurrency primitives
10:30:12 <cocreature> phadej: ok great, thanks!
10:30:16 <mnoonan> namosca: forkIO ?
10:30:28 <EvanR> forkIO, MVar, Chan, etc
10:30:42 <namosca> mnoonan: Yes I tried forkIO, but I got soooo many type errors that I got scared
10:31:38 <EvanR> if youre having trouble, try pasting your example code and the errors youre getting?
10:31:46 <EvanR> on lpaste, not in here
10:32:02 <mnoonan> hmm, if you have a "foo x y z" in a do-block, you can just "_ <- forkIO (foo x y z)" to fire and forget it
10:32:26 <EvanR> you dont even need the _ <- really
10:32:51 <paolino_> but it has to be IO ()
10:33:26 <EvanR> it doesn't
10:33:58 <mnoonan> oops, yeah, i meant to write "foo x y z :: IO ()"
10:34:11 <mnoonan> otherwise, use void :: IO a -> IO () or something
10:35:20 <EvanR> exfalsoM :: IO Void -> IO a    :)
10:37:21 <paolino_> forkIO :: IO () -> IO ThreadId
10:37:58 <phadej> cocreature: fwiw, 0.0.0.1 was a "test in production", if I can make it work for Cabal, but there are still technical issues yet not resolved :/
10:38:24 <phadej> cocreature: it didn't change any code
10:38:42 <phadej> (not with tree-diff, but with Cabal being "special")
10:39:37 <phadej> which is a pity, because Cabal was the place the predecessor was made
10:43:05 <namosca> About the forkIO, here is the code that doesnt work> http://lpaste.net/360052. WHen I turn IO on, my incoming messages disappears
10:43:47 <dminuoso> Im looking for a function that does something along those lines.(a -> b -> c) -> (x -> a, x -> b) -> [b] -> c
10:44:06 <dminuoso> I have a feeling that Im barking up the wrong tree, because this is nowhere to be found
10:44:13 <monochrom> namosca: If the main thread finishes, the whole program dies.
10:45:02 <namosca> monochrom: even if the forked thread has a forever loop? Interesting...
10:45:26 <monochrom> Read the doc of Control.Concurrency for how to wait for a thread.
10:45:31 <dminuoso> Oh wait! I mistyped.
10:45:54 <dminuoso> (a -> b -> c) -> ([x] -> a, [x] -> b) -> [x] -> c
10:45:58 <namosca> monochrom: thanks
10:48:22 <namosca> Control.Concurrency says that forkIO waits for the result of the thread:
10:48:22 <namosca> "Sparks off a new thread to run the given IO computation and returns the ThreadId of the newly created thread paired with an IO computation that waits for the result of the thread. "
10:49:09 <maerwald> pasaNenBraune2#
10:50:26 <paolino_> :t fmap (($) *** ($)) 
10:50:28 <lambdabot> Functor f => f (a1 -> b1, a2 -> b2) -> f (a1 -> b1, a2 -> b2)
10:50:33 <monochrom> That's not in mine. Where can I read this?
10:50:56 <namosca> Here: https://hackage.haskell.org/package/threads-0.3/docs/Control-Concurrent-Thread.html
10:50:58 <monochrom> Err Control.Concurrent
10:51:13 <cocreature> namosca: take a look at https://hackage.haskell.org/package/base-4.10.0.0/docs/Control-Concurrent.html#v:forkIO
10:52:04 * monochrom hates the Internet.
10:52:25 <cocreature> monochrom: don’t worry, I’m sure parts of the internet hate you too
10:53:44 <monochrom> Also even the threads package's forkIO doesn't wait.
10:54:08 <monochrom> There is a difference between "forkIO waits" and "forkIO returns an IO computation that waits"
10:54:59 <monochrom> A forkIO that would wait would belong to the ACME category.
10:55:25 <namosca> Ok... I am a haskell beginner, so I am getting too lost hehe...
10:56:46 <dminuoso> So IO is not actually a container?
10:57:07 <dminuoso> It's just some.. magical thing with kleisli arrows?
10:57:10 <cocreature> dminuoso: define “container” and we can tell you if IO fits your definition :)
10:57:12 <monochrom> IO has never been a container.
10:58:21 <monochrom> getLine waits for me to enter a line. But I'm not sure what I want to enter, I'll think about it and enter tomorrow.
10:58:26 <dminuoso> cocreature, well IO String does not actually "contain a string"
10:58:41 <dminuoso> Or does it in some bizarre "future" kind of way?
10:58:42 <monochrom> Now explain to me how getLine or IO contains the line I will decide to enter tomorrow.
10:58:54 <monochrom> Even I don't contain it.
10:59:17 <cocreature> dminuoso: by point is that without defining what exactly a container is, it’s really easy to claim that things fit or don’t fit that description
10:59:27 <dminuoso> cocreature, yeah I hear you.
10:59:49 <namosca> ok people... so I put a forever $ do before forking and the program doesnt terminate... but I get only garbage and my computer freezes
10:59:52 <Forlorn> Hi, I am trying to parse a tree in Haskell following the tutorial "Gentle Introduction to Haskell" where they use `lex` and list comprehenesions.
10:59:56 <Forlorn> http://sprunge.us/BaPK
11:00:07 <Forlorn> in the link above you may find the function for readsTree.
11:00:28 <monochrom> namosca: So you mean you fork infinitely many threads?
11:00:42 <Forlorn> It only works for `readsTree "<5|3>" :: [(Tree Int, String)]` it seems but not for e.g `readsTree "<5|<3|2>> :: ..."`
11:01:15 <namosca> monochrom: yes.. hehe
11:01:16 <Forlorn> It just provides me an empty list
11:02:40 <namosca> monochrom: I know its a bad idea, but at least I am sure of it :)
11:05:00 <monochrom> I think you need to take a step back and think over some common sense. Common sense that isn't specific to Haskell (so "I'm new to Haskell" is irrelevant).
11:05:42 <monochrom> If your main threads forks a thread, and afterwards the main thread has nothing else to do apart from waiting for the new thread, then what's the point of forking?
11:06:42 <namosca> monochrom: Its just a small test. After this I will have another thread to read my user inputs and send it to the connection
11:07:02 <monochrom> OK, but read the doc of Control.Concurrent for an example of how to wait.
11:07:34 <namosca> Yes, I think I found it.. .it says "If you want the program to wait for child threads to       finish before exiting, you need to program this yourself.  A       simple mechanism is to have each child thread write to an       MVar when it completes" 
11:07:41 <dminuoso> cocreature, this was really interesting. Ive been using the notion of container for so long, but without any clear definition of what it is.                                 
11:07:55 <dminuoso> cocreature, the only way to express it cleanly is "functor" - and in that perspective yes IO is a functor.
11:08:59 <dminuoso> cocreature, though much like promises (say from JavaScript) I think the notion of "IO" kind of holds a value of known type but (yet) unknown value might not be that horrible.
11:09:05 <namosca> monochrom: Do you think this MVAR thing to force waiting its a sound direction to take, or I will waste my time with it?
11:09:05 <cocreature> dminuoso: I’m not sure I’d agree that functor is a good description of what being a container entails but debating nomenclature without precise definitions is a waste of time so I’ll shut up now :)
11:09:21 <dminuoso> cocreature, yeah, the essence was a really helpful hint. Thanks a lot.
11:09:31 <monochrom> Yes to the former.
11:10:01 <monochrom> MVar is also worthwhile to learn for many other purposes.
11:10:14 <namosca> monochrom: Thanks, so I will read it more
11:11:13 <monochrom> I'm wondering if you will find the async library easier to use, or is it just more information overload.
11:12:27 <monochrom> But certainly async already provides waiters so you don't have to code one up yourself.
11:12:53 <Zemyla> I'm wondering if a ContraClosed class would be useful.
11:13:11 <monochrom> And it really is plural waiters because there is one for "wait for this thread" and there is one for "I have two threads, wait for the earliest finishing one" etc etc
11:13:11 <Zemyla> class Profunctor p => ContraClosed p where contraclosed :: p a b -> p (b -> x) (a -> x)
11:20:06 <namosca> monochrom: Can you give me a link with async please? The link with MVAR didnt work well for me. The program still terminates, look here: http://lpaste.net/6722579263779766272
11:23:55 <geekosaur> namosca, you never wait on the MVar
11:24:20 <geekosaur> you need to finish up main with a takeMVar to wait for it to have a value
11:24:43 <a6a3uh> hi! if I have long stack of transformers something like EitherT () (StateT s (ReaderT r (WriterT Log  ... how to make zoom on StateT?
11:25:25 <geekosaur> a6a3uh, newtype with newtype deriving of MonadState s, MonadReader r, MonadWriter Log, ...
11:25:37 <geekosaur> and their methods will then tunnel into the stack as needed
11:30:37 <a6a3uh> geekosaur: should I use some sort of automatic deriving? just write deriving (...)?
11:31:58 <geekosaur> newtype MyStack s r a = MyStack (EitherT () (StateT s (ReaderT r (... ) ) ) ) deriving (MonadState, MonadReader, ...)
11:32:08 <geekosaur> but I suspect you want to read up on monad transformers
11:32:16 <geekosaur> @google monad transformers step by step
11:32:17 <lambdabot> https://page.mi.fu-berlin.de/scravy/realworldhaskell/materialien/monad-transformers-step-by-step.pdf
11:34:09 <namosca> geekosaur: THANKS SOO MUCH!! NOW IT WORKS :). See here http://lpaste.net/3023094606251163648
11:36:42 <a6a3uh> geekosaur: when Im doing: newtype Game s r a = Game (EitherT () (StateT s (ReaderT r (WriterT Log (MemoQV Int Double)))) a) deriving (MonadState s, MonadReader r, MonadWriter Log)
11:36:59 <a6a3uh> it tells • No instance for (Monad (Game s r))
11:36:59 <a6a3uh>         arising from the 'deriving' clause of a data type declaration
11:37:00 <a6a3uh>       Possible fix:
11:37:00 <a6a3uh>         use a standalone 'deriving instance' declaration,
11:37:02 <a6a3uh>           so you can specify the instance context yourself
11:37:03 <a6a3uh>     • When deriving the instance for (MonadState s (Game s r))
11:37:27 <a6a3uh> and so on for each of typeclass
11:37:36 <geekosaur> you have that declared incorrectlyt
11:38:40 <geekosaur> I think
11:38:43 <EvanR> so if I want to use a version of a library which depends on older version of base, do I need to use an older version of GHC
11:38:56 <geekosaur> and I think you need to derive Monad on your newtype as well
11:39:03 <EvanR> stack seems to be ignoring me when i say install an older base
11:39:12 <geekosaur> EvanR, yes
11:39:23 <geekosaur> you annot install an older base, it's wired into ghc
11:39:33 <EvanR> alright... ill try to figure out which snapshot will work
11:39:38 <geekosaur> a6a3uh, I pointed to the monad transformers document for a reason
11:39:54 <ggVGc> is the general spirit in haskell that we should avoid creating type classes unless we really have to?
11:40:11 <a6a3uh> geekosaur: it was working before with just a type and without EitherT. I used zoom on state and magnify on ReaderT. But after adding Either all thet broken...(.  Thanks. Will try to figure out through documentation
11:40:38 <geekosaur> yes, EitherT probably doesn't derive those things
11:40:49 <geekosaur> in fact that probably means newtype deriving won;t work through it either
11:41:13 <namosca_test> geeoksaur, now thanks to you my small haskell irc client works :)
11:41:19 <EvanR> ggVGc: well, you never have to
11:41:31 <EvanR> now the question is when do you want to
11:41:40 <ggVGc> right
11:41:50 <ggVGc> I never really know when it's in my best interest to introduce a type class
11:41:57 <EvanR> and theres a few situations i know of that work nicely
11:41:58 <ggVGc> don't have a good feel for it
11:42:53 <namosca_test> geekousaur, I am writing from my small haskell irc client you just helped me with hehe
11:42:57 <namosca_test> thanks a lot for the help :)
11:43:22 <ggVGc> does it handle unicode?
11:43:44 <EvanR> if you start with a record of functions, to use as an interface, then decide if converting that to a type class will make your life easier to harder
11:43:51 <EvanR> easier or harder
11:44:01 <EvanR> it could go either way i think
11:44:12 <tdammers> ggVGc: in Haskell, it's almost more difficult to not support unicode than to support it
11:45:11 <geekosaur> tdammers, but IRC's relationship to unicode is a bit odd
11:45:43 <namosca_test> testing channel
11:45:49 <namosca_test> testing channel 2
11:45:54 <namosca_test> testing channel 2
11:45:57 <geekosaur> IRC protocol itself is based on an ISO8859 variant that uses `[]{}\| as letters. individual messages may be ISO8859 or UTF8
11:46:39 <ggVGc> EvanR: yeah I've been down that path, both ways
11:46:46 <lyxia> namosca: don't write testing messages into a public channel.
11:46:47 <tdammers> geekosaur: agree
11:46:56 <ggVGc> thing is, I have no intuition on when one is more appriopriate
11:47:05 <ggVGc> I usually start by not mkaing a type class
11:47:11 <ggVGc> but sometimes it ends up being shit
11:47:40 <EvanR> instances are tied to types, records are not, thats another thing
11:49:04 <ggVGc> so it comes down to the great general programming answer of IT DEPENDS
11:49:05 <ggVGc> :((
11:49:52 <EvanR> its kind of a personal choice right, the main difference is if your implementation is implicitly passed around instead of explicitly
11:50:07 <Tuplanolla> Just answer one simple question, ggVGc: will it alleviate your suffering?
11:50:28 <ggVGc> Tuplanolla: sure, usually I only know that after I've suffered a while
11:50:33 <ggVGc> which is the whole problem
11:50:48 <EvanR> suffering oriented programming
11:50:56 <ggVGc> EvanR: yeah, that's why I generally try not to make type classes. I prefer to stay as much on value level as I can
11:50:59 <ggVGc> in any language
11:52:03 <EvanR> i like records for drivery things, and type classes for implementations that are specifically tied to a type or about a type
11:52:51 <ggVGc> what are drivery things?
11:53:13 <EvanR> like OOP
11:53:20 <ggVGc> stop swearing
11:55:55 <EvanR> actually i can see a downside to basic records... if you have 2 different records for the same interface, you could mix and match the calls to them and get nonsense
11:56:17 <EvanR> you would need to attach phantom types to avoid getting them mixed up, if you dont trust yourself
11:56:34 <ggVGc> tat can
11:56:38 <ggVGc> that can also be a benefit...
11:56:51 <ggVGc> I mean, it adds some flexibility, which might be a positive or a negative thing
11:56:54 <EvanR> if your interface has no side effects yeah
11:57:00 <ggVGc> for me it's been positive fairly often
11:58:08 <ggVGc> EvanR: doing things like using a dict of implementations for some evaluation and mapping over it is very natural for me
11:58:22 <ggVGc> err, I meant list or whatever
11:58:24 <ggVGc> not dict
11:58:34 <ggVGc> point is, I think that's a more natural way than doing it at the type level
11:59:13 <EvanR> yeah there are hoops to put a variety of existentials in the same list
11:59:40 <ggVGc> right, but with the dicts it's just a very natural straightforward thing
12:06:57 <EvanR> the container thing is kind of weird
12:35:02 <Mr_Root1> hello
12:37:04 <erisco> hello
12:37:44 <shapr> hi erisco
12:37:48 <shapr> hi Mr_Root1 
12:38:21 <erisco> this greeting protocol has a O(n^2) complexity
12:44:41 <dsal> Is there a conceptual difference between the various *parsec libraries?
12:44:59 <dsal> i.e., are they roughly the same, but with more or less convenience bits?
12:45:01 <EvanR> sdl2-1.3.1 doesnt require lens... so the bindings hypothetically will let me use SDL2 on rpi. but i would need an old GHC. unless i hack that package to lift the version bounds. or i could write my own SDL bindings. uhg ;_;
12:45:20 <defanor> i'd like to embed a text file as a string into a haskell program, thinking of generating a module containing it using a custom Setup.hs (using cabal, ghc) -- but what would be a suitable hook for that, is it preBuild? and/or perhaps there are nicer ways to do it?
12:45:30 <shapr> dsal: from what I hear, megaparsec gives better errors, attoparsec is much faster
12:46:23 <dsal> Makes sense, but are they roughly the same in spirit?  i.e., is the knowledge generally transferrable across them?
12:46:25 <jrabe> erisco, if you pingspam the entire channel you'll get it down to O(n)
12:46:38 <dsal> I've used megaparsec some, and attoparsec a bit more.  They seem similar enough.
12:47:52 <erisco> dsal, they differ on what languages they can recognise. The "smaller" versions eschew language recognition for speed
12:48:29 <dsal> Do you mean like megaparser's understanding of operators and precedence?
12:48:34 <dsal> er, megaparsec
12:48:41 <erisco> there is also difference in what information you get on parse errors
12:48:43 * dsal just had a thing named megaParser that's being renamed in another window
12:48:59 <dsal> I've used attoparsec's <?> to some good results.
12:49:47 <erisco> there are certain things you need to do to recognise some languages, but this can come at increased cost
12:49:53 <cocreature> dsal: the knowledge is definitely transferable between the different libs but you’ll have to pay attention to some details e.g. attoparsec backtracks by default while parsec doesnt
12:50:37 <dsal> It sounds like they're fairly similar at a high level.  attoparsec has been very useful for what I've been doing.  I've occasionally run into things that I think would be nice, but would really just come at a higher cost of knowing that stuff exists (e.g., I was parsing a string of 16-bit numbers and had to chop them up and parse separately)
12:50:53 <dsal> cocreature: Oh, that's a good point. I saw something in attoparsec about having a parsec compatible interface for that.
12:51:07 <dsal> backtrack by default is super convenient.  That'd probably be hard for me to retool my brain around.
12:52:22 <shapr> cocreature: does megaparsec follow parsec in not backtracking by default?
12:52:33 <cocreature> shapr: yes
12:52:39 <cocreature> trifecta does too
12:53:07 <shapr> I used parsec for a bunch of years, I'd have to see an example to understand the difference
12:53:11 <dsal> My path to parsers is a bit weird.  I used megaparsec a bit back for a really simple language thing.  Then I decided to use it as part of a optparse-applicative thing.  Then I was watching Stranger Things and suddenly it made sense to me.  I should write a tutorial.
12:53:34 <cocreature> shapr: although megaparsec backtracks on some primitives, e.g., iirc it backtracks automatically on string literals
12:53:57 <dsal> The backtracking is great because a bunch of my code is just a whole lot of  pa <|> pb <|> pc  because this stuff I'm parsing is rather messy.
12:54:16 <cocreature> dsal: you can always slap "try" in front of things to get backtracking
12:54:17 <dsal> In some cases, the spec seems to observe things that people have done and just said, "Or like, they can do that."
12:54:28 <cocreature> but your error messages suffer and performance does too
12:54:32 <dsal> Yeah, it should be alright.  I'll just be doing it all over the place.
12:54:50 <dsal> Yeah, I took out all of my <?> because they don't do anything since the bulk of my parsing is an arrangement of <|>
13:07:46 <shapr> Is there a good way to call Haskell code from inside Python?
13:08:13 <merijn> shapr: via FFI is the only option, so it depends on what you consider "good"?
13:08:16 <tdammers> FFI from Haskell to C, wrap as a Python module?
13:08:23 <tdammers> otherwise, use subprocesses maybe
13:08:37 <merijn> shapr: Calling Haskell from C is trivial enough. So the real question is "how easy is the Python FFI"?
13:08:46 <shapr> hm, ok
13:09:33 <nawi_is> Hello, maybe I am in the wrong channel. How good / bad does it work to create Android / iOS apps with Haskell ?
13:09:53 <[exa]> There are some IIRC
13:10:19 <[exa]> You might want to take a look at links here https://wiki.haskell.org/Android
13:11:41 <dminuoso> Can you folks suggest something as good as WinCompose for macOS?
13:11:51 <nawi_is> I saw the posts in the wiki, thanks. I meaned more the point recommand to do (not so the how todo at the moment).
13:11:56 <dminuoso> Having some real trouble writing down all this category theory stuff with copy pasteing from tables. :S
13:13:09 <[exa]> nawi_is: also try googling for "reflex haskell on android", I certainly recall that people from reflex had something interesting running there
13:13:25 <Tuplanolla> How about LaTeX, dminuoso?
13:13:41 <dminuoso> Tuplanolla, I mostly just care about easy UTF8 for Haskell and IRC
13:13:42 <nawi_is> Thanks [exa]> nawi_is: also try googling for "reflex haskell on android", I 
13:13:42 <nawi_is>                certainly recall that people from reflex had something interesting 
13:13:47 <dsal> C-\ in emacs is awesome.
13:14:05 <nawi_is> ops, soory. Meaned thanks [exa]
13:15:41 <Tuplanolla> I use a set of completions that translate things, like \to turning into →, dminuoso.
13:16:18 <[exa]> nawi_is: found it. Look at this paper here https://dl.acm.org/citation.cfm?doid=3136534.3110246
13:16:59 <[exa]> and the example project is here https://github.com/ivanperez-keera/haskanoid
13:20:49 <[exa]> dminuoso: mac doesn't have .XCompose?
13:21:05 <geekosaur> only in X11 apps
13:21:15 <geekosaur> mac native is not X11 based
13:21:21 <[exa]> oh noes
13:21:25 <nawi_is> [exa]: wow, looks good, thanks for sharing. Would be nice to skip Java and Swift and focus on Haskell ;-)
13:21:42 <dminuoso> [exa], https://github.com/gnarf/osx-compose-key
13:21:48 <dminuoso> looking at that it seems trivial to DIY
13:21:52 * dminuoso sighs
13:21:53 <dsal> The normal built-in keyboard stuff does expansions for you, but not in like, emacs.  In emacs, I can do C-\ or whatever insert-char is bound to.
13:21:56 <dminuoso> I want RTW.
13:22:26 <dminuoso> dsal, well the thing is I want something that words in both weechat as well as vim :P
13:22:42 <[exa]> nawi_is: I guess it's not quite ready for production but also not as bad as full droid SDK :D
13:22:45 <dsal> emacs will emulate vi and connect to various chat networks.  :P
13:22:56 <dminuoso> Haha
13:23:02 <dsal> but yeah, I don't have something that's "universal"  Worst case, I just type stuff in emacs and then copy it.
13:23:25 <[exa]> there are terminals that have nice utf input
13:25:10 <dminuoso> Quick CT question: T ≅ Id ⚬ T --{ η ⚬ T } -> T ⚬ T
13:25:32 <dminuoso> What exactly is η ⚬ T? How can you compose a natural transformation with a functor?
13:26:09 <nawi_is> [exa]: For now it is enough, if there is something what works.
13:26:11 * dsal used ≅ in a typeclass once.
13:26:31 <dminuoso> dsal, what can I say. It is the best symbol I found for isomorphism :P
13:27:23 <dsal> I used it kind of as an equality while ignoring unimportant bits.  It wasn't perfect, but I gave up trying to find a better symbol.
13:28:01 <dsal> ≈ didn't seem quite right.
13:28:52 <Tuplanolla> The T stands for identity at T there, dminuoso.
13:30:14 <dminuoso> Tuplanolla, not sure I follow
13:32:51 <Tuplanolla> If you were mapping TTT into TT, you could either do it via μT or Tμ.
13:33:36 <dminuoso> Tuplanolla, I just understood natural transformations, so bear with me. Whats the deal with that type of composition μ . T, or T . μ ?
13:34:17 <dminuoso> Are we purposefully ignoring morphisms?
13:34:52 <Tuplanolla> The T in μT or Tμ signifies which TT in the TTT go through the μ and which T goes through as-is, like identity.
13:35:15 <Tuplanolla> The composition is more like "in parallel" than "in series".
13:35:52 <dminuoso> Tuplanolla, you lost me there. For a while I thought that since both natural transformations as well as Functors map objects to objects, that you could compose them in that sense.
13:36:12 <Tuplanolla> If you're trying to ask why mathematicians come up with stupid notations, I don't have an answer.
13:36:32 <dminuoso> No. Im trying to _understand_ that notation.
13:37:06 <Tuplanolla> Let's use Haskell as our vehicle then.
13:37:31 <dminuoso> Tuplanolla, thats the thing. In haskell terms that statement is just join . return = id
13:37:40 <dminuoso> But why does this work?
13:38:46 <dminuoso> Essentially the entire thing reads T ≅ Id . T --{ η . T }--> T . T --{ μ }--> T
13:38:46 <Tuplanolla> You have T -> TT via η and then TT -> T via μ. In Haskell terms, you have `m a -> m (m a)` via `pure` and then `m (m a) -> m a` via `join`.
13:39:18 <dminuoso> My goal is to understand the category side of things.
13:39:18 <Tuplanolla> On the other side you have T -> TT via η and then TT -> T via μ. In Haskell terms, you have `m a -> m (m a)` via `fmap pure` and then `m (m a) -> m a` via `join`.
13:39:48 <Tuplanolla> Now there is ambiguity on the mathematical side: because the `fmap` is implicit, the notation looks identical.
13:40:13 <Tuplanolla> So instead you say that you have T -> TT via ηT or Tη and then TT -> T via μ.
13:41:07 <dminuoso> <dminuoso>30 What exactly is η ⚬ T? How can you compose a natural transformation with a functor?
13:41:10 <dminuoso> This is the bit Im curious about.
13:43:41 <dminuoso> does η . T just means: forall objects x in C. ηx . Tx ?
13:44:14 <Tuplanolla> There are two ways to compose functors: horizontally and vertically.
13:44:39 <Tuplanolla> That's what I alluded to when I said "in parallel" and "in series".
13:44:52 <dminuoso> I have absolutely no clue what "in parallel" means.
13:45:06 <dminuoso> "in parallel" implies concurrency
13:49:30 <dminuoso> <Tuplanolla> The T stands for identity at T there, dminuoso.
13:49:38 <dminuoso> heh that actually helped more than it seemed at first.
13:50:03 <dminuoso> So ηT just means  η . I_T ?
13:51:02 <Tuplanolla> Yes, but I want to call question to whether you know what that dot really means, dminuoso.
13:52:02 <dminuoso> Tuplanolla, it seems it would just be a morphism in the category of natural transformations.
13:53:28 <dminuoso> Tuplanolla, see my understanding lied in seeing η . T as composing a natural transformation with a functor which made absolutely no sense to me.
13:53:33 <dminuoso> Or rather my confusion.
13:53:44 <dminuoso> And when you said Identity on T, I just read that as "Identify Functor"
13:53:52 <dminuoso> Which did not help :P
13:55:01 <dminuoso> Tuplanolla, (or rather the "dot" would just be composition of morphisms in the category of functors.
13:56:26 <Tuplanolla> Well, there you go. Mystery solved.
14:14:32 <dminuoso> Tuplanolla, so the purpose of composing these two together is so there's a sensible way of talking about TTT ?
14:15:10 <dminuoso> By representing TTT as TT . T, which forces me to compose two natural transformations together μ . I_T, or using the standard notation μT
14:16:14 <Tuplanolla> I wouldn't say that's the purpose of the notation, but that does happen as a consequence.
14:17:37 <dminuoso> Tuplanolla, mmm. for the commutativity diagram it allows me to express associativity of join I guess. It lets me specify "which two shells to smash together"
14:17:53 <dminuoso> Anyway. Thank you and sorry about my mood. Lack of food and frustrating has made me moody.
14:19:36 <Tuplanolla> As if I'd notice.
14:20:15 <maerwald> you should be well fed
14:20:24 <dminuoso> but arrows!
14:21:28 <Tuplanolla> This mess stems from another unfortunate choice of notation.
14:22:13 <dminuoso> Tuplanolla, yeah. If this had read mu . I_t, it would have just been a matter of figuring out what I_t was.
14:22:26 <dminuoso> Ah well, I can accept this notation, seen much more quirky things
14:27:37 <dminuoso> Tuplanolla, one last silly question if you dont mind. Is there a particular reason why Tμ is used over μ?
14:27:44 <dminuoso> Are they not the same?
14:28:02 <Tuplanolla> They're not the same for the same reason as in the other diagram.
14:36:51 <Tuplanolla> Let me explain the unfortunate part.
14:38:05 <Tuplanolla> Given a functor F : C -> D with some objects a : C and b : C and some morphism f : a -> b, people write F a : D for the mapping of the object and F f : F a -> F b for the mapping of the morphism.
14:39:22 <Tuplanolla> More precisely we should write a : Obj C, b : Obj C and f : Hom C a b, in addition to F a : Obj D and F f : Hom D (F a) (F b).
14:39:48 <Tuplanolla> See, there are actually two differently typed F in play here. One of them is F : Obj C -> Obj D and the other one is F : Hom C a b -> Hom D (F a) (F b).
14:40:57 <vwhite> If you want readline like functionality, but want your program to run and build in windows without too much trouble, is there an alternative to the haskell readline wrapper?
14:41:15 <dminuoso> Tuplanolla, based on the definition of natural transformations I learned today, it appears as if they are only concerned about objects (and morphisms are left intact)
14:42:53 <Tuplanolla> This is bad for us Haskell programmers, because our moral category is Cartesian closed, so leaving the type of F implicit means `fmap` vanishes from the notation.
14:42:56 <dminuoso> Tuplanolla, but what you said makes sense, and Ive had to remind myself of that a few times.
14:43:41 <dminuoso> Tuplanolla, (and in Haskell I learned to live with the fact that fmap is just the F : Hom C a b -> Hom D (F a) (F b) portion (the other part being fulfilled by the type constructor of the Functor)
14:43:52 <Tuplanolla> We can have F a -> F b and F (a -> b) at the same time and they are different.
14:44:52 <dminuoso> Tuplanolla, you just completely lost me at cartesian closed.
14:45:30 <Tuplanolla> It was just a segue to the next point.
14:51:24 <haskellnoob> hi there, maybe someone can help me with my assignment, I have to define a new type Nat without deriving Show, and others, but I can't wrap my head around on how to implement Show
14:52:02 <haskellnoob> basically I already have "data Nat = Null | N Nat, so it could be very big
14:52:46 <haskellnoob> how should one implement a show function for that ? it's supposed to return (e.g. N( N Null)) the value as a number
14:53:13 <ggVGc> haskellnoob: ignore the type class Show for a second. Just write a function that translates your data type to a string representation
14:53:20 <ggVGc> it's up to you what that is
14:53:29 <ggVGc> basically a Foo -> String for your type Foo
14:54:05 <ggVGc> haskellnoob: it will need two cases right. One for the Null case, and one for when it has a value
14:54:21 <haskellnoob> yes, thanks I will get back to you once that is done ggVGc 
14:57:27 <haskellnoob> here are a few things that I would try to implement, that I think is wrong ggVGc; try to either add a counter in the method, where I count each time i somehow remove one N from my N (N Null) - but that wont work since I can't remove something from the type because it's not a string
14:57:52 <haskellnoob> I basically can't wrap my head around on how to count, when I have a new type which I can't do anything with
14:58:15 <jle`> haskellnoob: like other ADT's, you can start out just by pattern matching and handling every case
14:58:23 <jle`> showNat Null = ????
14:58:26 <Tuplanolla> The point was that if we are given a type t and a functor F, saying F t in the way mathematicians do is ambiguous, because in Haskell our morphisms are also our objects, dminuoso.
14:58:28 <jle`> showNat (N n) = ????
14:58:32 <haskellnoob> yes that is "0" jle` 
14:58:53 <jle`> er wait, are we talking about the same thing
14:59:01 <jle`> "implementing Show" should be showNat Null = "Nll"
14:59:04 <jle`> * "Null"
14:59:19 <jle`> unless you mean you want to convert it to an Int, and then 'show' that Int
14:59:24 <haskellnoob> I guess I forgot to mention that Null is equal to 0
14:59:26 <codeshot> Tuplanolla, our morphisms aren't our objects
14:59:29 <haskellnoob> and N Null is 1
14:59:34 <haskellnoob> and N (N Null) is 2
14:59:35 <jle`> sure, Null and N Null can represent 0 and 1
14:59:36 <haskellnoob> and so on
14:59:41 <jle`> but that doesn't meant that that is what 'show' should do
14:59:43 <haskellnoob> well that is infinite tho
14:59:48 <jle`> in this case it's probably easier to write a Nat -> Int
14:59:55 <haskellnoob> yes yes I know
14:59:56 <jle`> and then just 'show' that resulting Int
15:00:10 <jle`> and so, toInt Null = 0
15:00:15 <jle`> toInt (N n) = ???
15:00:22 <codeshot> I don't know the correct category theoretic terms yet, but our morphisms are elements of some of our objects
15:00:32 <codeshot> which is different from being the objects themselves
15:02:54 <haskellnoob> ok I have a function now that works but I think I'm not implementing it in a clever way
15:03:15 <Tuplanolla> If s is a type and t is a type, then s -> t is also a type, codeshot.
15:03:51 <jle`> haskellnoob: clever is not a really a 'good' thing
15:04:00 <jle`> so if your code is not clever, that's a benefit :)
15:04:04 <haskellnoob> its > "showAsInt:: Nat -> Int -> Int; showAsInt Null a = a; showAsInt (N b) a = showAsInt b (a+1)"
15:04:12 <jle`> the more clever your code is, the worse it is.
15:04:24 <haskellnoob> and i call it with showAsInt (N Null) 0
15:04:39 <haskellnoob> which is basically the same as using a for loop but idk how to solve it otherwise
15:04:52 <jle`> what happened to the template i suggested?
15:04:57 <jle`> toInt Null = 0
15:04:59 <jle`> toInt (N n) = ???
15:05:01 <haskellnoob> yes
15:05:06 <haskellnoob> thats the problem
15:05:16 <haskellnoob> If i just call that recursively with n
15:05:21 <haskellnoob> It will return 0
15:05:29 <jle`> well, yeah
15:05:34 <jle`> but toInt (N n) is not the same as toInt n
15:05:36 <jle`> is it?
15:05:48 <haskellnoob> if n is of type N
15:05:57 <jle`> that's like saying that N (N Null) is the same thing as N Null
15:06:04 <jle`> they aren't...
15:06:09 <jle`> N (N Null) represents 2, right?
15:06:12 <haskellnoob> yes
15:06:14 <jle`> N Null represents 1, right?
15:06:18 <jle`> how do you get 2...from 1?
15:06:25 <jle`> is there like, some math you can do
15:06:38 <haskellnoob> well either add oder multiply
15:06:42 <jle`> if you have `toInt n`, how could you compute `toInt (N n)` ?
15:07:03 <haskellnoob> thats not what I ment
15:07:14 <jle`> if i told you that `toInt n` was 3, what is `toInt (N n)` ?
15:07:48 <haskellnoob> are you trying to give me clues 
15:07:51 <jle`> yes
15:07:55 <jle`> well, you can answer, too
15:07:59 <jle`> and i can tell you if you're on the right track
15:08:11 <phaazon> hey, what is that ghc-nopie stuff?
15:08:20 <phaazon> I can’t compile my packages anymore on my remote server
15:08:32 <phaazon> I get a very weird error message about .rodata and relocatable objects
15:08:53 <ggVGc> oh dear
15:08:55 <phaazon> I guess it stands for no position independent
15:09:00 <phaazon> but what the heck is that?
15:09:19 <ggVGc> it essentially boils down to "static linking on linux is fucked"
15:09:20 <jle`> haskellnoob: if i gave you some Nat x, and told you that x represneted the number 3 ... what would `N x` represent?
15:09:38 <haskellnoob> N(N(N Null)
15:09:41 <phaazon> ggVGc: can you be more explicit?
15:09:45 <haskellnoob> if that is what you actually want to know jle` 
15:09:53 <jle`> haskellnoob: i'm asking for a number
15:09:59 <jle`> if 'x' was 3, what is 'N x' ???
15:10:05 <phaazon> oh
15:10:06 <jle`> is it 18?
15:10:08 <phaazon> Church encoding
15:10:09 <haskellnoob> no
15:10:11 <jle`> is it 183?
15:10:26 <jle`> what is it, then? :o
15:10:35 <phaazon> easy
15:10:36 <phaazon> π!
15:10:42 <haskellnoob> an invalid instance of my new type Nat
15:10:50 <phaazon> oh
15:10:52 <jle`> is it invalid, really?
15:10:56 <phaazon> it’s not invalid
15:11:12 <haskellnoob> well if x is 3
15:11:17 <haskellnoob> and I do N 3
15:11:20 <jle`> if x represented 3, i mean
15:11:25 <jle`> if x was N (N (N Null))
15:11:27 <haskellnoob> ok
15:11:32 <jle`> if x represented 3...what would N x represent?
15:11:34 <haskellnoob> then N with that is 4
15:11:36 <jle`> would it represent....5?
15:11:41 <jle`> ok, good.  4 sounds reasonable
15:11:51 <jle`> what if 'x' represented 50.  what would N x represent?
15:11:53 <phaazon> who wrote that datatype?!
15:12:02 <phaazon> having Null is a bad idea :/
15:12:02 <haskellnoob> fifty one
15:12:06 <jle`> cool
15:12:22 <jle`> do you see the pattern?
15:12:24 <haskellnoob> but I don't see how, when I have the bigger number I can get to the lower one without counting
15:12:27 <haskellnoob> sure I see it
15:12:37 <phaazon> haskellnoob: try to write the code
15:12:42 <haskellnoob> did you read my code?
15:12:46 <jle`> if i gave you 'toInt n', how would you give me toInt (N n) ?
15:12:48 <haskellnoob> the very dumb one
15:13:09 <jle`> how do you get 51 from 50?  how do you get 4 from 3?
15:13:19 <jle`> what is the mathematical operation?
15:13:42 <haskellnoob> come on
15:13:44 <codeshot> Tuplanolla, yes s -> t is a type, but s -> t is not a morphism
15:13:54 <phaazon> I haven’t, but for that exercice, the typical way to kick it off is to start to write toInt for Null
15:14:00 <phaazon> then for N Null
15:14:04 <phaazon> then for N (N Null)
15:14:07 <haskellnoob> yes
15:14:08 <jle`> haskellnoob: you.... add ... one?
15:14:08 <Tuplanolla> What is, then, codeshot?
15:14:08 <phaazon> then generalize
15:14:13 <jle`> does that sound reasonable?
15:14:13 <ggVGc> phaazon: I am no expert. it has to do with gcc mainly I believe. This post seems to detail some things, https://stackoverflow.com/questions/2463150/fpie-position-independent-executable-option-gcc-ld
15:14:14 <ggVGc> phaazon: the issues arise because you have libraries on your system that might or might not be built with PIE
15:14:14 <ggVGc> but, honestly, there seems to be a lot of complications regarding this that I personally don't understand the details of yet
15:14:14 <codeshot> It's an object
15:14:19 <codeshot> id is a morphism
15:14:22 <jle`> so toInt Null = 0
15:14:23 <codeshot> but it's not a type
15:14:39 <jle`> toInt (N n) = (somethign with `toInt n`)
15:14:39 <phaazon> thanks, I’ll have a look ggVGc 
15:14:45 <haskellnoob> yeah jle
15:14:51 <jle`> do you know how to add something to 1
15:14:53 <jle`> in haskell?
15:14:58 <Tuplanolla> Oh, I see what you're saying now.
15:15:01 <haskellnoob> to the number 1?
15:15:03 <haskellnoob> yes?
15:15:06 <jle`> you need to add 'toInt n' to 1
15:15:11 <jle`> to get toInt (N n)
15:15:13 <phaazon> haskellnoob: 
15:15:15 <phaazon> 00:14 < jle`> toInt (N n) = (somethign with `toInt n`)
15:15:33 <haskellnoob> alright let me post my already done code
15:15:39 <haskellnoob> which I posted because I think it's stupid
15:15:46 <phaazon> yeah show it
15:15:50 <phaazon> (your code)
15:15:54 <phaazon> (don’t be that filthy!)
15:16:14 <haskellnoob> showAsInt :: Nat -> Int -> Int; showAsInt Null a = a; showAsInt (N b) a = showAsBin b (a+1)
15:16:15 <haskellnoob> here 
15:16:38 <haskellnoob> it's dumb because I count how often i can call it 
15:16:38 <phaazon> why two args?
15:16:50 <haskellnoob> yeah that's the stupid part
15:17:02 <haskellnoob> it's a counter arg
15:17:06 <phaazon> keep in mind that you’ve encoded a natural number with a Nat
15:17:10 <phaazon> so you want a function Nat -> Int
15:17:15 <jle`> haskellnoob: okay, so, if i gave you 'toInt x', how would you compute toInt (N x) ?
15:17:19 <phaazon> do you know how to convert from a Nat to Int?
15:17:29 <phaazon> for Null it’s pretty straightforward, right?
15:17:53 <haskellnoob> what do you mean "convert", I would call showAsInt Null 0
15:17:56 <haskellnoob> then I get 0
15:18:06 <phaazon> why two args?!
15:18:11 <haskellnoob> because it counts for me
15:18:16 <phaazon> it… counts?
15:18:19 <haskellnoob> yes
15:18:22 <haskellnoob> like a loop
15:18:22 <ggVGc> phaazon: I think the real solution to your issue is that you need to do something related to your distro. E.g this is a compatibility issue, and your distro as a whole should be handling it for you, hopefully
15:18:25 <jle`> haskellnoob: okay, so, how about i named y = toInt x.  how would you get toInt (N x ?)
15:18:29 <jle`> * toInt (N x)
15:18:37 <jle`> it's ... y + 1, right?
15:18:39 <phaazon> ggVGc: it’s a fresh archlinux :/
15:18:48 <ggVGc> haha.. I was suspecting it was archlinux
15:18:51 <phaazon> haskellnoob: do you fully understand what Nat is?
15:18:53 <jle`> toInt (N x) is... toInt x + 1, right?
15:19:02 <ggVGc> hm, wait, I had this issue the other day... I did something to fix it
15:19:03 <ggVGc> ah, wait
15:19:10 <phaazon> ggVGc: I’ll take it!
15:19:11 <haskellnoob> yes jle` 
15:19:14 <ggVGc> phaazon: I have your solution, just let me dig it up. You need a package from the AUR
15:19:14 <jle`> haskellnoob: do you believe me when I say that toInt (N x) = toInt x + 1 ?
15:19:22 <phaazon> ggVGc: wtf happened?!
15:19:28 <haskellnoob> I have no Idea jle`, but I guess
15:19:31 <phaazon> haskellnoob: :/
15:19:34 <phaazon> 00:18 < phaazon> haskellnoob: do you fully understand what Nat is?
15:19:41 <jle`> haskellnoob: that's (toInt x) + 1
15:19:52 <ggVGc> phaazon: try installing this, https://aur.archlinux.org/packages/ncurses5-compat-libs/
15:19:57 <haskellnoob> Nat is a new defined data type? phaazon 
15:20:00 <jle`> like you said earlier...if toInt x was 50, then toInt (N x) was 51
15:20:00 <phaazon> ncurses?!
15:20:03 <phaazon> haskellnoob: yeah
15:20:07 <phaazon> but what does it represent?
15:20:07 <jle`> if toInt x = 3, then toInt (N x) = 4
15:20:16 <haskellnoob> a number phaazon 
15:20:20 <phaazon> yeah!
15:20:23 <phaazon> a natural number
15:20:25 <haskellnoob> yes
15:20:30 <phaazon> something going from 0
15:20:32 <phaazon> to +Inf
15:20:41 <phaazon> data Nat = Z | N Nat
15:20:47 <phaazon> what is Z and what is N Nat here?
15:21:00 <phaazon> or Null, if you name it null
15:21:03 <phaazon> data Nat = Null | N Nat
15:21:14 <haskellnoob> exactly phaazon, the second version
15:21:21 <phaazon> ok, so, explain to me
15:21:26 <phaazon> how do you understand Null here?
15:21:27 <phaazon> what is it?
15:21:32 <phaazon> and what is that N Nat stuff?
15:21:36 <haskellnoob> yeah I know it's 0
15:21:49 <haskellnoob> sorry
15:21:51 <phaazon> it’s “0 encoded as a Nat”, yes
15:21:55 <phaazon> what is N Nat?
15:22:33 <haskellnoob> it's hard to put in words for me
15:22:38 <phaazon> basically, that type is a complete definition of natural numbers, because you have a constructor to build one natural number, 0, and a constructor to build EVERY OTHER natural numbers… so what does it represent?
15:23:13 <phaazon> ok ok
15:23:18 <phaazon> let’s rephrase 
15:23:23 <haskellnoob> I understand jle` s solution, I just was too confused on how to "count" when it's a new number, thanks for that btw jle` 
15:23:28 <haskellnoob> now for your theoretical question
15:23:29 <jle`> no problem!
15:23:30 <phaazon> how would you, personnaly, define natural numbers, without quoting them all?
15:23:36 <jle`> i'm not sure what you mean by counting
15:23:49 <jle`> since that's already the full solution :)
15:23:59 <ggVGc> jle`: counting is the act of giving a number to a set of items
15:24:05 <jle`> toInt Null = 0; toInt (N n) = toInt n + 1
15:24:16 <jle`> ggVGc: ty ggVGc 
15:24:19 <ggVGc> np
15:26:05 <haskellnoob> well jle`, how to increase the value on each recursion probably defines my stupid problem the best
15:26:24 <jle`> there is no value to increase
15:26:26 <ggVGc> don't worry, many programmers don't understand recursion
15:26:31 <haskellnoob> for phaazon I know what "Null | N Nat" means and stands for 
15:26:33 <jle`> we aren't really...increasing anything
15:26:37 <jle`> were *defining* things
15:26:39 <phaazon> haskellnoob: tell me then
15:26:42 <phaazon> if you tell me
15:26:44 <phaazon> you’ll get it all
15:26:52 <ggVGc> haskellnoob: you're just thinking about it in a weird way. Think about it in the sense that one thing is the result of another thing
15:26:55 <jle`> `toInt Null` is *defined* as 0.  `toInt (N n)` is *defined* as `toInt n + 1`
15:26:55 <ggVGc> and it becomes easier
15:27:00 <jle`> there isn't any incrementing happening
15:27:09 <ggVGc> haskellnoob: you're just specifying relationships between things
15:27:26 <haskellnoob> ok and +1 is not increasing something?  jle` 
15:27:41 <phaazon> haskellnoob: data Nat = Z | N Nat means that a Nat is defined by either 0 or by incrementing something that is already a Nat
15:27:46 <jle`> haskellnoob: yes, we're defining that whatever `toInt n` is, `toInt (N n)` is + 1 of that
15:27:47 <ggVGc> haskellnoob: you're saying that something is something else + 1
15:27:51 <ggVGc> and that covers all the cases after that
15:27:53 <phaazon> the only thing besides incrementing something we defined is… 0
15:27:57 <haskellnoob> phaazon: an object of type Nat is either Null or N with another object of type Nat 
15:28:01 <phaazon> so can only build nat with 0 and x + 1
15:28:19 <phaazon> haskellnoob: you didn’t get it
15:28:23 <haskellnoob> yeah but I only know that after the fact of jle` explaining it to me for 1 hour
15:28:28 <phaazon> « n with another object of type Nat »
15:28:45 <phaazon> the N is for “next”, for instance
15:29:00 <phaazon> so 
15:29:03 <phaazon> toInt (N x) = …
15:29:29 <phaazon> « I wanna transform a Nat to an Int, and I know that this Nat is the next of another Nat… so let’s just take the Int value of that other Nat, and add 1 »
15:29:39 <haskellnoob> yes phaazon 
15:29:47 <phaazon> and by recursion
15:29:53 <haskellnoob> I understand on how you are doing it
15:29:55 <phaazon> you eventually end up on:
15:29:56 <haskellnoob> technically 
15:30:09 <phaazon> “I wanna transform a Nat to an Int, and I know that this Nat is Z, which is 0 »
15:30:12 <phaazon> no more recursion
15:30:19 <haskellnoob> I also know how recursion works anyway
15:30:24 <phaazon> you have a looooooooong chain of 1 + 1 +  + … + 0
15:30:27 <haskellnoob> yes
15:30:35 <phaazon> haskellnoob: ok so what’s the problem then? :D
15:30:55 <haskellnoob> well there is none really
15:31:01 <phaazon> eh
15:31:02 <haskellnoob> just that my previous solution was bad
15:31:17 <phaazon> I hope you have more intution thanks to what jle` told you then
15:31:45 <jle`> haskellnoob: it's not 'bad', it's just like...an imperative solution :)
15:32:06 <haskellnoob> I know that it was imperative
15:32:52 <haskellnoob> It's just how I would have done it in another language
15:33:24 <haskellnoob> like ggVGc said I thought about it "in a weird way"
15:33:25 <ggVGc> if you try to be imperative in haskell, you're gonna have a tough time
15:33:38 <ggVGc> but it's always hard to switch your thinking in the beginning
15:33:45 <ggVGc> it'll click eventually
15:34:42 <haskellnoob> I hope it will ggVGc 
15:34:47 <haskellnoob> thanks for the help guys
15:35:16 <ggVGc> haskellnoob: try to think about things in the way that you are defining relationships between things, rather than "calculating things"
15:35:21 <ggVGc> not sure if that makes sense
15:35:22 <ggVGc> but it will
15:35:57 <wz1000> could it every be a good idea(with regards to performance), to represent arrays as functions (Integer -> a) from an index to a value?
15:36:39 <jle`> wz1000: it depends on what sort of operations you want to do with it
15:36:50 <jle`> it is efficient for some operations and inefficient for some others
15:37:02 <jle`> as opposed to continguous-memory arrays
15:37:06 <ggVGc> wz1000: if I understand correct, that's just a lookup table, and it's how things like sin and cos are often implemented
15:37:29 <ggVGc> (in hardware)
15:37:59 <wz1000> you get fast insertion, append etc. with this
15:38:12 <ggVGc> hm, maybe I misunderstood
15:38:12 <wz1000> but O(no of updates) indexing
15:39:42 <wz1000> something like append a b = \i -> if i < length a then a i else b (i - length a)
15:40:02 <wz1000> of course, you would need to keep track of length seperately
15:40:15 <Tuplanolla> Congratulations, you reinvented the `D` representation of Repa, wz1000.
15:45:13 <ggVGc> phaazon: did you try installing that library? did it help?
15:45:25 <ggVGc> I had the same issue as you on arch a month ago, and that fixed it for me
15:45:34 <ggVGc> I'm not sure exactly why...
15:45:35 <ggVGc> but it did
15:46:19 <ryantrinkle> is there an alias for [minBound..maxBound] :: (Enum a, Bounded a) => [a] ?
15:47:01 <jle`> there isn't, and i think the main reason is that it doesn't mean anything
15:47:02 <geekosaur> no predefined alias but it's just enumFrom minBound
15:47:30 <ryantrinkle> jle`: how do you mean?
15:47:35 <jle`> the Enum/Bounded laws aren't strong enough to tell you *anything* about [minBound .. maxBound]
15:47:39 <ryantrinkle> geekosaur: good point
15:48:06 <jle`> it doesn't cover the entire type
15:48:09 <ryantrinkle> jle`: i don't get it; if i have a simple enumeration type, and i just want to get all of its values, is there a more straightforward way?
15:48:23 <jle`> yes it makes sense for specific types
15:48:25 <jle`> like enumeration types
15:48:30 <geekosaur> well, given a well behaved type at least. given that Double has an Enum instance, jle` is correct
15:48:32 <jle`> but not for all Enum's and Bounded's
15:49:05 <wz1000> ryantrinkle: universe/universeF from Data.Universe
15:49:16 <geekosaur> athough I don't think we claim Bounded for Double. but the fact that we allow such Enum instances means there's no general definition
15:49:44 <ryantrinkle> jle`: fair enough, although i think you could apply that logic to Enum alone as well - that thing is pretty busted
15:49:45 <jle`>  [minBound .. maxBound] is a reasonable way to do it for enumeration types, but, having a general alias for all Enum/Bounded is where it's kind of weird
15:49:50 <ryantrinkle> otoh, Eq Double is also busted
15:49:53 <jle`> mhm
15:50:07 <ryantrinkle> but i'm not gonna stop abstracting over Eq :P
15:50:25 <jle`> i mean, this doesn't stop an alias for [minBound .. maxBound] from being useful
15:50:30 <jle`> but it just makes me a bit queasy :)
15:50:33 <phaazon> ggVGc: ==> Verifying source file signatures with gpg...
15:50:34 <phaazon>     ncurses-6.0-20170902.tgz ... FAILED (unknown public key 702353E0F7E48EDB)
15:50:37 <ryantrinkle> wz1000: cool, i'll check that out :)
15:50:45 <phaazon> by using the link you provided
15:50:52 <phaazon> I won’t install that 
15:51:00 <ryantrinkle> jle`: yeah, it's nbd, i was just curious :)
15:51:18 <ryantrinkle> in a sane world, perhaps it could be called, you know, "enumerate"
15:51:21 <ggVGc> phaazon: read the first comment on the AUR page
15:51:32 <ryantrinkle> but i'll grant that that's an overly nice name for something with so few laws, lol
15:51:51 <wz1000> https://hackage.haskell.org/package/universe-1.0/docs/Data-Universe.html
15:52:12 <haskellnoob> adding onto my previous exercise, if I was to implement Eq, I would intuitively solve it with x == y = asInt(N x) == asInt(N y) - it works, but is it a good solution?
15:52:29 <phaazon> ggVGc: I still don’t quite get why is it relevant to my issue.
15:52:46 <phaazon> and if it is, I find it very suspicious, so I’ll go read some code about it
15:52:58 <jle`> haskellnoob: it's not a bad solution, but you can be "more efficient"
15:53:04 <jle`> if you pattern match for (==)
15:53:15 <jle`> list all of the possible patterns
15:53:42 <ggVGc> phaazon: it has to do with system libraries as I said. But yeah, read up on it. Basically your haskell code links to libraries that are incompatible, and this package supplies the correct ones
15:53:59 <jle`> there's a version of (==) you can write that can be more efficient, and can also work in the case where one of the inputs is infinite and the other is finite
15:54:03 <phaazon> I don’t even use ncurses!
15:54:12 <phaazon> why should I care about that?
15:54:15 <phaazon> seriously wtf
15:54:25 <jle`> haskellnoob: in the version you gave, (Nat for infinity) == (Nat for 3) will never return an answer
15:54:50 <jle`> even though we can 
15:54:53 <geekosaur> phaazon, do you use ghci?
15:55:05 <phaazon> geekosaur: yes
15:55:19 <geekosaur> line editing requires the terminfo component of ncurses
15:55:28 <phaazon> oh fuck.
15:55:38 <phaazon> I don’t like adding that kind of thing from the AUR
15:55:38 <geekosaur> most linuxes don't build terminfo and ncurses separately (but do often have a compatibility symlink)
15:55:51 <haskellnoob> jle`: let me try to "pattern match": Null == Null = True; but how am I supposed to compare anything above without calculating its "real" value ?
15:55:53 <jle`> haskellnoob: just think of all the possible combinations of patterns you can use for two inputs
15:55:59 <jle`> haskellnoob: think of the patterns
15:56:06 <jle`> N x == Null = ???
15:56:12 <jle`> Null == N y = ???
15:56:15 <jle`> N x == N y = ???
15:56:24 <haskellnoob> I could just write False on the middle two?
15:56:30 <jle`> mhm
15:56:58 <haskellnoob> now to the part which apparently I can't get in my head (again), how do i compare them, if not with their respective value (asInt)
15:57:19 <jle`> well, let's look at (N x) == (N y)
15:57:33 <jle`> let's say i tell you that x == y is True
15:57:39 <haskellnoob> ah
15:57:41 <jle`> then what can you say about (N x) == (N y) ?
15:58:01 <jle`> and, what if i told you that x == y was False.  what does that tell you about N x == N y ?
15:58:08 <haskellnoob> I guess it's either to late in the night already or I'm just really dumb today
15:58:11 <haskellnoob> (or always)
15:58:24 <jle`> i mean, this stuff does take some getting used to :)
15:58:55 <phaazon> I wonder whether there’re viruses in the AUR
15:59:00 <phaazon> I’m pretty sure there are.
15:59:16 <haskellnoob> thanks very much jle`, again
15:59:40 <jle`> no problem :)
15:59:49 <phaazon> jle`: at work, I have to present and teach Haskell to people that have never done anything else but C#
16:00:09 <phaazon> it’s… incredible how I feel “far” from the steep learing curve I was fighting against years ago
16:00:17 <phaazon> like, “come one, it’s completely logical!”
16:00:19 <jle`> haskellnoob: but yeah, notice that becuase Haskell is lazy, this lets you "short circuit" without stepping through the entire type
16:00:34 <phaazon> whilst 6 / 7 years ago, I was like “WHAT THE FIX IS THAT FIX FUNCTION?!”
16:00:37 <phaazon> :D
16:00:43 <phaazon> s/FIX/F*CK*/ for the first
16:00:50 <jle`> phaazon: heh yeah, it's easy to forget how differently things used to seem ><
16:00:56 <phaazon> yeah
16:01:02 <phaazon> it’s an interesting bias
16:01:08 <jle`> haskellnoob: for example, for something like 100 == 0, you can "finish" immediately
16:01:22 <jle`> haskellnoob: because you get something like N (N ....) == Null = False
16:01:33 <jle`> and you don't have to peek into the internal number at all
16:01:39 <phaazon> jle`: nooo
16:01:41 <ggVGc> I tried using Fix but my software still had bugs :(
16:01:42 <phaazon> N _ == Null
16:01:53 <phaazon> N Null == Null would short circuit as well ;)
16:02:07 <jle`> i'm comparing to their original answer
16:02:13 <jle`> which was x == y = toInt x == toInt y
16:02:16 <phaazon> 01:01 < jle`> haskellnoob: because you get something like N (N ....) == Null = False
16:02:19 <phaazon> I meant this
16:02:24 <phaazon> you said N (N _)
16:02:30 <phaazon> which is true but not the more general 
16:02:39 <jle`> heh yeah.  mostly i got lazy trying to type out 100
16:02:43 <jle`> but def :)
16:02:44 <phaazon> :D
16:03:01 <phaazon> now
16:03:05 <phaazon> encode Float.
16:03:10 * phaazon sneaks out
16:03:24 <jle`> haskellnoob: also note you can even handle infinite numbers this way, since x == y will quit with False as soon as it sees a single Null
16:03:27 <haskellnoob> oh that is nice phaazon , so I can use N _ == Null for any N _ to return False
16:03:28 <phaazon> ggVGc: ok, I’m installing the lib
16:03:36 <haskellnoob> thats pretty cool
16:03:39 <phaazon> haskellnoob: exactly :)
16:03:47 <phaazon> in Haskell, _ means “whatever”
16:04:06 <phaazon> or “tell me about that type” if you use the TypeHoles extension
16:04:12 <phaazon> but don’t worry about that
16:04:15 <phaazon> you’ll get to it :D
16:04:30 <phaazon> but
16:04:31 <phaazon> you know
16:04:38 <phaazon> you can be ever more pragmatic, haskellnoob 
16:04:43 <phaazon> _ == Null = Null
16:04:46 <phaazon> Null == _ = Null
16:04:52 <phaazon> N a == N b = a == b
16:04:54 <phaazon> tada
16:05:03 <haskellnoob> believe me or not, I know about the underscore, but I did not once think about using it
16:05:06 <phaazon> (you don’t need parens)
16:05:12 <phaazon> haskellnoob: use it!
16:05:14 <haskellnoob> that's an even sweeter solution, thanks
16:05:15 <phaazon> it’s a very good ally :)
16:05:35 <phaazon> it’s powerful how “simple thoughts” resolve to the best solution in Haskell
16:05:38 <phaazon> (not always, but often)
16:05:53 <haskellnoob> oh and because I have Null = Null above the underscore cases that one clicks first right
16:05:59 <phaazon> ggVGc: ok, the library is installed
16:06:01 <haskellnoob> genius
16:06:02 <phaazon> let’s test… :D
16:06:10 <phaazon> haskellnoob: yep
16:06:15 <phaazon> you cover all the patterns with that
16:06:23 <phaazon> you have to write both the first lines though
16:06:31 <phaazon> because Haskell doesn’t know the (==) is commutative
16:06:54 <phaazon> (it could be not… why not)
16:07:34 <haskellnoob> alright I got both covered, thanks a lot again
16:09:00 <phaazon> ggVGc: https://gist.github.com/phaazon/365702e637c7f4ab4c2affeed8188091
16:09:02 <phaazon> I wanna cry.
16:09:52 <haskellnoob> just because I just tested it, how come that when I try Null /= Null it returns False even tho I didn't implement it?
16:10:12 <phaazon> haskellnoob: that is because in Haskell, you can provide default implementations
16:10:20 <phaazon> the Eq typeclass has two default implementations
16:10:29 <phaazon> a == b = not (a /= b)
16:10:34 <phaazon> a /= b = not (a == b)
16:10:40 <haskellnoob> oh and that works automatically?
16:10:42 <phaazon> that means you can implement the one you want
16:10:45 <phaazon> if you provide one
16:10:46 <haskellnoob> ohhhhhh
16:10:50 <phaazon> it has enough to build the other
16:10:55 <haskellnoob> that's nice
16:10:59 <phaazon> but you can provide the other one if you want 
16:11:23 <phaazon> https://hackage.haskell.org/package/base-4.10.0.0/docs/Data-Eq.html#t:Eq
16:11:26 <phaazon> you can see the hints here:
16:11:30 <phaazon> Minimal complete definition
16:11:30 <phaazon> (==) | (/=)
16:11:59 <phaazon> you can actually write code with that default feature if you want to
16:12:11 <phaazon> if you have a typeclass of your own, I mean :)
16:12:21 <phaazon> it’s “nice” but sometimes it’s not
16:12:47 <phaazon> at work I have an example of a typeclass we use that has a default that will make the code loop infinitely if you don’t provide an implementation
16:12:50 <phaazon> that’s so silly
16:13:09 <phaazon> and it’s froooooooooom
16:13:12 <phaazon> seeeeeeeeervaaaaaaaaaaaaaaaaaaaaant
16:13:12 <phaazon> :D
16:13:15 <phaazon> alp: ^
16:13:33 <jle`> doesn't Eq fall under that tent?
16:13:46 <phaazon> jle`: if you provide none, yeah
16:13:54 <phaazon> huh
16:13:54 <phaazon> no
16:13:56 <phaazon> it won’t
16:14:01 <phaazon> jle`: it won’t compile
16:14:07 <phaazon> because of the “minimal complete definition” rule
16:14:14 <jle`> oh i thought those were only warningsp
16:14:20 <phaazon> are they?
16:14:27 <jle`> also i'm not sure how that typo happened
16:14:34 <phaazon> which typo?
16:14:35 <phaazon> ah
16:14:36 <phaazon> :D
16:14:43 <jle`> @let data MyType
16:14:44 <phaazon> what keyboard layout are you using?
16:14:45 <lambdabot>  Defined.
16:14:48 <jle`> @let instance Eq MyType
16:14:49 <lambdabot>  .L.hs:164:10: warning: [-Wmissing-methods]
16:14:49 <lambdabot>      • No explicit implementation for
16:14:49 <lambdabot>          either ‘==’ or ‘/=’
16:14:54 <phaazon> it’s a warning :(
16:14:57 <phaazon> bad GHC, bad!
16:15:00 <jle`> just normal qwerty
16:15:35 <phaazon> https://hackage.haskell.org/package/http-api-data-0.3.7.1/docs/Web-Internal-HttpApiData.html#t:FromHttpApiData
16:15:38 <phaazon> jle`: ^
16:15:46 <phaazon> it’s okay because my colleagues and I read the docs
16:15:49 <phaazon> but… it’s silly
16:15:50 <phaazon> :D
16:15:59 <jle`> lol
16:16:10 <ivans> reading docs is silly, I agree
16:16:13 <jle`> i...don't think i'd ever use DeriveAnyClass
16:16:15 <jle`> for anything
16:16:30 <phaazon> jle`: I don’t either, but a colleague does
16:16:40 <phaazon> I use GeneralizedNewtypeDeriving, or Generics
16:18:06 <phaazon> good night!
16:18:11 <phaazon> good luck with the learning haskellnoob :)
16:18:38 <qqkami> hello, what does the -> in a type declaration mean / do? e.g. type foobar = foo -> bar
16:18:55 <qqkami> can't really wrap my head around what it means in hte context of a type
16:19:04 <phaazon> it’s a simple function
16:19:13 <ivans> (a -> b) is the type of a function that takes a parameter of type a
16:19:19 <ivans> and returns a vallue of type b
16:19:28 <qqkami> so foobar is a type of functions?
16:19:32 <ivans> yes
16:19:34 <qqkami> ahh
16:19:35 <qqkami> thanks
16:19:40 <ivans> foobat takes one parameter of type foo
16:19:42 <phaazon> Foobar*
16:19:46 <jle`> qqkami: type is like a lexical alias
16:19:52 <ivans> and the return value is of type bar
16:20:08 <jle`> `type FooBar = Foo -> Bar` is a type alias... whenever haskell sees FooBar, it replaces it with 'Foo -> Bar'
16:20:29 <qqkami> oh, thanks jle` and ivans - makes sense!
16:20:43 <jle`> no problem :)
16:20:46 <jle`> yeah it's not any special syntax
16:21:01 <jle`> 'Int -> Bool' is just a normal type, like Int or Bool or [Char] or anything
16:21:20 <zachk> @type (+)
16:21:21 <lambdabot> Num a => a -> a -> a
16:21:23 <jle`> although you won't be blamed for thinking it is special syntax, since -> is one of the most overloaded tokens in haskell syntax
16:21:33 <koz_> I'm getting some weirdness from Liquid Haskell. When I run 
16:21:53 <koz_> 'stack exec liquid src/Foo.hs' on a file with no Liquid Haskell annotations, I get this:
16:22:21 <koz_> http://lpaste.net/360054
16:22:26 <koz_> What's going on here?
16:22:29 <haskellnoob> thanks phaazon, good night!
16:22:41 <jle`> night phaazon !
16:22:57 <ivans> using the same syntax for values and types can be confusing at first, even if it makes sense after you "get it"
16:23:15 <ivans> the * in C is a good example, and to some extent -> in haskell is similar
16:23:59 <jle`> haskell has a lot of really overloaded tokens, heh
16:24:27 <haskellnoob> jle`: for instance Ord Nat, can I work with asInt here, or is there a better way you can think of? and if yes, why?
16:24:42 <jle`> you can probably do it using pattern matching too
16:24:57 <jle`> and for the same reasons --- you can 'quit' earlier and be moer efficient, and then also handle infinite inputs in some situations
16:25:10 <haskellnoob> alright I guess I'll do it for learning purposes jle`  thanks
16:27:25 <lyxia> koz_: You'll probably have more luck on the LH slack channel.
16:28:06 <koz_> lyxia: Thanks.
16:29:04 <haskellnoob> jle`: can I list all cases where it returns true, and otherwise false?
16:30:16 <jle`> haskellnoob: yeah that's one way to do it
16:30:24 <jle`> i like just listing all 4 possibilities though
16:30:36 <jle`> it's good practice in general
16:30:51 <jle`> because for some types, if you ever decide to add new constructors, GHC will warn you
16:31:06 <haskellnoob> when I implement compare tho, there are 16 cases ? but I guess that I'm thinking too far again?
16:31:08 <jle`> but if you have wildcards, those pattern matches will still compile
16:31:15 <jle`> haskellnoob: if you have two inputs, there are only 4 cases
16:31:28 <jle`> Null Null, N Null, Null N, N N
16:31:35 <haskellnoob> yeah but for <, <=, >=, and > ?
16:31:43 <jle`> same, since they both only take two inputs
16:32:06 <haskellnoob> how is it not 16 cases when I have 4 operators and 2 inputs ?
16:32:27 <ivans> 4 cases per operator
16:32:33 <jle`> well for each of the operators you have 4 cases
16:32:33 <haskellnoob> ok
16:32:39 <haskellnoob> so in sum... 16 
16:32:39 <ttrx> anyone wanna fight
16:32:45 <ivans> ttrx: fight me
16:32:50 <lyxia> Maybe it could be useful to be able to make a new constructor temporarily unmatcheable by wildcards so we get incomplete pattern warnings again.
16:32:54 <jle`> haskellnoob: but you can define >, <=, >=, and > in terms of compare
16:33:08 <haskellnoob> with that LT, GT thing? jle` 
16:33:13 <jle`> yeah
16:33:15 <dsal> :t compare
16:33:17 <lyxia> Not sure how to deal with situations where the type is nested though.
16:33:17 <lambdabot> Ord a => a -> a -> Ordering
16:33:44 <ivans> compare is very natural to define
16:33:52 <ivans> for the 4 cases
16:34:09 <Welkin> 4?
16:34:10 <ttrx> Is there an HTTP library for Haskell?
16:34:23 <dsal> ttrx: yes.
16:34:23 <Welkin> LT, LTE, GT, GTE
16:34:27 <Welkin> okay
16:34:37 <Welkin> what about just EQ
16:34:48 <Welkin> oh
16:34:50 <haskellnoob> you can work with EQ LT and GT 
16:34:52 <dsal> You can't have both LTE and GTE.  That'd be confusing.
16:34:57 <Welkin> for Ordering it is just LT, GT, or EQ
16:35:16 <ivans> Welkin: I meant 4 cases in terms of the parameters
16:35:28 <ivans> 2 parameters, 2 forms for each
16:35:38 <ivans> 3 return values of course
16:37:54 <dsal> Data.Maybe has an Ord instance.  Not sure how to @src that, though.
16:38:03 <Welkin> @src is made up
16:38:03 <lambdabot> Source not found. Sorry.
16:38:06 <Welkin> it's not the real source
16:38:13 <Welkin> look for the source on hackage
16:38:27 <dsal> Oh, it's derived.
16:38:45 <dsal> @src compare
16:38:45 <lambdabot> compare x y | x == y    = EQ
16:38:45 <lambdabot>             | x <= y    = LT
16:38:45 <lambdabot>             | otherwise = GT
16:38:54 <dsal> heh.  thanks
16:39:38 <dsal> Anyway, Data.Maybe does something sensible for Ord.  Finding out what that is somehow isn't as easy as I'd like.
16:39:40 <ivans> interesting that that's using <= instead of <
16:39:58 <ivans> I guess it aligns better
16:46:54 <haskellnoob> jle`: I don't understand how compare x y can have the case x <= y, when <= is not even implemented yet, (from this site http://zvon.org/other/haskell/Outputprelude/Ord_c.html)
16:48:43 <lyxia> haskellnoob: you're supposed to implement at least one of them
16:48:46 <geekosaur> you can provide either <= (which in conjunction with the required Eq instance gives you compare and thereby the other comparison methods), or you can provide compare (and the other comparison methods come from that)
16:49:15 <geekosaur> if you don't provide either, then the typeclass instance won't work (will go into an infinite loop)
16:50:37 <haskellnoob> how is <= implemented in the link I sent lyxia ? (it's not?)
16:51:50 <ivans> haskellnoob: the type class definition doesn't need to define it
16:51:54 <ivans> instance definitions do
16:52:05 <ivans> for example, you can see that for Ord Char, it's defined
16:52:20 <ivans> type class definitions can have "circular defintions"
16:53:34 <ivans> the other instance definitions are unfortunately not included in that link
16:54:39 <haskellnoob> so lets say i do instance Ord Nat where, then copy the code block from the provided link, I write another function (<=) x y, and define that for all cases, then I'm done?
16:55:02 <ivans> yes, defining (<=) is enough to create an instance of Ord
16:55:17 <haskellnoob> but I need the code block from the link right?
16:55:36 <ivans> the definition of Ord is part of the Prelude
16:55:52 <ivans> you should be able to define an instance of Ord without including anything
16:55:58 <qqkami> i have to code a natural number data type and have it represented as binary strings (via show) - this is what i have so far https://pastebin.com/sM1ncJeS - i'm somewhat stuck and would appreciate any pointers - also if there are any grave mistakes please let me know
16:56:25 <haskellnoob> seems fun qqkami
16:56:25 <ivans> omg are you both in the same class
16:56:47 <haskellnoob> what a coincidence
16:56:51 <ivans> xd
16:57:19 <qqkami> wait what - LOL
16:57:51 <haskellnoob> I like how qqkami just compares the numbers instead of taking the high road xD
16:57:53 <ivans> maybe you two should get a room
16:57:58 <hpc> lol
16:58:33 <haskellnoob> qqkami: do you hate the exercise as much as me?
16:58:38 <ivans> qqkami: what is the part you're stuck at
16:58:40 <Welkin> what is this?
16:58:46 <Welkin> a haskell class?
16:58:49 <Welkin> what school is this?
16:58:55 <haskellnoob> it's university
16:58:59 <ivans> Welkin: haskell school of functional programming
16:59:00 <Welkin> which one?
16:59:18 <ivans> damn I remembered the title wrong
16:59:27 <ivans> *haskell school of expression (is a book)
16:59:29 <hpc> Welkin: https://www.youtube.com/watch?v=7ffj8SHrbk0
17:00:04 <qqkami> haskellnoob: somewhat - it's kind of annoying that the script is kinda useless - exercise two is quite easy though
17:00:36 <haskellnoob> qqkami: I'd hope the 2nd one is easier, I'm wasting all my time on the 1st one
17:01:16 <ivans> hpc: you have no idea how well that fit with the bgII sound track I have playing
17:01:29 <qqkami> its a functional programming class and we use haskell Welkin - the lecture itself however is.. well.. somewhat disappointing imo
17:02:08 <Welkin> what university?
17:02:45 <haskellnoob> before qqkami answers that, would you give the internet information about your location?
17:02:55 <ivans> huh
17:03:04 <Welkin> ?
17:03:12 <haskellnoob> ?
17:03:24 <haskellnoob> didn't that make sense to u?
17:03:29 <Welkin> wtf
17:03:30 <ivans> why would you ask that
17:03:36 <ivans> is the wtf
17:04:02 <haskellnoob> uhhh idk what if I asked where you live
17:04:02 <hpc> "didn't that make sense to" is a weird name for a university ;)
17:04:03 <Welkin> trace my ip address if you want to know
17:04:05 <Welkin> it's not hard
17:04:24 <ivans> I can do it in visual basic
17:04:26 <qqkami> i think hes saying he doesnt want to mention his university 
17:04:39 <qqkami> i wouldnt exactly mind but i'll respect his choice 
17:04:43 <qqkami> his or her
17:04:54 <ivans> the thing is, a link to the exercises helps a lot in answering questions
17:05:11 <ivans> if the course has a website
17:05:22 <haskellnoob> I feel like cheating if I provide the exercises if we're being honest
17:05:27 <Welkin> I only know of a few universities that have haskell courses
17:05:33 <qqkami> also we aren't exactly allowed to do that i think
17:05:35 <Welkin> I am always curious to see how that grows
17:05:36 <ivans> I've heard of quite a few
17:05:54 <Welkin> there is no reason to be an ass about it haskellnoob 
17:06:18 <ivans> in general I don't have a problem with helping people get the university work done
17:06:25 <Welkin> first of all, you are here asking us for help on your homework assignments
17:06:28 <haskellnoob> Welkin: Im sorry if that came across in such a way
17:06:40 <ivans> the people who come to IRC with questions usually care a bit about learning the stuff
17:07:28 <qqkami> not exactly sure if thats the case for me - i'm kinda hard stuck so i decided to take my chances haha
17:07:33 <haskellnoob> for qqkami, since I already got the help from here, for Show, use pattern matching
17:07:35 <hpc> qqkami: back on topic, i think you have some bugs in your math code, i suggest trying it out in ghci
17:07:47 <hpc> qqkami: try something like 5 + 5
17:08:14 <haskellnoob> oh no qqkami I'm misled, does your code work in any way?
17:08:23 <ivans> :D
17:08:43 <ivans> I think rephrasing the homework questions in a way that gets answer from IRC is a good task on its own
17:09:15 <ivans> but I went to uni before IRC even existed so I wouldn't know
17:09:20 <hpc> haskellnoob / qqkami: did you learn how to use ghci yet?
17:09:32 <haskellnoob> we have to use hugs hpc 
17:09:34 <haskellnoob> "have"
17:09:39 * hpc dies inside
17:09:43 <qqkami> haskellnoob: ok i'll take a look at that - thanks and i have another set of instance Num Nat implementation stha somewhat work - couldnt exactly test them though
17:09:43 <haskellnoob> it is recommended because it is tested with hugs
17:09:47 <qqkami> hpc we actually use hugs
17:09:57 <ivans> hugs :|
17:10:01 <qqkami> yeah ;/
17:10:26 <ivans> I remember that hugs was very inferior to ghc 10 years ago
17:10:29 <hpc> if it's not too far out of the way, i suggest installing ghc and using that to get your code prototyped
17:10:30 <ivans> when I first learned haskell
17:10:33 <haskellnoob> qqkami: uh no wait, do you need help with Show? because that's not where u need pattern matching
17:10:48 <ivans> I can only assume the disparity has grown
17:11:06 <haskellnoob> idk if the rest of your code is working already tho, so I'm quite dumbfounded atm qqkami 
17:11:19 <hpc> then you can run "ghci yourfile.hs" to load it in a REPL
17:11:41 <hpc> you can write expressions to evaluate them, like Null + Null
17:11:52 <ivans> I agree that ignoring hugs at first is a good idea
17:11:58 <hpc> and when you make edits to the file based on that, you can use :r to reload it
17:12:04 <ivans> if you have code that works with ghc, then check it with hugs
17:12:09 <hpc> you'll be able to test waaaaaaaaaay faster
17:12:24 <haskellnoob> if you believe it or not, this is the 4th exercise, first three were quite a bit easier (for me)
17:12:25 <ivans> ghci is one of the big strengths of haskell
17:12:28 <jle`> hugs :(
17:12:32 <qqkami> haskellnoob: it seems like it works at least (with the other set of + and - implementations) - i was thinking the simle N x + N y = N (x + y) might work if i change something else but iguess it doesnt
17:12:51 <qqkami> haskellnoob: but i have no idea on how to go about the show thing - its not like we can use modulo or something
17:12:57 <ivans> the exercises seem simple enough that the ghci/hugs difference might not matter in the end
17:13:03 <haskellnoob> qqkami: what I learned here is that there are two ways to solve this assignment 
17:13:06 <jle`> `N x + N y = N (x + y)` doesn't sound...right...
17:13:20 <ivans> jle`: that's last millenium
17:13:37 <haskellnoob> implement a toInt function, and call that on every N for every instance you are trying to implement
17:13:38 <hpc> ivans: if they're having enough trouble to ask here, teaching them to fish is going to serve them better for the rest of the course
17:13:49 <ivans> hpc: affirmative
17:14:08 <haskellnoob> or do it the "hard" way with pattern matching and some stuff qqkami 
17:14:13 <hpc> hugs was last updated in 2008 or so
17:14:14 <ivans> I think "fishing" means "use ghci" here
17:14:26 <haskellnoob> so qqkami for Show, you need a toInt function
17:14:43 <haskellnoob> toInt :: Nat -> Int
17:14:44 <qqkami> haskellnoob: i agree that they were easiert (curry stuff was somewhat weird though) - and as mentioned.. the second task is easier ^^
17:15:07 <qqkami> haskellnoob: okay, thanks for the tip and i'll also look at pattern matching to think about another solution
17:15:32 <hpc> haskellnoob / qqkami: so yeah, there's a core loop that programmers get into which is write code, compile, test, repeat
17:15:46 <hpc> and what ghci ultimately gets you is the ability to go through that loop really really fast
17:16:11 <haskellnoob> hpc: what I do is, I have notepad open where I edit code, then I do :r in WinHugs, and then I can test 
17:16:13 <hpc> when you get really into it with this many lines of code, something like 10-20 seconds per iteration
17:16:22 <hpc> oh, hugs has a repl too?
17:16:27 <ivans> yyes
17:16:32 <haskellnoob> what's a repl
17:16:37 <ivans> it was one of the big selling points of hugs back in the day
17:16:39 <hpc> read-eval-print-loop
17:16:40 <ivans> a good repl
17:16:53 <ivans> but ghci eclipsed it
17:16:55 <haskellnoob> oh okay
17:17:09 <hpc> the term gets its etymology from lisp, but it's basically what ghci and i guess winhugs are
17:17:34 <hpc> but yeah if you have even a basic one you've satisfied the point i was trying to make
17:17:45 <qqkami> haskellnoob: do you still attend the lectures btw?
17:18:03 <qqkami> sadly it's really hard to understand our lecturer.. even though he uses a mic
17:18:21 <hpc> i think the advice i would give for your (Nat -> Int) function is to look at the (Int -> Nat) function you've already written and try doing it in reverse
17:18:56 <haskellnoob> qqkami: i was in the first two lectures, but I don't attend anymore
17:19:10 <hpc> using constructors turns into pattern matching, (+) turns into (-), f (g x) turns into ginverse (finverse x)
17:19:13 <haskellnoob> first it's at 8 o clock in the morning 
17:19:22 <qqkami> haskellnoob: yes, same :/ its actually a shame imo - but it's literally useless to attend
17:19:24 <haskellnoob> and secondly, the prof isn't really good
17:19:26 <qqkami> hpc alright thanks!
17:21:53 <koz_> If anyone's curious - my Liquid Haskell issues stem from NoImplicitPrelude.
17:22:07 <jle`> koz_: huh interesting
17:22:46 <koz_> jle`: Relevant feedback: https://github.com/ucsd-progsys/liquidhaskell/issues/1165
17:22:50 <hpc> haskellnoob / qqkami: oh, final advice - while you're not writing the Show instance, add "deriving (Show)" to the end of that data definition
17:22:53 <hpc> @undefine
17:22:53 <lambdabot> Undefined.
17:23:02 <hpc> @let data Nat = Null | N Nat deriving (Show)
17:23:04 <lambdabot>  Defined.
17:23:15 <haskellnoob> nah hpc we're not allowed to derive
17:23:19 <hpc> > N . N . N . N $ Null
17:23:22 <lambdabot>  N (N (N (N Null)))
17:23:37 <hpc> haskellnoob: only for convenience so you can see the actual structure of the data you're manipulating
17:23:40 <qqkami> hpc we have to write it ourselves
17:23:44 <haskellnoob> oh okay
17:23:45 <hpc> then remove it so you can write your own Show instance after
17:23:47 <haskellnoob> that's a good reason then
17:24:02 <haskellnoob> qqkami: need help with Show? I needed 1 hour in here to understand it
17:24:04 <qqkami> hpc: oh well, i've actually done that while "debugging" already ^^
17:24:10 <hpc> hah
17:24:27 <qqkami> haskellnoob: i'll try for a bit but i think i'll manage - thanks!
17:24:34 <hpc> and you're about at the limit of advice i can give before it feels like giving away the whole game
17:24:39 * hpc toodles
17:24:49 <qqkami> haskellnoob: just a tip for task 2 btw, don't think that its complicated - it only looks complicated
17:24:55 <Welkin> limit as hpc -> inifnity
17:24:57 <haskellnoob> qqkami: thanks
17:25:01 <Golden_Skittle> Im trying to learn haskell, is it better to start by using normal ghc, or should I use stack?  
17:25:16 <Welkin> Golden_Skittle: stack is a build system. GHC is the compiler
17:25:17 <qqkami> hpc: thanks for all your help^^
17:25:43 <Welkin> Golden_Skittle: I recommend using stack, for either beginners or advanced users
17:26:16 <Welkin> but learn what stack is doing and how you can compile directly with hc on the command line
17:26:20 <Welkin> ghc*
17:29:52 <haskellnoob> qqkami: are you in my statistics course as well?
17:32:24 <Golden_Skittle> no, but if your just learning too im interested
17:33:58 <qqkami> haskellnoob: yes - I'd guess so? ^^
17:34:28 <Golden_Skittle> oh you werent talking to me
17:38:32 <qqkami1> rip inet
17:38:41 <ivans> rip
17:49:09 <optimus\prime> The l0de Radio Hour is LIVE ! Call in now /join #LRH on irc.efnet.org !!! JOIN ! NOW ! 
17:54:29 <haskellnoob> ivans: any ideas on how to implement (*) for (N x) (N y) when i have the other cases: Null _ = Null; _ Null = Null ?
17:55:30 <qqkami1> haskellnoob: im thinking about just calling plus a couple of times xD
17:55:55 <haskellnoob> I mean that works qqkami1 
17:55:56 <Eduard_Munteanu> haskellnoob, multiplication for naturals? Repeated addition, recurse on one argument.
17:56:13 <haskellnoob> oh well I guess thats the correct way lol, thanks Eduard_Munteanu 
17:56:20 <haskellnoob> you got the show function to work qqkami1 ?
17:57:01 <Eduard_Munteanu> S m * n = n + m * n
17:57:57 <ivans> haskellnoob: yeah it's quite simple
17:58:15 <ivans> but did you try it yourself before asking?
17:58:38 <qqkami1> haskellnoob: not yet - fiddling wiht N x + N y and - atm
17:58:44 <ivans> the implementation is almost like the definition of *
17:59:12 <haskellnoob> yeah ivans I tried with N( x * y ), that didn't work, then I tried to write an own function to convert it to an integer and get the N from that number, wouldn't have been good 
17:59:54 <geekosaur> think about what you did for Show
17:59:58 <Eduard_Munteanu> Note  that converting to an integer makes it strict, though.
17:59:58 <Golden_Skittle> does anyone have some generic source code or programs they wrote when they were starting out with haskell that I could look through
18:00:33 <haskellnoob> wow geekosaur I think it's impressive that you didn't say a single word and still know how my Show is implemented
18:00:49 <geekosaur> I did say a couple words
18:00:57 <geekosaur> and I do check the channel quite often
18:01:05 <haskellnoob> then I apolgze, I'm usually not awake at 3 am
18:01:13 <haskellnoob> apologize*
18:02:04 <haskellnoob> :r
18:02:09 <haskellnoob> woopsie
18:02:26 <haskellnoob> I think that maybe the bed might be a good choice right now
18:05:42 <Golden_Skittle> or can anyone recommend a tutorial for getting started
18:06:16 <erisco> conal,  27:3 [If] the abstraction body is a variable … we must have the identity function on τ  (λx → x) ≡ id :: τ → τ
18:07:02 <erisco> conal, 27:4 If the body of an abstraction is an abstraction, we can curry a translation of the uncurried form  λx → λy → U ≡ curry (λ(x,y) → U)
18:07:05 <ivans> Golden_Skittle: I think this tutorial/book is quite nice: http://learnyouahaskell.com/
18:07:20 <haskellnoob> qqkami1: do you know how the part with Integer conversion to Nat is supposed to work? 
18:07:33 <ivans> it looks silly but it's actually written buy a guy who really knows what he's talking about
18:08:20 <haskellnoob> qqkami1: or what negate is supposed to do
18:08:22 <ivans> Golden_Skittle: for alternative learning materials, just pick something from https://www.haskell.org/documentation
18:08:39 <erisco> conal, ah never mind, the English must be less precise than intended …
18:09:21 <qqkami1> haskellnoob: you don't need to implement negate if you already implemented (-)
18:09:27 <erisco> conal, point being that I could choose  x  or  y  for U and it would not be right to say  λ(x,y) → U ≡ id
18:09:42 <qqkami1> and you convert it to a Nat so your Nat number can work with it
18:11:11 <haskellnoob> qqkami1: not implementing will really not work when there are no negative numbers...
18:11:18 <haskellnoob> not implementing negate*
18:11:57 <erisco> conal, but I don't know what to do with this form now… what if instead it translated to  λz → case z of (x,y) → U  and then I can use the case/of rule?
18:12:05 <qqkami1> its automatically implemented if oyu implement (-)
18:12:18 <qqkami1> shoudlnt really matter imo
18:12:41 <qqkami1> you check the rest with abs and stuff like that anyways haskellnoob 
18:12:44 <haskellnoob> qqkami1: what if you negate (N Null), what is the negative version of that?
18:12:47 <qqkami1> haskellnoob: also at fromInteger
18:13:27 <qqkami1> haskellnoob: its 0 but if you have it implemented so N x - N Null allows for that negate is uatomatically generated in that way - thats how i understood it at least
18:14:42 <haskellnoob> qqkami1: is the negation always 0 ? (Null) ? then I can implement that, since negate Null gives me errors
18:15:55 <qqkami1> negate Null gives me N Null in my scenario, but thats because of the way i coded (-) 
18:16:30 * dmwit_ . o O ( data Int = Int Nat Nat; instance Eq Int where Int a b == Int c d = plus a d == plus c b )
18:16:32 <qqkami1> haskellnoob: since i want my representation to have a N at the start, as in K 4 + K 4 = K 8.0
18:16:41 <erisco> conal, then also on 27:4 for the translation of case/of,  z  is not in scope
18:16:49 <ivans> you guys know this conversations makes mostly no sense to people not on your class?
18:16:58 <haskellnoob> qqkami1: yeah but what is it supposed to return? is negate Null supposed to be (N Null)?
18:17:16 <haskellnoob> cschneid: I guess it makes zero sense to you, sorry for that
18:17:57 <ivans> I think the irony is complete in that tab-complete
18:18:19 <erisco> conal, I am guessing  w = scrut  was supposed to be  z = scrut
18:39:53 <haskellnoob> since I'm still stuck, how does negate call (-) ? because when I try to negate anything it won't work
18:40:19 <erisco> > (\x -> -x) 5
18:40:21 <lambdabot>  -5
18:40:38 <dmwit> haskellnoob: The default definition of `negate` is `negate x = 0 - x`.
18:41:05 <dmwit> https://hackage.haskell.org/package/base-4.10.0.0/docs/src/GHC.Num.html#negate
18:41:58 <haskellnoob> this is gonna sound weird, but my problem is that I wrote a new (-) operator for a new type, and it crashes when doing negate, does negate call 0 - x here as well ?
18:42:21 <haskellnoob> in other words what does haskell assume is "0" in my new type?
18:42:35 <dmwit> 0 is `fromInteger (0 :: Integer)`.
18:42:58 <haskellnoob> so it wants to subtract my new type from 0 ?
18:43:02 <haskellnoob> oh I need fromInteger first ?
18:43:09 <haskellnoob> that's the problem, thanks
18:45:56 <dmwit> Numbers in Haskell are pretty special compared to other languages. There were a bunch of very pragmatic choices made during the language design that have potentially surprising consequences.
18:52:24 <koz_> I'm having trouble understanding what went wrong with my shuffle definition here: http://lpaste.net/360055 In particular, I don't understand why this doesn't shuffle anything.
18:53:01 <dmwit> koz_: Almost certainly you meant `[V.length v - 1, V.length v - 2 .. 0]`.
18:53:05 <dmwit> > [3 .. 0]
18:53:06 <lambdabot>  []
18:53:07 <dmwit> > [3, 2 .. 0]
18:53:09 <lambdabot>  [3,2,1,0]
18:53:10 <koz_> dmwit: OH....
18:53:14 <koz_> That'd be it then.
18:54:24 <koz_> dmwit: Is there a more elegant/idiomatic way I could have written that?
18:54:30 <koz_> Because it looks hideous.
18:55:54 <dmwit> There's a couple other choices.
18:56:10 <dmwit> Most of them start with `where n = V.length v`.
18:56:27 <ivans> "do theNeedful" is a very cute choice of words
18:56:45 <dmwit> Then you can `[n-1, n-2 .. 0]` which looks less hideous. Or `reverse [0..n-1]`. Or `[0..n-1]` and then `getRandomR (i, n-1)` inside `theNeedful`.
18:57:33 <koz_> dmwit: I could also use the lowest-to-highest shuffle, which I only just realized exists...
18:57:49 <dmwit> I think that was my third suggestion above.
18:58:37 <koz_> dmwit: Yeah, I think I'll do that. My question was more structural - is my use of the state monad appropriate here? I assume it is, but given that this is Haskell, I wouldn't be surprised if there was some one-liner that did that and then some.
18:59:22 <dmwit> I think it would be more reasonable to use a mutable vector. Clone your immutable one into a mutable one, do the swaps, then freeze once at the end. It can still have a pure interface thanks to ST.
18:59:38 <dmwit> As it is each swap is probably copying the entire vector.
18:59:55 <koz_> Ouch. OK,I'll do that instead then.
19:01:40 <dmwit> koz_: http://hackage.haskell.org/package/random-fu-0.2.7.0/docs/Data-Random-List.html#v:shuffle
19:02:13 <dmwit> Huh, there is not a corresponding one for Vector.
19:02:21 <dmwit> Seems like an obvious missing feature. =P
19:03:21 <koz_> dmwit: Is that a linear shuffle, or a quadratic one?
19:03:58 <qqkami1> got show to work haskellnoob, thanks
19:04:12 <qqkami1> haskellnoob: in fact i only need to code (*) now if you need any help
19:05:12 <haskellnoob> qqkami1: currently busyy with the Enum thing
19:05:25 <merijn> koz_: More important in the context of shuffle is: is it a properly randomised shuffle, because getting them wrong is easy :p (although, since you're using Fisher-Yates, getting it right is also easy :p)
19:06:42 <qqkami1> haskellnoob: i literally used the code from the script i think
19:07:34 <dmwit> koz_: It claims to be based on this discussion: http://okmij.org/ftp/Haskell/perfect-shuffle.txt
19:08:13 <koz_> dmwit: Quadratic then. It's not got a vector version because Oleg's definition is much more list-friendly.
19:08:29 <koz_> I remember reading that, feeling happy, and then seeing the time complexity...
19:09:00 <merijn> ugh
19:09:12 <merijn> I skipped over the complexity, but yeah, no
19:09:20 <dmwit> koz_: perfect-shuffle.txt claims O(n*log n).
19:09:28 <koz_> dmwit: Oh? 
19:09:30 <merijn> dmwit: Fisher Yates he has now is O(n)
19:09:31 <koz_> I should re-read it then.
19:09:44 <dmwit> merijn: No, not the one he has now. =)
19:09:50 <dmwit> merijn: The mutable version of it, yes.
19:09:55 <merijn> dmwit: Ok, then I understood the one he has now wrong :p
19:10:14 <koz_> merijn: According to dmwit, because of my use of modify, I'm likely getting a copy on each swap.
19:10:15 <dmwit> merijn: You probably assumed that swap was O(1), but I kind of doubt that for immutable vectors.
19:10:26 <merijn> koz_: Right, so why not use a mutable vector?
19:10:37 <merijn> koz_: Can even use ST to avoid IO
19:10:48 <koz_> merijn: I thought by using modify I was getting those benefits.
19:10:53 <koz_> Apparently I was wrong.
19:10:59 <dmwit> merijn: Welcome to 15 minutes ago. ;-)
19:11:25 <haskellnoob> qqkami1: I think I'm done
19:11:35 <haskellnoob> qqkami1: this was literally the worst haskell exercise ever
19:11:55 <merijn> koz_: hold on, can you link the code again?
19:12:02 <haskellnoob> qqkami1: I needed like 4 hrs for the 1st exercise, that's either me being dumb, or it's just way too late for me to be awake
19:12:24 <koz_> merijn: http://lpaste.net/360055
19:12:40 <merijn> koz_: oh, yes
19:12:50 <merijn> koz_: The problem is that you're doing only one swap in a modify
19:13:05 <merijn> koz_: You wanna restructure so you have ALL swaps THEN call modify
19:13:22 <merijn> koz_: modify groups all mutations into in-place updates, but only for that one function passed in
19:13:34 <koz_> merijn: In that case, I might as well write in ST.
19:13:49 <merijn> koz_: Well, yes
19:14:21 <merijn> koz_: IMO that's probably the easiest way to do it
19:14:34 <koz_> merijn: Alrighty, I'll do that then.
19:15:02 <merijn> koz_: basically, what modify does is "copy input vector into ST vector, run mutable update, freeze and return"
19:15:16 <koz_> Ah, OK. Then yes, that'd be awful.
19:15:18 <merijn> koz_: So if you only run a single swap, you still incur the copy + freeze every swap
19:15:26 <dmwit> I am sort of skeptical of the claim in the documentation for modify. "The operation will be performed in place if it is safe to do so." That sounds like a linearity analysis, something which I was under the impression did not yet exist.
19:16:35 <koz_> Which package do I need for ST?
19:17:03 <merijn> koz_: ST is in base
19:17:11 <merijn> koz_: Control.Monad.ST
19:17:16 <koz_> merijn: OK, thanks.
19:18:13 <merijn> koz_: Basically, in GHC ST and IO are both newtypes around GHC's lower level version of ST
19:18:17 <dmwit> ...or perhaps a dynamic analysis of a kind which I believe is neither done nor planned.
19:18:21 <koz_> RealWorld right?
19:18:34 <merijn> dmwit: It's unconditionally cloning :p
19:18:35 <koz_> dmwit: Is the maintainer of vector around here? Maybe we should ask them.
19:18:39 <merijn> dmwit: According to the source
19:19:04 <dmwit> merijn: I doubt your claim, too. I believe there might be some rewrite rules that can fire if things can be inlined well.
19:19:14 <dmwit> Certainly they won't fire in the presence of a fold.
19:19:28 <merijn> Maybe
19:19:32 <geekosaur> koz_, RealWorld is just a specific 's' for ST / the internal State# type
19:38:29 <koz_> dmwit: I'm unsure how to write my shuffle using a mutable copy. I've started here: http://lpaste.net/360055, but I don't know where to go next.
20:02:58 <inkbottle> Is there a nice name to designate the right hand side of bind (>>=); like [5] >>= return . (+1); a function like "return > (+1)"?
20:19:38 <tabemann> I'm trying to find a portable way of manipulating file paths
20:20:07 <tabemann> so that they are handled properly both on Unixen and on Windows
20:20:25 <tabemann> but I've been unable to find such a thing in Hackage
20:20:42 <merijn> tabemann: I think it's fundamentally impossible?
20:20:51 <tabemann> even though I swear I've seen a function for concatenating parts of pathnames before
20:21:18 <merijn> tabemann: Since on linux/BSD file paths are "just bytes" whereas OS X and Windows specify a specific unicode encoding (and even in some cases normalisation?)
20:21:52 <merijn> tabemann: Well, that should be easy, because windows can use both forward and backward slashes as seperator
20:22:14 <tabemann> so just use / in all cases then?
20:22:23 <merijn> tabemann: Any reason why filepath doesn't work for you?
20:23:03 <tabemann> oh that's what I'm looking for
20:23:11 <tabemann> thanks
20:23:19 <merijn> tabemann: That's the "standard" filepath utility, although it's not 100% correct (since it takes unicode filepaths which aren't always sensible)
20:23:32 <merijn> It should work for all the common cases, though
20:24:34 <raynold> Ahh it's a wonderful day
20:31:40 <Eduard_Munteanu> inkbottle, what do you mean? I'm not sure I understand.
20:34:01 <inkbottle> Eduard_Munteanu: Just a fancy name as a shorthand to "a function that takes a value and returns a monadic value" (which is always the type of the rhs of >>=)
20:34:07 <inkbottle> fancy but usual
20:34:44 <inkbottle> actually, doing search, I didn't find any specific technical name for that
20:35:06 <inkbottle> despite that we always use it as rhs of >>=
20:40:53 <monochrom> You can call it a Kleisli arrow.
20:41:11 <koz_> monochrom: Of outrageous fortune.
20:44:07 <inkbottle> monochrom: Oh yes? What a nice name :) Thanks
20:44:47 <nakai> Greetings all, does anyone have time to answer what I hope to be a quick question for a haskell newbie?
20:47:36 <koz_> nakai: Don't ask to ask. Just ask. :)
20:47:50 <nakai> Ok, thanks.
20:48:06 <nakai> I'm looking for an idiomatic way to do some basic I/O stuff.
20:48:30 <inkbottle> monochrom: Exactly was looking for (https://wiki.haskell.org/Arrow_tutorial#Kleisli_Arrows)
20:48:37 <nakai> Specifically, the file I'm reading from has a header section that is terminated with 3 newlines.
20:48:54 <nakai> How would you recommend I go about skipping this header section idiomatically in Haskell?
20:49:54 <nakai> I'm familiar with the basics of Haskell syntax and a few I/O functions as I've recently read through, "Learn You a Haskell", so you don't need to explain the basics.
20:50:02 <nakai> *Hopefully XD
20:53:43 <monochrom> perform "getLine" 3 times.
20:54:07 <monochrom> But I ignored the part about "idiomatically". I just get the job done.
20:54:55 <EvanR> terminated with 3 newlines in a row, but there are newlines not-in-a-row first, i am guessing
20:55:00 <nakai> Thanks monochrom. The real issue for me is actually finding the termination sequence. The header can be of variable length but is guaranteed to end with 3 newlines.
20:57:07 <nakai> Ooh, I actually just found dropWhile in the standard library docs. Do you think I would be able to use that to drop lines until I encounter an empty line and then just verify that the next one is also empty?
20:58:40 <nakai> Yes, EvanR, you are correct. I didn't see your message.
20:58:51 <EvanR> what you could do is convert the lazy (IO) list of lines into list of "three lines at a time, with the rest of the original list past this point"
20:59:08 <EvanR> then dropwhile, then take that original list 
20:59:44 <EvanR> or two lines a time if thats what youre testing
21:00:10 <EvanR> [String] -> [(String,String,[String])]
21:00:28 <nakai> So chunk the list of lines and then try and match against two empty strings in a row?
21:00:50 <EvanR> its not really a chunk
21:00:56 <EvanR> its a moving window
21:01:10 <nakai> Ok, I see. That makes sense.
21:01:26 <EvanR> [(A,B,[C..]), (B,C,[D..]), ...
21:02:04 <nakai> Alright, I think that should do me for now. Thanks EvanR and you too monochrom.
21:02:56 <EvanR> dont ask me what happens near the end of the file
21:03:21 <EvanR> i guess it looks like (Y,Z,[])
21:03:25 <EvanR> ]
21:03:46 <nakai> Haha, that's ok. This program is to validate the format of the file, so if it runs all the way to the end without finding the two empty lines in a row, it can just crash for now.
21:03:52 <EvanR> youre not expecting to get that far, but might happen given the info i have
21:04:13 <nakai> Always important to consider the edge cases though.
21:04:35 <nakai> Anyhow, thanks again.
21:05:21 <nakai> G'night.
21:38:09 <koz_> How would I use MonadRandom to get a pair of Ints (x, y) such that x /= y?
21:41:04 <Axman6> do x <- randomR (low,hi); y <- randNot (lo,hi) x; pure (x,y) where randNot (lo,hi) x = y <- randomR (lo,hi); if x /= y then pure y else randNot (lo,hi) x
21:41:05 <EvanR> there might be something in monad-loops for that
21:43:12 <mniip> koz_: (\x y -> (x, if y >= x then y + 1 else y) <$> randomR (lo, hi) <*> randomR (lo, hi - 1)
21:43:59 <EvanR> haw haw haw
21:44:55 <koz_> mniip: Wow, that's really clever, thank you!
21:49:02 <mniip> is it tho
21:50:56 <EvanR> i thought it was a joke
21:53:48 <EvanR> why do i find it funny that this ... works'
21:54:00 <EvanR> --{-# NOINLINE foo #-}
21:54:19 <EvanR> the commented out ... pragma comment ... now has no effect
21:54:41 <totom> in the definition of 'quad' http://lpaste.net/360057 there is no argument used. Is it because there is only one argument so it can be skipped?
21:54:58 <EvanR> maybe commented out normal comments should suddenly have an effect
21:55:25 <mniip> totom, no
21:55:49 <mniip> totom, twice is a function that takes a function and returns a function
21:56:12 <mniip> 'twice square' is already a function
21:57:06 <totom> but quad needs an Integer
21:57:14 <mniip> hmm?
21:58:26 <mniip> @let square :: Integer -> Integer; square x = x * x
21:58:28 <lambdabot>  Defined.
21:58:40 <mniip> @let twice :: (Integer -> Integer) -> Integer -> Integer; twice f x = f (f x)
21:58:41 <lambdabot>  Defined.
21:58:45 <mniip> :t twice square
21:58:46 <lambdabot> Integer -> Integer
21:58:57 <mniip> no problem here
21:59:47 <totom> Yes, I understand that twice takes a function and returns a function
22:00:29 <mniip> then what's the issue?
22:00:54 <totom> but quad type signature says the it requires Integer. But we are not providing an Integer in it's definition
22:01:22 <totom> Is it taking the Integer from square?
22:01:44 <mniip> I think you've got a few things mixed up...
22:02:08 <mniip> when you say
22:02:11 <mniip> f x y z = ...
22:02:16 <mniip> that's just syntactic sugar for
22:02:19 <mniip> f = \x y z -> ...
22:03:38 <totom> ok
22:04:17 <mniip> just because the type says 'a -> b -> c -> ...'
22:04:30 <mniip> doesn't mean you have to literally have '\x y z ->' in your code
22:04:44 <mniip> just like not every Integer is 3
22:05:05 <mniip> no like
22:05:12 <mniip> not every Integer that is 3, is spelled "3"
22:05:20 <mniip> sometimes it may be "1 + 2"
22:05:58 <totom> oh ok i think i got it now
22:06:06 <totom> thank you very much mniip 
22:06:24 <EvanR> "1 + 2" "is" 3
22:06:37 * EvanR gripes
22:07:12 <seafood> Does ghc -prof automatically produce p_hi files or do you also need to use the -hisuf flag?
22:07:14 <mniip> EvanR, "twice square" "is" "\x -> twice square x"
22:07:23 <dminuoso> Good morning. This topic has kept me awake for 2 hours already. Given those commutative diagrams for monads, Im looking at things like mu.T or T.mu. Seeing as T really just means I_T, which seems to be the identity of natural transformations, is mu.T not the same as mu?
22:07:27 <EvanR> agreed
22:07:52 <dminuoso> https://upload.wikimedia.org/wikipedia/commons/0/07/Coherence_law_for_the_unit_of_a_monad.svg this diagram in particular.
22:07:54 <mniip> dminuoso, T really just means I_T?
22:08:16 <dminuoso> mniip: You cant compose a natural transformation with a functor
22:08:19 <dminuoso> Id doesnt make sense
22:08:51 <geekosaur> seafood, it should be automatic
22:08:54 <dminuoso> It even. Unless it does, but I just dont see how.
22:09:08 <seafood> geekosaur: It doesn't appear to be.
22:09:35 <dminuoso> mniip: as far as I have learned and been told, its that mu . T is just a strange notation. But given that T cannot be a Functor, it must be just short for I_T
22:10:15 <EvanR> erm i thought sometimes a weird pre or post composition of a NT with a functor was defined
22:10:41 <dminuoso> NT?
22:10:47 <dminuoso> do you mean muT?
22:10:51 <dminuoso> or etaT ?
22:10:53 <EvanR> arguably an abuse of notation just to make people who formalize this stuff miserable
22:11:09 <EvanR> natural transformation
22:11:15 <dminuoso> Ahh
22:11:16 <geekosaur> hm, actually I think that might be Cabal installing a library
22:12:23 <dminuoso> EvanR: Well, up until regular natural transformations everything was good and fine. But I just dont see what the composition (T . mu) means in  T . (T . T) --{ T . mu }--> T . T 
22:12:39 <dminuoso> I have a feeling whats going on but its quirky and disturbing as heck.
22:12:56 <mniip> dminuoso, \eta T is the \eta_(TX) natural transformation here
22:13:08 <mniip> well err
22:13:15 <mniip> (\eta T)_X = \eta_(TX)
22:13:32 <dminuoso> mniip: So the component of eta at TX?
22:13:38 <mniip> yes for every X
22:13:48 <mniip> that makes another natural transformation
22:14:14 <dminuoso> mniip: so using the composition symbol is just for maximum confusion?
22:14:42 <EvanR> would you rather they write compositions as juxt 
22:14:49 <mniip> no one uses composition in there
22:15:00 <dminuoso> mniip: not in that diagram but Ive seen plenty other papers
22:15:19 <mniip> if you read the relevant article https://en.wikipedia.org/wiki/Monad_(category_theory)
22:15:37 <mniip> it says \mu \circ \eta T
22:15:42 <mniip> not \eta \circ T
22:17:56 <tabemann> what is the best way to create a file if it does not already exist if one is opening a file in appending mode?
22:20:19 <geekosaur> @tell seafood so it looks like Cabal library takes care of renaming the profiling files (.o, .hi) to include the build ways
22:20:19 <lambdabot> Consider it noted.
22:21:40 <monochrom> Yeah cabal calls GHC twice, once without -prof and then second time with -prof (and -hisuf and -osuf for obvious non-collision reasons)
22:22:24 <monochrom> Whereas GHC on its own just follows the path of least resistance.
22:23:15 <monochrom> GHC is like one of those devil's deals.
22:24:26 <monochrom> One day, a guy cut a deal with the devil, the guy will pay some price to buy "I will be adored by all women". The devil fulfills that by turning the guy into an adorable baby.
22:24:52 <seafood> Okay, I can confirm now. A 'ghc -prof -c File.hs' does not produce a .p_hi file, only a .hi file
22:25:06 <monochrom> Yeah but the *.hi file already contains the profiling stuff.
22:25:21 <monochrom> And an adorable baby.
22:25:31 <monochrom> But boy is it adorable.
22:25:35 <monochrom> and profiling.
22:25:44 <EvanR> i daresay that GHC did some common subexpression elimination, which blew up in my face 
22:25:55 <EvanR> foo = unsafePerformIO IVar.new
22:26:03 <EvanR> bar = unsafePerformIO IVar.new
22:26:18 <EvanR> writing to foo then bar or vice versa crashes "already written"
22:26:24 <EvanR> lol
22:27:18 <dminuoso> mniip: https://bartoszmilewski.com/2016/12/27/monads-categorically/
22:27:35 <monochrom> Yeah there are numerous cases you write two copies of the same code "f = ...; g = ..." and GHC just generates the f copy and then just says "g = f".
22:27:39 <monochrom> (in Core)
22:27:39 <dminuoso> mniip: This is one of the couple places where I got I_T ∘ μ from
22:28:17 <EvanR> NOINLINE fixed it, but its not an inlining phenomenon it seems
22:28:34 <monochrom> Anyway p_hi and p_o and p_a are cabal conventions not GHC conventions.
22:28:55 <monochrom> (And don't worry there is no p_so.)
22:29:11 <mniip> dminuoso, ahhhhh
22:29:15 <mniip> horizontal composition
22:29:26 <monochrom> However, dyn_hi and dyn_o are GHC conventions, -dynamic-too
22:29:30 <mniip> usually you would denote it with a different symbol
22:29:35 <mniip> like \cdot
22:30:19 <geekosaur> seafood, you were out of channel when monochrom explained it, after I tracked down the start of it: Cabal arranges to build the profiling version with -hisuf and -osuf so they can coexist with the normal versions. likewise dyn, p_dyn, and other build ways
22:30:37 <seafood> geekosaur: Thanks
22:30:45 <seafood> I came to the same conclusion after some more messing around.
22:31:29 <monochrom> Unfortunately there is no p_dyn "both profiling and dynamically linked" combination yet.
22:32:08 <tabemann> does anyone know why openFile is failing when attempting to open a nonexistent file in AppendMode?
22:32:27 <tabemann> the documentation indicates that it should create a file in this case
22:32:30 <seafood> monochrom: There isn't?
22:32:57 <geekosaur> monochrom, this started with ghc complaining that it couldn't find exactly that combination
22:33:05 <tabemann> wait i know why it isn't working
22:33:07 <tabemann> forget it
22:33:18 <mniip> dminuoso, if you have a : F --> G, and b :: J --> K, then there's a . b :: JF --> KG
22:34:35 <monochrom> GHC cannot do it yet. I don't know whether it's "we haven't figured out how" or "we haven't found time".
22:34:44 <k> i get pinged in here a lot. does k mean something special? is it for constants?
22:35:09 <mniip> k, often user kind variable
22:35:12 <mniip> used*
22:35:23 <nshepperd> kind, konstant, kontinuation
22:35:25 <mniip> :kind Proxy
22:35:33 <mniip> @kind Proxy
22:35:35 <lambdabot> k -> *
22:35:38 <k> mniip: ah, thanks.
22:35:45 <nshepperd> occasionally a loop index
22:36:04 <monochrom> Kinderella
22:36:47 <nshepperd> it would surprising if you wouldn't get pinged a lot with a one letter username
22:37:20 <k> this channel in particular comes up a lot. i may disable mentions from here.
22:37:42 <EvanR> k
22:38:00 <mniip> dminuoso, (a . b)_X = b_GX . J a_X = K a_X . b_FX
22:38:48 * EvanR puts in a dibs on λ for when freenode allows unicode nicks
22:39:08 <dminuoso> mniip: Ohh. This is whats sometimes referred to as whiskering, isn't it?
22:39:26 <dminuoso> Okay, Ive seen this. I need to get acquianted with this. Time to draw even more diagrams. Thank you mniip 
22:39:47 <monochrom> Rename yourself to q or z for less collision and higher Scrabble score.
22:40:16 <EvanR> /nick moneypenny
22:40:17 <dysfun> or have both
22:40:20 <mniip> k, as an owner of a single-char nickname myself, I say you pretty much subscribed for it
22:40:31 <dysfun> 'caziques' is the highest scoring scrabble world played in a competition
22:40:49 <k> i did. it doesn't really come up that often, though.
22:40:56 <k> just a few times a day.
22:41:17 <nshepperd> i would guess that probably l is the least common single letter in a programming room
22:41:21 <dysfun> i get a random highlight about once a week with this nick
22:41:44 <dysfun> apparently lots of people consider their workplaces dysfunctional
22:43:21 <jle`> does dysfunctional highlight you?
22:44:51 <nshepperd> huh, apparently I'm wrong. j is rarest in #haskell followed closely by q
22:44:51 <dysfun> yes
22:44:54 <^|{`-}_{> I virtually run into no accidental highlights. :)
22:45:00 <dysfun> :)
22:45:09 <dysfun> or intentional ones, probably
22:45:42 <jle`> hm, i don't think my client highlights matches that are a part of other words
22:47:10 <geekosaur> some do, mine doesn't. I ended up adding some 'additional highlighting words'
22:47:12 * dysfun is using irssi
22:47:42 <dysfun> incidentally it is only #haskell where i seem to get these random mentions
22:47:56 <dysfun> nobody else says dysfunctional, apparently
22:48:35 <dminuoso> mniip: Is it better to fully understand horizontal composition for this, or is it safe to just think of this horizontal composition of alpha . beta as a transformation of one horizontal path to another?
22:52:26 <dminuoso> mniip: ohh in that notation a and b are natural transformations!
22:52:42 <dminuoso> And JF and KG respectively are functor composition
22:54:52 <mniip> yes
22:54:56 <mniip> in haskell,
22:55:06 <mniip> NT f g = forall a. f a -> g a
22:55:26 <mniip> horiz :: NT f g -> NT j k -> NT (Compose j f) (Compose k g)
22:55:44 <mniip> vert :: NT g h -> NF f g -> NT f h
22:57:41 <m> hi k
22:57:54 <k> m: greetings, m.
22:58:18 * m a -> (a -> m b) -> m b
22:58:22 <osa1> anyone know if the recent Haskell survery results will be published?
22:59:39 <osa1> survey*
23:01:49 <dminuoso> mniip: Just to be on the safe side, Compose refers to Data.Functor.Compose ?
23:02:14 <mniip> yes
23:02:35 <mniip> actually in this case the encoding of NT needs to be a little more tricky
23:02:47 <mniip> almost all the way to edwardk's Hask
23:04:40 <mniip> data NT f g where NT :: (Functor f, Functor g, Dom f ~ Dom g, Cod f ~ Cod g) => (forall a. Ob (Dom f) a => f a -> g a) -> NT f g
23:05:28 <geekosaur> osa1, https://github.com/haskellweekly/haskellweekly.github.io/issues/125
23:05:35 <a6a3uh> If I have a type:
23:05:35 <mniip> o wait
23:05:35 <a6a3uh> newtype Game s r a = 
23:05:37 <a6a3uh>     Game { unwrap :: (EitherT Error (StateT s (ReaderT r (WriterT Log (MemoQV Int Double)))) a) }
23:05:37 <a6a3uh>     deriving ( Functor
23:05:39 <a6a3uh>              , Applicative
23:05:39 <a6a3uh>              , Monad
23:05:41 <a6a3uh>              , MonadState s
23:05:41 <a6a3uh>              , MonadReader r
23:05:43 <a6a3uh>              , MonadWriter Log )
23:05:43 <a6a3uh> What is correct syntax to derive Zoom for s and Magnify for r? Without these instances zoom and magnify not works. Or any other ways to make zoom and magnify work?
23:06:02 <osa1> thanks geekosaur
23:06:13 <dminuoso> mniip: Oh hoh. Scary. Well, I suppose I can just live with the fact that NT exists somehow. :P
23:07:14 <mniip> @tell edwardk I think I found a way to settle the Dom/Cod associated tyfam/explicit tycls parameter dispute: type FunctorC f k l = (Functor f, Dom f ~ k, Cod f ~ l)
23:07:14 <lambdabot> Consider it noted.
23:08:56 <jle`> a6a3uh: if you use ExceptT instead of EitherT you should be able to just derive Zoom using generalized newtype deriving
23:09:03 <jle`> you should probably be using ExceptT instead of EitherT anyway
23:10:06 <mniip> jle`, I forget, what's the difference between ExceptT, ErrorT, EitherT?
23:10:35 <mniip> monoidal applicative instance?
23:11:01 <jle`> ErrorT had a weird constraint on the error type for its Monad instance
23:11:11 <jle`> EitherT exists in a non-standard library to fix that problem
23:11:14 <mniip> oh is it the one that actively uses 'fail'
23:11:25 <a6a3uh> jle`: thanks for reply! no way to use Either? I just using it to indicate some normal conditions like gameover not for exceptions exclusively. So using something with name ExceptT will be a bit misleading...
23:11:26 <jle`> ExceptT is EitherT but actually *in* transformers/mtl, a standard library
23:12:04 <a6a3uh> jle`: I see what you mean. ExceptT should has desired behaviour albeit its name.
23:12:06 <jle`> a6a3uh: ExceptT and EitherT are the same
23:12:18 <jle`> it's just that ExceptT is in a standard library
23:12:24 <jle`> and EitherT is a non-standard/obscure one
23:12:44 <a6a3uh> jle`: Thanks a lot. Will give it a try.
23:12:45 <jle`> the reason why ExceptT works is that the lens library actually defines a Zoom instance for ExceptT e ...
23:12:55 <jle`> since ExceptT is a standard type
23:13:18 <jle`> but it doesn't define one for EitherT because it's in an esoteric package, so doesn't pull that in as a dependency for the library
23:14:23 <jle`> mniip: yea ErrorT actually uses fail yeah, so asks for a constraint on the error type to implement fail
23:15:07 <jle`> mniip: you might be thinking of another type from transformers, 'Errors'
23:15:15 <jle`> that's the one that is only Applicative and accumulates errors monoidally
23:15:27 <jle`> it's also often called Validation because names are confusing
23:15:32 <cocreature> I wonder what the original reasoning behind the constraint on ErrorT was. I’ve never heard anybody say that it was a good idea.
23:15:58 <a6a3uh> jle`: I changed Either for Except. Still it complains about it:
23:15:58 <a6a3uh> No instance for (Zoom
23:15:59 <a6a3uh>                          (Game World Settings') (Game GameWorld Settings') World GameWorld)
23:15:59 <a6a3uh>         arising from a use of ‘zoom’
23:16:02 * jle` just now realizes that there is both an Errors and ErrorT which are very different
23:16:17 <a6a3uh> Should I derive Zoom and Magnify somehow as well?
23:16:27 <jle`> oh yeah, that's waht i mean
23:16:33 <jle`> you should put that in the deriving list
23:17:20 <a6a3uh> but Zoom typeclass has parameters that I cant provide in advance.
23:17:56 <a6a3uh> ‘Zoom’ is not a unary constraint, as expected by a deriving clause
23:18:29 <jle`> ah hm
23:18:33 <jle`> yeah it can't be partially applied in a nice way
23:18:37 <jle`> like MonadState and MonadReader were
23:19:28 <jle`> i wonder if you can StandaloneDeriving it
23:20:29 <a6a3uh> jle`: never used it as Im still learning the Haskell. Will read about.
23:20:35 <jle`> deriving instance Zoom (Game s r a) (Game t r a) s t
23:21:17 <jle`> that might work
23:21:39 <jle`> but also for some reason there is no Magnify instance for ExceptT which makes me think that it might be impossible
23:22:17 <jle`> it isn't obvious to me that it should be impossible
23:23:10 <a6a3uh> jle`: at least having zoom will make most of the job as Im using magnify only once. is standalone deriving could be mixed with normal deriving? or should I standalone derive now all the instances?
23:25:12 <jle`> you can mix the two together
23:26:00 <tdammers> a6a3uh: you can mix them, and I would only use standalone deriving when you can't use regular deriving for some reason
23:32:39 <a6a3uh> jle`: it required me to remove 'a' like this {deriving instance Zoom (Game s r) (Game t r) s t} and add FlexibleInstances and MultiParamTypeClasses and still unable to derive claiming: The last argument of the instance must be a data or newtype application
23:33:03 <jle`> oh yea sorry that's my fault
23:33:11 <jle`> it should be (Game s r) (Game t r) s t, yeah
23:33:28 <jle`> ah
23:33:49 <jle`> yeah it looks like it requires a specific ordering for typeclass parameters, maybe
23:34:05 <jle`> that's unfortunate
23:34:26 <jle`> you might just have to manually define the instance by wrapping and unwrapping then
23:34:50 <jle`> zoom l (Game g) = Game (zoom l g), or something like that
23:35:13 <jle`> zoom l = Game . zoom l . unwrap     ...?
23:35:54 <a6a3uh> jle`: yeah. this is the last resort )
23:36:16 <jle`> GeneralizedNewtypeDeriving is basically a mechanical process and only works for very simple cases
23:37:05 <cocreature> the problem is probably the Zoomed type family
23:37:24 <cocreature> ghc 8.2 can do GND for typeclasses with associated type families in some cases but even that won’t help here
23:37:31 <jle`> oh huh i didn't even notice that type family
23:37:39 <jle`> it's not even an associated type family for some reason
23:38:31 <jle`> but yeah because Zoomed exists, you'd need to write an instance for Game s r too
23:40:28 <a6a3uh> cocreature: unfortunately ghc 8.2 is not in stack lts yet. so most of packages not works without manually writing them in stack.yaml I did tried to switch already.
23:40:49 <cocreature> a6a3uh: you can use a nightly snapshot
23:41:09 <a6a3uh> cocreature: good point. never used it. thanks)
23:42:08 <cocreature> lts is kind of weird. it’s really convenient since you get a lot more caching by sticking to a relatively stable snapshots but almost nobody actually provides bugfix updates for old releases so you’re mostly stuck on outdated, unmaintained versions of packages
23:42:48 <dysfun> i think you should periodically upgrade stack snapshot and fix the breakage
23:43:19 <dysfun> it's a bit annoying it lags behind, but i haven't had cabal hell since i started using stack, so i'm not giving that up
23:45:05 <cocreature> sure, my point is upgrading to the newest lts snapshot still leaves you with unmaintained versions of packages in quite a few cases since lts doesn’t upgrade to new major releases
23:45:36 <dysfun> it's not *that* far out of date
23:46:25 <cocreature> depends on how frequently maintainers do major releases :)
23:49:08 <dysfun> still, seems like a small price compared to the pain of living at head
23:50:09 <dysfun> i just love when my build randomly fails and i have to clone some packages and tweak the dep versions
23:50:28 <cocreature> well a nightly snapshot still gives you reproducible builds
23:50:42 <dysfun> are they all tested to build together though?
23:50:46 <cocreature> yes
23:50:50 <dysfun> oh cool
23:51:04 <ventonegro> https://neilmitchell.blogspot.ch/2015/12/whats-point-of-stackage-lts.html
23:52:48 <dysfun> that makes a lot of sense tbh
23:53:55 <dysfun> i don't think it will convince risk-averse megacorps though
23:54:12 <ventonegro> Do those use Haskell?
23:54:31 <EvanR> new whitepaper, the risks of java
23:54:33 <cocreature> there are not that many risk-averse megacorps using Haskell :)
23:56:53 <AliasSpider> �
23:56:55 <AliasSpider> ʺ
23:56:56 <AliasSpider> ʸ
23:57:00 <AliasSpider> 衷
23:58:35 <a6a3uh> jle`, ocreature: did testest latest nightly. while error messages are nicier still error the same. Could you pls explain briefly why automagic type deriving fails in that case? Or point to where to read about this?
23:59:29 <dysfun> ventonegro: a few do, yeah
23:59:44 <dysfun> actually i would have thought where neil works was one of them, but i guess they let him do what he wants
