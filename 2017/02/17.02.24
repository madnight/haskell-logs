00:00:05 <Cooler> is the ecmascript 2017 async await the monadic bind?
00:00:20 <Cooler> the syntax looks very much like do notation
00:00:25 <ezyang> Cooler: Almost nearly but apparently not actually? 
00:13:06 <osa1> mmm why there isn't a hoistEither for ExceptT?
00:15:24 <cocreature> osa1: there is one in the "errors" package
00:16:02 <osa1> nice thanks cocreature
00:16:14 <osa1> I was using either package but EitherT doesn't have instances I need
00:18:42 <phadej> hmm, that's less general then either throwError return
00:20:13 <osa1> ugh
00:20:20 <osa1> it's specific to a transformer
00:20:21 <osa1> why
00:20:32 <osa1> I was assuming that it's the general version
00:21:26 <osa1> one more function to add to Utils.hs :-)
00:23:10 <phadej> osa1: errors doesn't depend on mtl :)
00:24:03 <osa1> good point. unfortunately that's not an advantage in my case
00:32:44 <srk> hm. so I've managed to us lifted-async to make test = app1 `concurrently` app2 work in my monad but it uses readTBQueue to feed apps so events are only received by one of them
00:33:22 <srk> not sure how to wrap it so both apps can receive all messages
00:36:51 <cocreature> srk: you could have one queue per thread and an additional thread that reads from the original queue and inserts into both of these queues
00:37:42 <cocreature> srk: alternatively you could try to find a queue structure that has something like "dupTChan"
00:37:47 <srk> I think my double queue interface is weird
00:37:50 <cocreature> it looks like TBQueue itself doesn’t provide this
00:37:56 <srk> yeah, TBChan does
00:38:42 <cocreature> I’m not actually sure if there is anything that prevents TBQueue from providing that operation
00:41:20 <phadej> cocreature: the implementation
00:41:40 <MarcelineVQ> srk: a possibly relevant and short section http://chimera.labs.oreilly.com/books/1230000000929/ch12.html#sec_server-state
00:41:57 <srk> this is what I've got now https://git.48.io/zre/c/bdeeb2ef0d495d9362671835f3f22bac562827d7?branch=master
00:41:57 <MarcelineVQ> the broadcast chan idea mostly
00:41:58 <phadej> tchan is "read :: STM a, write :: a -> STM ()", tqueue is "TVar [a]"; oversimplified
00:42:29 <phadej> there is: https://hackage.haskell.org/package/stm-chans-3.0.0.4/docs/Control-Concurrent-STM-TBChan.html
00:43:40 <halogenandtoast> I just want to take a moment and say the #haskell community is amazing, and I appreciate all the contributors
00:43:55 <halogenandtoast> After spending a day in the #lisp channel, I really appreciate the environment we have here
00:44:33 <srk> like I'm already using STM internally but now I want to hide this implementation behind monadic interface
00:46:07 <srk> the problem is that readZ function uses ask to get TBQueue from reader and reads from that queue
00:46:34 <srk> I would somehow need to split queues for each app
00:50:48 <jle`> halogenandtoast: :D
01:17:48 <halogenandtoast> jle`: yeah, why are you some damned helpful?
01:18:24 <halogenandtoast> I actually expected haskell as a language to attract exactly the opposite kind of person (pretentious), but this channel has always proven the opposite.
01:20:41 <MarcelineVQ> I'm reasonably pretentious
01:20:49 <halogenandtoast> I can accept that.
01:25:02 <srk> feels like I've ended up with yet another bug ridden implementation of erlang :D
01:25:18 <halogenandtoast> Are you writing an Erlang interpreter/compiler?
01:25:29 <srk> nah, just p2p messaging
01:25:50 <srk> it's quite limited compared to erlang. testing mocked up workers 
01:26:57 <srk> try if you want, it's an implementation of zre protocol using zeromq as a backend - https://git.48.io/zre/
01:29:51 <halogenandtoast> srk: awkward territory, untrusted code :p I have to now read all of your code.
01:30:25 <srk> :D
01:30:40 <jle`> \
01:30:41 <jle`> \
01:31:22 <jle`> i consider myself better than others because i am not pretentious, unlike other people
01:32:24 <halogenandtoast> srk: hmm I don't trust this: zgsSig = 0xAAA0 :: Word16
01:32:34 <halogenandtoast> looks like buffer overflow to me...
01:32:44 <srk> woot :D
01:33:42 <srk> it's a zgossip protocol identifier, gossip is not yet implemented, only protocol part
01:34:20 <srk> with gossis peers don't have to rely on working multicast and can find peers in other subnets
01:34:52 <srk> otherwise there's a UPD beacon to port 5670 for matching peers
01:35:18 <srk> they then establish zmq connections and follow zre protocol
01:37:07 <halogenandtoast> So many files to go through
01:38:21 <srk> yeah, and it needs even more refactoring/splitting of API and internal types..
01:38:37 <halogenandtoast> no tests...
01:38:50 <srk> yeah, still learning to write that
01:39:16 <halogenandtoast> I actually found writing tests in Haskell easier than writing tests in any other language
01:39:19 <halogenandtoast> which susprised me.
01:39:25 <srk> I'm now testing this against reference C implementation and if works quite nice
01:39:37 <halogenandtoast> hspec is top class imo
01:39:52 <halogenandtoast> as is quickcheck.
01:40:03 <srk> indeed
01:40:18 <srk> but who needs tests when you have STM :D
01:40:50 <srk> nah, just joking. they would be handy for protocol encoding stuff
01:49:31 <ertes> how can i "touch" a value without actually evaluating it?  i just want to tell GC that i still need that value
01:49:41 <ertes> (needing IO is ok)
01:50:57 <alexbiehl> ertes: Try Control.Monad.Primitive.touch
01:51:03 <alexbiehl> But why would you want that? 
01:51:10 <ertes> background: i need this in an unsafePerformIO-block that would otherwise let-float to the top-level
01:52:39 <ertes> yeah, touch/touch# seem to do it, although i would feel safer if they were actually documented =)
01:52:42 <ertes> thank you
01:53:37 <ertes> i could just use (const action x), but then GHC will likely optimise the dependency on x away
01:58:10 <ertes> actually…  is there a variant of 'touch' that doesn't even prevent GC?  i really just need to *refer* to the value somehow in order to prevent let-floating
02:05:06 <MarcelineVQ> ertes: is  inline  at all appropriate? I've never used it
02:05:21 <MarcelineVQ> specifically http://hackage.haskell.org/package/base-4.9.1.0/docs/GHC-Exts.html#v:inline
02:07:57 <ertes> MarcelineVQ: not sure how to use it for this purpose
02:08:34 <ertes> nevermind, i've enabled -fno-full-laziness
02:09:00 <ertes> would have preferred not to do that though
02:13:37 <MarcelineVQ> let floating is an echoing arcane mutter that dredges up images of oily vines obscured in a quaking and somehow ribald shadow, grasping, grasping, or something, ghc source has some interested leads on it when searching the word 'prevent' though
02:17:12 <MarcelineVQ> reallyUnsafePtrEquality# for example mentions that it must not be floated, and seems to use a flag called can_fail to do so, so it'd be interesting if there was one whos only purpose was to apply that tag to something else. appearantly can_fail and has_side_effects on primops both prevent let from floating out, though not from in
02:18:37 <MarcelineVQ> though there's probably a really simple language trick somewhere to accomplish this
02:31:18 <ertes> MarcelineVQ: there is also the "i really don't want to research this right now" trick: -fno-full-laziness =)
02:31:19 <MarcelineVQ> looks like touch does what that line of thinking would do anyway
02:34:58 <MarcelineVQ> :>
02:36:37 <bennofs1> ertes: how about putting your code into a no-inline Int -> IO () function and calling that function with some constant?
02:37:02 <bennofs1> oh, that problably means the whole expression `f constant` can be lifted out though..
02:37:28 <bpa2> I am knew to #haskell and do not know what is meant to happen? Can somebody explain?!?!
02:37:58 <bennofs1> bpa2: what do you need help with?
02:38:05 <bpa2> Haha now I see my own post!!
02:39:23 <bpa2> Thank you bennofs I will get back when I have formulated my question. This was just an experimental run....
02:59:46 <bpa2> I started with Haskell a couple of days ago and am working through Learn You a Haskell...the first example in Syntax in Functions gives lucky :: (Integral a) => a -> String as the function signiture. Why is Integral the typeclass? As opposed to Num for example. I have tried using Num and it produces errors - but why? In the third examle the signiture is factorial :: (Integral a) => a -> a. Again with Integral. Why? factorial could have 
03:01:03 <bpa2> The last part of my question should be:Again with Integral. Why? factorial could have a simple factorial :: Int -> Int as the signiture so why use the typeclass Integral? Is there a newbie friendly chart showing the hierarchy of number typeclasses and types?
03:02:32 <Ferdirand> Num alone would not be enough to define a factorial, I think
03:02:41 <quchen> bpa2: There is one in the Haskell Report. The Haskell number class hierarchy is fairly messy, but in practice you rarely need more than Num and sometimes Integral. https://www.haskell.org/onlinereport/classes.gif
03:03:03 <quchen> bpa2: Num gives you + - *, Integral adds div/mod ontop of that.
03:03:12 <quchen> That’s the basic summary. :-)
03:03:24 <Ferdirand> Integral also implicitely adds Enum
03:03:37 <Ferdirand> that you would need if you used the [a..b] notation
03:05:01 <quchen> bpa2: GHC infers the most general type for your expressions, not just »one that works«. Your function works for anything Integral, so GHC will not give you the specialized Int version of that.
03:05:20 <Ferdirand> Num gives you a way to subtract 1, but you would still need to test for equality
03:05:37 <av_> bpa2: you could restrict the type to Int -> Int, but that would remove polymorphism altogether, and wouldn't be a good choice anyway because factorial can reach the maxBound of Int fairly quickly -- use Integer instead, if you really need a monomorphic function
03:06:39 <av_> bpa2: the minimum set of type classes you need to specify (unless you omit the type declaration altogether) depends on the operations you use in your definition
03:06:49 <Ferdirand> and by the way, you can use :info in ghci to get info about typeclass definitions
03:07:47 <av_> bpa2: as a hint, omit the type class, then run ghc on it with -Wall, that will give you a general (or even the most general?) type definition in its warnings
03:08:36 <av_> bpa2: newer GHCs even warn of redundant class constraints, which is very useful sometimes
03:09:19 <bpa2> Thank you guchen and Ferdirand...yes I see its not just the type of number but the operations that can be performed on them! Wow av that would be a big factorial - but yes I get your point.
03:12:14 <bpa2> Thank you for directing me to the chart guchen!
03:15:47 <Bish> hi, when people are talking about "creating DSLs in haskell to not have to write haskell at all" what do they mean by it
03:16:00 <Bish> does that always mean, parsing bytestrings ors imiliar in runtime, to express what you want to do?
03:17:34 <pacak> Bish: Look at xmonad config. I make my first one long before I was able to program in haskell.
03:17:35 <Bish> if so: why is this considered so great in haskell, i mean you always have the overhead of interpreting your own language
03:18:57 <Bish> i think i've seen something like that when playing with awesome?
03:19:01 <Bish> i hated it :D
03:19:35 <liste> yes, awesome has Lua as config language
03:19:52 <Bish> really? okay, then i was using xmonad
03:19:56 <pacak> I'm still using more or less the same config. The only difference is that I know what those things mean,.
03:19:58 <Bish> i am sure it was haskell
03:20:25 <Bish> but still.. i don't quite understand, if i am parsing my own language in runtime, doesn't that make a lot of overhead?
03:20:47 <Tuplanolla> It's usually compiled, Bish.
03:21:37 <Bish> how do i compile my own language? that's the thing i don't get
03:21:53 <Bish> i mean, can i tell haskell how to compile my dsl and integrate it into my binary, or whats the deal?
03:23:26 <Tuplanolla> I mean the usual way is to put the language into a Haskell library and have GHC compile it like the rest of your code. It's not really a separate language on its own.
03:23:56 <Tuplanolla> Some projects diverge from this and interpret or compile such languages at runtime.
03:24:04 <pacak> One approach - xmonad. You can also write a parser with parsec and write some interpreter for resulting AST
03:24:06 <liste> Bish: if you want to modify the behaviour at runtime, you'd of course compile/parse it at runtime, and if you want compile time, you can use eDSLs or TH, for example
03:24:35 <Bish> so there is a way, so my dsl gets compiled to assembler, too?
03:25:13 <lyxia> It's easy if it's an eDSL
03:25:33 <Bish> e as in embedded?
03:25:37 <Bish> so that's the thing i need to google
03:25:39 <liste> yes
03:25:59 <Bish> so i kinda can write my own language in haskell, and use it inside haskell?
03:26:07 <Bish> that'd be interesting
03:26:21 <Tuplanolla> Are you familiar with Scheme, Bish?
03:26:24 <Bish> no
03:26:48 <Tuplanolla> They've done the same thing for much longer and more comprehensively.
03:27:15 <Bish> Tuplanolla: thanks looking into it
03:27:21 <liste> here's an example of an eDSL: https://hackage.haskell.org/package/lucid
03:27:43 <Bish> what started my interest in that is a ORM that expresses SQL very convieniently
03:27:50 <Bish> i decided i want to see more of that
03:28:08 <Bish> that one was in ruby, which has these weird block "DSLs"
03:28:35 <Bish> which kinda looks like programming via configurations, i really liked that and searching for a language which does that more natively, thought haskell might be this one language
03:28:39 <Bish> looking at scheme now
03:29:25 <liste> scheme is even more flexible eDSL-wise, but with Haskell you get more "for free"
03:29:39 <liste> type checking, tools, etc
03:30:15 <liste> racket, a scheme-based language, is even more dsl-oriented than scheme
03:30:50 <Bish> but when looking at lucid, it doesn't look much like a dsl
03:31:00 <Bish> i mean, that's just haskell syntax
03:31:01 <Bish> isn't it?
03:31:33 <liste> Bish: it is Haskell, that's what edsls are, a "language within a language"
03:31:53 <Bish> well, what makes it it's own languagte than
03:32:04 <Bish> i would call that domain specific expressions
03:32:22 <Bish> i was hoping for something that "wraps" that
03:32:23 <Tuplanolla> I would prefer that name.
03:33:18 <Bish> aww :( i wished that all would be so much cooler
03:34:57 <Bish> i was kinda hoping that you can do something like.. express something like jade inside a haskell sourcefile, what then gets wrapped by haskell, then compiled, and be fast, beauty and optimizeable
03:35:13 <alexbiehl> Bish: take a look at QuasiQuoters
03:35:30 <Tuplanolla> @hackage inline-java
03:35:30 <lambdabot> http://hackage.haskell.org/package/inline-java
03:35:47 <alexbiehl> A nice blog post on that topic: https://www.well-typed.com/blog/2014/10/quasi-quoting-dsls/
03:35:51 <liste> Bish: that's done too, extensively by the Yesod framework
03:36:08 <Bish> that looks more like it
03:36:48 <liste> Bish: see http://www.yesodweb.com/book/shakespearean-templates#shakespearean-templates_interpolation for example
03:37:00 <Bish> so this code inside thesequasiquotations
03:37:06 <Bish> gets somehow translated to haskell
03:37:09 <liste> yes
03:37:13 <Bish> and will be optimizeable in compilation?
03:37:18 <Bish> thats just fucking great, isn't it?
03:37:34 <liste> it'll be just like any other piece of Haskell code
03:38:24 <liste> I think there's certain advantages to leveraging Haskell syntax instead of using quosiquotation though, if you have the option to specify a new language
03:39:00 <Bish> sure, this will have disadvantages, but still
03:39:17 <Bish> haskell is pretty ugly, but pretty great, if you could remove the most ugly parts with something like this
03:39:23 <Bish> would make it a lot better
03:40:01 * kuribas wishes ghc would automatically vectorize numeric code with simd instructions...
03:41:12 <Bish> thanks for the insight, i will look into these things
03:41:24 <yushyin> Haskell isnt ugly at all :(
03:41:38 <Bish> it's sooo freakin ugly, :( but it's opinion
03:41:44 <Tuplanolla> Nonsense. It's a mess.
03:42:07 <yushyin> Ghc haskell is ugly :)
03:42:18 <Bish> imho it begins with .. postfix..infix.. whatever we do both whenever we want
03:42:19 <LordBrain> well, what about old school haskell98
03:42:22 <LordBrain> that was pretty
03:42:36 <merijn> What's ugly about GHC haskell?
03:42:45 <Tuplanolla> Even old Haskell has warts like the lack of `ArgumentDo`.
03:43:05 <merijn> Is ArgumentDo even a thing yet?
03:43:16 <Tuplanolla> No and that sucks.
03:44:11 <LordBrain> merijn, well... we have a slew of language pragragmas these days at the top of every program... our imports are a mile long typically.... and of course magic hash is not exactly aesthetic.. we have nice pretty sugar for lists, but use things like pipes and conduits instead, and then there's dependent type foo
03:44:35 <LordBrain> i love haskell... don't get me wrong...
03:45:01 <LordBrain> but it's value on prettiness has gone down significantly i think
03:45:11 <LordBrain> as peopel prioritize everything else i guess
03:45:42 <LordBrain> we have a hundred different kinds of strings...
03:45:47 <merijn> Magic hash only really appears in GHC internally
03:45:49 <Tuplanolla> Here's a good heuristic for beauty: if two language designers independently arrive at the same syntax, there has to be something intrinsically appealing about it.
03:45:56 <LordBrain> fair enough merijn 
03:46:15 <Tuplanolla> It's similar to how good mathematics is "discovered", not "invented".
03:46:19 <merijn> As for different kinds of Strings, the alternative would be to have one and make things suck if it doesn't do what you want
03:46:25 <phadej> indeed
03:46:36 <merijn> I think ezyang's Backpack work will make things a lot better in terms of "strings"
03:46:46 <LordBrain> but for example, say we decided to take things really more toward the higher level, we might remove implementation details to the point where the programmer doesnt even know if its a linked list or a bytestring or what
03:46:59 <merijn> LordBrain: I also don't see the problem with a long list of imports? What's the alternative?
03:47:06 <phadej> LordBrain: but list of characters is different from the array of bytes!
03:47:13 <LordBrain> of course it is
03:47:16 <phadej> even semantically
03:47:24 <LordBrain> well...
03:47:26 <halogenandtoast> Bish: What language do you consider pretty?
03:47:51 <phadej> LordBrain: you cannot ask "textLength" from bytestring, without giving the encoding informaation too, for example
03:47:59 <LordBrain> but we don't need "string" to be hard coded to either.... the default could just be to use something that adapts, maybe even having some online tickies to figure out the best adaptation at run time
03:48:24 <LordBrain> i mean if that was our priority
03:48:47 <LordBrain> we could make things a lot prettier
03:48:48 <merijn> LordBrain: See my remark about Backpack
03:49:40 <LordBrain> well, i don't think haskell is ugly
03:50:12 <LordBrain> but i try to be fair to the negative feedback.. and understand why people might feel differentlyl than i do, when my impressoin was formed about an earlier haskell
03:50:51 <LordBrain> realisitically, it does feel like that priority has been put off in comparison to others
03:51:11 <mlehmk> list and bytestring are fundamentally different, it's on the level of developing, not just the level of how it is stored
03:51:29 <LordBrain> mlehmk, so, programming can be ambiguous until specified
03:51:39 <LordBrain> you can say, just use something here i don't care 
03:51:54 <LordBrain> and then later, hard bake it as necessary
03:51:58 <kuribas> scheme is pretty
03:52:12 <kuribas> but I prefer static types...
03:52:14 <mlehmk> this is not what one wants, that's why one uses a list sometimes and bytestring at other times. It just depends
03:52:18 <LordBrain> yeah me too kungp 
03:52:21 <LordBrain> kuribas, 
03:52:24 <LordBrain> sorry tab complete
03:52:43 <LordBrain> huh?
03:53:09 <LordBrain> look, i'm just saying lets be aware of our priorities and such, we don't have to be in denial about it, we tend to put "pretty" lower than a lot of other concerns
03:53:39 <mlehmk> writing functions that create new strings from other strings, just use lists, as that's the thing that works nicely with the functional style
03:53:47 <LordBrain> and that has increased with time as we have increased our priorities on performance and practicality and yadda yadda
03:53:52 <phadej> Well, the "not breaking existing stuff" conflicts with "pretty" unfortunately a lot
03:54:15 <mlehmk> if you communicate with other apps/servers etc. use the bytestring, as they have an encoding and a byte representation useful to exchange plain byte buffers
03:54:37 <LordBrain> sigh
03:54:46 <LordBrain> i know this stuff mlehmk 
03:55:01 <mlehmk> and that is the whole point in that thing
03:55:17 <LordBrain> i think my entire point went right over your head
03:55:24 <mlehmk> I have yet to find a language that makes no distinction between strings and a byte buffer
03:55:32 <mlehmk> well, C was one
03:55:32 <LordBrain> so????
03:55:42 <LordBrain> we would make distinction
03:55:51 <mlehmk> and we have
03:55:52 <LordBrain> its just htat string would be a complicated adapting-container
03:56:04 <LordBrain> and we'd have non-adapting when we want that too
03:56:10 <LordBrain> but for a lot of pruposes, we'd just go with the default
03:56:24 <mlehmk> and you just end up having two different types again
03:56:37 <LordBrain> yes, but in practice a lot of code gets cleaner
03:56:46 <phadej> I think this is wrong mentality. ByteString and say Text are entirely different concepts
03:56:51 <LordBrain> whatever
03:57:06 <LordBrain> i am not saying we throw away the fixed implementation types
03:57:28 <mlehmk> I think the code is cleaner if there's actually a distinction between strings and a byte representation in source
03:57:32 <phadej> even with backpack it would make sense to have abstraction "things with characters" and "things with bytes"
03:57:41 <mlehmk> as they are different things and handled differently
03:57:42 <LordBrain> yes
03:58:17 <mlehmk> although, one thing that might be possible is hide the implementation details of a list and store it as some utf-8 string internally
03:58:51 <mlehmk> though you still need the bytestring if you want to send/receive buffers
03:58:53 <phadej> but then there's textbuilder, bs-builder and DList String
03:58:54 <LordBrain> right, adapting-lists actually is superior to adapting-strings
03:59:33 <phadej> and those are different too
03:59:38 <LordBrain> god
03:59:49 <LordBrain> what about me says i am ignorant of the distinctions?
04:00:33 <phadej> but at some point (when it's a "real" program), you cannot be ignorant of the low-level details
04:00:52 <phadej> IMHO it's a beauty of haskell, thata you can write high level abstractions for quite low-level stuff
04:00:56 <LordBrain> well we always sacrafice prettiness at some point, but that doesnt mean give up entirely
04:01:10 <LordBrain> well yes
04:01:16 <LordBrain> we don't expect every program to be beautiful
04:01:44 <phadej> and FWIW, imho the string story is much more "beautiful" than numbers
04:02:06 <LordBrain> how would you make numbers prettier?
04:02:07 <phadej> at least we don't have "not so good" abstractions in base/prelude
04:02:22 <phadej> (for strings)
04:02:38 <liste> phadej: what's wrong with the numbers story?
04:02:45 <liste> numbers tower?
04:02:50 <phadej> liste: yes
04:03:11 <liste> or should it be "numbers tree" in this case :D
04:03:19 <phadej> DAG
04:03:23 <phadej> :)
04:04:07 <LordBrain> another thing that has uglied up a bit is our type sigs, as neil pointed out they would... when we generalized a lot of stuff in the prelude... so we have things like monoid m => m a, instead of just [a]
04:04:37 <LordBrain> minor, but still
04:04:38 <phadej> foldable?
04:04:43 <LordBrain> same yes
04:04:54 <phadej> your type signature is kind-incorrect
04:04:55 <LordBrain> well, foldable, my point was we have a type class
04:05:04 <LordBrain> i'm not writing code
04:05:06 <LordBrain> i'm talking, on irc
04:05:24 <phadej> ... and half of people think foldable/traversable stuff is good thing
04:05:31 <LordBrain> oh i think its good too
04:05:41 <LordBrain> my point isn't "bad" about anything
04:06:04 <phadej> I don't follow
04:06:36 <LordBrain> phadej, to say we don't prioritize pretty as highly as other things is not the same thing as to say that we should do so
04:06:56 <lyxia> How do you define "pretty"
04:07:09 <phadej> well, uglier type-signatures <-> prettier in use-sites?
04:07:29 <LordBrain> well, thats a good question lyxia, you can't really get a hard metric on that, but one thing is fewer tokens
04:07:36 <LordBrain> as a general rule
04:07:47 <LordBrain> sometimes symbols are improvements over textual words
04:07:53 <LordBrain> other times its the other way around
04:08:12 <phadej> well, then my trade-off applies to it
04:08:42 <LordBrain> it didn't change the use site, maybe the import list
04:08:45 <phadej> more general type-signatures: more lexemes, but when you use them, you have less "adoption boilerplate"
04:09:02 <phadej> but if you use e.g. `Vector` ?
04:09:55 <phadej> if the [a] is the only container you ever use, then indeed the generalisation is net loss for you
04:10:01 <LordBrain> well, i am pointing out something i think is a discerning observation about the relative ranking of values and the evolution of ghc, you can disagree if you want, but i don't want to debate this utterly subjective judgment call.
04:10:21 <phadej> I agree on that, it's highly subjective
04:11:56 <LordBrain> it had to be this way too, imo, if they prioritized pretty over other stuff, nothing would ever get done
04:12:43 <LordBrain> people would bikeshed syntax forever
04:13:15 <LordBrain> but being aware of it is valuable i think, because it means you can consider how to clean it up
04:13:33 <LordBrain> without sacrificing other priorities
04:17:29 <kuribas> Why isn't any work being done on automatic simd vectorization for operations on numeric vectors?
04:26:37 <merijn> kuribas: There is some work on that
04:26:54 <kuribas> there is?
04:27:03 <merijn> kuribas: Recent GHCs have simd primitives and carter (and people in #numerical-haskell) are working on simd support too
04:27:12 <kuribas> ah, great
04:27:22 <merijn> kuribas: But that needs to crystalise before that can really be used by vector
04:27:35 <kuribas> merijn: simd only works for llvm, and only for a particular version.
04:27:51 <kuribas> merijn: so using simd in my project becomes somewhat of a hassle for users.
04:28:15 <kuribas> merijn: unless I write in C using the FFI
04:28:16 <merijn> kuribas: It becomes a hassle anyway, because using SIMD means compiling for a specific CPU architecture
04:28:38 <merijn> kuribas: Like, do your users support SSE? Which version? AVX? AVX2? etc.
04:28:56 <kuribas> merijn: true, but I think most modern desktops use x86 with SSE2 support.
04:30:21 <merijn> I think some people are making the LLVM backend less of a pain (i.e. generating bitcode resulting in GHC becoming portable to newer LLVM versions)
04:30:29 <merijn> Since it's currently tied to a specific one
04:30:50 <kuribas> right
04:31:34 <kuribas> merijn: a fractional instance for simd vectors would be very useful.
04:32:14 <kuribas> I guess currently the FFI is my best bet.
04:32:53 <merijn> kuribas: Yeah, I think so. Although this is also an area where minor knowledgeable contributions can be very helpful, since this doesn't/shouldn't require advanced knowledge of GHCs type system and things :)
04:33:41 <kuribas> merijn: you think adding more SIMD instructions to base would be appreciated?
04:35:48 <piyush-kurur> kuribas: as the developer of raaz I would appreciate having native SIMD in Haskell
04:36:15 <merijn> kuribas: I think things like SIMD primitives are very much something people would like to have, but something that, say, the Simon's aren't very focussed on doing
04:36:31 <kuribas> yeah, they have enough stuff to do...
04:36:47 <merijn> kuribas: So they remain "undone" until someone contributes them. See also the ability supply custom Cmm primitives which you might be able to leverage
04:37:18 <merijn> piyush-kurur: As I mentioned there is some SIMD in GHC already (maybe only when using llvm)
04:37:31 <merijn> piyush-kurur: Have a look at GHC.Prim: https://hackage.haskell.org/package/ghc-prim-0.4.0.0/candidate/docs/GHC-Prim.html
04:38:01 <carter> It's also work to fix the current simd design. I have ideas. But holly crap it's work I can sneak into day job
04:38:14 <carter> *can not
04:38:24 <carter> Also missing all the good stuff
04:38:29 <carter> And wrongly abstracted
04:38:35 <kuribas> There should be fund for this kind of stuff.
04:39:00 <carter> kuribas: that's not the bottle neck.
04:39:10 <kuribas> carter: what is?
04:39:53 <carter> More like the folks who know /want the better design directions are super busy ATM. And there's some huge sub tasks
04:40:06 <carter> In the mean time use c
04:40:15 <carter> For simd inner loops
04:40:32 <piyush-kurur> merijn: that is quite a bit in fact looking at it I would like to code chacha20 in haskell ;-)
04:41:05 <merijn> kuribas: Like I said, if you can contribute even some small amount of work, I think it'd be very welcome :)
04:41:13 <kuribas> merijn: right :)
04:41:57 <carter> Like help cleanup native code gen first
04:42:15 <kuribas> what's bad about it?
04:42:29 <piyush-kurur> merijn: do you know how to use llvm backend with stack
04:42:36 <piyush-kurur> I mean I want to try it out
04:42:38 <carter> Currently peephole optimizations are adhoc added to the asm pretty printer.
04:42:47 <carter> I mean idk take a look and dig in
04:42:57 <merijn> piyush-kurur: I don't know how to use stack, so no? ;)
04:44:46 <piyush-kurur> merijn: is it the case that the SIMD is supported only with llvm
04:44:47 <piyush-kurur> >
04:44:48 <piyush-kurur> ?
04:45:04 <kuribas> carter: I am reading about optimizing assembly language, it's pretty complicated.
04:45:20 <merijn> piyush-kurur: I don't know, tbh. Ask carter, he's the expert ;)
04:45:25 <kuribas> piyush-kurur: yes
04:45:46 <halogenandtoast> Does anyone have any suggestions on how to make this function better? https://gist.github.com/halogenandtoast/b92a1a12e981fa1e5e624d40f8c321e7#file-game-hs-L143
04:45:50 <merijn> kuribas: Well, yes ;) What kinda work are you doing that you're looking at optimising asm? :)
04:45:53 <halogenandtoast> I really don't enjoy the plethora of lets
04:46:07 <carter> kuribas: intel optimization manual is super readable
04:46:07 <merijn> halogenandtoast: You can define multiple variables with a single let :p
04:46:18 <merijn> Intel optimisation manual is great
04:46:50 <merijn> Also, if you're doing unpayed (so hobby stuff) or can convince your employer to pay for a license, grab Intel VTune Amplifier
04:46:50 <kuribas> carter: I am reading agner fog's stuff.
04:47:00 <merijn> VTune <3
04:47:08 <merijn> I will never optimise without it again
04:47:17 <merijn> Well, unless I'm not running on Intel :p
04:47:23 <merijn> I just it's GUI wasn't so slow
04:47:24 <kuribas> merijn: I want to optimize my graphics library.
04:47:27 <carter> Read the intel guide.  It's full of love
04:47:44 <halogenandtoast> merijn: Good point!
04:48:09 <kuribas> merijn: I want to use it for realtime applications, and there is a lot of computations that can be parallelized.
04:48:11 <merijn> halogenandtoast: In fact, you can move all of them (I think?) into the where clause
04:48:15 <halogenandtoast> That's marginally better
04:48:28 <carter> Agner is good too. But you need both. Agner is kinda what you read after intel optimization
04:48:31 <merijn> kuribas: See VTune and Intel optimisation manual recommendations :)
04:49:11 <carter> As merijn knows, it's not worth talking about any other recs till those are explored
04:49:11 <halogenandtoast> merijn: yes they can all be in the where, whis is furthermore marginally better
04:49:30 <carter> merijn: sadly there's no vtune for mac /:
04:49:37 <merijn> carter: Tell me about it :((
04:49:56 <merijn> carter: I've considered simply running a linux VM and VTune in there, should work, I think?
04:50:00 <sjpet> If I have a "type BigThing = (SmallerThing, OtherThing)", what's the generally used term for a function "thisFunction :: (SmallerThing -> SmallerThing) -> BigThing -> BigThing"? I can't quite use fmap, I feel, as this is more restrictive. Or is there a more idomatic way of dealing with incresingly complex data types that I'm overlooking?
04:50:23 <kuribas> carter: do those techniques work on AMD?
04:51:03 <carter> Probably.  Or read amd guide and compare.
04:51:56 <alexbiehl> sjpet: try `fmap (first f) bigThing`
04:52:26 <alexbiehl> no, that doesn't seem right
04:53:20 <alexbiehl> just `first` should suffice
04:56:49 <merijn> halogenandtoast: You can probably rewrite it a bit nicer, still, but I don't have too much time to puzzle about it at the moment 
04:57:44 <sjpet> alexbiehl: I'm not so much interested in how to do it as what to call it.
04:58:28 <merijn> sjpet: Well, a more specific abstraction for this specific example would be Bifunctor
04:58:41 <lyxia> halogenandtoast: is hand' used anywhere
04:58:59 <merijn> :t Data.Bifunctor.first
04:59:02 <lambdabot> Bifunctor p => (a -> b) -> p a c -> p b c
04:59:26 <merijn> > Data.Bifunctor.first succ ('a', True)
04:59:29 <lambdabot>  error:
04:59:30 <lambdabot>      Not in scope: ‘Data.Bifunctor.first’
04:59:30 <lambdabot>      No module named ‘Data.Bifunctor’ is imported.
04:59:33 <merijn> awww
05:04:59 <sjpet> Ah, that's probably gonna cover my needs. Thanks both of you.
05:05:41 <MarcelineVQ> @let import Data.Bifunctor
05:05:43 <lambdabot>  Defined.
05:10:35 <kuribas> how accurate is the ghc profiler in finding cost centers?
05:11:38 <eklavya> the state in ST is just a token or can it be used for something?
05:12:02 <merijn> kuribas: In what sense?
05:12:22 <hpc> ST is abstract, and doesn't expose any way to interact with the s type parameter
05:12:28 <kuribas> merijn: in finding bottlenecks...
05:12:28 <hpc> so i guess the answer is "it doesn't matter"
05:12:37 <merijn> eklavya: The state is a token that prevents leaking of mutable variables outside of the operation
05:12:47 <eklavya> hpc: why do we need both IO and ST then?
05:12:50 <merijn> eklavya: Or rather, it doesn't prevent leaking, but prevents using those that have leaked
05:12:59 <merijn> eklavya: You can exit ST, but not IO
05:13:05 <hpc> because ST is a pure interface to mutable computation with no external effects
05:13:11 <merijn> eklavya: So you can use ST inside a pure function
05:13:22 <eklavya> right
05:13:24 <hpc> and IO expresses arbitrary effects (which happens to include mutation)
05:13:30 <merijn> eklavya: In GHC (this is an implementation detail!) IO and ST use the same primitve implementation
05:13:48 <merijn> eklavya: IO just allows the visibility of mutation to escape the scope of the actual operation
05:13:57 <eklavya> yeah that's why I wondered why two and not one
05:14:22 <merijn> eklavya: Whereas ST restricts the scope where ST is implemented, so you can still implement functions that are internally using mutable state, but externally pure :)
05:14:25 <hpc> eklavya: eventually you may find yourself wanting to write your own restricted IO types as well
05:14:42 <hpc> it's a pretty handy way to exclude operations that you don't want to be able to do by mistake
05:14:44 <eklavya> understood
05:14:49 <merijn> eklavya: See also the, very readable, Lazy Functional State Threads paper: https://www.researchgate.net/profile/Simon_Peyton_Jones/publication/2295326_Lazy_Functional_State_Threads/links/0046351edd5bb96287000000.pdf
05:15:01 <kuribas> merijn: according to agner fog, profiling is not reliable.
05:15:08 <eklavya> so ST is special cased because mutation if local is not evil?
05:15:16 <hpc> correct
05:15:35 <eklavya> thanks hpc merijn :)
05:15:40 <merijn> kuribas: Pretty sure that statement depends on a few things, such as the type of profiling and regularity of work :p
05:16:27 <merijn> kuribas: For example, gprof and some other tools rely on sampling at an interval (assuming that if you have a bottleneck, you're more likely to sample during the bottleneck than out of it)
05:16:43 <kuribas> right
05:16:47 <merijn> kuribas: Which is unreliable if your slowdown comes from frequent use of VERY small operations (likely to be unsampled)
05:17:07 <merijn> kuribas: But, for example, VTune uses the CPU profiling counters which are MUCH more granular and unlikely to miss such things
05:17:09 <kuribas> "Unfortunately, profilers are often unreliable. They so metimes give misleading results or fail completely because of technical problems."
05:17:27 <merijn> kuribas: Additionally, profiling can mislead you if you're not profiling "real world data"
05:18:02 <kuribas> but ghc profiler doesn't suffer from this problem right?
05:18:46 <merijn> kuribas: I'm honestly not sure how the GHC profiler works
05:21:20 <delYsid> Is ByteString lazy?  Or, if I request a big .bz2 file via wreq and run it through Codec.Compression.BZip.decompress followed by a aeson decode, will the full decompress ByteString be in memopry, or will it somehow happen in chunks?
05:21:36 <hpc> there's a couple of questions in there
05:21:54 <kuribas> delYsid: you can choose between lazy or strict bytestrings.
05:21:58 <hpc> ByteString has both a strict version (whole thing in memory) and a lazy version (which is effectively a linked list of strict bytestrings)
05:22:24 <hpc> how much of the file will sit in memory depends on both which version is being used, and the algorithm itself
05:22:56 <ertes> i'm almost certain that wreq will read the whole response into memory, *even if* you choose lazy ByteString
05:22:58 <hpc> i thiiiiiiink you should see that it only keeps what is in memory though
05:23:02 <hpc> oh, or that
05:23:13 <delYsid> version of what?  The hackage libraries, or the BZip algo?
05:23:22 <Bish> halogenandtoast: ruby
05:23:27 <hpc> bzip2 iirc has the index structure at the beginning, so it's at least possible
05:23:46 <Bish> which is like the exact opposite to functional languages, with the state going everywhere
05:24:14 <ertes> delYsid: what you need is the response *stream* (most likely a Source) and feed that one into a streaming version of bzip2 like the bzlib-conduit package
05:24:22 <delYsid> well, having the bz2 stream in memory isn't that much of an issue, but the decompressed stream would need to be lazy for aeson to do its thing, without having to fully uncompress the whole thing first.
05:25:53 <ertes> delYsid: unfortunately wreq doesn't seem to support streaming, so your only alternative seems to be to use foldGet
05:26:09 <ertes> or abandon wreq and use http-conduit directly
05:26:28 <ertes> i generally found that wreq gets in my way more often than it helps, so …
05:27:10 <delYsid> well, I just used it once, and it served me very well so far, but I have no experience with the alternatives either.
05:28:31 <halogenandtoast> Bish: yeah I do a lot of ruby, it is kind of pretty
05:28:47 <zipper> ertes: Well I don't think you can have a thunk that says read more from network
05:28:55 <halogenandtoast> but I think once you try to do what haskell is doing the syntax just falls out kind of naturally
05:29:00 <delYsid> hmm, I guess laziness doesn't mean low memory consumption?  IOW, if a lazy value goes to a single consumer that parses it, nothing is freeing already read bytes until the wholee operation is finished, right?
05:29:19 <ertes> delYsid: if you decide to go with http-conduit, you can use httpSink or (in this case more likely) withResponse
05:29:51 <ertes> this should compose directly with bzlib-conduit, and then you can feed the result incrementally into aeson
05:30:20 <delYsid> ok, need to look into that, thanks.
05:36:47 <ertes> delYsid: laziness doesn't "save" memory in the sense that it lets you write an algorithm that consumes less memory…  what it does is to let you write and compose algorithms that work on partial data without explicitly representing that partiality in your code…  otherwise your whole code would need to handle incremental processing/partiality explicitly
05:37:45 <ertes> example:  filter p . map f  -- this starts to compute as soon as the first value is found, and it starts to produce as soon as the predicate lets a value pass
05:38:05 <ertes> > (take 1 . filter even . map (^2)) [1..]
05:38:08 <lambdabot>  [4]
05:38:51 <Bish> halogenandtoast: i believe that, that's why i am here
05:38:57 <ertes> what it does *not* give you is "lazy loading"…  it's a similar, but incompatible concept
05:40:32 <slaterr> hi
05:40:50 <zipper> delYsid: I had an issue that I solved by reading a lazy bytestring over the network
05:41:06 <zipper> Then `take`ing x bytes
05:41:10 <zipper> and parsing those
05:41:22 <slaterr> what is that function name where map (<function> f g) xs will give me [(f x1, g y1), (f x2, g y2)..]
05:41:28 <zipper> idk why but now it doesn't fill memory than when I was reading all of it
05:41:50 <zipper> delYsid: So I'd say try with taking the bytes you need from a lazy bytestring
05:42:03 <zipper> If that fails go for something greater
05:42:17 * zipper should learn pipes/conduit
05:45:27 <zipper> hpc: So the linked list sits on disk?
05:48:52 <lyxia> slaterr: (***)
05:49:07 <slaterr> :t (***)
05:49:10 <lambdabot> Arrow a => a b c -> a b' c' -> a (b, b') (c, c')
05:49:34 <slaterr> hmm doesn't seem to work
05:49:37 <slaterr> > map (id *** (2^)) [0..10]
05:49:40 <lambdabot>  error:
05:49:40 <lambdabot>      • Could not deduce (Enum (b, b'0))
05:49:40 <lambdabot>        from the context: (Num (b, b'), Num c', Integral b', Enum (b, b'))
05:49:56 <lyxia> slaterr: okay I misunderstood. What is y1
05:50:44 <lyxia> > map (id &&& (2^)) [0 ..]
05:50:47 <lambdabot>  [(0,1),(1,2),(2,4),(3,8),(4,16),(5,32),(6,64),(7,128),(8,256),(9,512),(10,10...
05:50:58 <slaterr> thanks, that was it
05:51:15 <slaterr> I never learned (or understood) arrows, so I couldn't hoogle the type
05:51:23 <stevenxl> Hi folks. Does the term polymorphic type imply ad-hoc or parametric polymorphism? For example, [a] is polymorphic, but (a, b) -> a is not? Or are both polymorphic?
05:51:54 <liste> stevenxl: parametric usually
05:51:58 <liste> stevenxl: both are polymorphic
05:52:11 <liste> there's type variables present
05:52:25 <stevenxl> liste: Sweet! Thank you .
05:53:05 <liste> stevenxl: you're welcome
05:53:06 <slaterr> when would one use ***?
05:53:15 <liste> :t (***)
05:53:17 <lambdabot> Arrow a => a b c -> a b' c' -> a (b, b') (c, c')
05:53:33 <merijn> lyxia: Isn't that better replaced with bimap nowadays?
05:53:46 <lyxia> is it?
05:53:51 <merijn> :t bimap
05:53:53 <lambdabot> Bifunctor p => (a -> b) -> (c -> d) -> p a c -> p b d
05:53:56 <merijn> :t (***)
05:53:58 <lambdabot> Arrow a => a b c -> a b' c' -> a (b, b') (c, c')
05:54:38 <lyxia> right they're equivalent in this context. That seems like a better choice. merijn++
05:54:51 <stevenxl> huh - null is an expression.
05:55:01 <stevenxl> oh i see
05:55:01 <stevenxl> is the list empty
05:55:01 <liste> stevenxl: it's a function actually
05:55:08 <liste> > null []
05:55:11 <lambdabot>  True
05:55:42 <stevenxl> liste: I'm not trying to be pedantic. Well actually I am because it really helps me to learn things when I know the terminology (hence my previous question). But isn't a function an expression?
05:56:00 <liste> stevenxl: it is, you're correct
05:56:13 <liste> stevenxl: an expression, more specifically a function
05:56:20 <liste> also a value
05:56:24 <stevenxl> Yea! :-)
05:58:45 <stevenxl> I don't know if it's because I've tried this a million times, but Yet Another Haskell Tutorial is the tutorial that is finally clicking.
05:58:52 <stevenxl> (So far, of course).
05:59:17 <stevenxl> Only on chapter 4 so still early.
05:59:20 <ertes> slaterr: zipWith
05:59:31 <ertes> together with (***)
05:59:52 <ertes> > zipWith (f *** g) [1,2,3] [10,20,30]
05:59:54 <merijn> stevenxl: If we're gonna be pedantic, saying a function is an expression is a confusion of meta levels :)
05:59:55 <lambdabot>  error:
05:59:55 <lambdabot>      • Couldn't match type ‘(c0, c'0)’ with ‘Integer -> c’
05:59:55 <lambdabot>        Expected type: (b0, b'0) -> Integer -> c
06:00:13 <lyxia> almost
06:00:16 <ertes> > zipWith ((^2) *** (*100)) [1,2,3] [10,20,30]
06:00:18 <lambdabot>  error:
06:00:19 <lambdabot>      • Couldn't match type ‘(b0, b'0)’ with ‘Integer -> c’
06:00:19 <lambdabot>        Expected type: (b0, b'0) -> Integer -> c
06:00:20 <merijn> stevenxl: expressions are a grammar concept (i.e. what does the language look like) whereas functions are more of semantics concept
06:00:23 <ertes> err…
06:00:24 <ertes> oh
06:00:35 <ertes> curry (***), then
06:00:58 <ertes> > map (f *** g) (zip [1,2,3] [10,20,30])
06:01:02 <lambdabot>  error:
06:01:02 <lambdabot>      • Ambiguous type variable ‘c0’ arising from a use of ‘show_M133918705352...
06:01:02 <lambdabot>        prevents the constraint ‘(Show c0)’ from being solved.
06:01:24 <stevenxl> merijn: Luckily  for me, I understand what semantics vs syntax so I get what you are saying there. I haven't had it explained to me as such. 
06:01:53 <ertes> lambdabot: i hereby terminate our friendship
06:02:15 <stevenxl> Can you explain what you mean by saying that an expression is more of a syntax? 
06:02:21 <stevenxl> related to syntax**
06:03:34 <merijn> stevenxl: Expressions are a syntax notion (i.e. what was written in the source file), when it comes to executing things, you generally deal with values (and functions are values)
06:04:01 <stevenxl> merijn: That makes sense. OK got it. Thank you. 
06:08:43 <hits1911> can we pattern match arguments such as "f n (2*m) = ....", that is, "f 3 4" gets n to match 3 and m to 2?
06:09:35 <merijn> hits1911: No, but you might be interested in viewpatterns?
06:10:03 <merijn> > let f n ((`div` 2) -> m) = (n, m) in f 3 4
06:10:06 <lambdabot>  (3,2)
06:10:54 <ahihi> or perhaps ((`divMod` 2) -> (m, 0)) instead
06:11:47 <merijn> Alternatively, you can use pattern guards to avoid needing an extension
06:12:11 <merijn> > let f n m' | m <- m' `div` 2 = (n, m) in f 3 4
06:12:14 <lambdabot>  (3,2)
06:13:12 <hits1911> is viewpatterns a ghc extension?
06:13:34 <merijn> Yeah
06:13:48 <merijn> The second one using pattern guards is standard Haskell2010, though
06:14:14 <hits1911> I'll go with pattern guards. Thank you very much.
06:18:09 <ahihi> > let f n m' | m <- m' `div` 2 = (n, m) in f 3 5 -- note that this will match odd numbers as well, which might not be what you want
06:18:11 <lambdabot>  (3,2)
06:18:47 <merijn> yeah, if you wanna avoid that you can mix ahihi's solution with pattern guards
06:39:14 <brynser> > showFFloat (Just 2) 1.0 ""
06:39:17 <lambdabot>  "1.00"
06:39:27 <brynser> https://hackage.haskell.org/package/base-4.9.1.0/docs/Numeric.html#v:showFFloat
06:39:38 <brynser> "if digs is Just d, then at most d digits after the decimal point are shown."
06:39:55 <brynser> To me, that makes it sound like the result should be 1, not 1.00
06:44:12 <kuribas> Is there a strict tuple type?
06:44:27 <kuribas> preferably unboxed?
06:45:34 <merijn> Unboxed tuples exist, but they're not what you'd naively expect
06:45:51 <merijn> unboxed tuples don't exist at runtime at all, they pass results in registers
06:46:39 <kuribas> merijn: I want to calculate two values, but not create a closure.
06:47:12 <kuribas> merijn: should I use seq instead?
06:47:24 <merijn> kuribas: Right, sounds like unboxed tuples might work for you
06:47:39 <merijn> It requires a bit of messy wrapping, though
06:47:54 <kuribas> merijn: Can I map them over a vector?
06:48:08 <merijn> Well, the thing is you have to reuse them immediately
06:48:17 <merijn> They don't exist in the sense of values
06:48:41 <merijn> So you can't, say, compute a vector of them
06:48:45 <kuribas> maybe I should just use seq with a normal tuple, to make sure both are evaluated...
06:48:54 <merijn> Or define a custom type?
06:49:34 <kuribas> merijn: that will get messy with Vector
06:59:42 <kuribas> Got a 8x performance increase by strictifying my function.  Who said lazyness is good for performance?
07:02:41 <dminuoso> Hi there. Im coming from a decent C++ and strong Ruby background and just compiled my first Haskell program and would like to get a serious taste. Is there a good book list you folks can recommend? Something akin to the Stroustrups TC++PL ?
07:06:28 <kuribas> dminuoso: dead tree book?  Or also online material?
07:08:29 <dminuoso> kuribas: I want to get some quality book, not a random "Learn what a Haskell monad is in 9 days" :)
07:08:39 <dminuoso> That is unless there's a really excellent online resource.
07:08:47 <lambdafan> does that book exist?
07:08:49 <kuribas> dminuoso: there is
07:09:19 <lambdafan> There's no better introduction right now other than the purple book
07:09:25 <lambdafan> but it's going to cost you
07:09:28 <kuribas> dminuoso: https://github.com/bitemyapp/learnhaskell
07:09:42 <dminuoso> lambdafan: Im willing to spend money, if it's a quality book.
07:09:46 <lambdafan> oh yeah that's the same guy that does the book I am talking about
07:10:30 <dminuoso> lambdafan: Do you mean this book? http://haskellbook.com/
07:10:30 <lambdafan> http://haskellbook.com/
07:10:34 <dminuoso> Heh.
07:10:35 <lambdafan> that's the one
07:10:39 <kuribas> dminuoso: I haven't used it, but I heard many positive things about that course.
07:10:48 <dminuoso> Boy that looks cheap.
07:10:57 <dminuoso> The way you phrased it I was expecting 100+ dollars.
07:11:18 <dminuoso> Which I have paid for my dragon book a while ago, so I wasn't scared. :P
07:11:25 <lambdafan> well, for what you get it's a bargain
07:12:22 <lambdafan> what I know about haskell is from patchwork sources, and so I have missing pieces. This book solidifies my knowledge.
07:12:29 <kuribas> is that $59 for an ebook?
07:12:39 <lambdafan> yep
07:13:07 <dminuoso> I wonder whether there's a hard copy.
07:13:09 <lambdafan> it's comprehensive, and well-done
07:13:17 <kuribas> At least I hope it goes to the authors, not to a publisher.
07:13:24 <lambdafan> that may be a future plan, I'm not sure about now
07:13:37 <lambdafan> oh yes the authors are self-publishing from what I can tell.
07:13:56 <kuribas> dminuoso: I learned from "Real World Haskell", but that's a long time ago, and they didn't update it.
07:14:08 <kuribas> which is a shame
07:14:16 <lambdafan> yeah "Real World Haskell" has some traps for you because it's outdated.
07:14:33 <lambdafan> but if it is not your first book, you can see the traps and avoid them
07:15:21 <dminuoso> lambdafan: Alright, it looks like they have a fairly extensive sample from the beginning (the first 4 chapters), Ill just try it and if it feels good Ill just buy the hard copy then. :)
07:15:32 <dminuoso> Though I can't find where to buy said hard copy.
07:15:36 <lambdafan> oh there's a hard copy? Nice!
07:15:53 <lambdafan> I don't think there is one, I was suprised to think so.
07:16:40 <lambdafan> the purple book was made by people who understand pedagogy.
07:17:00 <lambdafan> real world haskell was not, although the authors of that book are clearly experts in haskell, they are not in teaching haskell.
07:17:09 <lambdafan> you can tell by the nature of the exercises
07:17:14 <lambdafan> compare the two books
07:17:26 <lambdafan> the exercises in the purple book clearly build on each other 
07:17:45 <dminuoso> I just hope they don't give me the LSD trip kind of feeling that Haskell programs have invoked in me when staring at some complicated source code.
07:17:48 <dminuoso> :-)
07:17:51 <lambdafan> I asked one of the authors of RWH about their exercise choices, and they said they didn't put much thought into it.
07:17:58 <tapirus> ha
07:18:33 <lambdafan> ha?
07:19:21 <shapr> lambdafan: RWH was a huge improvement over the books that were published before.
07:19:36 <kuribas> lambdafan: uh, I disagree.  I think RWH is well written.
07:19:46 <lambdafan> shapr: no arguement there. And perhaps there would have been no purple book without it
07:19:49 <shapr> lambdafan: Before that, most coders didn't think Haskell had real world applications.
07:19:53 <shapr> lambdafan: Yes, I think so.
07:20:02 <glguy> dminuoso: "Programming in Haskell" is good
07:20:16 <lambdafan> that was the book that put the idea in my head, that haskell had practical application.
07:20:16 <shapr> Mind you, I much prefer "Haskell from First Principles" these days.
07:20:21 <glguy> and has been recently revised, and is available in hard copy
07:20:28 <robertkennedy> Parallel and Concurrent Programming in Haskell by Simon Marlow is amazing imo
07:20:37 <shapr> Oh yeah, I want to read the second edition of graham hutton's book.
07:20:39 <lambdafan> it's not a first book though
07:20:48 <shapr> oh, and I just started on Richard Bird's recent Haskell book.
07:20:54 <lambdafan> I mean the Marlow book
07:20:54 <tapirus> lambdafan: sorry, it just seemed like a strange thing to come from an author's mouth, it wasn't intended as snark or disbelief towards you :)
07:21:35 <shapr> Of course, I think #haskell is the best of all the options ;-)
07:21:41 <lambdafan> tapirus: oh he meant the order in which they were presented. I always appreciate exercises that build on prior work, and the exercises seemed individually useful but patchwork in their order.
07:21:55 <robertkennedy> Oh yeah sorry I missed that it was for someone that fresh
07:21:57 <tapirus> ah right
07:21:58 <lambdafan> so the author was referring to the order of the exercises, and their relationship to each other
07:22:22 <tapirus> that's a bit less extreme then :) rather than "we had a 2 hour brainstorming session and came up with some fizz buzz problems"
07:22:32 <shapr> I do think haskellbook.com has much better exercise ordering, and gradiations in difficulty
07:22:48 <tapirus> or whatever the real world equivalent of fizz buzz is..."read a file and parse a list of numbers"
07:24:20 <lambdafan> "make a crud app in yesod" <- real world fizzbuzz
07:36:58 <robertkennedy> Making a crud app in yesod was my first big Haskell project lol
07:37:46 <lambdafan> robertkennedy: I mean the simplest version of such.
07:41:35 <Aku> How to get started with IRC channel?
07:42:40 <Tuplanolla> Observe it quietly until you feel confident enough that you can pretend you've always been around, Aku.
07:43:06 <jackhill> Aku: welcome. I don't think you have much more to do to get started, you can feel free to ask questions or comment on what others say.
07:43:10 <shapr> Aku: although I like your approach by asking directly how to get started :-D
07:43:36 <shapr> Aku: Have you written any Haskell code recently?
07:43:47 <Aku> Guys it feels friendly here!
07:44:17 <Aku> Ya reading Learn you a Haskell
07:44:23 <mlehmk> of course it is friendly here
07:44:39 <Aku> I keep getting parse Errors
07:44:48 <Aku> How do I go about it?
07:45:09 <mlehmk> examine it, also mind; sometimes whitespace is important
07:45:31 <shapr> Aku: you go to lpaste.net, paste in the code that has an error, click "public" and then share the link here in the channel.
07:45:49 <mlehmk> especially indentation should be consistent if the examples have it
07:52:53 <Aku> shapr: Thanks
07:54:13 <lpaste_> Aku pasted “trying” at http://lpaste.net/8840934035556401152
07:55:30 <Tuplanolla> That's incomplete, Aku.
07:55:33 <Aku> This is giving me parse error
07:55:56 <Aku> Tuplanolla : How?
07:56:32 <Sornaensis> Aku: what is Alter a, what is IsRec
07:56:41 <Sornaensis> what is Name
07:57:37 <Aku> Ohh..I have the code for it below and havent pasted in lpaste, it's giving me a parse error
07:58:10 <Habib> Does anyone know a version of fmap that may return a totally different functor?
07:58:32 <halogenandtoast> :t fmap
07:58:34 <lambdabot> Functor f => (a -> b) -> f a -> f b
07:58:37 <halogenandtoast> nope
07:58:46 <Tuplanolla> Lenses come to mind, Habib.
07:58:57 <Habib> Sorry, I mean a function that's similar to fmap.
07:59:13 <halogenandtoast> Tuplanolla: Lenses or Prisms?
07:59:15 <Habib> Thanks, Tuplanolla.
07:59:20 <Habib> I'll take a look.
07:59:34 <lyxia> traverse
08:00:20 <halogenandtoast> traverse :: (Traversable t, Applicative f) => (a -> f b) -> t a -> f (t b)
08:00:30 <Tuplanolla> @info Lens
08:00:31 <lambdabot> Lens
08:00:36 <Tuplanolla> Not useful, lambdabot.
08:00:47 <Habib> Is Lens a typeclass?
08:00:56 <Habib> :i Lens
08:01:11 <Tuplanolla> It's a type alias for `Functor f => (a -> f b) -> (s -> f t)`, Habib.
08:01:52 <halogenandtoast> type Lens s t a b = forall f. Functor f => (a -> f b) -> s -> f t
08:02:17 <lpaste_> Aku revised “trying”: “Getting parse error on |” at http://lpaste.net/8840934035556401152
08:02:23 <Habib> Ah, I'll take a look, guys, thanks. Gotta dash to the library. See ya. Thanks for your help.
08:02:27 <Aku> Help?
08:02:35 <Tuplanolla> Since you want mappings between functors, most likely natural transformations, lenses provide nice ways to define them, Habib.
08:04:13 <Aku> I am getting parse error, I don't know why, can somebody help?
08:04:42 <lpaste_> Aku annotated “Getting parse error on |” with “Getting parse error on |” at http://lpaste.net/8840934035556401152#a1338909608751661056
08:05:43 <halogenandtoast> Aku: it's probably spacing, I removed the newlines and it compiled for me.
08:06:20 <Aku> halogenandtoast: Ohh..thanks
08:06:41 <halogenandtoast> if you are inconsistent it can hurt you.
08:07:08 <halogenandtoast> well incosistent and don't do it the way haskell is expecting...
08:07:16 <Aku> Can post the code after removing new Lines @ halogenandtoast?
08:07:40 <halogenandtoast> I guess...
08:07:56 <Aku> I don't get how it is inconsistent?
08:08:22 <lpaste_> halogenandtoast annotated “Getting parse error on |” with “Getting parse error on | (annotation)” at http://lpaste.net/8840934035556401152#a352944
08:08:50 <Aku> halogenandtoast: Thanks
08:09:53 <halogenandtoast> no problem. Heading to bed it's 1:10 am here :[
08:12:48 <Geekingfrog> In the middle of a conduit I'd like to add something which inspect the values and modify some internal state before putting the value back in the stream. Is there a combinator to do that or do I need to use some MVar + await/leftover ?
08:14:09 <merijn> Geekingfrog: You can use StateP or whatever it's called to keep local state inside a conduit
08:14:40 <thomasd> what are the goto lang extensions/type level trickery that allow me to do something like `foo True = 5` and `foo False = "hello"`? or is this just absurd? a coworker mentioned this needs dependent types.
08:15:22 <johnhw>  In the first theorem of http://www.cs.cmu.edu/~rwh/papers/chitt/popl17.pdf, what does the notation bool [\cdot] mean?
08:17:49 <johnhw> Context: If M ∈ bool [·] then either M ⇓ true
08:18:49 <Geekingfrog> merijn: there is Data.Conduit.State which may work for what I need. Thanks
08:19:50 <tapirus> Anyone here familiar with the fork and hook constructs of the J programming language? Anyone feel that Haskell would benefit from something similar? (or if it would benefit from any of the other features of J)
08:20:05 <rozencrantz> thomasd: As far as I know this is impossible even with dependent types. Although I may be wrong if anyone would like to correct me.
08:20:25 <shapr> tapirus: I think Haskell needs more APL influence
08:20:48 <shapr> tapirus: but less humorously, I've seen some papers about making everything postfix in Haskell
08:21:27 <shapr> hm, the only one I can find on google is okasaki from 2002, but I'm certain I've seen more recent papers
08:23:24 <thomasd> rozencrantz: damn... yeah I guess runtime values influencing return types is more like dynamic typing. what I want is `foo :: SomeConstraint b => MyType -> b` where `b` is decided upon by which constructor of MyType is supplied. and then to write all other functions in the form of `f :: SomeConstraint a => a -> b`
08:23:45 <thomasd> scratch the part about dynamic typing
08:23:52 <tapirus> taking a look at that Okosaki paper
08:25:38 <shapr> tapirus: Maybe something way more APL would be to mix the GPU array-oriented libraries with some reimplementations of the original APL operators?
08:28:30 <mnoonan> johnhw: I definitely don't know this stuff, but later in the paper (pg 685) they talk about types of the form T[Psi]
08:29:01 <shapr> tapirus: my viewpoint on that is: J and K are spiritual successors to APL, and one of APL's strong points was array operations. Today's popular array ops are done in GPUs.
08:29:20 <mnoonan> Psi is some kind of relation, so presumably . is some kind of trivial case?
08:29:29 <Tuplanolla> My guess is that the dot is a placeholder for a parameter, like `_` in Haskell, johnhw.
08:29:53 <tapirus> shapr: sounds smart to me
08:30:02 <mnoonan> oh, that's probably right
08:30:24 <johnhw> mnoonan: yes, that's indeed it (someone else already told me).
08:30:47 <johnhw> mnoonan: unfortunately, on that same page they are themselves inconsistent in the notation.
08:31:00 <mnoonan> heh
08:31:06 <johnhw> mnoonan: see the reference to M \in bool without the brackets after it.
08:32:41 <rozencrantz> thomasd: Yeah I'm almost certain your example impossible even with dependent types. The return type of functions is known at compile time. You could do something like `foo :: Num a => MyType -> a`. But to actually use foo in the program, you'll need to know the type of a unambiguously.
08:33:24 <rozencrantz> Please let me know if I'm not using IRC correctly, this is my first time ever using it.
08:33:46 <thomasd> rosencrantz: exactly
08:33:54 <thomasd> and unfortunately
08:34:21 <stphrolland> Hi everyone. I'm just wondering if something doing that exists: like we have $ for function application, is there something for type application, that would allow to write something like f :: a -> IO $ Maybe a   instead of f :: a -> IO (Maybe a)     put the wanted type operator instead of the $ I put there in IO $ Maybe a  ???
08:34:26 <thomasd> I'm not convinced, though, that this idea is impossible. maybe with the particular type signatures I gave it is.. but the underlying idea might still be valid
08:34:27 <stphrolland> I'm just wondering if that exists
08:35:33 <thomasd> stphrolland: if it doesn't, you could try to define it as a type level operator that's basically syntactic sugar for parenthesis
08:35:46 <mauke> @let type a $ b = a b
08:35:48 <lambdabot>  Defined.
08:35:48 <Benzi-Junior> is it possible to tell ghc to ignore what is exported from a particular module and import it as if everything in it was exported
08:35:48 <Geekingfrog> mmm, wait. The stateful conduit stuff are actually in a very old version of the library. On the most recent ones I couldn't find anything :/
08:35:58 <Benzi-Junior> for testing purposes
08:36:18 <mauke> > return . return :: a -> IO $ Maybe a
08:36:19 <mnoonan> stphrolland: https://hackage.haskell.org/package/type-operators-0.1.0.4/docs/Control-Type-Operator.html
08:36:21 <lambdabot>  error:
08:36:21 <lambdabot>      • No instance for (Typeable a0)
08:36:21 <lambdabot>          arising from a use of ‘show_M514524567618444706125178’
08:36:25 <thomasd> mauke: nice
08:36:25 <mauke> neat
08:37:19 <stphrolland> mnoonan ... just wow...
08:38:21 <johnhw> Monad transformer stacks where in vogue a decade ago. Are they replaced by something else in the meanwhile? 
08:39:37 <byorgey> johnhw: well, effect handlers are a cool new alternative, but I certainly wouldn't say they have "replaced" monad transformer stacks.
08:39:47 <byorgey> monad transformer stacks are still quite prevalent in practice.
08:40:12 <johnhw> byorgey: are all the action evaluations compiled away?
08:40:24 <mauke> > [] :: [] $ Maybe Int
08:40:26 <byorgey> johnhw: I don't understand your question.
08:40:28 <lambdabot>  []
08:40:54 <byorgey> mauke: wat
08:41:04 <byorgey> oh, I see the @let above
08:41:15 <merijn> Good, I was almost worried
08:41:28 <shapr> Silly question, how do I send money to the people curating libraries for stack? Is there a Patreon or something?
08:42:05 <rozencrantz> thomasd: I may be wrong. But you'll have to define some type class which encapsulates all return values of foo. I think instead you could wrap the return values of foo in a newtype like this: `foo:: Constraint a => MyType -> Wrapper a` then have all f be of the following `f:: Constraint a => Wrapper a -> b'
08:42:19 <johnhw> byorgey: if you compose lots of monads, there is lots of pattern matching going on, which wouldn't exist if one would just write everything in the I/O monad with e.g. Refs. In order for monad transformer stacks to be used in innerloops, they should be a zero cost abstraction.
08:42:48 <stphrolland> mauke I'm not sure [] $ Maybe Int  is more readable that [Maybe Int] ... but it's nice to see it possible
08:44:40 <byorgey> johnhw: OK, I see.  They are not zero cost, though there are tricks you can use to get the overhead down pretty low (e.g. using CPS-encoded versions of things, etc.)
08:45:23 <byorgey> abstractions are almost never zero cost.
08:46:22 <stphrolland> just tried it, it needs to use the {-# LANGUAGE TypeOperators #-} , but works like a breeze it seems
08:47:24 <shapr> tapirus: Is J's fork something like &&& in Control.Arrow?
08:48:53 <dolio> shapr: It's more like defining Num instances for functions, I think.
08:49:06 <shapr> huh, I don't even
08:49:09 <dolio> So you can say 'sin^2 + cos^2'
08:49:27 <dolio> Instead of \x -> (sin x)^2 + (cos x)^2
09:02:29 <Aku> How do I got about learning Monads?
09:05:08 <Aku> I have started learning Haskell quite recently, how should I go about leaning Monads?
09:05:17 <Aku> *learning
09:06:53 <orion> Aku: Study the definitions. Do not let metaphors enter your mind.
09:06:58 <orion> And the laws.
09:07:27 <tapirus> shapr: I'm not sure, I'm still a beginner when it comes to haskell. But in J the classical pedagogical example of a fork would be average =: (+/ % #), or (sum divide mean)
09:07:43 <Aku> orion: Ohhkay
09:07:57 <chreekat> Aku: Know that 'monad' is an abstract concept, and do as orion says, and practice *using* individual instances of Monad, like Maybe and Reader
09:08:04 <koala_man> Aku: some say the best way is to just write programs with IO, State and Writer until you're comfortable with them, and then look at the definitions to see how they're related
09:08:09 <chreekat> ^
09:08:13 <tapirus> shapr: so for instance, '(sum divide mean) list' would be interpreted as (divide (sum list) (mean list)) in prefix notation
09:08:17 <koala_man> like chreekat just did
09:09:09 <Aku> Ohhkay..
09:10:47 <tapirus> shapr: or more generally, '[x] ( f g h k l) y' is equivalent to ((x f y) g ((x h y) k (x l y))) etc. in infix (where [x] means x is optional)
09:11:03 <tapirus> does that relate to &&& at all?
09:17:17 <nitrix> Aku: I'd say, stick stricly to their definition and then look at a few simple examples like the Maybe monad. Ideally, you'd have a strong understanding of Applicatives and Functors first (and type classes in general).
09:18:03 <Tuplanolla> @where burritos
09:18:03 <lambdabot> I know nothing about burritos.
09:18:42 <nitrix> Aku: Alternatively, we can walk the steps and I can attempt to make you "discover" monads on your own intuitively on #haskell-beginners. I have some free time :P
09:22:27 * orion wants to witness that
09:23:49 <nitrix> Depending on your background, we could talk about endofunctors and monoids too to arrive at monads :P
09:25:30 <Aku> Thanks nitrix for help but I am bit busy, I was gonna start Monads soon so just asked!
09:25:55 <nitrix> Feel free to come for help if you get stuck :)
09:26:28 <Aku> Sure :D
09:26:56 <Aku> How to quit from the channel?
09:26:58 <Aku> I am new!
09:27:10 <hydraz> ./part
09:27:14 <nitrix> /part will leave the channel, /quit will disconnect completely.
09:27:30 <Aku> And then how to join back?
09:27:36 <byorgey> tapirus: I'm familiar with J, and I don't think adding fork/hook to Haskell would be a good idea.  Too much syntactic complexity for not much benefit.
09:27:36 <nitrix> /join #haskell
09:27:47 <Aku> ohhkay
09:29:04 <byorgey> tapirus: on the other hand, the way J automatically resizes arguments to match is really cool.  For a nice take on that in a functional context, see this recent paper by Jeremy Gibbons: http://www.cs.ox.ac.uk/publications/publication10857-abstract.html
09:33:14 <byorgey> tapirus: note in Haskell you can just define e.g. fork as a higher-order function
09:33:34 <byorgey> > let fork f g h y = (f y) `g` (h y) in  fork sum (/) genericLength [1..6]
09:33:37 <lambdabot>  3.5
09:34:11 <byorgey> which is why I claim there would not be much benefit to including it as built-in syntax.
09:34:23 <Tuplanolla> > liftA2 (/) sum genericLength [1 .. 6]
09:34:26 <lambdabot>  3.5
09:34:40 <Tuplanolla> Even better: we already have it.
09:34:53 <hydraz> nice
09:35:03 <byorgey> yes, good point, although that is not going to make sense to a beginner yet =)
09:35:48 <Tuplanolla> It's nice to notice that J has implicit `liftAn` for all `n`.
09:35:58 <tapirus> byorgey: interesting, thanks :) reading this now
09:36:30 <byorgey> Tuplanolla: not exactly, it kind of iterates liftA2
09:36:57 <byorgey> if you give a big string of operations it builds a binary tree out of them
09:37:11 <Tuplanolla> Okay, almost.
09:37:13 <zennist> quick question: what function do you use such that: Traversable t => t a -> [a] 
09:37:28 <zennist> i can think of using folds like: foldMap pure
09:37:33 <zennist> but that's for Foldable instance
09:38:06 <byorgey> zennist: well, every Traversable has to be Foldable.
09:38:14 <zennist> obviously you can go for something like: getConst . traverse (Const . pure)
09:38:15 <byorgey> zennist: you want Data.Foldable.toList
09:38:22 <zennist> but something shorter please
09:38:39 <zennist> ah okay - my oversight then; awesome
09:51:15 <zennist> question again: can you implement 'unzip' via traverse
09:52:03 <zennist> i got this working by: traverse (\(a, b) -> ([a], b))
09:52:10 <zennist> but don't think very satisfactory
09:53:51 <dolio> You can implement map with traverse, and unzip is a combination of map fst and map snd.
09:54:35 <monochrom> Neat.
09:54:52 <zennist> well the output structure is different, so it's not just fold
09:55:22 <zennist> not just *map*
09:55:49 <zennist> although you can do it with just folding
09:56:32 <dolio> Yes, you can also do that.
09:57:18 <zennist> didn't try it out, but you should be able to write: unzip = foldMap (bimap pure pure)
09:58:22 <zennist> but obviously a naive implementation of foldMap means we go through the list twice
09:58:33 <zennist> sorry - no, my fault
09:59:16 <c_wraith> the problem with naive unzip is that it's spine-strict in the list. 
10:00:02 <shapr> Aw, aku left
10:00:46 <zennist> which 'naive unzip' you are referring to?
10:01:11 <c_wraith> just about any unzip that isn't very careful with laziness. 
10:01:41 <drostie> yeah the (Monoid a, Monoid b) => Monoid (a, b) instance is definitely where I'd be going with that.
10:02:26 <drostie> foldMap (bimap pure pure) should indeed get you there.
10:02:56 <dolio> That one might not be lazy enough.
10:03:03 <dolio> Not sure.
10:03:15 <c_wraith> yeah, I worry about mappend for that instance. 
10:05:43 <drostie> You're right.
10:05:55 <drostie> let t10 (x, y) = (take 10 x, take 10 y)
10:06:25 <zennist> mhh imagine it's just (a, b) <> (c, d) = (a <> c, b <> d);; why is tup1 <> tup2 <> tup3 would cause the evaluation of the insides..? (I'm not good with laziness analysis) ; wouldn't it stop at the toplevel (<>) operator?
10:06:31 <drostie> t10 . unzip $ zip [1..] [0..] completes, t10 . foldMap (bimap pure pure) $ zip  [1..] [0..] doesn't.
10:07:24 <c_wraith> zennist, you want to watch for forcing the (,) constructor in the second argument. 
10:07:25 <dolio> Right, it'd need to be at least (a, b) <> ~(c, d) = (a <> c, b <> d)
10:07:58 <zennist> i see
10:08:03 <zennist> makes sense
10:08:22 <drostie> zennist: suppose we define `data Right x = E | R x` and then `instance Monoid Right where mempty = E, mappend (R x) y = case y of E -> R x; R z -> R z`
10:08:35 <dolio> It's not hard to write unzip correctly without thinking about it, though.
10:08:38 <dolio> Depends on your habits.
10:09:15 <drostie> zennist: foldMap doesn't know that that's not your monoid, and that monoid needs to have all the list information present before it can give you an answer. I think.
10:09:17 <dolio> If you always reach for case, you won't. If you prefer let instead, you will.
10:09:39 <c_wraith> I'd you write it correctly without thinking about it, it was an accident. :) 
10:10:20 <dolio> Just like if you write it wrong without thinking about it.
10:10:23 <zennist> okay logically it makes sense; but on the detail level - say  t10 forces the (,) constructor, but wouldn't the x, y arguments still stay as unevaluated thunks? then the take 10 x and take 10 y can do their magic
10:10:32 <c_wraith> dolio, I agree! 
10:11:00 <zennist> i'd imagine at the point of calling t10, x and y are just tons of builds ups of 1<>2<>3<>....?
10:11:08 <shapr> byorgey: thanks for the link to that Jeremy Gibbons paper, I should just subscribe to his RSS publications feed.
10:11:30 <c_wraith> zennist, the problem is how much work needs to be done before producing the result (,) constructor 
10:11:31 <dolio> zennist: t10 isn't a recursive function.
10:12:20 <zennist> c_wraith: wow, that's a brilliant way of thinking
10:12:57 <c_wraith> zennist, I wrote an answer on SO that applies here.. http://stackoverflow.com/questions/42150614/why-is-the-lazy-pattern-match-version-of-splitat-function-faster/42151076#42151076 
10:13:12 <c_wraith> zennist, that may provide more detail, if you want it. 
10:13:19 <shapr> Aha, so it's Carl_wraith
10:13:27 <c_wraith> my secret code! 
10:13:33 <c_wraith> broken forever! 
10:13:37 <shapr> :-)
10:14:18 <zennist> haha
10:14:26 <zennist> thanks guys - learned something today
10:24:25 <drostie> Ah c_wraith, I see what you were saying. Took me a second. Restating in my own words: the instance `mappend (a, b) (c, d) = (a <> c, b <> d)` is strict in both arguments because it pattern matches on both of them, so even though foldr is lazy in general foldr mappend mempty for this instance needs to destruct every single tuple in the input list. Is that about right?
10:25:06 <c_wraith> drostie, yes indeed. that's exactly it. 
10:25:18 <drostie> (And `foldMap f` is just `foldr mappend mempty . map f` in this case hence the result applies to foldMap too.)
10:28:05 <c_wraith> this is a case where there are unfortunate choices to be made. some applications of mappend for tuples want that strictness. some do not. 
10:28:51 <c_wraith> the best we can really manage is utility newtypes for this sort of thing, I think. 
10:30:17 <kmelva> any selenium/webdriver users here? How to reuse an already open session in my tests?
10:30:43 <kmelva> trying to use REPL to write tests, but have no clue how to reuse an already open session... nothing in the webdriver docs that helps :/
10:31:56 <Jello_Raptor> Can someone help me understand this tweet? https://twitter.com/parametricity/status/558492726178881537
10:32:41 <byorgey> shapr: you can't go far wrong subscribing to anything by Jeremy Gibbons.
10:32:49 <Jello_Raptor> It's a joke, and it looks like the function is like Profunctor.lmap with a context. 
10:33:47 <dmwit> I suspect that type is not implementable.
10:33:58 <byorgey> Jello_Raptor: sounds to me like you already understand the tweet.
10:34:02 <Jello_Raptor> oh nevermind, I can see a practical example of how you'd use it, at least if you change the constraint to `Monad f => ...` 
10:34:34 <Jello_Raptor> byorgey: yeah :P it took me a bit though
10:34:46 <byorgey> the type is definitely not implementable with only Profunctor and Functor constraints.
10:34:55 <Jello_Raptor> oh? 
10:35:03 <dmwit> e.g. specializing `f` to `[]` and `p` to `->`, we would get `(a -> [b]) -> (b -> r) -> [a -> r]` which... seems unlikely.
10:35:16 <Jello_Raptor> byorgey: how'd you go about showing that? 
10:36:20 <byorgey> Jello_Raptor: to prove it formally?  I dunno, something something parametricity something.  But intuitively, all you can do is map stuff, there's no way to pull an f across the p like that.
10:36:36 <dmwit> right
10:37:08 <byorgey> or give a counterexample as dmwit has done.  You could probably come up with an even simpler counterexample and give a proof by cases that no such function can exist.
10:37:26 <Jello_Raptor> byorgey: ahh, yeah the intuitive explanation was all I'm looking for. Is there a `class Profunctor p => ??? p` which would let you do that? 
10:37:29 <byorgey> though to be completely formal it will still ultimately depend on parametricity.
10:37:30 <dolio> Probably don't need parametricity.
10:37:46 <byorgey> Jello_Raptor: I don't know.  Good question.
10:37:47 <dolio> Just choose the right values for the variables and get a contradiction.
10:38:02 <byorgey> dolio: hmm, you're probably right
10:38:20 <byorgey> ah, yeah, showing that no such function can exist is easier than showing that all such functions must have property P
10:38:23 <dolio> Or something that can only be bottom in Haskell's case, I guess.
10:38:32 <Jello_Raptor> dmwit: right :/ that type really doesn't make sense. 
10:38:49 <dmwit> The beauty of `Functor` is that it's such a weak constraint that practically everything is a `Functor`. The downside is that you can hardly do anything if all you know is `Functor`. Ditto with `Profunctor` everywhere.
10:39:14 <Jello_Raptor> all: i looks like I didn't get the joke, or at least one of the jokes, thanks :) 
10:39:35 <dmwit> I think you got it just fine.
10:40:18 <dmwit> They're simultaneously roasting the "don't need docs because types" attitude and the "let's have lots of operators" attitude. The fact that their joke type isn't implementable is just icing on the already delicious cake.
10:40:31 <Jello_Raptor> dmwit: didn't realize the unimplementability :P the other bits I got 
10:40:43 <dmwit> right
10:41:55 <mauke> > let foo :: (a -> [b]) -> (b -> r) -> [a -> r]; foo _ _ = [] in ()
10:41:58 <lambdabot>  ()
10:42:09 <dolio> dmwit: Is the joke that you figured out what it does just by looking at the type?
10:42:38 <dmwit> mauke: Yeah. The thing is we want an implementation of the concrete type that only uses things available to all Functor/Profunctor's.
10:42:43 <dmwit> Which... `[]` isn't.
10:43:55 <Jello_Raptor> unrelatedly I've got a hacky thing I'm using to get better error messages for a project of mine, but there's got to be an actual pattern for this sort of thing: http://lpaste.net/352946 
10:44:26 <Jello_Raptor> This is mostly just a kludge for time reasons, but I'd like to know what I should be doing instead. 
10:45:55 <Jello_Raptor> basically each monadic function gets a "errContext contextString $ do ..." tacked in front of it, but this can't be the right way of annotating errors 
10:45:58 <dmwit> That's basically fine. The only thing I'd do mildly differently is have a real structured type for your errors, and a separate pretty-printer for turning that into `String`s.
10:46:27 <Jello_Raptor> dmwit: huh, okay. Good to know, thanks. 
10:46:27 <dmwit> So that you can programmatically do other things with the errors than turn them into `String`s if you need to.
10:46:52 <AWizzArd> Are the builds at http://ghcjs.luite.com/ trustworthy?
10:47:25 <shapr> luite is trustworthy
10:47:30 <dmwit> Jello_Raptor: See also `mapExceptT`. And you might like the new `CallStack` stuff in GHC.
10:47:53 <Jello_Raptor> part of why I chose String is to tie my hands a bit, I'm using that exceptT for places where I should be using `error`, but with the added benefit of better notes on why that error was triggered. 
10:48:11 <Jello_Raptor> dmwit: mmm?
10:48:15 <dmwit> Jello_Raptor: https://hackage.haskell.org/package/base-4.9.1.0/docs/GHC-Stack.html
10:51:21 <kmelva> here it is: http://stackoverflow.com/questions/42446192/reuse-existing-selenium-session-with-haskell-and-webdriver ... if anyone has an idea, hit me :)
10:52:54 <Jello_Raptor> dmwit: cool, annotated source locs would be very nice. Thanks. Though i don't see a way to append Human readable metadata to the call stack. Which would allow me to use it as a more general error message mechanism. 
11:11:46 <Sonolin> j
11:12:16 <Sonolin> sorry, I think I need a filter on my irc for vim keys...
11:24:34 <Jello_Raptor> Does anyone know why the haskell spec doesn't allow for infix operators with normal characters in them? something like `var_op ::= infix_char ([infix_char,alphaNum]*infix_char)?` so that the parser can tell operators apart since the first and last char of the operator is from the current set of allowed chars, but internal characters can come from the full range? 
11:24:38 <Jello_Raptor> I'm just curious about the reasoning since, in many cases, operators like `<pSub>` or whatnot would let us have nice oeprators with a much lower cost for readability
11:25:23 <Jello_Raptor> also, the fact that we can have `.<.` `.>.` `.^.` but not `.v.` annoys me  
11:26:07 <Tuplanolla> Then people would have to separate tokens with spaces like civilized human beings.
11:26:12 <Tuplanolla> Can't have that.
11:26:17 <Jello_Raptor> :| 
11:26:21 <Jello_Raptor> seriously? 
11:26:25 * Jello_Raptor grumbles 
11:27:28 <Jello_Raptor> literally the only place where that even makes a tiny amount of sense is composing lenses and even then only if you insist on making it look like record update syntax in more imperative languages. 
11:27:39 <Jello_Raptor> or well, the only place I've seen where it makes sense. 
11:27:41 <taktoa> Jello_Raptor: use the "binary or symbol"
11:27:55 <Tuplanolla> Consider the excited spectator operator `\o/` for example.
11:28:02 <taktoa> lol
11:28:20 <Tuplanolla> Suddenly everyone who writes `\x -> x` instead of `\ x -> x` would have broken programs on their hands.
11:28:20 <Jello_Raptor> taktoa: I add spaces around `||` <_<
11:28:27 <Tuplanolla> I wouldn't mind that, but I know some people would.
11:28:46 <taktoa> no I meant the unicode symbol that looks like "v"
11:29:21 <taktoa> http://www.fileformat.info/info/unicode/char/2228/index.htm
11:29:59 <Jello_Raptor> ahh yeah, problem is that unicode symbols require more special pleading to be useful. You have to set up your dev environment to support it. I'd much rather stick to standard ascii when possible. So that I'm not adding additional hurdles for other people working on my projects. 
11:30:30 <taktoa> ah, true. as an emacs user I have a weird perspective on that ;^)
11:30:47 <monochrom> No, consider the excited robot operator \∩/
11:31:58 <taktoa> I definitely agree that a more Agda-like perspective on Haskell tokens would be nice
11:32:05 <taktoa> maybe even just with a language pragma
11:35:02 <cdidd> Is there a programming language with strict evaluation, full type inference for core language subset, and with parametric polymorphism/type clases/higher kinded types ?
11:40:07 <monochrom> probably Idris
11:40:50 <cdidd> monochrom, I thought you need to annotate everything with types in Idris. No?
11:41:02 <monochrom> Ah, nevermind.
11:46:03 <taktoa> cdidd: only principal types, which you really should be annotating anyway
11:54:42 <dolio> That's not what "principal types" means. :)
11:59:31 <flxw> Hi all. I'd liketo ask a theory question. The idea of free theorems seems usually to be expressed using logical relations and parametricity. I have no experience with either of them. Lately someone mentioned in passing, that the free theorems idea could be explained using natural transformations as well. So as a concrete question: What kind of categorical setting would one need to show that id: a -> a has a unique inhabitant, nam
11:59:31 <flxw> x = x (modulo bottoms to make it easier for starters like me)?
12:00:56 <monochrom> There will be many different things, all called "identity" confoundingly, in this example.
12:01:29 <monochrom> id is a natural tranformation from the Identity functor to the Identity functor.
12:01:44 <dolio> I'm not sure the person was exactly telling you the truth.
12:02:07 <dolio> The free theorem for that type tells you that `id` is natural.
12:02:46 <dolio> But the reason for inventing naturality is that not all definable operations with that 'type' in mathematics are natural.
12:03:08 <robertkennedy> Philip Wadler's "Category Theory for the Working Hacker" talk proves that `(,) <$> fst <*> snd` is unique. Maybe look at that?
12:03:37 <monochrom> So, for all object A, id_A :: Identity A -> Identity A, which is a long way to say id_A :: A -> A, but you need to insert the Identity functor to see functors.
12:05:22 <lambdamu> maybe he was confusing free theorems with free constructions which arise from adjunctions which are certain functors related by natural transformations
12:05:56 <flxw> Oh, I see. Yes, id must be many things, if it's to become a natural transformation, one component for each type a.
12:08:54 <dolio> I suppose if you understood what naturality is, you could get some idea of free theorems from 'this operation is natural in a way related to the type', instead of thinking about relations related to the type.
12:08:59 <dolio> If that's what is meant.
12:08:59 <flxw> Yes, I'll have a look at wadler's talk. And as it seems, getting better acquainted with LR und parametricity will be unavoidable when I want to understand the free theorem stuff better ...
12:09:43 <dolio> There are types that aren't easy to see in those terms, though.
12:10:05 <flxw> dolio: yes, I meant it in that sense.
12:10:56 <flxw> as a start I'd be completely happy with seeing some types. Dosn't need to be all of them at once. :-)
12:11:30 <EvanR> cantors diagonal argument uses the notion of a listing of reals (in the form of digit sequences), and the notion of a listing which is complete. by assuming a complete listing you get a contradiction by constructing a witness to its incompleteness. So a complete listing is contradictory. However an incomplete listing is OK, and the diagonal construction still works for generating a "new" real. this process 
12:11:36 <EvanR> can be iterated indefinitely, prepending each new real to the beginning of the listing. will this process eventually reach all reals?
12:12:21 <monochrom> That's the continuum hypothesis.
12:13:38 <EvanR> it is?
12:15:57 <monochrom> I am taking "eventually" very liberally. But the next cardinal after the naturals is continuum-hypothesized to be the reals.
12:16:54 <dolio> I don't think that question is answered by the continuum hypothesis.
12:18:01 <lambdamu> is any question answered by the continuum hypothesis?
12:19:12 <dolio> I don't think it's specified clearly enough to answer, either, though.
12:19:16 <EvanR> i guess i should try to write the haskell code for that and see what happens
12:20:24 <monochrom> There are a few more words I need to say. The process looks more like ordinals than cardinals. But that difference can be bridged. The first uncountable ordinal = the first uncountable cardinal anyway.
12:20:48 <EvanR> type error
12:20:58 <dolio> How are you going to get to the first uncountable ordinal by adding 1 to countable ordinals?
12:21:02 <dolio> Repeatedly.
12:21:26 <monochrom> Oh yikes.
12:22:44 <monochrom> But Haskell code is not going to help. Any haskell program (any program in any language) can only be run for less than omega steps. You can't even observe the first time you add an extra real number.
12:22:51 <dolio> Also, what counts as a "listing", and are you sure that what you're doing with ordinals counts?
12:23:00 <Aruro> is there github repo mirroring wiki.haskell.org?
12:24:35 <EvanR> a listing of reals (in the form of digit sequences) is a function from N to N to D where D is the type of digits
12:24:51 <EvanR> so an infinite matrix
12:25:10 <EvanR> you can prepend to it like hilberts hotel
12:25:34 <EvanR> (in 2 directions)
12:26:06 <EvanR> thats what i expected, if you attempt to list all reals somehow, you ought not to be able to even get started
12:26:42 <dolio> Okay, so what does "eventually reach all reals" mean?
12:26:44 <ripread> Hey, Haskell newbie here. I'm having trouble getting recursion to work in ghci. ghci --version = 8.0.1.
12:27:00 <ripread> ghci> rec 0 = 1
12:27:09 <ripread> ghci> rec n = n + rec (n-1)
12:27:14 <ripread> ghci> rec 5
12:27:15 <Tuplanolla> The second line shadows the first one, ripread.
12:27:22 <EvanR> i guess it ends up being the same thing, technically, as being complete
12:27:31 <dolio> Having a complete listing?
12:27:32 <monochrom> I recommend you to put code in file and use :load. It's much simpler and editable.
12:27:54 <EvanR> so the process doesnt add anything new to the situation, despite adding infinite new things
12:28:06 <ripread> Tuplanolla is that something specific to the repl? cause they have very similar recursive code in LearnYouAHaskell
12:28:11 <monochrom> But if you insist, "let {rec 0 = 1; rec n = n + rec (n-1)}
12:28:19 <EvanR> however you can take the output of that process, and feed it into a second copy of itself to get even more new reals
12:28:23 <EvanR> and so on
12:28:24 <Tuplanolla> Shadowing is universal, ripread.
12:28:45 <EvanR> and repeat that indefinitely
12:29:10 <EvanR> so higher order computability like
12:29:19 <lyxia> ripread: if you insist on making it multiline http://lpaste.net/352948
12:29:44 <Tuplanolla> It's just that the top level in files and the prompt in GHCi are different contexts, ripread.
12:29:52 <monochrom> Also pretty sure LYAH tells you to use a file.
12:29:59 <EvanR> maybe not because the first level never gives you a listing with a beginning
12:30:18 <EvanR> ah so thats different
12:31:47 <EvanR> it generates a zipper, one way is the original incomplete listing and the other direction is the unfolding by diagonal manipulation
12:32:03 <EvanR> in that way you can observe it
12:35:36 <contiver> What's the usual consensus when uploading a package to hackage? Should the Text/Data/Control/ etc. structure be respected, or is it ok to upload something outside it (such as the Pipes library does)? I've been told that as long as the name isn't a generic one (say, Parser), it's fine.
12:35:53 <contiver> But it is my first time uploading something to Hackage, so I'm trying to avoid messing it up.
12:35:58 <Tuplanolla> What's the name, contiver?
12:36:02 <contiver> Hasmin
12:36:52 <Tuplanolla> I wouldn't care.
12:37:38 <byorgey> contiver: my sense is, Text/Data/Control is appropriate if and only if you are uploading a library which contains some sort of fundamental data structure or abstraction.
12:37:44 <myfreeweb> There was a "do not use Text/Data/Control in your packages" post on /r/haskell recently i think
12:38:24 <byorgey> for most things, I would not recommend putting your modules in those namespaces.  Pick whatever makes sense.
12:38:36 <Tuplanolla> Then again `Game.Hasmin` would make sense.
12:40:38 <myfreeweb> Com.Contiver.Packages.Games.Hasmin.HasminApp :D
12:43:18 <EvanR> Everything.EverythingReal.ProgrammingLanguageLibraries.Haskell....... .
12:43:21 <EvanR> someyhing
12:44:07 <contiver_> so I'm guessing people will be fine with it? 
12:44:10 <monochrom> Quaternion.Complex.Real.Integer.Natural.Programming.Haskell.Data.Double
12:44:32 <nshepperd_> I think do whatever you like as long as you don't collide with anything
12:44:53 <contiver_> good. Thanks for the answers :)
12:45:00 <mauke> Comp.Lang.Haskell
12:46:14 <dolio> EvanR: Anyhow, you need to nail down what stuff like "repeat that indefinitely" means.
12:47:21 <EvanR> unfoldr
12:47:26 <EvanR> iterate
12:48:01 <dolio> That gives you a list of things.
12:48:05 <Tuplanolla> It seems dubious that you could generate the reals with a countable process, EvanR.
12:48:20 <dolio> Each one of those things will have a finite number of constructions on your input.
12:49:02 <dolio> And if they're things subject to the diagonal argument each time, of course each one won't be complete.
12:51:33 <EvanR> yes, if the question is if the result is complete, were back to the beginning
12:52:05 <GusCE6> Is this a place for help with programming?
12:52:09 <EvanR> yes it seems dubious
12:52:26 <EvanR> but it seems interesting that i can seem to use the construction to get "even more reals" than before
12:52:28 <monochrom> No.
12:52:40 <EvanR> then refold the result, then do it again
12:52:43 <EvanR> and iterate that
12:52:44 <dolio> You already listed countably many reals.
12:53:00 <dolio> Why are you surprised that you can generate countably many more?
12:53:09 <dolio> And still not have all uncountably many of them?
12:53:11 <EvanR> how many times do i need to iterate that before something different happens
12:53:42 <EvanR> countable x countable is still countable
12:53:58 <EvanR> i mean, i guess, i vaguely remember
12:54:07 <GusCE6> Basically I'm trying to get started programming applications. I do understand basic website programming.
12:54:09 <int-e> GusCE6: if it's related to the Haskell programming language...
12:54:35 <monochrom> 10000 to 1 it is not.
12:54:40 <GusCE6> Oh- Do you know a chat room that can help me out here?
12:54:52 <monochrom> There is none.
12:54:53 <Tuplanolla> This is one, but you need to install GHC first, GusCE6.
12:55:23 <GusCE6> Actually, I'm trying to do so on a Windows CE6 ARM 8505 device...
12:55:34 <Tuplanolla> Forget what I said.
12:56:07 <sveit> in a functional heap/priority queue, is there a way around doing breadth-first search to find the node at which to insert an element?
12:56:13 <EvanR> i found this paper on the subject... "How Real are Real Numbers" by Gregory Chaitin https://www.cs.auckland.ac.nz/~chaitin/olympia.pdf
12:56:20 <GusCE6> It's a long story, but it boils down to an effort to keep weak outdated devices from becoming useless.
12:57:20 <sveit> of course i mean so that the heap stays balanced
12:57:32 <flxw> just to report back: Thanks for the input to my free theorems question. I was watching the wadler talk and indeed is beginning to help clear things up! ;) 
12:57:35 <Tuplanolla> Freenode is not the best place to look for Windows support, GusCE6.
12:58:05 <GusCE6> What would be a good place?
12:58:07 <flxw> *the talk is
12:58:12 <luite_> of purescriptq
12:58:15 <luite_> c
12:59:22 <Qwerasd> Every language I try to learn I try to codegolf a program that prints the numbers from 1 to 100 and then "Poof!" in. Considering I know basically nothing of Haskell maybe someone can show me what that would look like.
13:00:12 <EvanR> map show [1..100] ++ ["Poof!"]
13:00:42 <sdrodge> Qwerasd: main = putStrLn $ show [1..100] ++ "Poof!"
13:00:57 <Qwerasd> Very concise, I like it.
13:01:14 <EvanR> once you have the list, you can show it all with forM_ putStrLn theList
13:01:18 <robertkennedy> Need a mapM_ before your putStrLn
13:01:26 <EvanR> or do more processing on it first
13:01:52 <monochrom> mapM_ putChar "Poof!"
13:02:03 <sdrodge> robertkennedy: No, I'm letting it just print it out with the array formatting.
13:02:09 <Qwerasd> I hear you can make infinite lists in haskell, is that something like [1..] would give all integers above 0?
13:02:17 <sdrodge> If he just wants space separated numbers, then my program isn't what he asked for, I guess.
13:02:20 <robertkennedy> Then you'd need print, yes?
13:02:31 <sdrodge> robertkennedy: No.
13:02:32 <Qwerasd> How about that same program but with a newline between each number and before Poof! ?
13:02:50 <sdrodge> okay then people are correct about mapM_
13:02:59 <EvanR> er mapM_ putStrLn theList
13:03:00 <monochrom> mapM_ print [1..100]. You add the Poof part.
13:03:19 <Tuplanolla> No love for `unlines`?
13:03:19 <Habib_> Anyone know how to elegantly one-line 'result <- action :: IO Result; otherAction result :: OtherMonadIOType ()'?
13:03:50 <robertkennedy> Ahhh you didn't map show, tight. 
13:03:57 <lyxia> Habib_: liftIO action >>= otherAction
13:05:05 <sdrodge> Qwerasd: main = mapM_ print [1..100] >> putStrLn "Poof!"
13:05:06 <Habib_> Thanks so muh
13:05:23 <sdrodge> Qwerasd: That's the concise version that does exactly what you asked for.
13:05:32 <Habib_> Thanks so much, lyxlia; I've been trying all sorts of combinations of liftIO with bind, fmap, etc.
13:05:39 <Qwerasd> But does that have a newline after every number and
13:05:51 <Qwerasd> s/ and//
13:05:58 <lyxia> Habib_: haha
13:05:58 <sdrodge> Qwerasd: yes.
13:06:15 <monochrom> Qwerasd: Why don't you try it on your computer?
13:07:14 <Qwerasd> How do you compile haskell, I knew once, but forgot.
13:07:23 <sdrodge> Qwerasd: As to your other question, yes, [1..] is the infinite list containing all positive integers
13:07:47 <monochrom> Use ghc.
13:07:49 <Habib_> Qwerasd: ghc Main.hs?
13:07:52 <sdrodge> Qwerasd: If you don't want to bother with compilation just pop open ghci and type that line without the "main = " part
13:08:07 <Habib_> or runhaskell
13:09:12 <nshepperd_> EvanR: if by "hit all the reals" you mean that any given real is added at some point in the process, then it is not possible, since that would amount to a countable numbering of the reals (via countable x countable = countable)
13:09:15 <Qwerasd> what's the file extension for haskell files?
13:09:18 <Qwerasd> .hs right
13:09:22 <ongy> .hs usually
13:09:45 <sdrodge> nshepperd_: What about Skolem's paradox?
13:10:07 <nshepperd_> You would have to "iterate" the diagonal process an uncountable number of times to have a chance of getting all the reals, i guess
13:10:37 <monochrom> Recall that countable^countable is uncountable.
13:10:41 <Qwerasd> [1,1.1..] would be a list starting from 1 and incrementing by .1 infinitely right?
13:11:17 <monochrom> Yes apart from 0.1 getting screwed by floating point.
13:11:54 <monochrom> Or rather, by binary floating point.
13:12:20 <dolio> nshepperd_: Right, the question would then be what it means to 'iterate' that many times.
13:12:38 <dolio> And it's still not clear that the diagonalization process wouldn't miss things.
13:13:25 <dolio> If you could even define it sensibly.
13:17:36 <nshepperd_> sdrodge: don't know anything about it
13:18:52 <ongy> monochrom: countable^countable? I thought it's just squared?
13:20:06 <ongy> oh it's 2^countable_infinity if I see this correctly
13:21:08 <sdrodge> There's not much significance to that notation other than the fact that a finite powerset of P has size 2^(size P).
13:21:39 <sdrodge> So since the cardinality of reals is equal to the cardinality of the powerset of naturals, we keep using that notation.
13:21:48 <sveit> is it a cause for worry in the community that many data structures typically used in non-trivial algorithms have worse asymptotic and constant factors when implemented functionally? i'm not trying to be a troll, i'm asking if this is seen as a serious obstacle or (hopefully) there is some reason this concern is unfounded.
13:22:15 <Tuplanolla> It's a deterrent for some applications, sveit.
13:22:48 <Cale> sveit: Well, if it ever matters, you can still do all the imperative stuff.
13:22:51 <Cale> sveit: However, it really tends not to be a big deal in practice.
13:23:01 <sdrodge> sveit: It's not unfounded at all, and Haskell has a lot of ways to enter impurity land for when you actually need performance characteristics that only destructive mutability can provide.
13:23:14 <ezyang> To people who use lens out there: is there any practical utility to being able to swap out lens implementation? 
13:23:33 <Cale> There are a few places where we have data structures that are implemented via various degrees of cheating and which present a purely functional interface.
13:23:45 <Tuplanolla> We don't have enough number crunching scientists here.
13:23:57 <ezyang> (I'm asking because type synonym lenses actually need a new feature that we have thought about, but didn't implement because we couldn't think of a good use case for) 
13:24:26 <Cale> e.g. ByteString and Vector and Text and various other array libraries all use various low level hackery to implement the operations
13:26:09 <dolio> What does "swap out lens implementation" mean?
13:27:56 <sdrodge> sveit: You might be surprised how often impurity is not necessary for performance, though.
13:29:12 <dolio> Number crunching seems like a bad example for where asymptotics of complex data structures would matter, too.
13:29:42 <ezyang> dolio: I'm not sure! 
13:29:53 <ezyang> Someone mentioned it 
13:30:06 <ezyang> "An Iso can be used wherever a Lens is required.-- Could this be specified given abstract "Lens" and "Iso" such that Iso can be instantiated with any type, as long as it is subsumed by Lens" 
13:30:32 <dolio> I wouldn't call that "swapping out lens implementation".
13:30:57 <ezyang> :) 
13:31:17 <dolio> That's referring to the fact that there is induced subtyping on a bunch of stuff in lens.
13:32:46 <dolio> And it's very important.
13:32:54 <lyxia> I understood the question to be whether this subtyping can be induced by another representation of lenses than Van Laarhovens's.
13:33:40 <lyxia> but that seems unlikely
13:34:20 <jle`> sveit: there are functional algorithms and there are imperative algorithms, and you can implement both in haskell
13:34:49 <jle`> sveit: some imperative algorithms are truly the best for the job a lot of times, and haskell lets you implement them
13:34:59 <jle`> sveit: but the nice thing about haskell is that you start to learn how rarely you actually need those
13:35:29 <dolio> It's like the whole thing that makes lens worth using, whereas previous lens libraries weren't.
13:35:30 <EvanR> the nice thing about haskell is you can actually implement the functional algorithms without a huge pain in the ass
13:35:38 <dolio> (In my opinion.)
13:36:37 <EvanR> what if i had a list of all computable reals (in digit sequence form), then took the diagonal and flipped the bit...
13:36:39 <jle`> ezyang: yeah, the way that works is because Iso is a type synonym that is more general than lens
13:36:53 <jle`> ezyang: but any iso can be specialized to a lens using normal typeclass stuff
13:36:57 <EvanR> wouldnt that compute an uncomputable real
13:37:06 <ezyang> makes sense 
13:37:10 <raynold> ahh it's a wonderful day
13:37:12 <jle`> kind of like how you can give a Monad to any function that expects an Applicative
13:37:18 <dolio> EvanR: You can't compute a list of all computable reals.
13:37:28 <dolio> By the diagonal argument.
13:37:29 <EvanR> why not?
13:37:39 <jle`> you sort of just proved that you can't
13:37:41 <EvanR> theres a list of all programs
13:37:42 <ezyang> :) 
13:37:58 <EvanR> and all proofs, and all statements, etc
13:38:27 <jle`> EvanR: your list is obviously not complete, if that's possible
13:38:33 <sdrodge> I think Godel might have something to say about that assertion.
13:38:47 <jle`> because you showed a way to construct a number not on that list
13:38:50 <sdrodge> And I think Turing would be very interested to see your solution to the halting problem.
13:39:15 <EvanR> enumerating all programs or all turing machines or whatever is exactly what godel and turing did in their respective papers
13:39:27 <EvanR> by encoding them as numbers and doing [1..]
13:39:48 <sdrodge> And then they showed...
13:40:09 <EvanR> i didnt get that far yet ;)
13:40:25 <sdrodge> :D
13:40:56 <ezyang> "The set S of Gödel numbers, however, is not computably enumerable, even though the computable reals are themselves ordered. This is because there is no algorithm to determine which Gödel numbers correspond to Turing machines that produce computable reals." 
13:41:00 <sveit> wow, thanks for the detailed responses! are there some examples of non-trivial algorithms (roughly of the "complexity" of sort or a* graph search) that are optimal when implemented functionally?
13:41:35 <EvanR> so its a classification issue?
13:42:05 <ezyang> It's kinda subtle 
13:42:44 <jle`> sveit: you might like okasaki's Purely Functional Data Structures 
13:42:55 <sdrodge> +1 for that suggestion.
13:43:02 <sdrodge> It's basically the bible on this topic.
13:43:41 <sdrodge> But as a shorter answer, sort can be done in n log n purely functionally.
13:44:21 <lyxia> sveit: sorting
13:44:57 <EvanR> from chaitin's paper "The set of all possible computer programs is countable, therefore the set of all computable reals is countable, and diagonalizing over the computable reals immediately yields an uncomputable real. Q.E.D." this is kind of the teaser... but from just this it sounds like a recipe for constructing an uncomputable number
13:45:20 <EvanR> which seems like a recipe for disaster
13:45:23 <EvanR> lol
13:45:29 <dolio> EvanR: You can do Cantor's argument in a setting where the reals are the computable reals, and the functions involved are computable functions from the naturals to the computable reals.
13:46:09 <dolio> By the diagonal argument, no such computable function enumerates all computable reals. This means the computable reals are what is called, "computably uncountable."
13:46:17 <Cale> sveit: Another thing to note is that while it's been proved there is a time penalty to be paid when you're using *strict* purely functional programming with no mutation, having laziness changes things somewhat, and at least the same proof which was used doesn't apply when lazy evaluation is involved. Practically speaking, most of the time you will just pay the log factor and not worry about it, but there are sneaky things that can be 
13:46:17 <Cale> exploited some of the time to remove it.
13:46:25 <sveit> lyxia: i thought quicksort was faster when implemented imperatively, esp. regarding allocations?
13:46:36 <dolio> Even though from some exterior, set theoretic perspective they are 'equally large' sets.
13:46:55 <Cale> (since laziness is effectively a form of one-time mutation)
13:47:25 <lyxia> sveit: I was thinking of asymptotic complexity but you are right.
13:47:39 <sdrodge> sveit: There's a trick you can do to do sort "almost" in-place and still present a pure interface.
13:47:41 <Cale> Okasaki exploits that a bunch, finger trees do as well.
13:47:49 <EvanR> i see
13:48:03 <sdrodge> sveit: But that might be getting too deep into the weeds for now.
13:48:23 <dolio> EvanR: Yeah, the thing is that saying the set is "countable" there doesn't mean "computably countable".
13:48:26 <sveit> Cale: but the point is the log factor is important if i want to implement the conceptually "non-trivial" parts of my application functionally as well. although it seems the answer is that one can encapsulate the imperative part in a functional interface.
13:48:43 <Cale> Sometimes, yes.
13:48:43 <dolio> As in, there's a computable surjection from the naturals.
13:49:18 <sdrodge> sveit: Or, even more typically, you simply use the purely functional parts of your progam in the larger imperative program.
13:49:19 <dolio> So he defines an uncomputable real in set theory (or whatever).
13:49:39 <Cale> sveit: It really depends to some extent on what the operations you're performing really are.
13:50:01 <dolio> But his enumeration of the computable reals was already uncomputable.
13:50:22 <dolio> It is just specified to exist in the theory he's using.
13:50:35 <sveit> sdrodge: i guess all programs eventually ending up in the IO monad encourages that style more than what I mentioned :)
13:51:16 <Cale> sveit: At the top level, your program will consist of an IO action, yeah.
13:51:51 <Cale> You still wouldn't want to unnecessarily clutter your API with IO-action-producing stuff if it's not necessary.
13:51:51 <sdrodge> sveit: :D yep, but don't get me wrong, the other pattern definitely exists too and is super neat.
13:52:30 <Cale> It's certainly much nicer to test and to reason about pure functions, rather than having to think about what IO actions are going to do when executed.
13:53:55 <sveit> Cale: that's exactly why i'd like to be able to implement the "guts" of my program functionally! complicated algorithms are exactly where most of the difficult reasoning ends up (at least for me). stringing blocks together is relatively easy no matter if you're imperative or functional.
13:53:57 <EvanR> this diagonal argument sure gets around
13:54:06 <Tuplanolla> Relating to this: is there a nice way to test socket programs, especially for rare failure conditions?
13:54:07 <EvanR> kind of suspicious
13:54:35 <sdrodge> sveit: If you give a concrete example, the answer would probably be a lot more enlightening.
13:54:52 <monochrom> ST is usually OK.
13:54:53 <EvanR> Tuplanolla: id like to see a testbench program to artificially cause all the conditions that cause those rare exceptions
13:55:06 <jle`> ST is a pure way to describe an imperative algorithm
13:55:09 <dolio> About 50% of the really famous proofs in the last 100 years are diagonal arguments. :P
13:55:16 <ongy> Tuplanolla: "socket" is pretty broad
13:55:31 <monochrom> Because you can and will have to localize ST.
13:55:48 <Tuplanolla> Let's say Internet stream socket, ongy.
13:55:52 <dolio> Maybe more than 50.
13:55:56 <ongy> for AF_INET(6) it may be reasonable to set up some stubs for testing, but socket has AF_UNIX and a few others aswell
13:56:06 <sveit> so just to have some intuition, an "almost-trivial" data structure is the heap (used in priority queues). imperatively it's very easy to implement (have an array, append to the end and propagate up the "tree" for insertion (just swaps), and pop off an element to get the minimum). functionally I have seen some more complicated examples but is there a simple version with good asymptotics?
13:56:33 <Tuplanolla> Realistically `AF_INET`, `AF_INET6` and `AF_UNIX` are the only options, ongy.
13:56:46 <ongy> Tuplanolla: if only... if only....
13:56:59 <sdrodge> jle`: That seems like a weird way to describe ST. Would you mind clarifying what you mean?
13:57:07 <Tuplanolla> There's `AF_CAN` and other old stuff, but I couldn't even get the SocketCAN kernel module to cooperate with them, ongy.
13:57:30 <jle`> sdrodge: you describe an imperative algorithm by purely assembling imperative primitives
13:57:41 <ongy> I'm used to AF_PACKET and AF_NETLINK. But I those aren't in network
13:57:47 <jle`> using combinators like (>>=)
13:57:48 <Cale> http://hackage.haskell.org/package/psqueues provides some efficient priority search queues
13:57:48 <lambdamu> sveit: i don't think, in persistent datastructures you alwayes have to armotize a log factor away where imperatively you would have a constant
13:57:57 <lambdamu> *i dont think so
13:58:07 <sveit> something like "data Heap a = Node a (Maybe (Heap a)) (Maybe (Heap a))". Is this a "reasonable" way to implement a heap? My main problem is insertion. Do I have to do BFS to find the best insertion point every time?
13:58:25 <jle`> sveit: there's the skew heap, which is a pretty close relative to the normal heap, but is purely functional
13:58:34 <sdrodge> jle`: Okay, sure, but I wouldn't describe STRefs as being pure.
13:58:57 <jle`> sdrodge: im not sure if it makes sense to label them either way
13:59:01 <lyxia> sveit: you don't need a BFS for that.
13:59:10 <jle`> i'm talking about the method by which you describe an imperative algorithm
13:59:14 <jle`> you describe it purely
13:59:29 <jle`> using primivites like newSTRef, readSTRef, and combinators like (>>=), (<$>), etc.
13:59:45 <sveit> lyxia: how else do i find where to insert a node? if i just insert it at my first opportunity the tree will become unbalanced very quickly
13:59:54 <sdrodge> Surely you'd agree that readSTRef doesn't exhibit referential transparency, though.
14:00:23 <erisco> of course it does
14:00:25 <dolio> I wouldn't agree.
14:00:43 <jle`> ' newSTRef >>= readSTRef ' returns the same imperative algorithm every time
14:00:50 <jle`> er, newSTRef x >>= readSTRef
14:00:58 <jle`> the description of the algorithm is done purely
14:01:07 <erisco> the whole point of these interfaces is that they work in a pure language
14:01:12 <jle`> the actual algorithm itself is imperative, or "impure"
14:01:18 <jle`> but the process of describing it is pure
14:01:33 <lyxia> sveit: I think you just need an integer to keep track of the size of the heap.
14:01:47 <jle`> you'd be in trouble if you used readSTRef on the same input, and then a different 'ST s a' came out every time
14:01:54 <jle`> or a different imperative program came out as a result every time
14:02:37 <erisco> just because something has side effects doesn't mean it isn't pure :P
14:02:39 <lambdamu> jle`: but couldn't the same be said about C doesn't you source code describe the same program every time?
14:02:42 <jle`> of course, >> doesn't commute, so 'x >> y' is different than 'y >> x' or 'x >> x >> y'
14:02:58 <jle`> but that's the same as 'x <> y' being different than 'y <> x' or 'x <> x <> y'
14:03:04 <sveit> lyxia: sure, then i would add to it every time i added an element. i meant i wanted something with the "minimal" data structure that I put.
14:03:05 <jle`> lambdamu: yes
14:03:06 <Cale> sveit: http://www.cs.ox.ac.uk/ralf.hinze/publications/ICFP01.pdf might be of interest -- it describes the implementation of a purely functional data structure for storing (priority, key) pairs, which acts both as an efficient priority queue, and can be searched by key.
14:03:06 <erisco> lambdabot, no. read uninitialized memory for example
14:03:26 <jle`> but you can't manipulate C source code progamatically very well
14:03:38 <jle`> it's definitely not as easy to manipulate as ST is
14:03:46 <sdrodge> Okay, maybe I'm missing something obvious, but consider newSTRef 4 >>= \x -> readSTRef x >> writeSTRef x 5 >> readSTRef x
14:03:55 <jle`> for sample 'x >> y' sequences two ST actions and performs them one after the other
14:03:59 <lambdamu> erisco: but even then isn't that just an effect? you can get a random number in haskell IO, too, e.g.
14:04:06 <sdrodge> Surely you can't claim readSTRef is exhibiting referential transparency here.
14:04:08 <jle`> 'cat foo.c bar.c' outputs a program that probably isn't the sequence of two C programs
14:04:16 <Cale> er, sorry, (priority, key, value)
14:04:18 <sdrodge> The total computation is, sure, but readSTRef is not.
14:04:22 <jle`> so c source files ar emuch harder to manipulate than ST is
14:04:33 <jle`> sdrodge: what you you mean by referential transparency there
14:04:33 <erisco> lambdamu, if you are not familiar with how IO is pure I will try and find a description for you
14:04:56 <Cale> *Execution* of IO is not pure.
14:04:58 <dolio> `newSTRef 4 >>= \x -> let e = readSTRef x in e >> writeSTRef x 5 >> e`
14:05:02 <Cale> *Evaluation* of IO actions is pure.
14:05:06 <monochrom> IIRC there is a dons post about "C is pure too! It's just a recipe!"
14:05:16 <ongy> erisco: isn't that considered undefined behaviour and therefore breaking any reasoning either way?
14:05:24 <Cale> monochrom: IIRC that was Conal Elliott
14:05:34 <monochrom> At which point you have to be either very broad or very hair-splitting
14:05:36 <sdrodge> jle`: My understanding is that referential transparency of an expression means that it evaluates to the same result always.
14:05:36 <jle`> lambdamu: i think comparing c source files and ST is fair.  i do it often.  but, it's a lot easier to manipulate, combine, sequence, modify 'ST' actions than it is to modify c source files
14:05:55 <jle`> sdrodge: yes, 'readSTRef x' evaluates to the same action every time
14:06:00 <sdrodge> jle`: So you could only argue referential transparency here in the sense that it represents a computation that looks up a value from a memory cell.
14:06:05 <jle`> sdrodge: it evaluates to the same 'ST s Int' every time you use it
14:06:07 <sdrodge> Oh, is that the argument that you are making?
14:06:09 <jle`> you'd be introuble if it doesn't
14:06:11 <lyxia> sveit: An array-backed heap would also need that integer FWIW.
14:06:30 <lambdamu> jle`: i get your point and i agree, mainly i wanted to say that it cuts both ways if you say io is a pure description of an algorithm, then so is C
14:06:33 <jle`> yes.  readSTRef is pure in that every time you apply it to the same STRef, you get the same ST action every time
14:06:35 <erisco> ongy, well I don't know C semantics, but the simple thing is that you cannot replace what was read with the instruction that reads
14:06:49 <lambdamu> jle`: modulo corner cases
14:07:01 <sdrodge> jle`: Okay, but that argument means that all imperative languages are pure too.
14:07:02 <jle`> lambdamu: yeah, and i think that's perfectly fine :o
14:07:09 <glguy> sdrodge: The the function that is pure, not the operation, we can name the result of applying readSTRef to a specific stref. Doing that we name the operation, not the result
14:07:18 <glguy> sdrodge: let example = readSTRef theRef
14:07:34 <glguy> not the result of actually reading the reference*
14:07:40 <dolio> lambdamu: There is no interesting pure computational framework around C's imperative "DSL" that people use.
14:07:43 <Cale> I've found it helpful to understand Haskell programs in terms of two separate but interconnected processes: evaluation, which transforms (runtime representations of) expressions into values, primarily for the purposes of pattern matching, and execution, which refers to carrying out the steps described by an IO action.
14:07:44 <sdrodge> Okay, sure, but by the same argument, all imperative languages are pure too.
14:07:46 <dolio> There is just program catenation.
14:07:52 <sveit> lyxia: fair point. i guess it
14:07:57 <jle`> sdrodge: like i've been saying earlier, the difference is that it's much easier to manipulate ST than it is to manipulate imperative language source code
14:08:07 <glguy> sdrodge: No, if you write   sometype x = readref(theread) in C, you don't get that same behavior
14:08:08 <sveit> is used in a more "straightforward" way in the imperative case
14:08:09 <lambdamu> jle`: yep what implication one draws from that is not my concern, i just dont like the zealtory, and ascribing absolute properties to haskell which just aren't there
14:08:09 <ongy> erisco: I have no idea what you are trying to say, but I don't think it's that important
14:08:34 <lambdamu> jle`: that it is a much more principled approach than C is completely true in my mind
14:08:37 <Cale> Evaluation is (barring low-level hooks that are not used every day), referentially transparent.
14:08:43 <glguy> sdrodge: That's not the application of a pure function naming an operation. In C application actually performs the read from the ref
14:08:51 <Cale> Execution is very much not, and can cause your computer to do anything that computers can do.
14:08:53 <sdrodge> glguy: So? Variable assignment represents the same computation every time, even if the result isn't the same every time. (see how that's the same argument).
14:09:01 <glguy> sdrodge: No, the variable represents the result
14:09:11 <monochrom> I don't think it's useful to talk of "this is pure, that is impure".
14:09:16 <glguy> x is not the named operation, it's the name of the result of the operation there
14:09:38 <monochrom> Instead, the fact that type X and type ST X are different is important.
14:09:50 <jle`> sdrodge: you can't manipulate variable assignment and c programs the same way that you can in haskell.  an 'ST s a' is a data structure that represents an imperative algorithm, and you can manipulate that data structure to describe new algorithms
14:10:01 <nitrix> sdrodge: In C, that computation would be evaluated eagerly and replaced by the resulting value. That value is what would be stored in the variable.
14:10:03 <dolio> There is no equivalent of Haskell's = in C, there is only IO's <-.
14:10:04 <monochrom> Whereas in C you don't have those two types, you have one union of them.
14:10:10 <sdrodge> jle`: Sure, I certainly agree with that.
14:10:30 <Cale> monochrom: Fair enough, but I do think it's helpful to understand that *evaluating* an IO action won't generally cause anything visible to happen.
14:10:31 <jle`> so 'variable assignment' in C isn't a value
14:10:34 <sdrodge> jle`: If that's what you mean by a pure way to describe imperative algorithms, then I agree.
14:10:34 <jle`> it's a statement/action
14:10:37 <ongy> dolio: can I cheat with CPP? :)
14:10:38 <sdrodge> Glad I clarified.
14:10:42 <nitrix> sdrodge: With Haskell, you're defining that the variable is synonymous for that computation and that's it. The evaluation wont happen until the runtime starts evaluating `main`, if ever.
14:10:54 <sdrodge> nitrix: Yeah, I understand.
14:10:54 <dolio> ongy: No, because CPP is another imperative DSL.
14:11:08 <Cale> dolio: And then only sort of -- IO's <- is actually pretty different from C's = too
14:11:14 <dolio> Sure.
14:11:19 <ongy> also I don't think CPP is even enough to get the same thing
14:11:29 <erisco> ongy, you cannot, for example, replace getchar() with 'm' because these are different programs, even if getchar() is 'm' one time
14:11:58 <nitrix> sdrodge: We're able to do this because functions are first-class citizens, and because IO computations are also first-class citizens. They're all values.
14:12:17 <sdrodge> nitrix: I see why you might think I'd be confused about that, but I promise I'm not.
14:12:27 <Cale> In C, names rarely if ever refer to values, and typically only refer to memory locations where values might live.
14:12:38 <ongy> erisco: yes? I don't get what argument you are trying to make
14:12:55 <erisco> someone said that you could say the same about C (referential transparency)
14:13:20 <Cale> The thing on the left of an <- for IO is a name for the value which is the result of the IO action on the right of it, rather than a name for a memory location where that result is to be stored.
14:13:24 <dolio> Anyhow, IO is referentially transparent with respect to =, but not to <- (if you take them to be analogous things).
14:13:31 <Cale> yeah :)
14:14:02 <erisco> that's a bit of a strange description
14:14:14 <erisco> you could say similarly of Reader
14:14:19 <Cale> Which is another way of saying what I'm saying about the distinction between evaluation (which corresponds to equality), and execution, (which corresponds to <-)
14:14:27 <Cale> Yes, that's true.
14:14:28 <erisco> or State, rather
14:14:31 <dolio> Yes, of course.
14:14:47 <sdrodge> jle`: For the record, I have read your related blogpost, and I think it's great.
14:14:56 <jle`> oh, thanks :)
14:15:43 <Cale> Execution of reader actions is not referentially transparent. In fact, that's almost the original sense of referential opacity -- cases where you have a way to refer to "something" that is not intrinsic to the expression at hand.
14:15:54 <Cale> that is, the Reader's environment
14:17:16 <erisco> not sure I can wrap my head around that one
14:17:36 <erisco> other than it not involving <-, it is like we're just talking about abstraction
14:17:39 <Cale> Well, okay, what is the result of ask?
14:17:46 <Cale> If I write  v <- ask
14:18:58 <Cale> There's no way to know what v will be, until you get all the way outside of the description of the reader action, and find the corresponding runReader
14:19:13 <erisco> it isn't really a question different than asking what x is in \x -> x
14:19:34 <dolio> The "original" meaning that Cale is referring to has expressions with variables being referentially opaque, too.
14:19:50 <Cale> Well, it's important to recognise that referential transparency is always with respect to a particular context.
14:19:55 <Cale> and yeah
14:19:59 <dolio> Like, what is the value of `x + 1`? In the context of 'let x = 5 in x + 1' it's 6, but in `let x = 6 in x + 1` it's 7.
14:21:02 <ezyang> Is there a way to declare a "higher order" type class constraint; for example, forall f. (forall a. Show a => Show f a) => f Int -> f Bool -> Int 
14:21:08 <monochrom> Use "v <- getChar", it makes life more difficult.
14:21:36 <erisco> yes, fine, I am on board with that :)
14:21:42 <dolio> ezyang: edwardk has some hacky ways that may or may not be completely unsafe in practice.
14:21:46 <monochrom> Or "v <- "xyz"", it makes life more plentiful.
14:21:52 <ezyang> dolio: Haha 
14:22:01 <dolio> Might depend on GHC version.
14:22:17 <ezyang> oh this looks related: https://ghc.haskell.org/trac/ghc/ticket/2256 
14:22:20 <jle`> ezyang: there's one in the constraints library, yeah
14:22:28 <monochrom> OK that's a great example of "not referentially transparent" :)
14:22:41 <ezyang> jle`: Forall? 
14:23:12 <jle`> oh hm i'm not sure if it's exactly the same
14:23:35 <monochrom> Certainly related.
14:23:44 <dolio> ezyang: I think supporting constraints like that is probably pretty difficult.
14:24:00 <dolio> It'd be nice to be wrong, though.
14:24:04 <ezyang> I just realized that Backpack lets you do this haha 
14:26:36 <sdrodge> jle`: The more I think about the purity / referential transparency status of readSTRef, the more uncomfortable I'm getting, because it seems like while it's perfectly true that it's referentially transparent by denotational semantics and by interpreting it as a pure value, that view leaves a lot to be desired about it from a practical standpoint.
14:26:52 <sdrodge> jle`: Is there a formal operational semantics defined for this stuff anywhere?
14:27:42 <jle`> most of the API follows well established laws
14:27:47 <jle`> fmap f . fmap g = fmap (f . g)
14:27:55 <johnw> sdrodge: if you reduce the expression to System FC, then yes
14:28:09 <jle`> so `fmap f . fmap g $ readSTRef x` is the same as `fmap (f . g) $ readSTRef x`
14:28:23 <dolio> ezyang: Lets you support constraints like that, or lets you write code that expects constraints like that to be supported? :)
14:28:26 <jle`> great things happen when you can manipulate actions purely
14:28:52 <sdrodge> jle`: Sure, but it seems to me that this point of view is leaving a lot of the "actual" semantics on the floor.
14:29:11 <ezyang> former :) 
14:29:15 <dolio> Cool.
14:29:19 <jle`> the semantics come from the data type
14:29:39 <sdrodge> Which I suppose is intentional in much of the design of Haskell, but it seems very odd to take a purely denotational approach to understanding IO and ST actions, for example.
14:29:49 <sdrodge> Though of course it's very nice that you can take that approach.
14:29:55 <sdrodge> Yay for EDSL magic.
14:29:59 <jle`> well, you need to understand the imperative things that ST is describing
14:30:12 <jle`> so you do need to understand the 'impure' thing it represents
14:30:31 <jle`> the nice thing is that you can assemble these things purely
14:30:40 <sdrodge> Definitely. No argument here.
14:30:51 <jle`> it's like you have to understand how the english language works in order to 'read' a string describing english text
14:30:54 <sdrodge> Do you happen to know of a paper describing the operational semantics of ST, though?
14:30:55 <jle`> but you can assemble String's purely
14:30:59 <jle`> using ++ and stuff like that
14:31:06 <ezyang> sdrodge: Lazy functional state threads? 
14:31:09 <jle`> but the thing they represent might require some actual understanding of the domain
14:31:17 <jle`> (that is, how to read english)
14:31:27 <sdrodge> ezyang: Thank you. Exactly what I'm looking for.
14:31:51 <monochrom> Until you run into a sentence like "every person is a human"
14:32:04 <monochrom> At which point you need a delimited continuation.
14:32:10 <Cale> http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.50.3299&rep=rep1&type=pdf
14:32:15 <erisco> legal person, or?
14:32:17 <dolio> Delimited continuations are easy, though.
14:32:27 <dolio> They make much more sense than undelimited continuations.
14:32:54 <Cale> The first thing that came up for me was an older version of the paper which didn't have the semantics
14:33:03 <sdrodge> jle`: That's actually a really good analogy.
14:33:06 <sdrodge> jle`: Thanks.
14:33:10 <jle`> no problem!
14:33:25 <orion> Inside ST, must all references have the same type?
14:33:32 <sdrodge> no
14:34:06 <mauke> is IORefs + unsafePerformIO a valid implementation of ST?
14:34:23 <Cale> mauke: Yeah, that'll do.
14:34:24 <monochrom> I think yes.
14:34:40 <Cale> Well, if it's not, we have problems, because it's basically what we have ;)
14:34:46 <dolio> It better be, because they're all the same thing inside GHC.
14:35:45 <sdrodge> orion: The (a) parameter in (STRef s a) is the type of what is stored in it.
14:36:13 <Cale> You can implement the ST type and its monad instance, and STRef, without needing IO. It's basically just runST which needs a bit of magic.
14:37:36 <Cale> see also http://hackage.haskell.org/package/vault
14:38:10 <orion> sdrodge: But, how is it permissible to have foo <- newSTRef 0 and bar <- newSTRef 'x' at the same time?
14:38:29 <Cale> orion: why would that be an issue?
14:38:36 <Cale> :t newSTRef
14:38:38 <lambdabot> a -> ST s (STRef s a)
14:38:44 <Cale> ^^ it's polymorphic
14:39:12 <sdrodge> orion: In the same way that it's okay to have foo <- readLn and bar <- getLine in the same io action
14:39:12 <orion> hmm, right
14:39:39 <dolio> They aren't the same 'memory cell' obviously.
14:39:42 <orion> sdrodge: Very good point. It's clear to me now, thanks.
14:39:43 <dolio> That'd be bad.
14:39:51 <monochrom> A compiler will have to do the type-checking and then translate it to typeless Scheme.
14:40:06 <orion> Cale: What is the benefit of vault over plain IORefs?
14:40:33 <Cale> orion: You can read and write to a vault in pure code.
14:40:49 <Cale> You only need IO to make new keys
14:41:03 <orion> Oh, cool.
14:42:00 * monochrom invents Data.Heist, a system and method for stealing data from Data.Vault using unsafePerformIO.
14:42:08 <mauke> hmm. this looks unsafeCoerce-able
14:42:12 <nshepperd_> @hackage persistent-refs
14:42:12 <lambdabot> http://hackage.haskell.org/package/persistent-refs
14:42:26 <nshepperd_> St Monad implemented with intmap :)
14:42:52 <monochrom> But Data.Unique.Really is a funny name.
14:44:45 <jle`> i'll admit i tried to hack together an IntMap-based st/stref kinda thing before just realizing that i was re-implenting ST
14:45:01 <monochrom> This is strange. Why doesn't vault need Typeable?
14:45:15 <mauke> why would it?
14:45:18 <jle`> why would it need it
14:45:30 <monochrom> Oh nevermind, I guess the key's type does it.
14:45:35 <mauke> right
14:45:44 <jle`> St Monad, the patron saint of monads
14:45:50 <sdrodge> jle`: At least an IntMap version would have lg n overhead on everything! :P
14:47:14 <monochrom> Hahaha the source code of Data.Vault.Lazy is simply: #define LAZINESS Lazy \n  #include "IO.hs"
15:00:29 <c_wraith> I've never seen a convincing version of ST written against something like IntMap
15:00:36 <c_wraith> and I've written them before. 
15:00:56 <c_wraith> all the ones I've seen have a very bad garbage collection story. 
15:01:57 <dmwit> hah, ouch, yes
15:10:27 <nitrix> Having observed on #haskell-beginners that `and []` and `or []` can be understood as "Are all elements True" and "Is any element True", this made me think of forall and exists. What would be the notation for such propositions?
15:10:34 <nitrix> forall x. x `elem` xs, x == True
15:10:42 <nitrix> exists x. x `elem` xs, x == True
15:11:05 <nitrix> Do these seems like reasonable propositions? Also, it bothers me that the quantification is used to introduce values rather than types. Is this normal?
15:13:34 <erisco> :t and
15:13:36 <lambdabot> Foldable t => t Bool -> Bool
15:13:39 <nitrix> Besides, I'd like to explain that this is for the `and` and `or` functions and that `xs` is originating from a parameter. Is there a notation for that?
15:14:03 <erisco> ∀x. x∈xs ⇒ x    and  ∃x. x∈xs ∧ x
15:14:26 <erisco> > or []
15:14:29 <lambdabot>  False
15:15:09 <erisco> is that the kind of notation you were looking for?
15:16:02 <nitrix> erisco: That's familiar to me. I was more hoping for some :: alternative that'd like me express this relation inline.
15:16:22 <erisco> you mean you want these propositions as a type
15:16:22 <nitrix> e.g. and xs := forall x. x `elem` xs, x == True
15:17:04 <nitrix> erisco: Can it even exist at the type level? Proposing that all elements are True?
15:17:06 <johnw> have free arrows been made into a Hackage package yet?
15:17:15 <erisco> yes
15:18:22 <nitrix> erisco: How? Some sort of type equalities but with values?
15:18:27 <nitrix> Bool ~ True !? ahah.
15:18:50 <nitrix> Or a type whose only possible value is True?
15:19:10 <erisco> singletons and DataKinds will be involved
15:19:49 <nitrix> erisco: To be honest, I was originally curious how people expressed this denotationally, let's say, in a paper or similar, but now I'm also interested how I can propose this using the type system.
15:20:41 <nitrix> erisco: Do you mind sharing? Or I can actually experiment and see if I find the solution on my own?
15:20:44 <erisco> one way is to construct the lists which have the property
15:21:11 <erisco> or we can make a decision procedure that determines if a particular list has the property
15:21:58 <nitrix> I think I prefer constructing the list, as this gives us a proof, right?
15:22:11 <erisco> yes
15:23:10 <nitrix> erisco: Going home, I'll play a little and report on my progress :D
15:23:22 <erisco> good luck! :)
15:55:01 <jle`> erisco: yeah you can definitely do this at the type level
15:55:29 <jle`> i think there might be some libraries that already have them
15:55:32 <erisco> what about proving the associativity of const, gr
15:55:51 <jle`> what are those
15:56:02 <erisco> by gr I mean grrrrr
15:56:10 <jle`> actually i've directly used these types
15:56:14 <jle`> sometimes i've defined them from scratch
15:56:32 <erisco> the two in question are both simple
15:56:39 <erisco> but all good inductive definitions are :)
15:56:44 <jle`> data All :: k -> [k] -> Type
15:56:56 <jle`>   AZ :: All a '[]
15:57:06 <jle`>   AS :: All a as -> All a (a ': as)
15:57:13 <jle`> data Any :: k -> [k] -> Type where
15:57:58 <erisco> I don't know why you do not instead have All :: '[k] -> Type
15:58:12 <jle`> All (== x), some type
15:58:22 <jle`> so your original one would be All 'True
15:58:42 <jle`>   AnyZ :: Any a (a ': as)
15:59:00 <jle`>   AnyS :: Any a as -> Any a (b ': as)
15:59:04 <jle`> any is often called 'Index
15:59:06 <jle`> '
15:59:12 <jle`> http://hackage.haskell.org/package/type-combinators-0.2.4.3/docs/Data-Type-Index.html
15:59:56 <jle`> and All i've called Uniform before
16:00:22 <jle`> you can even do some skolemization here which is kinda cute
16:02:11 <jle`> er i can't figure it out right now
16:02:45 <erisco> maybe  All :: (p :: a -> *) -> [a] -> Type   AZ :: All p '[]   AS :: p x -> All p xs -> All p (x ': xs)
16:03:20 <jle`> SkolemAll :: (Any a as -> a :~: b) -> All b as
16:03:22 <jle`> there ya go
16:03:41 <jle`> er no that's wrong
16:04:16 <jle`> data All a as = All (forall b. Any b as -> a :~: b)
16:04:21 <erisco> that corresponds to all :: (a -> Bool) -> [a] -> Bool
16:05:57 <erisco> All ((:~:) 'True)
16:08:03 <erisco> then you can have lists of even naturals and all sorts of stuff :)
16:09:01 <jle`> is there any way i can change the type of 'a -> Maybe b -> (c, d)' to show that 'c' cannot depend on the 'Maybe b' ?
16:09:17 <jle`> originally i had `a -> (c, Maybe b -> d)`, but...there are issues.
16:09:37 <jle`> that might be complicated to explain and are very specific to my application heh
16:10:02 <jle`> but 'a -> Maybe b -> (c, d)' works, i'm just not happy that the type is less descriptive
16:10:39 <erisco> I have encountered this before and unfortunately I don't know
16:10:43 <jle`> (a -> c, a -> Maybe b -> d) would be as descriptive
16:10:49 <jle`> but is unideal because it requires recomputing things
16:11:03 <erisco> there is this notion of intermediate values
16:11:07 <jle`> hm this actually looks like a similar pattern to lens
16:11:26 <jle`> maybe i can van laarhoven this up
16:11:45 * jle` channels the spirit of twan
16:12:00 <MarcelineVQ> ordoesit :O can join points cut the recomputing
16:12:05 <jle`> laarhovenization
16:12:08 * Axman6 readies the functors
16:12:40 <phz_> god I feel stupid
16:12:46 <phz_> I have stack complaining about a dep:
16:12:46 <phz_>       vector: needed (>=0.12 && <0.13), 0.11.0.0 found (latest applicable is 0.12.0.0)
16:12:51 <phz_> so I just tried a 
16:12:56 <phz_> stack install 'vector-0.12.0.0'
16:13:00 <phz_> Error parsing targets: Specified target version 0.12.0.0 for package vector does not match snapshot version 0.11.0.0
16:13:03 <phz_> wtf?
16:14:06 <erisco> newtype Intermediate a b c = Intermediate (a -> (b, c))   maybe you can make something happen with it
16:14:27 <phz_> any idea?
16:14:29 <MarcelineVQ> phz_: I looks like the snapshot you're using has 0.11.0.0 but you need at least 0.12.0.0 so you either need to add the version you want to extra-deps so it pulls it from hackage or you need a newer lts
16:14:50 <erisco> Intermediate a c (Intermediate (Maybe b) d ())
16:15:02 <phz_> MarcelineVQ: lts-8.2
16:15:22 <phz_> https://www.stackage.org/lts-8.2/package/vector-0.11.0.0
16:15:26 <phz_> it’s in that lts snapshot
16:15:29 <phz_> so I don’t get it.
16:15:55 <glguy> lts is just behind on vector versions. 0.12.0.0 is released
16:16:02 <MarcelineVQ> yes, but your deps want 0.12.0.0 it says, most likely due to other dependencies
16:16:10 <phz_> ah yeah
16:16:14 <phz_> I need the nightly I guess
16:16:17 <phz_> goddammit I’m tired :D
16:16:20 <phz_> thanks
16:16:48 <erisco> if you had  newtype Intermediate a b = Intermediate (a -> (b, Intermediate a b))   that might be neat
16:16:49 <MarcelineVQ> extra-deps:\n- vector-0.12.0.0 in your stack.yaml will fetch it from hackage if you want to give that a shot
16:17:21 <MarcelineVQ> you may need to run  stack solver  afterwards if the new one needs other newer deps though
16:17:35 <erisco> some sort of refining computation
16:18:19 <erisco> but guided
16:23:51 <jle`> the problem is that the intermediate state is the state of an ST computation
16:24:02 <jle`> and all of its STRefs
16:24:47 <jle`> if I could somehow freeze the state...
16:35:14 <recursion-ninja> I'm trying to `stack build` with the `-debug` flag passed to GHC but it fails with a linking error: "/usr/bin/ld: cannot find -lHSrts_debug_p"
16:35:35 <recursion-ninja> Any ideas on how to pass the library to ld?
16:45:16 <wedify> does it exist?
16:50:13 <robertkennedy> Yesterday we tried to construct a g so `g show 7 True == ["7","True"]` and `g fromIntegral (7 :: Int) (8 :: Integer) == [7,8]`. 
16:50:23 <wedify> recursion-ninja: if it does i think you would use -optl
16:50:57 <lyxia> robertkennedy: did you succeed
16:51:30 <recursion-ninja> wedify: could you elaborate on how to use -optl ?
16:52:09 <recursion-ninja> wedify: Would I add -optl as a GHC command line option?
16:52:32 <robertkennedy> We got something like `g f x y = [f x, f y] :: forall c x y b. (c x, c y) => (forall a. c a -> b) -> x -> y -> [b]`
16:53:45 <robertkennedy> Which compiles, but I can't actually input `g show True (7 :: Int)`
16:53:53 <erisco> are you related to Bob Kent?
16:54:44 <erisco> by the identity relation? :P
16:57:09 <monochrom> (forall a. c a -> b)? (forall a. c a => b)
16:57:29 <monochrom> No, neither.
16:57:49 <monochrom> forall a. c a => a -> b
16:58:04 <robertkennedy> Sorry yeah that should be what you put
16:58:13 <robertkennedy> Let me lpaste this
17:00:43 <wedify> recursion-ninja: have you confirmed the existence of the library somewhere? it sounds to me like the debugging version of the rts hasn't been built for some reason
17:02:09 <recursion-ninja> wedify: I have no idea where to look for the library on my system. It very well could be missing. Do you know how to confirm it's existance?
17:02:14 <lpaste_> robertkennedy pasted “g show True 7 == ["True","7"]” at http://lpaste.net/352952
17:03:21 <erisco> I tried to do it myself and ran into the same "untouchable" thing
17:03:50 <wedify> recursion-ninja: you on a linux system? if so i'd look in '/lib' or '/usr/lib'. on windows and mac i've no idea
17:04:08 <wizard1337> so why am i as the programmer expected to optimize my haskell code,... what prevents the compiler from handling any known form of optimization?
17:04:27 <wedify> wizard1337: undeciablity
17:04:48 <recursion-ninja> wedify: yes, a linux system.
17:05:08 <wizard1337> wedify: can you elaborate?
17:05:49 <monochrom> robertkennedy: It seems the computer is saying it doesn't know that c0=Show will work beautifully.
17:06:01 <robertkennedy> erisco: I've been bumping up on that a lot lately - ever since I started to use the real gadt power
17:06:24 <erisco> robertkennedy, enable TypeApplications and use g @Show show
17:06:27 <wedify> recursion-ninja: on my system the rts stuff is in '/lib/ghc-8.0.1/rts'
17:07:49 <robertkennedy> Ahhah!
17:08:03 <robertkennedy> Weird syntax
17:08:15 <erisco> it seems it won't infer the constraint
17:08:23 <erisco> not sure if that is a bug or oversight or purposefully not a feature
17:11:10 <erisco> not sure what you can do from fromIntegral as you cannot partially apply type synonyms
17:11:19 <erisco> seems to carry over to context synonyms
17:12:11 <erisco> you can define  type NumIntegral b a = (Num a, Integral b)   but  g @(NumIntegral Int)  is an error
17:12:29 <monochrom> @type fromIntegral
17:12:32 <lambdabot> (Num b, Integral a) => a -> b
17:13:06 <monochrom> I think it suffices to have @Integral near the beginning, and add :: Int at the end (for b)
17:13:06 <erisco> I have it backwards XD
17:13:19 <erisco> but it doesn't matter because you still cannot partially apply it
17:14:43 <erisco> you're completely right
17:15:11 <erisco> okay, but for any time we need a context with more than one constraint we're stuck
17:16:59 <wedify> wizard1337: it's offtopic so i won't say much but basically to figure out that you need an optimization requires an algorithm and finding algorithms that can run in minutes or seconds is really, really hard
17:19:32 <lyxia> robertkennedy: there's no way to infer which c you mean, because you can always specialize any function to add arbitrary constraints.
17:20:03 <erisco> it can't take the minimal?
17:20:14 <lyxia> robertkennedy: you would need a type on the right of "=>" to carry c
17:20:36 <erisco> it takes the minimal any other time it is inferring which constraints you need
17:26:49 <barrucadu> wizard1337: In general, the problem is truly impossible to solve, this is proven. In specific cases, things can be done. By throwing lots of manpower at the problem, more and more specific cases can be figured out.
17:27:15 <monochrom> "It's only a matter of time."
17:28:31 <erisco> genetically evolve your optimisations
17:28:43 <lyxia> erisco: It might be sound to "just" unify c with Show because it's the only constraint that is there but that rule looks mighty fishy.
17:29:09 <erisco> how many millennia would that take?
17:29:43 <monochrom> If you happen to have written a really slow bubble sort
17:29:54 <erisco> how long until it evolves quicksort :P
17:29:59 <monochrom> It's faster to just wait for it to finish
17:30:16 <monochrom> than to wait for a future compiler to recognize it and replace it with a decent sort.
17:30:52 <monochrom> But that's kind of my ideal of code optimization.
17:31:16 <monochrom> You write a really slow exhaustive search brute force program for Project Euler #498
17:31:54 <monochrom> And the optimizer simply sees "oh it's PE 498" and substitute the fastest solution known to humanity.
17:32:39 <robertkennedy> Project Euler is how I learned about Haskell - that and Mathematica dominate the top spots
17:32:45 <monochrom> You will no longer come to #haskell to ask "my code has been running for two days, what can I do to optimize it by hand? is list comprehension slow?"
17:33:18 <monochrom> Instead, you will come to #haskell to ask "this is too fast, it can't be faith to my code. how can I make it slower?"
17:33:27 <monochrom> s/faith/faithful/
17:33:43 <erisco> but this is incapable of finding novel optimisations
17:34:34 <monochrom> You don't know that, because I haven't said how it can be done.
17:35:17 <monochrom> But observe that #haskell could collectively do that kind of things, and #haskell cannot really be described as "incapable of finding novel optimizations"
17:35:21 <erisco> "known to humanity"
17:36:11 <dmwit> erisco: I have a constructive proof that there is an algorithm which occasionally spits out novel optimizations. Step one in the proof is to observe that compilers do not have an empty set of optimizations.
17:36:24 <dmwit> Step two is to ask, "where did those optimizations come from?".
17:36:49 <robertkennedy> I've wondered if we couldn't get orders of magnitude speed increases by forking computations optimistically. Like if you do `if x == y then f else g`, it could first compute that `hash x == hash y`, and begin computing f while it waits to confirm
17:37:09 <dmwit> robertkennedy: See `par`
17:37:33 <dmwit> The original paper about it even used the word "optimistic" just like you. =)
17:37:52 <erisco> merely the combination of known optimisations isn't what I'd consider novel
17:38:02 <erisco> no more than a collage is novel
17:38:12 <robertkennedy> Oh tight ty dimwit
17:38:34 <monochrom> I forgot to say that I gave only one minute to my hypothetical optimizer, so it settled for Googling.
17:38:41 <dmwit> erisco: I feel that monochrom and I have utterly failed to communicate with you somehow. But I feel helpless to correct this problem.
17:38:48 <nshepperd> -O15: compiler searches stack overflow for keywords related to the snippet of code to be optimised, filter results for things that can be proven to have same semantics via source-to-source transformation, take the fastest under some microbenchmark
17:39:05 <monochrom> If you give it a year, it will first develop a proof of P=NP, then apply it to PE 498.
17:39:52 <erisco> dmwit, seems so!
17:39:58 <monochrom> Or failing that, it will plagiarize someone else.
17:40:38 <erisco> to me the news will run!
17:40:38 <dmwit> erisco: I wonder: do you believe there is an algorithm that could act exactly as you do when put in the same situation?
17:41:41 <erisco> dmwit, well then the optimiser is far more elaborate than 'the optimizer simply sees "oh it's PE 498" and substitute the fastest solution known to humanity'
17:42:06 <erisco> so I failed to receive the intended generality of this optimiser
17:42:18 <robertkennedy> dmwit: I can't seem to find that paper, do you remember the author?
17:42:48 <monochrom> I talked optimistically and you interpreted pessimistically.
17:42:53 <erisco> not to toot my own horn or anything
17:43:15 <erisco> I just razored it
17:43:21 <erisco> but seems I cut it to pieces in the process
17:45:08 <dmwit> You might like to start here: https://hackage.haskell.org/package/parallel-3.2.1.0/docs/Control-Parallel-Strategies.html
17:50:11 <systadmin> heyo
17:51:24 <erisco> dmwit, and to answer your question: yes. I also hope this doesn't happen so soon else I'm out of a job
17:51:32 <robertkennedy> Our Haskell directory is up to 25k LOC, and our "package manager" is a stack.yaml file. What's the next step upwards in scale?
17:52:03 <monochrom> Splitting.
17:52:15 <robertkennedy> ?
17:52:29 <monochrom> Split into 10 projects.
17:53:31 <robertkennedy> Well we have ~25 packages and 6 continuous projects, but to have those projects call the same packages?
17:54:10 <monochrom> Oh, you already have multiple projects?
17:54:41 <monochrom> Then I guess "split into even more projects" may hurt.
17:54:56 <monochrom> Then I don't know.
17:55:15 <robertkennedy> Well idk if it's actually multiple projects 
17:55:24 <robertkennedy> Idk if that's a codified word in haskell
17:55:29 <monochrom> What do C people do when they have 25M LOC?
17:55:50 <robertkennedy> I think they load code to databases?
17:57:26 <erisco> I have to wonder if any program has 25M LoC and actually needs 25M LoC
17:57:57 <erisco> or if it is more like 1M LoC and and decades of legacy
18:01:23 <monochrom> "need" is not a very useful boolean for this. The more useful boolean is one of opportunity cost: You have 1000 person-hours (just an example), would you spend it on crunching down that 25M LoC to 1M, or would you rather spend it on a novel project?
18:02:40 <erisco> 26M LoC it is
18:27:56 <maksim__> is this really accurate? https://hackage.haskell.org/packages/top
18:28:07 <maksim__> the most popular package is warp with a whopping 500 downloads?
18:29:37 <hpc> it's probably not counting everything, like maybe cabal downloads are ignored?
18:29:48 <hpc> or it's downloads within a week or something
18:44:19 <robertkennedy> 500 downloads is behind cdn
19:07:05 <glguy> maksim__: those numbers are for the last 30 days and might be impacted by the cdn cache (I'm not sure)
19:07:25 <glguy> That package has "106844 total (502 in the last 30 days)"
19:38:28 <Cooler> how does (+1) $ (*2) typecheck?
19:38:36 <Cooler> says it has type (+1) $ (*2) :: (Num a, Num (a -> a)) => a -> a
19:40:16 <Cooler> but when i do let x = (+1) $ (*2) it says to use Flexible contexts
19:40:57 <Welkin> Cooler: Where is Freezer?
19:41:10 <Cooler> he's dead
19:41:33 <Welkin> > (+1) . (*2) $ 1
19:41:37 <lambdabot>  3
19:41:38 <Welkin> :t (+1) . (*2)
19:41:40 <lambdabot> Num c => c -> c
19:41:44 <Welkin> looks fine to me
19:42:02 <Cooler> what? not . , $
19:42:14 <Welkin> it ends up being the same
19:42:16 <Welkin> in this case
19:42:33 <Welkin> :t (+1) $ (*2)
19:42:34 <lambdabot> (Num (a -> a), Num a) => a -> a
19:42:54 <Welkin> :t (+1) $ (*2) $ 1
19:42:56 <lambdabot> Num a => a
19:43:07 <Welkin> > (+1) . (*2) $ 1
19:43:08 <Cooler> $ has a different type than .
19:43:09 <lambdabot>  3
19:43:10 <Welkin> > (+1) $ (*2) $ 1
19:43:13 <lambdabot>  3
19:43:17 <Welkin> it doesn't matter
19:43:42 <Cooler> well it does
19:43:54 <Cooler> because you applied an argument
19:44:03 <Welkin> `a . b` becomes `\c -> a b c`
19:44:04 <Cooler> to just the right side of $
19:44:18 <Welkin> `a . b` becomes `\c -> a b c`, which is the same as `a $ b $ c`
19:44:40 <Cooler> don't apply the argument
19:44:43 <Welkin> er
19:44:56 <Welkin> a (b c) is the same as a $ b $ c
19:45:02 <Cooler> there is no c
19:45:09 <Welkin> c is the parameter
19:45:13 <Cooler> omg
19:45:25 <Welkin> that is eliminated due to eta reduction
19:45:56 <Welkin> `f c = a . b $ c` becomes `f = a . b`
19:45:59 <Cooler> we are talking about f $ g not f $ g c
19:46:50 <MarcelineVQ> it typechecks because the `a` of `Num a` can be anything, so there can be an instance of  Num (a -> a)    there isn't one, but it's certainly possible
19:50:22 <mekeor> :t (+)
19:50:29 <lambdabot> Num a => a -> a -> a
19:51:15 <mekeor> :t ($)
19:51:17 <Cooler> does instance Num a => Num (a -> a) where (+) f g = \x -> (f x) + (g x) work?
19:51:18 <lambdabot> (a -> b) -> a -> b
19:52:53 <MarcelineVQ> you'd need to define all the required methods but give it a try
19:53:04 <Cooler> instance Num b => Num (a -> b) where (+) f g = \x -> (f x) + (g x)
19:53:20 <Cooler> same for all the others?
19:54:33 <mekeor>  is ($) infixr or infixl?
19:54:42 <MarcelineVQ> mekeor: r
19:54:45 <glguy> mekeor: You can ask GHCi
19:54:46 <MarcelineVQ> Cooler: whatever makes sense, idk I've never needed to do this :> There's likely many different ways it could be done, otherwise the instance would probably exist
19:54:48 <glguy> :i $
19:58:57 <glguy> Cooler: consider https://hackage.haskell.org/package/NumInstances-1.4/docs/Data-NumInstances-Function.html
20:03:28 <rgrinberg> Is there anything an arrow based parsec can do that an applicative one can't? I'm reviewing this wiki page https://en.wikibooks.org/wiki/Haskell/Understanding_arrows#Static_and_dynamic_parsers and it seems like this whole static/dynamic thing can just as easily be done with applicative.
20:07:00 <mekeor> Cooler: it works http://sprunge.us/ATfM -- (((+1) $ (*2)) $ 3) == 7
20:08:49 <Welkin> sprunge? O.o
20:08:56 <Welkin> sounds like a porn site
20:09:15 <c_wraith> sprunge is from Futurama
20:09:32 <Welkin> > (==7) . (+1) . (*2) $ 3
20:09:35 <lambdabot>  True
20:09:42 <Welkin> reads better
20:10:51 <mekeor> or -- 3 * 2 + 1
20:14:58 <glguy> or -- True
20:16:55 <mekeor> (||) True == const True
20:21:53 <Welkin> :t const id
20:21:55 <lambdabot> b -> a -> a
20:24:29 <mekeor> :t fix id
20:24:32 <lambdabot> a
20:24:39 <mekeor> :t fix (const id)
20:24:42 <lambdabot> a -> a
20:27:12 <robertkennedy> :t g f x y = [f x, f y]
20:27:14 <lambdabot> error:
20:27:14 <lambdabot>     parse error on input ‘=’
20:27:14 <lambdabot>     Perhaps you need a 'let' in a 'do' block?
20:28:58 <lpaste_> Aku revised “Getting parse error on |”: “Getting parse error on |” at http://lpaste.net/6400890151794376704
20:29:00 <centril> I have:  class PhaseIndex pi where phaseId :: f pi -> PhaseId    , is it possible to generalize this to more parameters automatically w/o resorting to more classes?
20:29:32 <Aku> Can someone look at my code, I am getting parse error on |?
20:29:52 <centril> Do I have to use template-haskell for this ?
20:30:20 <geekosaur> Aku, "where" attaches to a declaration
20:30:47 <geekosaur> the where on line 6 ends the definition of clex, and the | on line 9 has nothing to attach to
20:30:47 <Aku> Can you elaborate?
20:31:12 <Welkin> Aku: line 12 should be `| otherwise = [c] : clex cs`
20:31:24 <Aku> ohhkay
20:31:46 <geekosaur> actually that's fine as is just not optimal :_
20:31:48 <geekosaur> :)
20:31:59 <Aku> welkin : If I do so then how do i include the case [] = []
20:32:02 <Aku> ?
20:32:08 <Welkin> just leave it
20:32:10 <geekosaur> ^
20:32:19 <Aku> okay
20:32:42 <Aku> geekosaur : can you post the working code for the same extract?
20:33:04 <Welkin> `where` clauses apply to the entire definition
20:33:17 <Welkin> and must go at the bottom
20:33:34 <Aku> Ohh..I get it
20:34:55 <lpaste_> geekosaur annotated “Getting parse error on |” with “Getting parse error on | (annotation)” at http://lpaste.net/6400890151794376704#a352956
20:35:16 <geekosaur> although I would drop the rest_cs-s and just write the expression, since it's not worth abstracting it for reuse
20:35:58 <Aku> Thansk geekosaur
20:36:31 <geekosaur> (also note I did not test it and I am the typo king >.> )
20:37:00 <Aku> :D
20:37:27 <Aku> would it not be the case that otherwise won't allow the control to reach to '[]'?
20:39:18 <Welkin> no
20:39:37 <Welkin> functionName (a:as) = ...
20:39:59 <Aku> Hmm...I got it!
20:40:27 <mekeor> challenge: how many characters do you need for an expression evaluating to [1,2,3,1,2,3,1,2,3] ?
20:40:29 <Welkin> is just sugar for `functionName str = case str of (a:as) -> ...; [] -> `
20:40:52 <Welkin> it all gets turned into case expressions
20:40:56 <Welkin> even the | eguards
20:41:02 <Welkin> guards*
20:41:24 <Welkin> so you are branching down isolated paths as you match
20:42:48 <Aku> ohkay
20:47:07 <MarcelineVQ> hmm
20:47:14 <dramforever> > [1..3]>>[1..3]
20:47:16 <lambdabot>  [1,2,3,1,2,3,1,2,3]
20:47:41 <dramforever> I suppose that was not good after all....
20:48:10 <MarcelineVQ> > length (show [1,2,3,1,2,3,1,2,3])
20:48:13 <lambdabot>  19
20:48:16 <monochrom> It's good, but you can make it even shorter because the exact content of the first list doesn't matter.
20:48:21 <monochrom> > "abc" >> [1..3]
20:48:25 <lambdabot>  [1,2,3,1,2,3,1,2,3]
20:48:37 <dramforever> monochrom: Aah Strings!
20:49:02 <monochrom> You got the main idea (I couldn't think it up), I just fine-tuned it. :)
20:51:30 <dramforever> monochrom: Isn't the whole point of codegolf fine-tuning :)
20:51:50 <Welkin> > repeat [1..3]
20:51:53 <lambdabot>  [[1,2,3],[1,2,3],[1,2,3],[1,2,3],[1,2,3],[1,2,3],[1,2,3],[1,2,3],[1,2,3],[1,...
20:52:02 <Welkin> > cycle [1..3]
20:52:04 <lambdabot>  [1,2,3,1,2,3,1,2,3,1,2,3,1,2,3,1,2,3,1,2,3,1,2,3,1,2,3,1,2,3,1,2,3,1,2,3,1,2...
20:52:11 <Welkin> :D
20:54:04 <osa1> do I have to use .markdown extension for my changelog files for Hackage to show it in the package page?
20:54:25 <Welkin> osa1: or maybe .md
20:54:35 <osa1> Welkin: I was thinking that but wanted to make sure
20:54:39 <osa1> .markdown is too long
20:55:07 <osa1> ah OK, criterion uses .md: http://hackage.haskell.org/package/criterion
20:56:29 <monochrom> I think *.md is universally understood, even github honours it.
21:29:17 <lpaste_> Aku revised “myCode”: “myCode” at http://lpaste.net/1060007270206668800
21:29:19 <lpaste_> Aku revised “myCode”: “myCode” at http://lpaste.net/1060007270206668800
21:29:55 <Aku> Can somebody look at my code and help?
21:30:19 <Aku> I have written in comments what I have to achieve!
21:35:36 <Aku> Is anybody up there?
21:43:10 <glguy> Aku: You can add a line number argument to your definition of clex
21:44:29 <Aku> glguy: Ya then how do I update it as I go scanning the list?
21:46:24 <glguy> the same way you "update" the existing parameter of clex
21:47:19 <glguy> when you apply clex to the new input string apply it to the correct line number
21:51:25 <Aku> okay
22:02:43 <robertkennedy> How often per x are you unable to suggest constructive criticism b/c inebriation
22:03:21 <glguy> robertkennedy: Did you have a question about Haskell?
22:03:48 <robertkennedy> Not this time sorry
22:05:08 <robertkennedy> Sorry x100
23:21:40 <adelbertc> how does one generally determine the version bounds they want for a particular dependency?
23:26:05 <ongy> if the library does the intended versioning, you usually want to restrict the major version. So if something is currently 1.3.2.1. I'd say >= 1.3 < 1.4. Where the lower bound is a bit more "moving" depending on what you need
23:27:33 <adelbertc> ongy: gotcha, makes sense
23:27:34 <adelbertc> thanks!
