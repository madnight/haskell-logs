00:06:22 <rabbi11> pacak: jared-w great! I am new to haskell.. 
00:06:55 <cocreature> saurabhn_: that instance just reuses the instance for String which is also a unicode string so I’m not sure what makes you think that it produces invalid unicode strings
00:06:58 <spinus> Hello, struggling with OO -> Func mindset transition
00:06:58 <spinus> have:
00:06:58 <spinus> class My c where
00:06:58 <spinus>    const_for_c :: String
00:06:58 <spinus>    do_something :: c -> String
00:06:58 <spinus>    do_something thing = show c ++ const_for_c
00:06:58 <spinus> Of course haskell tell that const_for_c is to general, is there any way to do that better?
00:07:25 <jared-w> rabbi11: what got you interested in Haskell?
00:07:33 <saurabhn_> cocreature: hmm...
00:07:49 <jared-w> spinus: A good rule of thumb that really helped me out
00:08:12 <MasseR> spinus: what is your reason for using type classes for this? Usually when moving from OO to haskell, people use type classes when they shouldn't
00:08:14 <jared-w> 90% of the time in functional programming, if you *think* you want a class, what you *probably* want is an ADT or GADT
00:08:21 <saurabhn_> cocreature: because of what is happening when you use those instances... it seems to be generating Text values that pg-simple is being unable to handle. 
00:08:35 <cocreature> saurabhn_: well then pg-simple probably can’t handle arbitrary unicode strings
00:08:49 <rabbi11> jared-w: I am basically a designer, hated programming. Loved the concept of FP, as loved mathematics a lot during school days. 
00:09:02 <jared-w> spinus: https://lukepalmer.wordpress.com/2010/01/24/haskell-antipattern-existential-typeclass/ have you seen this?
00:09:04 <saurabhn_> cocreature: should it not be able to?
00:09:17 <spinus> MasseR: I have few data types (json), I wanted to have common interface to request the data from server.
00:09:46 <iqubic> :t pure
00:09:46 <spinus> jared-w: thank you, I didn't seen that yet, I'll look now
00:09:47 <lambdabot> Applicative f => a -> f a
00:10:07 <jared-w> spinus: If you have some data types, you can write a toJSON, fromJSON instance for those data-types
00:10:31 <iqubic> :t empty
00:10:32 <lambdabot> Alternative f => f a
00:11:03 <cocreature> saurabhn_: well lpsmith already said in that issue “Also, you are taking a Text type and treating it as an hstore. All that postgresql warranties in that case is that there is no injection (so long as you don't do anything bad in the template, like that kludge), not that it is valid hstore syntax. Which is what is happening here.”
00:11:56 <saurabhn_> cocreature: nope. I'm not taking a text type and treating as hstore. I'm taking a [(Text, Text)] and treating as HStoreMap. Which is what the PG-simple interface provides.
00:12:02 <spinus> jared-w: I can, yes. But I wanted to have common function which will handle requests/errors/retries, the only difference between functions are: url and returned data type
00:12:34 <saurabhn_> cocreature: there are TWO issues being discussed on the PG-simple issue. The first one was raw text related. The second one is related to QC, where the arbitrary instance is going via HStoreMap, not raw text.
00:12:35 <spinus> something like
00:12:35 <spinus> request :: String -> [DataType1]
00:12:35 <spinus> request :: String -> [DataType2]
00:12:55 <saurabhn_> cocreature: this one ==> https://github.com/lpsmith/postgresql-simple/issues/218#issuecomment-308947912
00:13:05 <cocreature> saurabhn_: fair enough. I’m not familiar with hstore but I don’t see how quickcheck-instances is to blame for this
00:13:36 <iqubic> :t absurd
00:13:37 <lambdabot> Void -> a
00:13:53 <saurabhn_> cocreature: I don't know. I'm digging deeper. Have you looked at https://hackage.haskell.org/package/quickcheck-text ? It seems to claim that **something** is wrong with the Text instance and it's trying to fix that. I am unable to understand the subtleties here.
00:13:53 <cocreature> saurabhn_: also it would be great if you could stop asking your questions in several places at once, e.g., irc, stackoverflow, github issues, …. that only duplicates the effort of people trying to help you
00:13:59 <jared-w> spinus: why would you want a single function to handle all requests, all errors, all retries, etc?
00:14:21 <rabbi11> jared-w: finding hard to get ring of OO concepts from head… 
00:14:55 <saurabhn_> cocreature: I now use one persistent channel (GH or SO), and one ephemeral channel (IRC).
00:15:16 <jared-w> Most everyone has a hard time getting OO out of their head :p
00:15:19 <saurabhn_> cocreature: using a persistent channel is VERY IMPORTANT, to leave a trail for other people who may face a similar issue in the future.
00:15:27 <cocreature> saurabhn_: but then don’t ask in IRC too
00:15:30 <jared-w> it's such an unintuitive concept that once you break your brain into learning it, it's hard to unbreak your brain
00:17:02 <spinus> I can copy paste this function 3 times, but I would prefer to reduce it to one
00:18:10 <jared-w> Ah, I see where you're getting at.
00:18:25 <spinus> so right now I have
00:18:25 <spinus> request = do
00:18:25 <spinus>    x<-make_request_with_url "/bla1"
00:18:25 <spinus>    return x :: DataType1
00:18:56 <Cale> spinus: You'll need parens around x and its type
00:19:41 <spinus> Cale: sure, sorry, this one is only toy, I have whole reflex machinery there :)
00:20:05 <jared-w> One key thing to think about here is that abstraction in functional languages is at the function level, not the type level. OO abstracts at the "type" level by classing and implementations all over the place. You write one function and cram it into as many types as possible. In FP you write several functions and compose them into as many types as possible :p
00:20:36 <jared-w> In FP you write several functions and compose them** (forget the second half of the sentence)
00:22:59 <jared-w> So you have some request :: String -> [Type] and you want to be able to write the same request function and have it work with a bunch of different 'Type's. Why not break that request up a little bit? You should be able to avoid needing several functions that do the same thing then. Another thing to look into is potentially dummy types... although that might be more work than it's worth (if it's even valid
00:23:01 <jared-w> for this situation)
00:23:03 <spinus> jared-w: would you advice how I could split/compose my request function around those few types? (I think the reason I'm strugling here is becasue I wanted to keep url and DataType1 tied together)
00:24:28 <spinus> ah, ok, I think I can formulate it better now, is it possible to have "case" on the function output type, so I can select correct URL based on the output type?
00:25:27 <Cale> I don't know if it's relevant to the discussion, but sometimes I like to think of OO as being programming that puts emphasis on coinductive data: you define things according to how they will be taken apart (i.e. the messages or methods that they respond to), whereas traditional FP puts emphasis on manipulating inductive data, where you define things in terms of how they're put together.
00:27:06 <spinus> Cale: yeah, reflex already made me to do some mind-bending in that direction 
00:27:16 <Cale> But there's nothing which says you can't do either of these things very nicely in the average functional programming language. You can do a pretty good job of OO just by building records of functions and such.
00:28:38 <Cale> At least -- the parts of OO that I think really matter.
00:29:10 <Cale> For some reason, most typed OO languages like to tie the implementations of methods to the types of objects via classes.
00:30:18 <tdammers> I think there are two reasons for that
00:30:44 <tdammers> one is historical; per-object vtables used to be prohibitively expensive, so people wanted to avoid them
00:30:45 <Cale> I see it as scrounging for performance in the earliest OO languages, which then got cargo culted along.
00:30:48 <Cale> yeah
00:31:01 <tdammers> the other one, I think, has to do with a crude notion of type safety
00:31:43 <tdammers> or rather, lack of theoretical underpinnings for type systems that could express the kind of constraints you need to do it per-object
00:32:16 <tdammers> you write a class that defines all the type-level properties of, well, a class of objects, and then per-object checks can be reduced to "is instance of"
00:32:53 <Cale> Yeah, it's not so much the type-level side which I object to though.
00:33:29 <hjulle> glguy: Done: https://ghc.haskell.org/trac/ghc/ticket/13833#ticket
00:33:30 <Cale> I mean -- insofar as classes behave like interfaces :)
00:34:16 <Cale> But I suspect there are good use cases for the kind of type safety that tying the method implementations in there gets you -- it's just I haven't seen many of them.
00:35:45 <jared-w> As for the type thing, I always saw that as people compensating for C's awful type system. Then Java came along with a halfway decent type system (bool isn't #DEFINE int), but still very very weak... So, compensating for a super weak type system leads to making tons of classes where you don't /really/ need them because you want some level of encapsulation
00:35:46 <tdammers> well yeah, it depends on the alternative - it's not like monkey-patching everything is a comfortable answer either
00:36:29 <spinus> ok, I think I have it
00:36:29 <spinus> so instead of having function "inside" class, I'll put function outside
00:36:30 <spinus> in the implementation of my class I'll do
00:36:30 <spinus> -- instnace My Bla where
00:36:30 <spinus> --   request = general_request "/bla" :: DataType1
00:36:30 <spinus> so, that's how I would tie them together
00:36:30 <tdammers> jared-w: also, Java insists on cramming too many concerns into classes - a class is a namespace, a module, a type, and an interface
00:36:30 <spinus> Does it make any sense? Or should I try something different?
00:37:30 <jared-w> tdammers: that's definitely a large part of my frustration with Java
00:38:27 <jared-w> spinus: can you give me a high level explanation of what your class is trying to do and what the functions do?
00:38:40 <tdammers> jared-w: I think Java would need 3 major changes in order to be somewhat nice: split up these 4 concerns, get rid of `null`, and make mutability explicit and controllable
00:40:41 <tdammers> come to think of it, C++ actually does most of that - it has namespaces, modules are crude (#include) but orthogonal, the only thing it lacks is a more explicit notion of interfaces; `null` exists, but references aren't nullable; mutability is explicit and controllable through `const`, and through by-ref vs. by-value arg passing
00:40:59 <Cale> tdammers: It's still got classes that mix things up though.
00:41:16 <jared-w> tdammers: C++ is getting a real module system sometime in the next 10 years
00:41:21 <Cale> (and it's a horrible mess)
00:41:24 <tdammers> Cale: it does. But at least using them that way isn't strictly mandatory.
00:41:46 <jared-w> You know the real sin of C++? 5 different usages of the word 'const'
00:41:55 <tdammers> nah
00:42:06 <jared-w> and absolutely all of them can be over-ridden whenever the programmer wants
00:42:10 <tdammers> the real sin is being 5 different languages in one, and nobody can agree on the right subset to use
00:42:22 <jared-w> hah, good point
00:42:27 <butter> Hey eeps
00:42:29 <butter> peeps*
00:42:32 <spinus> jared-w: I have a "view" function, which is rendering stuff. Inside that function. Based on the datatype I wanted this function to fetch and render data from correct url :-)
00:42:32 <spinus> for example: my_view :: DataType1 -- it should use /url1 to fetch data and render it 
00:42:33 <Cale> Only 5? The spec is long enough for 10 at least.
00:42:35 <jared-w> The other sin, to me, is trying so hard to be perl 2.0
00:42:35 <butter> I want to multiply two lists of equal length
00:43:04 <jared-w> <?%*)>@$)::>>k::wtf<>>?:
00:43:10 <butter> And I want to multiply each element of the first list with all the elements of the second list
00:43:13 <tdammers> Cale: 1. C-with-frills; 2. Object-Oriented C; 3. Template metaprogramming; 4. STL
00:43:18 <butter> And get back a list
00:43:22 <jared-w> butter: sounds like you want list comprehensions :)
00:43:30 <Cale> > liftM2 (*) [1,2,3] [1,10,100]
00:43:32 <spinus> thank you for help and advices
00:43:32 <lambdabot>  [1,10,100,2,20,200,3,30,300]
00:43:37 <tdammers> Cale: I added 5 to make it sound more impressive
00:43:40 <quchen> tdammers: C++ has a »const« keyword, but it hasn’t got much to do with immutability :-/
00:43:41 <Cale> Or indeed list comprehensions :)
00:43:52 <Cale> > [x * y | x <- [1,2,3], y <- [1,10,100]]
00:43:54 <lambdabot>  [1,10,100,2,20,200,3,30,300]
00:43:59 <butter> Ah yes
00:44:01 <butter> Thanks!
00:44:17 <jared-w> quchen: depends on where you use const
00:44:24 <jared-w> and/or static
00:44:36 <jared-w> http://duramecho.com/ComputerInformation/WhyHowCppConst.html
00:44:48 <tdammers> quchen: it's kinda leaky, and the immutability is scoped, and it doesn't usually quite work the way to expect, but I'd still call it mutability control
00:44:58 <quchen> ReinH: Woooo! That looks like a great start!
00:45:02 <jared-w> const int * const Method(const int*const&) const; // valid C++
00:45:31 <jared-w> of course all of that can be bypassed instantly by using const_cast or mutable inside the function :p
00:45:34 <merijn> jared-w: That's only 2 different uses
00:45:52 * quchen patiently waits for mauke to join in and whip everyone
00:46:24 <jared-w> pretty sure mauke is the one that yelled at me the last time I linked that const post in here
00:46:34 * tdammers has given up actually understanding C++
00:46:38 <Cale> spinus: It's rather interesting to choose a URL based on a type, but yeah, a type class makes that doable.
00:46:52 * jared-w has given up wanting to do anything with C++
00:46:53 <tdammers> class HasUrl a where { url :: a -> URL }
00:46:58 <tdammers> something like that
00:47:05 <tdammers> ^ spinus 
00:47:11 <quchen> No matter what you say about C, mauke will correct you. Worse even: he is correct in doing so.
00:47:38 <merijn> Yes, if I'm too lazy to look up the spec I bait mauke into correcting my wrong claims :p
00:48:11 <jared-w> Ah, yep, they were the one that corrected me last time :p
00:48:34 <quchen> mauke: »Wait merijn! There are actually 5 different uses of const, you can use it as an infix parameter to return to reverse-allocate the heap« – stuff like that ;-)
00:48:37 <tdammers> merijn: "how to get Linux nerds to help you: waltz in and complain that you can't do X with Y. Within seconds, someone will be outraged enough to tell you exactly how to do X with Y to disprove your claim."
00:48:37 <merijn> I'm getting there, wrt C. C++ not by a longshot
00:49:07 <mauke> tdammers: yeah, it's called godwin's law
00:49:36 <jared-w> I like how you suddenly appeared the second someone mentioned goodwin's law
00:49:36 <tdammers> mauke: it is?
00:49:44 <tdammers> mauke: the generalized version I take it?
00:50:19 <quchen> merijn: Here’s a linux challenge: try quickly writing nub on the command line, i.e. a function that removes all duplicates, but does not change the order of entries.
00:50:30 <jared-w> Now... I'm not quite sure what that has to do with nazis
00:50:43 <quchen> merijn: sort -u? Hah no, that sorts. uniq? Hah no, that won’t globally remove duplicates. uniq --unique (wat)? Hah no, also something else.
00:51:07 <mauke> damn, it didn't work :-)
00:51:16 <merijn> quchen: eh..."uniq --unique" looks like crazy linuxism
00:51:20 <butter> Hey peeps, so I now want to find the smallest positive integer that divides evenly by [1, 20]
00:51:31 <butter> i.e. 1, 2, ..., 20
00:51:43 <merijn> butter: Sounds like least common multiple?
00:51:48 <butter> Yeap
00:51:51 <mauke> :t lcm
00:51:52 <lambdabot> Integral a => a -> a -> a
00:51:57 <quchen> merijn: Almost PHP levels of tomfoolery. uniq_real_unique_plox
00:51:58 <tdammers> quchen: perl one-liner?
00:52:11 <quchen> tdammers: Everything is a perl one-liner it seems.
00:52:14 <mauke> > foldl1' lcm [1 .. 20]
00:52:16 <lambdabot>  232792560
00:52:43 <butter> Wow.
00:52:54 <butter> *jaw drops*
00:53:03 <quchen> Welcome to #haskell
00:53:23 <jared-w> "hey guys how do you reverse a list"
00:53:28 <mauke> quchen: perl -ne 'print unless $seen{$_}++'
00:53:32 <geekosaur> one-liner in any language with real data structures, at least. (bash, enh. could probably do it, but hitting yourself with a brickbat is probably more pleasant)
00:53:46 * jared-w waits for absolutely unreadable point-free gibberish to spontaneously be created
00:53:48 <geekosaur> (zsh can also, even more directly, but same applies)
00:54:36 <butter> I can't find documentation on foldl
00:54:38 <butter> foldll*
00:54:50 <quchen> butter: »reduce« in many other languages
00:54:59 <cocreature> butter: http://hoogle.haskell.org/?hoogle=foldl1%20is%3Aexact
00:55:07 <geekosaur> butter, are you misreading "foldl1"?
00:55:16 <butter> Ah it's foldl"one"
00:55:16 <LiaoTao> Fonts!
00:55:17 <LiaoTao> :D
00:55:18 <quchen> jared-w: Hold on a second. :-)
00:55:18 <geekosaur> the last character is digit 1 not lowercase l
00:55:37 <tdammers> $ runghc <('main = (unlines . nub . lines <$> getContents) >>= putStr')
00:55:44 <mauke> quchen: perl '-pe$_ x=!$h{$_}++'
00:55:52 <jared-w> one liners have me thinking about that one article where donald knuth wrote a super beautiful literate program and gets absolutely #rekt by McIlroy
00:56:24 <merijn> @quote xslt
00:56:24 <lambdabot> darius says: I imagine XSLT programmers say "It's a one pager" the way most other programmers say "It's a one liner".
00:56:35 <jared-w> lol
00:56:38 <mauke> tdammers: interact?
00:56:51 <tdammers> mauke: even better
00:57:10 <merijn> Pretty sure I have some data that that breaks on
00:57:25 <merijn> Incidentally, I learned "this one neat trick to speed up sort"
00:57:52 <merijn> Turns out that by default it tries to use stupid little memory
00:58:44 <jared-w> https://www.cs.princeton.edu/courses/archive/spring17/cos333/knuth-mcilroy.pdf  <-- this is the article I was talking about
00:59:19 <merijn> Turns out I can just pass "-S" to specify using more memory and be 2 orders of magnitude faster
00:59:22 <rabbi1> is there anything similar to twitter built on haskell?
00:59:39 <merijn> rabbi1: Define "built on"?
00:59:45 <merijn> Also, define "similar"
00:59:47 <jared-w> we should make a twitter clone
00:59:50 <jared-w> and call it hitter
01:00:03 <tdammers> might as well go all the way and call it hitler :x
01:00:11 <tdammers> (speaking of Godwin's Law)
01:00:17 <zomg> hsitter?
01:00:19 <jared-w> nah, call it hitter and then you swipe right on tweets to "hit dat"
01:00:28 <tdammers> also, this is -blah material methinks
01:00:41 <jared-w>  ¯\_(ツ)_/¯ it's 1AM, it's not like I'm being productive anyway
01:03:34 <butter> mauke: Wait, sorry can you explain the lcm example again?
01:03:47 <butter> And what's a good book where I can pick up functions like foldl?
01:07:08 <quchen> jared-w: nub3 = flip(foldr(ap(flip.(ap.).join.(liftA2 bool.).ap((.).(.).(:))(flip(.).(:)))elem)(const []))[]
01:07:13 <quchen> There you go, nub as a one-liner
01:08:34 <boj> my eyes >.<
01:10:26 <cocreature> quchen: let me guess, you’ve written perl in the past?
01:10:31 <LiaoTao> quchen, Are you human? :|
01:10:38 <quchen> cocreature: Never have I ever
01:10:57 <quchen> LiaoTao: Why, yes?
01:11:13 <LiaoTao> Just checking
01:11:25 <quchen> I mean I’m fairly confident that I am, up to Gettier getting in the way and shenanigans like that
01:12:02 <quchen> Lambdabot is not human, as far as I know. I may have had a little help for the pointfree part. :-)
01:12:14 <quchen> The version I gave to it was
01:12:15 <quchen> nub2 ys = foldr (\x xs cache -> bool (x : xs (x:cache)) (xs cache) (x `elem` cache)) (const []) ys []
01:12:22 <quchen> That one is handwritten.
01:12:56 <quchen> Needless to say, I write all my production code in this style, for no human shall ever touch my creations.
01:15:39 <jared-w> quchen: it's beautiful, fam
01:16:00 <LiaoTao> quchen, At least now I can sleep at night
01:16:07 <LiaoTao> lambdabot, Good boy
01:17:21 <quchen> LiaoTao: I write Haskell because I’m not particularly good at programming, much like you need a cane when you’re not particularly good at walking. So all this obscure stuff is just showing off the nice stickers I have on my cane. (I really hope »cane« is unabiguous here.)
01:17:58 <quchen> I have a lambdabot on it for sure! :-)
01:18:12 <merijn> Is there an easy way to turn a storable vector into a bytestring?
01:20:19 <jared-w> alright, I'm off to bed. See y'all later
01:21:27 <merijn> There's all the "unsafeWith", but that kinda bracket-like operation doesn't play well with a transformer stack
01:23:04 <quchen> merijn: fold it to a bytestring builder?
01:23:48 <merijn> quchen: I want 0 copy if at all possible
01:25:32 <quchen> merijn: That sounds hard – what if the original vector is deleted and the data is GC’d?
01:25:57 <quchen> I think bytestring manages its own pinned heap section, Vector.Storable maybe as well (with all its ForeignPtr business)
01:26:10 <quchen> So just a »smarter unsafeCoerce« probably won’t do
01:26:14 <merijn> quchen: bytestring is just a ForeignPtr
01:28:02 <ertes-w> merijn: storable vector is also ForeignPtr
01:28:17 <ertes-w> if you make sure to use the *same* ForeignPtr, you can probably reuse it
01:28:43 <ertes-w> although there is a caveat:  both ByteString and Vector do slice management
01:29:03 <merijn> ertes-w: I don't have slices, I'm just porting some C code using a struct array
01:29:03 <ertes-w> so they kinda have a mini-memory-manager built into the finalisers
01:29:31 <merijn> So all I have is a single storable vector that I wanna load/store into a database (which only groks ByteString)
01:29:51 <maerwald> anyone knows about secure coding rules for haskell?
01:29:52 <ertes-w> merijn: the best way would be to teach your database binding Vector
01:30:07 <merijn> The problem is, most of the stuff using ForeignPtr expects you to us withForeignPtr, but that doesn't work in a monad stack
01:30:26 <ertes-w> merijn: you can make it work via monad-control
01:30:33 <ertes-w> if you really need it…
01:30:46 <merijn> ertes-w: Yeah...no...I'm not going to patch my database bindings to get this working
01:31:06 <merijn> At that point I'd rather say fuck it, keep my C implementation and pass the ByteString to FFI code
01:32:29 <LiaoTao> findEndpoint = flip (find . flip flip 100 . ((<) .) . flip qd) . foldr (uncurry ((. (:)) . (.) . (:))) ([])
01:32:32 <LiaoTao> This pointfree stuff is fun
01:34:45 <maerwald> LiaoTao: http://perlobfuscator.com/
01:37:19 <LiaoTao> maerwald, Cool
01:37:41 <maerwald> you should code something like that for haskell, utilizing pointfree
01:41:58 <merijn> maerwald: Be the chance you want to see in the world! :)
01:51:01 <MasseR> Is the 'free' in free monad free as in beer or free is in free and open?
01:51:12 <maerwald> yes, it's all about beer
01:54:27 <merijn> I always prefer to license my academic code "free as in puppies". Yeah, you can use it, but you're going to have to pay to fix it and if it wrecks your house it's not my problem :p
01:58:27 <maerwald> "free as in vodka"
02:08:04 <Cale> MasseR: The latter -- it's free as in "unrestricted by equations"
02:08:34 <MasseR> Cale: Thanks.
02:08:41 <dredozubov_> is there any news considering munihac 2017?
02:08:44 <MasseR> Just wondering on how I should translate it
02:08:48 <Cale> MasseR: The term gets used in conjunction with a variety of different mathematical objects
02:09:08 <Cale> MasseR: Which language?
02:09:12 <MasseR> Finnish
02:09:48 <quchen> dredozubov_: We’ll move it back to Fall, because with ZuriHac and Hac Freiburg there are already lots of Haskell conferences in our neighbourhood. And there’s usually a shortage of conferences in fall/winter
02:10:35 <quchen> Haskell Exchenge and ICFP are much more global and in a different format, so we don’t see them as much in the same space as other weekend-long hackathons
02:10:53 <quchen> dredozubov_: We’re shooting for early November
02:11:02 <dredozubov_> cool, keep us posted
02:11:19 <Cale> MasseR: https://wiki.helsinki.fi/download/attachments/31429687/kahdeksasluku.pdf seems to be something on free groups in finnish :)
02:11:56 <dredozubov_> quchen: should we try to implement inductive tuples by the end of zurihac?
02:11:59 <dredozubov_> :P
02:12:06 <dredozubov_> munihac*
02:12:08 <MasseR> Apparently it is 'free as in open source' style of free. Thank you. ('ilmainen' vs 'vapaa')
02:12:24 <dredozubov_> characters were next to each other, it seems
02:12:36 <quchen> dredozubov_: Oh, you’re the inductive tuples guy!
02:12:46 <quchen> dredozubov_: I can’t stop thinking about how good that quote is
02:12:52 <dredozubov_> people never called me that before
02:17:27 <kosmikus> dredozubov_, quchen: go go! :) I'd certainly support inductive tuples ...
02:18:08 <dredozubov_> btw, do you guys know if anyone do any work regarding integers and rationals type literals?
02:18:21 <dredozubov_> i would love to see them supported by ghc
02:18:50 <quchen> kosmikus: The quote is something along the lines of »In Java they have problems because generics don’t allow inheritance and they have no module system, and here in Haskell land we worry (jazzhands!) ›oh no we need inductive tuples‹«
02:19:57 <dredozubov_> I think kosmikus was present when I said it :)
02:20:02 <quchen> Ooh.
02:20:08 <quchen> It’s a shame we did not record it
02:20:28 <dredozubov_> it would make a really unpopular vine video
02:23:20 <maerwald> does GHC have a formal memory model?
02:23:45 <mniip> do you mean an ABI?
02:24:05 <maerwald> no
02:24:12 <hjulle> Hmm, "Could not deduce (GetNr (n <=? 1)) from the context: (GetNr 'False, GetNr 'True)". What else would it be?
02:25:56 <hjulle> Does the Law of Excluded Middle not hold for DataKinds Bools?
02:28:51 <kosmikus> dredozubov_: yes, I remember the quote
02:29:03 <Logio> MasseR: "vapaa" is the right word
02:29:22 <hjulle> Isn't HList kind of like inductive tuples?
02:29:32 <hjulle> Or have I misunderstood what it means?
02:29:53 <mniip> hjulle, no lem doesn't hold
02:30:19 <hjulle> mniip: What would be an counter-example?
02:30:24 <hjulle> *a
02:30:43 <mniip> there doesn't need to be a counterexample
02:30:49 <mniip> there's just fewer things you can prove
02:31:46 <mniip> there's actually a problem with having (GetNr 'False, GetNr 'True) imply forall a. GetNr a
02:31:51 <hjulle> mniip: So is it true, but unprovable?
02:32:19 <hjulle> Or is it false without counter-examples? What would that even mean?
02:32:33 <mniip> "true" isn't the right word
02:32:35 <mniip> nor is "false"
02:32:46 <mniip> it's types we're talking about
02:33:02 <hjulle> ok, so what is the problem?
02:33:22 <mniip> hmm look
02:33:39 <mniip> if we state lem as 'forall (a :: Bool). p True -> p False -> p a'
02:33:44 <mniip> then that is false
02:33:53 <mniip> there's no such function
02:34:01 <mniip> (via C-H)
02:34:27 <mniip> I guess I forgot to add 'forall (p :: Bool -> *)'
02:34:56 <rizo_> Hey! I'm writing a combinator-based Pratt parser. My state is a parametirc type (on some `a` Expr). I think State monad is not enough in this case and I need an Indexed Monad. Are there any resources about parsing with indexed monads I can learn from?
02:35:43 <mniip> hjulle, the problem is that if such a function existed, and was applied to two different parameters,
02:36:04 <mniip> then the resulting 'forall (a :: Bool). P a' would not be "uniform" in a
02:36:11 <mniip> parametric polymorphism would break
02:36:53 <mniip> more formally some of the conditions in the definition of a profunctor end wouldn't hold
02:36:57 <mniip> (forall = end of a profunctor)
02:37:05 <mniip> I'm not sure I have a simpler explanation sorry!
02:37:23 <hjulle> mniip: What does uniform mean in the context?
02:38:08 <mniip> it's meant to be analogous to uniformity in calculus
02:38:39 <mniip> CT introduces its own ideas which are kinda analogous
02:39:17 <mniip> brb
02:40:28 <hjulle> rizo_: So your hidden state will change type during the execution of the monad?
02:40:35 <Cale> rizo_: My guess would be no, though that seems like something you could write a paper about if you explored it (and there will surely be some neat applications of it)
02:44:35 <hjulle> rizo_: I think most of the time, you can just reuse the implementations for non-indexed monads and let type inference determine what types the functions should have.
02:44:48 <mniip> hjulle, do you understand that due to parametricity, a function 'forall a. a -> Int' has to always return the same int?
02:44:54 <mniip> as in, the value cannot depend on the type a
02:45:51 <hjulle> mniip: Ah, right. That makes sense.
02:46:13 <rizo_> hjulle: For simple languages the where there's only one AST it's not a problem because I can partially apply the state (it doesn't change), but this way I can't mix multiple sub-ASTs.
02:46:50 <mniip> hjulle, for a similar reason, we can't say that the kind Bool only has two inhabitants, 'True and 'False
02:47:28 <rizo_> s/languages the/languages/
02:47:54 <mniip> it also has something like forall a. a
02:48:14 <hjulle> mniip: Where can I read more about the category theory behind haskell types? (e.g. what relation forall has with profunctors)
02:48:19 <mniip> hmm, no, ignore that. that's not a useful way to think about it
02:48:44 <rizo_> hjulle: Also my state is not hidden since I want to `get` it to access the symbol table (which stores other parsers).
02:48:50 <mniip> hjulle, well, the keywords are: extranatural transformation, profunctor, wedge, end/coend
02:49:08 <mniip> maybe try nlab or wikipedia
02:49:28 <Cale> rizo_: Wait, what? :)
02:49:52 <Cale> oh, something to do with the mechanism you're using to parse things
02:50:37 <hjulle> mniip: More specifically, what is the connection to haskell? I.e. formal specifications in terms of category theory?
02:50:56 <mniip> if you have a type T with a free variable a
02:51:23 <mniip> and you make a profunctor that would be parametrized by first all contravariant occurences of a, and then by all covariant occurences
02:51:43 <mniip> then the end of such profunctor would be isomorphic to forall a. T
02:52:11 <mniip> for example, forall a. a -> a
02:52:37 <mniip> the type 'a -> a' turns into /\a' a -> a' -> a
02:53:08 <mniip> uh
02:53:11 <mniip> the type 'a -> a' turns into /\a' a -> (a' -> a)
02:53:17 <hjulle> What is a'?
02:53:23 <mniip> the second parameter
02:53:31 <mniip> a profunctor maps two objects to an object
02:53:43 <mniip> it maps objects a' and a to the object (a' -> a)
02:54:44 <hjulle> What is /\?
02:55:13 <mniip> type-lambda
02:55:21 <geekosaur> capital lambda/type lambda
02:55:21 <mniip> I guess in haskell it would be expressed like
02:55:34 <mniip> data OurProfunctor a' a = OurProfunctor (a' -> a)
02:56:27 <piyush-kurur> Is it the case that the stage restriction of TH has been removed? Otherwise how is lenses able to deriveLenses in the same module where the type is defined?
02:58:44 <mniip> hjulle, yeah, so, the end of OurProfunctor is isomoprhic to ()
02:59:11 <mniip> which signifies that there's only one function of type 'forall a. a -> a'
02:59:48 <mniip> the end also gives you an explicit mapping, for every type A, from the inhabitants of () to A -> A
03:01:39 <hjulle> ok, I have a bit better understanding for it now. thanks
03:02:20 <MarcelineVQ> piyush-kurur: the restriction is the generating TH code being in the same module, not the splices, I think. since it can splice the derived code in anywhere after the datatypes are defined, which does still create a sequence restriction due to that
03:02:50 <piyush-kurur> MarcelineVQ: thanks 
03:02:52 <MarcelineVQ> idk though, pretty vague on all that
03:06:03 <rizo_> Cale: Sorry if I didn't make myself clear enough :) Here's a simplified version of the type I mentioned: https://gist.github.com/rizo/369b4510cf57f83cdbf23c1838208eb6
03:09:23 <rizo_> Cale: I'm not able to combine `Parser a` and `Parser b` because the state is not the same.
03:11:44 <Cale> What would you do to combine them?
03:12:09 <Cale> Does it actually make sense to combine them in some way?
03:13:02 <hjulle> rizo_: Yes, you should probably use indexed monads. They are basically the same as normal monads with an extra parameter.
03:13:40 <rizo_> Cale: I have separate ASTs for expressions, patterns, types, etc. and would like to be albe to have separate parsers for these sub-languages (`Parser Expr`, `Parser Pat`, etc). The problem is that the sub-languages are mutually recursive.
03:14:26 <hjulle> rizo_: you need to change the type to Parser s a though, otherwise it'll be impossible to combine.
03:14:52 <hjulle> rizo_: http://blog.sigfpe.com/2009/02/beyond-monads.html is a good article about it
03:14:56 <rizo_> For example, I want to parse a global declaration `Parser TopPhrase` and built it from `Parser Pat` and `Parser Expr` parsers.
03:15:34 <Cale> rizo_: Just to check, you're familiar with the usual style of monadic parsing library?
03:16:43 <rizo_> Cale: You mean Parsec? I just started a rewrite of a project from OCaml to Haskell, so my knowledge is a bit limited. I do understand the concepts but it's hard for me to map them to Haskell ATM.
03:16:57 <Cale> Yeah, stuff like Parsec
03:17:51 <rizo_> Cale: Yup, I wrote a Parsec clone in OCaml to learn it ;)
03:18:26 <Cale> So let's think about this
03:18:35 <Cale> What does PState a actually mean as a type?
03:18:48 <Cale> This is a thing which pairs up some input string
03:19:16 <hjulle> Apparently that article was parameterized about monads, not indexed. I'm not quite sure what the difference is.
03:19:26 <Cale> with a Map (keyed by String?) that contains functions that accept a String and produce a parser for values of type a
03:19:34 <hjulle> *about parameterized monads
03:20:09 <rizo_> It plays two roles: a) providing the input and b) storing the "symbols" (Token -> Parser a) for infix and prefix handlers (used in Pratt alg).
03:20:12 <Cale> hjulle: They're the same thing if you look at it the right way
03:20:16 <mniip> hjulle, indexed monads are things parametrized by "input" and "output"
03:20:27 <mniip> where "composition" only works if they match appropriately
03:20:33 <Cale> hjulle: But it's hard to unify them in Haskell -- easier if you have a dependent type system :)
03:20:37 <mniip> by which I mean <*> and >>=
03:21:05 <Cale> hjulle: Well, also that kind is still called an indexed monad -- the terms are pretty interchangeable
03:21:23 <Cale> hjulle: But it is, on the face of it, different
03:21:41 <rizo_> Here's my original attempt to write this in OCaml: https://github.com/fold-lang/fold/blob/master/src/Pratt.ml#L152
03:21:41 <Cale> You can imagine indexing a monad on the arrows of a category
03:23:25 <Cale> and so  join : M (f : a -> b) (M (g : b -> c) t) -> M (g . f : a -> c) t
03:23:42 <Cale> If that notation makes sense :)
03:24:11 <Cale> One kind of indexed monad is what you get when the category is a monoid (i.e. there's only one object, all arrows can compose)
03:24:24 <Cale> The other kind is what you get when the category is a preorder
03:24:50 <Cale> Or, I suppose, the indiscrete category
03:26:03 <hjulle> rizo_: What types should the state have and what does it depend on?
03:26:36 <geekosaur> piyush-kurur, I can't tell if someone answered you while my vps was being rebooted/upgraded, but the stage restriction as you described it applies only to functions being called. For data values, the only restriction is it must be completely defined before it is used.
03:26:44 <geekosaur> or rather, data types
03:27:09 <Cale> rizo_: So in your ocaml code, your State type doesn't appear to depend on the type of the result of the parser at all, does it?
03:27:48 <piyush-kurur> geekosaur: MarcelineVQ already answered it. thanks anyways 
03:27:51 <rizo_> The state depends on the lexer and curiously enough on the parser itself, they are recursive. Parser advances the state, sub-parsers ask the state for token handlers (other parsers).
03:29:07 <rizo_> Cale: yes, correct. This implementation is monomorphic(?) to `a`. I can't `bind` parsers of different types.
03:30:14 <Cale> I'm still not 100% clear on what the table :: Map String (String -> Parser a)  field of PState means
03:30:37 <Cale> The keys are what? Named symbols of some sort?
03:30:51 <rizo_> I can make state depend on `a` but it won't be general enough to implement a monad: `Parser a -> (a -> Parser a) -> Parser a`. Because the returned state has to be of the same type.
03:31:10 <sphinxo> Anyone used a c main alongside cabal/stack?
03:31:24 <rizo_> Cale: Yes, the keys are tokens that map to token parsers. "+" maps to an infix parser
03:31:25 <sphinxo> I can't seem to import my haskell modules from my c main
03:31:32 <rizo_> It's a grammar in a sense.
03:31:52 <Cale> and then the values are what?
03:32:40 <rizo_> The values in the map are prefix and infix parsers.
03:32:57 <rizo_> Cale: Here's a basic implementation of a Pratt parser in Haskell
03:32:58 <rizo_> https://gist.github.com/aisamanra/e52791fcea7b75905c68
03:33:46 <hjulle> rizo_: Why not just do this (for your first example)? https://gist.github.com/anka-213/78063e1f410e4ff2938ddfd5e44b4f90 
03:33:47 <rizo_> The grammar in this case is `Tables` type provided by the ReaderT.
03:34:22 <Cale> hjulle: That doesn't kind check
03:34:28 <Cale> You have Parser s there
03:34:35 <Cale> But you added a parameter to Parser
03:34:40 <Cale> So it needs another
03:35:18 <hjulle> Cale: Right. I've fixed it now
03:36:15 <Cale> Er...
03:36:42 <Cale> I had a tenuous enough grasp on what the parsers inside the table were used for, without the type being something weirdly different :)
03:36:43 <rizo_> Cale, hjulle: y, PState would have to depend on both: s and a
03:37:33 <hjulle> rizo_: Why?
03:38:21 <Cale> rizo_: So, if I provide this thing with some initial input and, say, an empty table
03:38:23 <rizo_> hjulle: `M.Map String (String -> Parser s)` is `Parser s` partially applied here?
03:38:34 <Cale> rizo_: I'm definitely getting a value of type a, no matter what?
03:38:43 <hjulle> rizo_: Refresh the page
03:39:03 <Cale> hjulle: That kind checks now, but I don't think it makes sense
03:39:31 <Cale> Why should the parser in the table start producing values of type s?
03:39:39 <rizo_> If the table/grammar is empty pratt parser will fail. But yeah, `a` is the result of parsing.
03:40:22 <Cale> rizo_: Well, you're still giving me a function of type PState a -> (a, PState a)
03:40:47 <Cale> So if I provide any value of type PState a, it looks like I get a result of type a, unconditionally
03:40:52 <Cale> even if the table is empty
03:40:56 <rizo_> That's the thing, this function is too restricted to mix parsers for different types.
03:41:39 <rizo_> Oh, the resulting `a` depends on the `a` in `PStat a` not on the resulting `PState`
03:42:01 <hjulle> rizo_: So, should the table be able to contain parsers of different types?
03:42:27 <rizo_> That's why I think need indexed monads. The resulting `PState` has to be generic enough to (monadically) compose with the other parsers.
03:43:19 <Cale> rizo_: Well, forget about the interface it'll eventually satisfy
03:43:24 <Cale> rizo_: What will the type be?
03:43:33 <hjulle> rizo_, Cale: My version is just a generalization which makes it a valid functor (and monad). If you use Parser a a, you get back the old type.
03:46:31 <Cale> rizo_: Maybe if I actually understood what a Pratt parser was, I would stand a better chance of helping here
03:46:43 <hjulle> But yes, if you want to combine Parser f a with Parser g b, you probably need an indexed monad.
03:46:46 <Cale> rizo_: Can you explain in English how it's meant to work?
03:47:27 <rizo_> The table contains parsers only for one type. Let's say we are defining the `Expr` sub-language: the table will have entries for function calls, values, etc. that will _only_ result in `Expr` type. But you may want to combin the Expr lang with Patt lang.
03:48:27 <rizo_> Cale: sure! Pratt parser is a top-down operator precedence parser. It operates on tokens instead of production rules.
03:48:53 <hjulle> rizo_: And what type will the combination have? Will it have a table with two different types?
03:48:55 <rizo_> Each token can be seen in two positions: infix and prefix. And the parser for that token is searched in the table/grammar.
03:50:48 <rizo_> The parser starts at the prefix position and requests token handlers (from the state in our case). Each infix token has a precedence that decides which parser will get the already parsed sub-tree. 
03:51:27 <Cale> rizo_: I'm really tempted to suggest that you implement some way to turn these parsers into a function which looks something like String -> [(a, String)] -- i.e. functions which take the input and produce a list of possible parses, each with a depleted output string
03:51:40 <Cale> and then from there, just do monadic parsing in the usual way :)
03:52:19 <Cale> Presumably when you actually *run* the parser, you'll need to be able to get something close to that
03:52:41 <Cale> String can be replaced with list of tokens if you prefer
03:53:11 <rizo_> Where would be the rules stored in this case?
03:53:23 <Cale> Somewhere in the definition of that resulting function
03:53:51 <Cale> i.e.
03:53:53 <rizo_> Here's another simpler implementation in Haskell: https://github.com/cbowdon/TDOP/blob/master/tdop.hs#L58
03:54:04 <Cale> Suppose you implement some  TableParser a
03:54:12 <rizo_> But all of them restrict the type of `a` for the parser.
03:54:42 <Cale> You need to eventually be able to supply it with input and run it to get some output value(s) of type a and depleted input string, right?
03:55:09 <Cale> So, then you have to be able to write  TableParser a -> String -> [(a, String)]
03:55:17 <Cale> (maybe that list has length at most 1 or something)
03:55:30 <rizo_> Yes, a state is created with the initial set of `Token -> Parser a` entries
03:55:43 <Cale> Sorry
03:55:54 <Cale> So, then you have to be able to write  TableParser a -> [Token] -> [(a, [Token])]
03:55:58 <Cale> is that clearer?
03:56:13 <Cale> I mean, the actual function you're finally going to use to run the dang thing
03:56:47 <Cale> If it's not obvious why there's a list, perhaps imagine that's a Maybe
03:57:00 <Cale> TableParser a -> [Token] -> Maybe (a, [Token]) -- good enough
03:57:19 <rizo_> I thought about that. I could have Reader on top of State. That would probably work. But... the problem is that I'm building a "lisp", I want to be able to change the Table as I parse (add new grammar entries). So the Table has to be part of the state :/
03:57:46 <hjulle> rizo_: How do you combine two parsers in the algorithm?
03:57:49 <Cale> nono, that's all fine
03:57:55 <Cale> I just mean
03:58:02 <Cale> You use a *completely separate monad*
03:58:06 <Cale> to combine these parsers
03:58:22 <Cale> and turn the parsers into actions in this monad, by effectively running them
03:58:37 <Cale> Your monad would be something like  StateT [Token] []
03:59:36 <Cale> and you can take the results from previous parsers, and use them to inform the construction of future parsers however you like
04:00:06 <Cale> Does that make sense?
04:00:23 <rizo_> I understand the core idea, by having a simpler State I avoid having to worry about the table.
04:00:42 <Cale> You will worry about the table, locally
04:01:35 <Cale> It's just that it doesn't appear to make any sense at all for the table for Expr to persist on to somewhere where you're parsing a Pat or something
04:01:43 <rizo_> I think it makes sense. Essentially it's a standard State with partially applied table.
04:03:14 <rizo_> Cale: Yes, it's confusing. Even with indexed monads I can't come up with a clear separation of parser tables.
04:03:57 <rizo_> Cale: I need to try your suggestion!
04:05:12 <rizo_> Cale, hjulle: Thanks for helping me with this, I'll let you know how it went.
04:08:50 <orion> In attoparsec I want to read a single Word8, then read that many bytes, and parse those bytes. Is there a way to do it without calling "parse"/"parseOnly" twice?
04:09:34 <merijn> orion: eh...why are you calling parse at all there?
04:10:13 <cocreature> use the monad luke
04:11:13 <merijn> anyWord8 >>= take . fromIntegral
04:12:16 <Cale> What's take?
04:12:42 <Cale> oh, that does exist
04:12:54 <Cale> it's in the type-specific libraries of course
04:13:08 <hjulle> Which gives back a ByteString, which (from what I understand) orion wants to parse. (Hence calling parse twice)
04:13:14 <orion> An example of what I need to parse is: "\x0bFOO_BAR_BAZ"
04:13:41 <hjulle> orion: What should the result be?
04:14:05 <orion> And the result should be FooThing { a = "FOO", b = "BAR", c = "BAZ" }
04:14:24 <orion> I can already do it without the length byte.
04:14:48 <hjulle> Just ignore the length byte then.
04:14:57 <orion> But the Parser I came up with relies on "endOfInput".
04:15:03 <orion> And I am reading from a TCP stream.
04:15:31 <lyxia> The problem with take is that you just get the bytestring and you have to run a separate parser to parse it.
04:15:42 <cocreature> can you change the parser of FooThing to make it rely on the length instead of endOfInput?
04:15:54 <cocreature> definitely not super pretty but it should work
04:15:57 <orion> lyxia: That's exactly the point of my question ("without calling "parse"/"parseOnly" twice").
04:18:03 <orion> cocreature: I'm not sure how to do that.
04:18:33 <cocreature> orion: basically have the parser of FooThing keep track of how many bytes it has consumed and compare that to the length you parsed before
04:18:42 <cocreature> but tbh I probably would just use "parse" twice
04:18:45 <atodorov> hi folks, I'm adding hspec tests for a project and I have something like 
04:18:45 <atodorov> parseFunc `parseSatisfies` expectedFunc  
04:18:45 <atodorov>   where
04:18:45 <atodorov>       expectedFunc r = ....
04:18:45 <atodorov> and this all works but when I try to add another it section afterwards the compiler complains with "parse error on input ‘it’" any idea what I'm messing up? I can send a GitHub link to the entire file if necessary
04:19:13 <cocreature> atodorov: a github link would be helpful but if I had to guess, you have probably forgotten a "do"
04:19:40 <ongy> or you have an alignment error
04:21:45 <atodorov> cocreature: https://github.com/weldr/haskell-rpm/pull/16/files#diff-a90acf530e450e0b2cc57855aa5dab9d. Even if I add a 'do' on line 14 I get the same error
04:21:51 <hjulle> orion: It's a good question. I've been wondering about it before. How do you do a length-restricted parser, without nesting "parse"?
04:22:53 <cocreature> atodorov: so you definitely need the do after the $ in line 14. you also need to make sure that the two "it"s start at the same column
04:24:16 <atodorov> cocreature: both of these are true from what I can tell. is there any way to get a better error message ?
04:24:37 <cocreature> atodorov: can you show us the full error message?
04:24:58 <atodorov> $ cabal test --show-details=always
04:24:58 <atodorov> Preprocessing library rpm-1...
04:24:58 <atodorov> Preprocessing test suite 'tests' for rpm-1...
04:24:58 <atodorov> [7 of 8] Compiling RPM.Parse_parseRPMSpec ( tests/RPM/Parse_parseRPMSpec.hs, dist/build/tests/tests-tmp/RPM/Parse_parseRPMSpec.o )
04:24:58 <atodorov> tests/RPM/Parse_parseRPMSpec.hs:116:5: error:
04:24:58 <atodorov>     parse error on input ‘it’
04:25:19 <cocreature> atodorov: there is no line 116 in the file you linked on github
04:25:26 <cocreature> so it’s kind of hard to tell you what’s going on
04:25:35 <atodorov> cocreature: I can commit this so you can take a look if you want
04:25:59 <cocreature> that would be helpful
04:26:27 <cocreature> atodorov: I would also highly recommend to pull some of that code out into separate definitions in particular the large bytestring. that makes it significantly easier to see if things line up like they should
04:27:36 <atodorov> cocreature: here's the new commit https://github.com/atodorov/haskell-rpm/commit/9d491d6f26ef547f2515cfa4eb77c994aad56b63
04:28:07 <atodorov> otherwise thanks for the tip
04:30:01 <hjulle> atodorov: Probably incorrect indentation
04:30:26 <cocreature> yeah something like that but with that giant definition it’s really hard to figure out where the problem is
04:30:31 <cocreature> the "it"s seem to align
04:30:34 <ongy> I think it's the where in line 80
04:30:45 <ongy> isn't that always at the end of a body?
04:30:53 <cocreature> oh right that seems wrong
04:31:58 <atodorov> ongy: what do you mean, please explain? 
04:33:56 <cocreature> atodorov: a "where" block needs to belong to a definition, i.e., something defined using an equal sign. but there is no definition here
04:35:32 <ongy> atodorov: the where block ends the body of 'spec =' so it tries to parse your 'it' in line 116 as part of the where, which fails
04:36:40 <hjulle> atodorov: Just move the it to above the where block and it should work.
04:38:18 <atodorov> hjulle: thanks, that worked
04:39:04 <atodorov> so this means I need to write all my tests and the at the bottom define all the functions which compare the results to whatever is expected
04:39:42 <butterthebuddha>   print (filter (isPrime) [2..])
04:39:44 <butterthebuddha> How do I make that stop when I have 7 primes?
04:39:54 <hjulle> atodorov: You can also replace the "where" with a "let" and place it above the code that uses it.
04:40:50 <quchen> butterthebuddha: Only take the first 7 primes!
04:40:57 <quchen> > take 7 [1..]
04:40:59 <lambdabot>  [1,2,3,4,5,6,7]
04:41:02 <ongy> atodorov: if you make the huge bytestring its own top level definition, it will be a lot more readable either way, then it probably doesn't make that much of a difference
04:41:25 <atodorov> hjulle: thanks, I like the let variant better
04:41:51 <hjulle> atodorov: But, no. Use ongy's variant instead. It's much better.
04:42:18 <atodorov> I will give it a try and see how it goes, thanks for the help
04:43:29 <hjulle> atodorov: Just move out every large expression to its own top level function and it will be much more readable.
04:45:58 <atodorov> hjulle: the main reason I wanted to keep the expectation and the bytestring variables together is to make it easier to navigate the code and be able to see the two with the least scrolling as possible
04:47:24 <anieuwland> Hi everyone :) Does anyone have experience with gi-gio? I'm trying to connect to signals from VolumeMonitor, but they don't seem to fire
04:52:24 <hjulle> atodorov: Btw, matchExpected doesn't typecheck. The expression is not a monad, it's bool.
05:02:32 <hjulle> atodorov: But honestly, just use a text-editor/IDE which allows you to easily jump between definition and usage. It'll save you a ton of time in the long run.
05:03:50 <hjulle> atodorov: What is that? Can you send a link?
05:04:09 <hjulle> anieuwland: What is that? Can you send a link?
05:05:13 <anieuwland> hjulle: They're haskell bindings to Gnome IO library (https://hackage.haskell.org/package/gi-gio and https://developer.gnome.org/gio/stable/)
05:06:41 <anieuwland> I registered callbacks to the onVolumeMonitorMountAdded etc. signals, but they never seem to fire
05:06:45 <anieuwland> The same code used to work for gtk2hs, so I think I'm doing it correctly, but...
05:06:47 <atodorov> hjulle: https://github.com/weldr/haskell-rpm/ - RPM parser in Haskell
05:07:25 <codedmart> A team member used type families and type level code to easily generate routes for different endpoints/models using the servant library. It builds and runs fine locally with OS X. When I tried to build it on the server it just chews up memory and disk space until all disk space is used and crashes. In this case about 16GB. Any ideas?
05:07:45 <hjulle> atodorov: (accidentally pinged the wrong person, sorry)
05:08:26 <cocreature> codedmart: same version of ghc and all deps?
05:09:10 <codedmart> cocreature: I assume so. We are using stack. The main difference is OSX locally and Ubuntu server.
05:09:56 <lyxia> hjulle, orion: I just opened an issue on attoparsec https://github.com/bos/attoparsec/issues/129
05:10:11 <ongy> anieuwland: I have no experience with gio, but do you have a way to verify the library works? Some monitor application that's build on it, or a minimal C (or whatever the original bindings are) application
05:10:21 <cocreature> codedmart: hm sorry then I’m out of ideas.
05:11:27 <codedmart> No worries,
05:12:57 <anieuwland> ongy: Hmm. Well I'm pretty sure the original library works, because different bindings (gtk2hs) work. Unfortunately those are otherwise out of date. I'm not sure how I'd go about testing if these new bindings (gi-gio) are correct
05:17:49 <mrkgnao> I've been wondering if a function-level ALLOW_AMBIGUOUS pragma might make sense. Type applications are really useful, but turning on AllowAmbiguousTypes module-wide kills compile-time errors for, e.g. spurious tyvars in foralls. 
05:26:29 <cocreature> mrkgnao: I would definitely like that. I’ve used proxies instead of type applications in the past just so I can avoid having to turn on AllowAmbiguousTypes
05:38:02 <mrkgnao> cocreature: I have, too. I suppose this might be a nice thing to hack on GHC for.
05:43:30 <neeb> Hi. Im getting "non-exhaustive patters.." error with this. How do I close the brakets? 
05:43:48 <neeb> primes n = filterPrime [2..n]
05:43:51 <neeb> where filterPrime (p:xs) = 
05:43:56 <neeb> p : filterPrime [x | x <- xs, x `mod` p /= 0]
05:44:54 <mrkgnao> neeb: what did you call the function `primes` with?
05:44:55 <cocreature> neeb: you’re getting that warning because filterPrim can’t handle an empty list
05:46:03 <neeb> mrkgnao: just with an integer
05:47:55 <mrkgnao> neeb: if you call it with the wrong integer, the argument to filterPrime is an empty list.
05:48:38 <mrkgnao> as cocreature said, it assumes the list has at least one element. you missed the "pattern" for the empty list (filterPrimes [] = blah).
05:48:56 <mrkgnao> hence "non-exhaustive". you haven't covered all the cases.
05:56:30 <dfordivam> Hello this is regarding a haskell reddit creation... I am interested in creating a sub-reddit for haskell for japanese audience... but it seems that requires a certain amount of *karma* ... So I was wondering can anyone help me with that
05:57:18 <Unhammer> If I want /usr/lib/x86_64-linux-gnu/odbc/ in extra-lib-dirs, should I put it in the .cabal, the project's stack.yaml or my user's stack.yaml?
05:58:34 <Unhammer> initially thought user since the path is linux-specific, but does it then change compiler flags for everything I compile? also might be helpful for other linux-users of the project
05:58:40 <Cale> dfordivam: I could make one for you, I suppose
05:59:22 <Cale> dfordivam: But it would probably be better for you to get some karma and do it yourself -- there are lots of options to set.
05:59:55 <Cale> I dunno, maybe I'll be able to mod you, I've never actually created a subreddit.
06:00:04 <mrkgnao> hoogle doesn't seem to help here: is there some known operator for \f g -> f >=> traverse g?
06:00:39 <Cale> :t \f g -> f >=> traverse g
06:00:41 <lambdabot> (Traversable t, Monad m) => (a1 -> m (t a)) -> (a -> m b) -> a1 -> m (t b)
06:00:53 <Cale> probably not
06:01:14 <mrkgnao> I tried copying that into hoogle and setting a1 = a and stuff.
06:01:22 <mrkgnao> Hm, thanks.
06:08:32 <codedmart> cocreature: OK how about this. I haven't tried locally without `--fast` -> stack `Turn off optimizations (-O0)`, but it builds fine on the server with `--fast`. Is turning off optimizations a bad thing or is there an in between setting?
06:09:19 <cocreature> codedmart: there is -O0, -O1 and -O2. turning off optimizations is a bad thing because they make your code faster and often significantly faster :)
06:09:29 <cocreature> codedmart: using -O1 instead of -O2 is not too bad usually
06:09:33 <cocreature> and sometimes even better
06:09:47 <codedmart> OK I will try that
06:09:48 <cocreature> but I wouldn’t want to use -O0 for production code
06:10:05 <erisco> I think pipes, iirc, is absurdly slow without optimisations
06:10:58 <codedmart> cocreature: Does it default to -02? This is what is in the cabal file now `-threaded -rtsopts -with-rtsopts=-N` So I can just add -01 do that.
06:11:22 <cocreature> codedmart: I don’t recall the default but adding -O1 should work
06:11:27 <Unhammer> … so if I first run "stack build --extra-lib-dirs=/usr/lib/x86_64-linux-gnu/odbc/", I can remove that arg if I put it in stack.yaml and it re-uses stack-work, but if I put it in the .cabal it seems it needs to rebuild (flags changed from ["--extra-lib-dirs=/usr/lib/x86_64-linux-gnu/odbc"] to []
06:11:29 <cocreature> codedmart: unless stack overwrites that somehow
06:11:45 <cocreature> codedmart: I would just pass it via --ghc-options to stack build
06:11:57 <neeb> thanks mrkgnao and cocreature. :) I had some problems with the indentation but I got it working now.
06:15:09 <orion> lyxia: Thanks.
06:15:56 <orion> Is it possible to write a pure function that returns a (Pipe a b, Pipe b a) where the Pipes share State?
06:16:23 <orion> I can do it impurely by using an IORef or MVar.
06:23:44 <L4rmbr> data Or a b = Fst a | Snd b 
06:23:52 <L4rmbr> instance Monoid (Or a b) where 
06:23:57 <L4rmbr>  mempty = mempty
06:24:14 <L4rmbr> can anyone explain the mempty behind the '=' ?
06:24:16 <L4rmbr> thx
06:24:35 <cocreature> that’s an infinite loop
06:24:51 <L4rmbr> I think so, but what is the ending condition?
06:25:00 <cocreature> there is none, that’s why it’s infinite
06:25:02 <erisco> that's called "begging the question"
06:25:37 <cocreature> also called “a bug”
06:25:52 <erisco> Or a b is not a Monoid
06:28:52 <L4rmbr> mempty `mappend` mempty  ,  the result is (),  can I say the identity still holds ?
06:29:03 <L4rmbr> in this case, i mean
06:32:40 <sproingie> anything appended with identity is itself, including identity
06:34:48 <Unhammer> Is it possible to only apply the --extra-include-dirs and --extra-lib-dirs to the local package? Since it all goes into .stack-work and gets deleted each time, it takes half an hour each time I experiment with settings :(
06:35:26 <L4rmbr> thx. Just for this "data Or a b = Fst a | snd b" case, I tried to find a mempty for its Monoid instance, I've tried seval ways but only this infinite definition compiles, but it seems just weird
06:35:46 <cocreature> L4rmbr: Or a b is just not a Monoid
06:37:18 <codedmart> cocreature: So far it isn't looking good. Using all 7GB of memory and has taken up about 7GB of disk space. There is no swap. I wouldn't think that would matter, but bringing it up in case it does.
06:40:39 <L4rmbr> cocreature, Thanks
06:41:46 <cocreature> L4rmbr: try writing mappend. what do you do for "mappend (Fst a) (Snd b)"
06:42:40 <L4rmbr> I first define its Semigroup instance, and reuse (<>) for the mappend, like this:
06:42:47 <L4rmbr> (Snd a) <> _ = Snd a
06:42:53 <L4rmbr> _ <> (Snd a) = (Snd a)
06:42:58 <L4rmbr> _ <> b = b 
06:44:22 <cocreature> alright. so let’s assume there is an identity, i.e., a valid implementation for mempty corresponding to that operation.
06:44:36 <codedmart> Yup doesn't help still uses all disk space and crashes.
06:44:49 <cocreature> it can’t be "Snd a" for some a because then Snd a <> Snd a' would be Snd a which means that Snd a is not the identity
06:45:33 <cocreature> so it has to be "Fst b" for some b. but then Fst b' <> Fst b = Fst b so it’s not the identity
06:45:53 <cocreature> thereby there is no identity element for your implementation of <>
06:46:05 <L4rmbr> agreed
06:46:07 <L4rmbr> thx
07:01:24 <byorgey> L4rmbr: that is an interesting semigroup.  What do you use it for?
07:03:45 <MagneticDuck> is it reasonable to run a computation that will take multiples days in the form of a fold through a lazy list?
07:04:28 <erisco> outrageous!
07:06:55 <MagneticDuck> so here's the thing with that
07:06:56 <barcabouna> how can i compile without all intermediate files as output?
07:07:41 <MagneticDuck> I wanted to write an Enumeration abstraction that represented the enumeration of a large search space -- maybe some state and a transition function
07:08:13 <MagneticDuck> but then I wanted to hide the state from the interface, and I realized that what I was describing was basically a Haskell list
07:08:19 <erisco> what is Enumeration?
07:09:16 <MagneticDuck> erisco: say S is some generic state, then maybe Enumeration a = Enumeration { init :: S, transition :: S -> Maybe S, progress :: S -> Integer }
07:09:50 <erisco> what is progress?
07:09:56 <L4rmbr> byorgey, Just an exercise, 
07:10:29 <erisco> :t unfoldr
07:10:31 <lambdabot> (b -> Maybe (a, b)) -> b -> [a]
07:10:45 <MagneticDuck> erisco: well, ignore progress
07:11:10 <MagneticDuck> maybe it would tell me how many more iterations I need to finish, but that can always be stuck in some other way
07:11:21 <erisco> you can see how Enumeration relates to the anamorphism
07:11:44 <MagneticDuck> well, yes
07:12:35 <MagneticDuck> but, it is really not possible to run a long computation through a lazy list?
07:12:56 <erisco> I don't see how computation time matters
07:13:42 <MagneticDuck> what's happening right now is apparently I haven't put seq in the right places, and the process takes up multiple gigabytes of memory within a few minutes
07:14:32 <MagneticDuck> my question is -- that is avoidable, right?
07:15:23 <erisco> that depends on what you're computing... by adding strictness you may reduce some thunks that are building up
07:15:49 <MagneticDuck> let's say I'm just searching for a maximum value
07:16:02 <erisco> sure, that can be done in constant space
07:18:32 <MagneticDuck> erisco: https://hastebin.com/irewucoxix.hs
07:19:33 <MagneticDuck> erisco: sorry, ignore `Enumeration` there, it's not relevant 
07:20:12 <MagneticDuck> and of course lines 17 through 19 are irrelevant too
07:22:11 <erisco> I don't know why you are forcing r to whnf, but I am also not sure why you are having memory problems with that
07:22:27 <erisco> is r constant space?
07:22:37 <MagneticDuck> yes
07:23:25 <MagneticDuck> one moment, I need to include a bit more
07:24:02 <codedmart> cocreature: Would you think this is a bug or related to my code if it builds fine with -O0 locally, but with -O1 and -O2 it doesn't? I just tried locally with -O1 and -O2 and it has been running for a while using 6.5GB of memory the whole time and so far the .stack-work folder is up to 25GB of disk space.
07:24:29 <erisco> does f v run in constant space?
07:26:41 <MagneticDuck> erisco: What state type would you use for a Computation that computes a fold over a list?
07:27:09 <erisco> it depends on what I am computing
07:27:49 <MagneticDuck> say we have a state `s`, an accumulator `(s -> a -> s)`, and an `[a]` to get through
07:27:51 <barcabouna> hello. if s = "0101010101..." how can i turn it into a list of 8char strings and then a list of ints?
07:28:00 <erisco> if f does not run in constant space and v keeps getting bigger then that could be your problem
07:28:23 <MagneticDuck> I'm using `([a], s)`, where `[a]` holds the list that hasn't been processed
07:28:51 <MagneticDuck> oh wow
07:29:36 <lyxia> barcabouna: you can write a recursive function that splits s into chunks, or you can use a function in the split library
07:29:56 <cocreature> codedmart: sounds like a GHC bug
07:30:02 <cocreature> codedmart: which version of GHC are you using?
07:30:04 <pavonia> barcabouna: Also the Numeric module is useful here
07:30:11 <codedmart> cocreature: 8.0.2
07:30:29 <MagneticDuck> erisco: https://hastebin.com/udujujasub.hs this is how I'm building Computations
07:30:32 <codedmart> Should I try a different version?
07:30:34 <cocreature> codedmart: it might be a good idea trying to reproduce it with the 8.2rc assuming that doesn’t require too much changes to your code
07:31:22 <barcabouna> lyxia: splitter s = take 8 s : splitter (drop 8 s)
07:37:35 <codedmart> cocreature: OK I will try that.
07:37:56 <c_wraith> Now I'm reading "codedmart" as "co-dedmart"
07:38:13 <codedmart> :)
07:39:43 <codedmart> Well that won't work. There are a number of packages that have `base <4.10`.
07:46:46 <barcabouna> does anyone know of a haskell counterpart to python's int(string, 2) ?
07:46:57 <barcabouna> basically in python bin(number) -> string 1010101
07:47:08 <barcabouna> and int(string,2) -> number
07:48:08 <barcabouna> i expect maybe read to help?
07:48:55 <MitchellSalad> you want "1010" = Just 10, "101z" = Nothing?
07:49:07 <barcabouna> "1001" => 9
07:49:32 <MitchellSalad> "abc" => ?
07:49:43 <barcabouna> i don't care. exception or maybe 
07:49:47 <merijn> barcabouna: You're probably looking for: https://hackage.haskell.org/package/base-4.9.1.0/docs/Numeric.html#v:readInt
07:49:53 <barcabouna> read "abc" :: Int
07:50:54 <Cale> > readInt 2 (`elem` "01") digitToInt "1001"
07:50:56 <lambdabot>  [(9,"")]
07:52:05 <Cale> (the list you get there is a list of possible parses, which will be empty if the parse fails, and will in this case have only a single element if it succeeds -- a pair of the number read, and the remainder of the input)
07:52:10 <Cale> > readInt 2 (`elem` "01") digitToInt "1001apple"
07:52:12 <lambdabot>  [(9,"apple")]
07:52:28 <Cale> > readInt 2 (`elem` "01") digitToInt "orange1001apple"
07:52:30 <lambdabot>  []
07:52:30 <benzrf> Cale: when might there be multiple results
07:52:50 <Cale> benzrf: When you're building up more complicated parsers
07:53:09 <benzrf> e.g.?
07:53:21 <dmwit> None of the parsers that ship with base ever return multiple results.
07:53:21 <barcabouna> it's actually better to get an exception maybe
07:53:31 <Cale> Well, consider a parser which matches zero or more of something
07:53:32 <dmwit> So only if you write an ambiguous parser yourself.
07:54:40 <benzrf> dmwit: kk
07:55:32 <MagneticDuck> why does foldr max 1 [1..product [1..20]]
07:55:42 <merijn> barcabouna: You can simply wrap the results of readInt
07:55:48 <benzrf> > foldr max 1 [1..product [1..20]]
07:55:50 <lambdabot>  *Exception: stack overflow
07:55:55 <MagneticDuck> (sorry, line break) consume a huge amount of memory?
07:56:01 <merijn> barcabouna: "case myResult of [(i, "")] -> Just i; _ -> Nothing"
07:56:02 <benzrf> MagneticDuck: because of laziness!
07:56:08 <MagneticDuck> and how should it be fixed to .. not do that?
07:56:10 <Cale> Because of strictness!
07:56:16 <benzrf> > foldl' max 1 [1..product [1..20]]
07:56:21 <dmwit> > foldr f z [a,b,c]
07:56:22 <lambdabot>  mueval-core: Time limit exceeded
07:56:23 <lambdabot>  f a (f b (f c z))
07:56:29 <Cale> (the strictness of max)
07:56:31 <MagneticDuck> well that was pretty easy
07:56:48 <MagneticDuck> @src foldl
07:56:48 <lambdabot> foldl f z []     = z
07:56:48 <lambdabot> foldl f z (x:xs) = foldl f (f z x) xs
07:56:55 <benzrf> MagneticDuck: no, not foldl - foldl'
07:56:56 <dmwit> ?src foldl'
07:56:56 <lambdabot> foldl' f a []     = a
07:56:56 <lambdabot> foldl' f a (x:xs) = let a' = f a x in a' `seq` foldl' f a' xs
07:56:58 <benzrf> foldl' is strict
07:56:59 <Cale> max pattern matches on both its arguments, and so eventually, many of these pattern matches will be waiting on the stack
07:57:11 <MagneticDuck> benzrf: yes, but I'm trying to get my head around why this happens!
07:57:24 <benzrf> MagneticDuck: do you know about things like "weak head normal form"?
07:57:42 <Cale> foldr max 1 [1..x] -> max 1 (foldr max 1 [2..x])
07:58:11 <Cale> and then max evaluates, and pattern matches on both its arguments -- the second argument isn't evaluated yet, so it waits on the stack
07:58:27 <Cale> (the pattern match of the second argument waits on the stack)
07:58:34 <dmwit> MagneticDuck: Before you can compute `max 1 (...)`, you have to know the answer to `(...)`. But you can't just throw the `1` away while you compute `(...)`, you have to store it somewhere.
07:58:35 <MagneticDuck> so you get max (1 max ( ... ))
07:58:39 <Cale> while foldr max 1 [2..x] gets evaluated
07:58:56 <dmwit> MagneticDuck: So you store `1`, then start computing `max 2 (...)`. Now you have to store `2`. And so one, all the way up to some ungodly big number.
07:58:57 <benzrf> MagneticDuck: do you know any conventional imperative languages like C or Python?
07:59:13 <MagneticDuck> benzrf: yes
08:00:06 <MagneticDuck> @src foldl'
08:00:06 <lambdabot> foldl' f a []     = a
08:00:06 <lambdabot> foldl' f a (x:xs) = let a' = f a x in a' `seq` foldl' f a' xs
08:00:07 <benzrf> MagneticDuck: ok, well, here's how you can imagine that haskell's evaluation model works (although this isn't quite how ghc implements it)
08:00:42 <benzrf> MagneticDuck: imagine that a haskell expression is stored as a syntax tree in memory, and this imperative program is in a loop that keeps simplifying the expression
08:01:04 <benzrf> now, there are a few basic types of expression involved here
08:01:17 <benzrf> there can be an application, or a lambda, or a constructor application, or a case
08:01:46 <benzrf> if the top-level expression is a lambda or a constructor application, we're done - we don't bother simplifying anything below those (for reasons that will become clear)
08:01:46 <dmwit> (...let?)
08:01:53 <benzrf> dmwit: whatever :P
08:02:01 <benzrf> so that just leaves cases and applications
08:02:44 <benzrf> if we have an application, we can immediately just substitute the right hand side into the left hand side - we'll need to recurse to do the substitution, of course, but once we've substituted we're done recursing, and we can move on to the next loop iteration
08:02:55 <benzrf> so simplifying a function application doesn't add anything to the stack!
08:03:20 <benzrf> note that lazy evaluation is important here - we don't need to first simplify the argument, we just substitute it in
08:03:46 <MagneticDuck> I'm listening
08:04:11 <benzrf> but if we have a case - we need to fully evaluate the thing we match on before we can simplify the case into one of its branches
08:04:27 <benzrf> so we recurse into simplifying the case scrutinee
08:04:40 <benzrf> therefore, we add stack layers precisely when we match on something
08:05:16 <benzrf> so if we have a function that's tail recursive, we get tail recursion completely for free, because we're just expanding the expression nodes
08:05:39 <benzrf> but if we have a function that recurses and then examines the result, we have to push a stack layer to focus on the thing it scrutinizes
08:06:41 <benzrf> also note that we're done simplifying as soon as we hit a constructor, so when we do the match, we substitute the unevaluated constructor arguments into the body, which is why this works:
08:06:59 <benzrf> > case undefined:undefined of [] -> 1; a:b -> 2
08:07:00 <lambdabot>  2
08:07:02 <benzrf> but this doesn't:
08:07:04 <benzrf> > case undefined of [] -> 1; a:b -> 2
08:07:06 <lambdabot>  *Exception: Prelude.undefined
08:07:36 <benzrf> (in practice ghc shuffles things around and optimizes in some ways that make it so you won't actually see a stack overflow from some things that would be stack overflows in what i'm describing)
08:08:15 <c_wraith> also recent version of ghc grow the stack when it fills.  It's hard to overflow the stack now.
08:08:45 <MagneticDuck> Hmm, alright
08:08:51 <benzrf> if you add primitive arithmetic operations on machine integers into the mix, you can just pretend that doing addition or whatever also forces a stack frame
08:09:13 <benzrf> because you similarly need to scrutinize the operands
08:09:30 <MagneticDuck> but I suppose there's one class of expression that you haven't mentioned yet, which is seq
08:09:34 <benzrf> aha!
08:09:59 <benzrf> seq forces a recursion that simplifies the left hand operand before just continuing normally on the right
08:10:06 <benzrf> which is useless in the model i'm describing, but
08:10:17 <benzrf> we can tweak it slightly:
08:10:46 <Cale> (seq is actually a little more subtle than that, but I'm not sure we want to get into it)
08:10:51 <benzrf> Cale: :P
08:11:03 <benzrf> MagneticDuck: when we simplify an application, instead of literally substituting the argument wholesale into the function body, substitute *the same single mutable reference* in each case
08:11:36 <benzrf> that way, if we have two cases on the argument in the function body, the first one will recurse and evaluate the argument, but then the second one will have a reference to the same, already-simplified subtree, and we save time
08:11:49 <benzrf> e.g.:
08:11:58 <Cale> It is semantically legal for the compiler to decide it wants to evaluate seq x y by first evaluating y, then evaluating x before the result of y is made available.
08:12:42 <benzrf> (\x -> case x of [] -> 1; a:b -> (case x of [] -> 1; -> c:d -> 3)) (expensive computation)
08:12:51 <benzrf> simplify this by substituting:
08:13:09 <benzrf> case (expensive computation) of [] -> 1; a:b -> (case (expensive computation) of [] -> 1; c:d -> 3)
08:13:25 <benzrf> now we need to recurse to simplify (expensive computation)
08:13:32 <benzrf> do that, and now our expression is simplified to:
08:13:54 <benzrf> case 'a':[] of [] -> 1; a:b -> (case (expensive computation) of [] -> 1; c:d -> 3)
08:14:05 <benzrf> _but_, if we instead share a reference to the same tree that gets simplified, we instead now have
08:14:12 <benzrf> case 'a':[] of [] -> 1; a:b -> (case 'a':[] of [] -> 1; c:d -> 3)
08:14:26 <benzrf> because mutating the tree to simplify it will show up to all references to that tree
08:14:40 <erisco> I just close my eyes and press "run"
08:14:42 <benzrf> this is "sharing", iirc
08:15:32 <MagneticDuck> I see
08:15:37 <benzrf> now, we can see that seq can be useful, because by forcing recursion and simplification of its left hand side, it may simplify a subtree that is shared with other parts of the expression
08:16:29 <benzrf> the reason we might care is - it may be much later in the expression that we end up actually caring about the result of that subtree, but
08:16:37 <benzrf> if we wait until then, the subtree will take up memroy
08:16:51 <benzrf> if we're recursing a lot, that can build up a huge expression tree before we end up simplifying each of the parts
08:17:04 <benzrf> we may be able to get much better memory usage by instead simplifying each node as it appears
08:17:05 <erisco> hindsight is 20/20 in computation
08:17:11 <benzrf> even though we only match on the result at the end
08:17:47 <benzrf> and *that* is why foldl' gets reasonable memory usage and foldr explodes
08:18:22 <benzrf> er, actually
08:18:22 <MagneticDuck> so, recalling:
08:18:24 <MagneticDuck> @src foldl
08:18:24 <lambdabot> foldl f z []     = z
08:18:24 <lambdabot> foldl f z (x:xs) = foldl f (f z x) xs
08:18:33 <benzrf> i would expect foldr to stack overflow, and foldl to memory explode
08:18:43 <benzrf> im guessing ghc is smart enough to make foldr memory explode instead of stack overflow
08:19:07 <erisco> does the runtime stack overflow? I haven't seen it
08:19:07 <benzrf> but in the naïve model i described, im pretty sure we'd see a stack overflow in foldr and a memory explosion in foldl
08:19:32 <benzrf> in foldr, we get:
08:19:45 <benzrf> > foldr f z [a, b, c] -- thanks dmwit
08:19:46 <lambdabot>  f a (f b (f c z))
08:20:58 <benzrf> so "foldr max 1 [1..product [1..20]]" will expand to "max 1 (...)", and then simplifying that forces a recursion (since max is a primitive operation on machine numbers)
08:21:09 <benzrf> then (...) expands to "max 2 (...)"
08:21:12 <benzrf> another recursion, etc
08:21:15 <benzrf> hence,
08:21:24 <benzrf> > foldr max 1 [1..product [1..20]]
08:21:26 <lambdabot>  *Exception: stack overflow
08:21:34 <benzrf> regular foldl will look like this:
08:21:34 <erisco> I guess so
08:22:39 <benzrf> "foldl max 1 (...)" expands to "foldl max (max 1 1) (...)" expands to "foldl max (max (max 1 1) 2) (...)"
08:22:52 <benzrf> at each stage, our root node is an application, so we expand the applicatoin
08:23:15 <benzrf> this builds up a massive subtree of maxes - we end up with an unforced max-list of the entire sequence of numbers
08:23:37 <benzrf> then at the end we finally have a max at our root, and THEN we can finally stack overflow
08:24:03 <benzrf> in foldl', we squish the max subtree at each stage before sticking it into the next subtree
08:24:07 <benzrf> @src foldl'
08:24:07 <lambdabot> foldl' f a []     = a
08:24:07 <lambdabot> foldl' f a (x:xs) = let a' = f a x in a' `seq` foldl' f a' xs
08:24:38 <MagneticDuck> yes I see
08:24:54 <benzrf> so not only do we not build a massive memory hogging tree, we also don't stack overflow
08:25:17 <MagneticDuck> I had realized before that seq helped prevent memory from getting hung up waiting for things to evaluate, but I didn't realize it worked because of "sharing"
08:25:28 <erisco> the short answer is if you evaluate it by hand, lazily, then you can just observe the size of the term to see what is happening wrt memory
08:25:38 <benzrf> :)
08:25:46 <MagneticDuck> which is a useful mental model for this
08:25:47 <erisco> take the first operand of seq to the first constructor
08:25:53 <c_wraith> MagneticDuck: if it forces evaluation of an unshared expression, it's not really doing much. :)
08:25:59 <benzrf> ghc really does work *kind* of like this
08:26:00 <MagneticDuck> I know, right
08:26:21 <benzrf> there really is a graph in memory that corresponds roughly to an expression being built up
08:27:05 <benzrf> but it's not a literal representation of an expression
08:27:34 <erisco> also, bang patterns evaluate strictly
08:27:48 <erisco> and tilde patterns not
08:28:12 <benzrf> each literal expression in the source code gets compiled to a routine in the code segment, and the correspondence in the data graph arises because of thunks
08:28:19 <erisco> inf ~(x:xs) = x : inf xs    -- my favourite little function I came up with
08:28:47 <benzrf> when there's a top-level application, that doesn't appear in the data at all - it's just the control flow jumping. applications only show up as a data structure when there's a thunk
08:29:10 <benzrf> (if that's too confusingly worded, don't worry - it doesn't matter too much for anything i said earlier)
08:29:27 <benzrf> erisco cute
08:30:37 <Akii> so I've a program that has an unusual high amount of CPU usage while idling; how can I inspect that it's doing?
08:31:03 <erisco> it is cons all the way down
08:36:26 <MagneticDuck> benzrf: I think I've absorbed something, thanks
08:36:48 <MagneticDuck> now I'm trying to apply this sharing nonsense to my own program
08:37:32 <MagneticDuck> https://hastebin.com/ebebaxoles.hs
08:38:12 <MagneticDuck> even for basic Computations, `execute` seems to memory explode. I'll try to replicate on ideone
08:38:20 <benzrf> :)
08:38:58 <benzrf> MagneticDuck: wait, you can always go from s to r?
08:39:20 <MagneticDuck> it's a bit of a funny model, but yes
08:39:32 <benzrf> odd
08:39:34 <benzrf> alright!
08:40:10 <erisco> sharing is critical for polynomial general CFG parsing
08:40:14 <benzrf> MagneticDuck: fyi, you can say "Just !r" instead of manually seq'ing, if you have BangPatterns on
08:40:34 <MagneticDuck> benzrf: it would probably be better to have the transition function s -> Either s r
08:40:43 <MagneticDuck> but this is besides the point
08:40:49 <MagneticDuck> benzrf: mm
08:40:52 <benzrf> MagneticDuck: also, try being strict in n
08:41:17 <MagneticDuck> benzrf: n is used for printing progress
08:41:43 <erisco> and you can see it in something as simple as fib, though realistically you use a different approach
08:41:45 <MagneticDuck> although it wouldn't hurt
08:42:17 <erisco> but you can use sharing to improve the exponential solution to linear
08:42:33 <MagneticDuck> eh well, thanks for the help, I'll get around to this later
08:42:49 <benzrf> MagneticDuck: oh wait i had an idea
08:42:54 <benzrf> what kind of Computations are you doing here?
08:43:11 <benzrf> remember, evaluation generally only goes up to the first constructor
08:43:25 <benzrf> if you have nested data for s, then seq there is not going to do anything to the nested stuff
08:43:43 <benzrf> you may be building a giant tree down there
08:43:54 <benzrf> e.g., if it's a list of ints
08:44:09 <benzrf> or even a tuple of ints!
08:44:19 <benzrf> seq is only gonna force it up to the (,) constructor
08:44:59 <benzrf> you could use deepseq or just make the runComp also strict
08:45:20 <benzrf> or make strict fields in your s
08:45:46 <benzrf> (if you mark a field strict, evaluation will descend into it when the constructor is forced)
08:52:00 <Philonous> Does someone have a function lying around that creates curl commands from http-types Request-s? 
08:52:27 <Philonous> (curl commands as in strings you can copy/paste)
09:39:32 <vise890> hi all. How can I implement a typeclass for a Type defined in my Types.hs in a separate file without running into a ciclic dependency?
09:43:08 <nojilo> Quick question, is there a way to always hide a certain function in a stack project, example when I type  `import Data.Aeson` it automagically hides ((.=)) ?
09:43:27 <nojilo> Would I need to create new modules and reexport stuff?
09:43:40 <kadoban> nojilo: You'd have to make your own module, yeah.
09:43:54 <kadoban> Or edit the library I guess, though that seems like a mistake.
09:44:24 <nojilo> What a pity, thought that might be a common problem and there's a way.
09:46:37 <geekosaur> vise890, painfully. https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/separate_compilation.html#mutual-recursion
09:48:15 <vise890> oh boy geekosaur, i think i'll stick with an endless Types.hs then.. thanks anyway
10:24:15 <ThreeFx> How would I transform one tree-like structure into another tree-like structure idiomatically using Lens?
10:31:11 <barcabouna> hey guys you know how the head function gets the first element of a list without pattern matching?
10:31:23 <barcabouna> is there something similar for Either? like left 
10:32:33 <cocreature> there is "fromLeft" in various packages but it suffers from the same problem that "head" does, i.e., it crashes if the input is not Left
10:33:06 <Tuplanolla> You need lenses in your life, barcabouna.
10:33:13 <barcabouna> cocreature: im cool with that
10:33:20 <barcabouna> i just need this for glue stuff
10:33:23 <barcabouna> like quick script
10:33:30 <barcabouna> if it crashes it's actually gud
10:33:37 <barcabouna> Tuplanolla: what are lenses?
10:33:45 <barcabouna> you need jeeesus in yo life
10:34:43 <barcabouna> also is there a simmetry function, like \x -> x
10:34:52 <cocreature> barcabouna: if you’re not already depending on a package that contains fromLeft, just define it yourself. I don’t think it’s worth the cost of adding a dependency
10:34:56 <cocreature> :t id
10:34:57 <lambdabot> a -> a
10:35:02 <cocreature> that is \x -> x
10:35:02 <barcabouna> loooks like either works
10:35:14 <barcabouna> either (\x->x) (\_->err "") e
10:35:21 <Tuplanolla> :t preview _Right :: Either a b -> Maybe b
10:35:22 <lambdabot> Either a b -> Maybe b
10:35:24 <barcabouna> kinda uglier than "left e" but ok
10:35:53 <cocreature> just define "fromLeft = either id (\_-> err "expected left")"
10:36:03 <barcabouna> either id error e
10:36:05 <barcabouna> works well!
10:37:16 <cocreature> it’s still almost always a bad idea to just let your program crash instead of handling the error somehow
10:38:03 <barcabouna> what about a maybe?
10:38:12 <barcabouna> cocreature: i handle it with a crash
10:38:21 <barcabouna> what am i to do with a value i don't need?
10:38:30 <barcabouna> a value i actuallye perceive as an error
10:38:30 <cocreature> well then at least provide a better error message
10:38:45 <barcabouna> you can carry around that error but in the end you're gonna have to crash
10:39:07 <barcabouna> either id (err "base64 error") e
10:39:20 <barcabouna> maybe err id m 
10:39:22 <barcabouna> doesn't work
10:39:35 <barcabouna> how can i do something for maybe?
10:46:00 <pavonia> barcabouna: What exactly are you trying to do with maybe?
10:47:33 <barcabouna> pavonia: i can't remember but i did find it somewhere
10:47:41 <barcabouna> im just future proofing here
10:48:00 <barcabouna> i wanna use haskell like a scripting language. and then if i wanna extend the script i'll add less exceptions
10:48:09 <barcabouna> so to me either a b e is perfect
10:48:14 <barcabouna> i can use it like i want
10:48:28 <barcabouna> maybe is also good in case i don't want to crash
10:48:46 <barcabouna> but if i do want to crash? is there something that goes from Maybe to Either so i can use either?
10:49:27 <sproingie> possibly you want ExceptT so as to generalize this pattern
10:50:13 <cocreature> :t maybe Right (Left ())
10:50:14 <lambdabot> error:
10:50:14 <lambdabot>     • Couldn't match expected type ‘a1 -> b -> Either a b’
10:50:14 <lambdabot>                   with actual type ‘Either () b0’
10:50:27 <cocreature> :t maybe (Left ()) Right
10:50:28 <lambdabot> Maybe b -> Either () b
10:50:29 <sproingie> @hoogle Maybe a -> Either b a
10:50:30 <lambdabot> Data.Either.Combinators leftToMaybe :: Either a b -> Maybe a
10:50:30 <lambdabot> Agda.Utils.Either maybeLeft :: Either a b -> Maybe a
10:50:30 <lambdabot> Music.Theory.Either fromLeft :: Either a b -> Maybe a
10:51:54 <sproingie> probably better to start with ExceptT and do explicit conversions only if you end up needing it
10:51:58 <Tuplanolla> Now my monad morphism sense tingles.
10:53:23 <ondrejs> Hello. I have `f x = x & y .~ ((x ^. y) + 1)` (where y is an attribute of the record x) and I want to make it a bit more abstract `f x lens = x & lens .~ ((x ^. lens) + 1)` but this gives me `Expected type: Getting b s b; Actual type: ASetter s b1 b b; In the second argument of ‘(^.)’, namely ‘lens’`. How can I make this work? Thank you
10:54:37 <lyxia> ondrejs: add a type signature to f
10:55:03 <lyxia> f :: X -> Lens' X Int -> X
10:55:11 <barcabouna> :t ExceptT
10:55:13 <lambdabot> m (Either e a) -> ExceptT e m a
10:55:56 <lyxia> ondrejs: lenses are polymorphic functions, and the compiler will not infer types for polymorphic arguments
10:56:18 <barcabouna> actually doing `either id error (maybe Left Right m)` is something i can put up with
10:56:31 <Tuplanolla> :t hoist -- Monad morphisms have signatures suitable for converting, say, between `Maybe` and `Either e`.
10:56:32 <lambdabot> (Monad m, MFunctor t) => (forall a. m a -> n a) -> t m b -> t n b
10:57:24 <ondrejs> lyxia: nice, thank you very much
11:04:46 <rblaze> is there a good idiom to split Just (a, b) to (Just a, Just b)?
11:05:59 <lyxia> rblaze: pattern matching is the best
11:06:52 <ezyang> > (\x -> (fmap fst x, fmap snd x)) (Just (3,4)) 
11:06:54 <lambdabot>  (Just 3,Just 4)
11:07:07 <ezyang> you can golf it more if you want :o) 
11:07:25 <ezyang> > (fmap fst `fmap` fmap snd) (Just (3,4)) 
11:07:27 <lambdabot>  error:
11:07:27 <lambdabot>      • Could not deduce (Num (b1, b0))
11:07:27 <lambdabot>        from the context: Num (b1, b)
11:07:31 <ezyang> foo 
11:07:49 <ezyang> > (liftM2 (,) (fmap fst) (fmap snd)) (Just (3,4)) 
11:07:50 <lambdabot>  (Just 3,Just 4)
11:07:52 <cocreature> :t fmap fst &&& fmap snd
11:07:53 <lambdabot> Functor f => f (b1, b) -> (f b1, f b)
11:08:38 <Tuplanolla> It looks like something `bisequence` could handle.
11:08:51 <blackdog> rblaze: bear in mind that you are strictly losing information there
11:08:57 <cocreature> Tuplanolla: yeah I’ve thought about that too but I can’t quite work out how it works
11:10:13 <Philonous> blackdog, How so? That transformation is reversible 
11:10:59 <blackdog> Philonous: that particular one, but once you have a (Maybe a, Maybe b), you don't statically know they're both either Nothings or Justs.
11:11:32 <ahihi> > ((&&&) `on` fmap) fst snd $ Just (1, 2)
11:11:34 <lambdabot>  (Just 1,Just 2)
11:11:43 <blackdog> so now if you want to be total you have to take into account cases that can't happen.
11:12:00 <cocreature> ahihi: that’s longer than the version without on :)
11:12:06 <lyxia> Philonous: (1+ab) /= (1+a)(1+b)
11:12:33 <ahihi> cocreature: but less repetitive!
11:12:47 <cocreature> ahihi: and less readable :P
11:14:44 <ReinH> :t (+~)
11:14:45 <lambdabot> Num a => ASetter s t a a -> a -> s -> t
11:15:00 <Frenchiie> can anyone refer me to a haskell book/website that isn't overly wordy to start with?
11:15:04 <ReinH> :t \f x -> f & x +~ 1
11:15:05 <lambdabot> Num a => s -> ASetter s b a a -> b
11:15:18 <ReinH> ondrejs: why not use that?
11:15:27 <Frenchiie> also is learnhaskell kept updated? http://learnyouahaskell.com/
11:15:50 <ReinH> That's short and clear enough that hardly needs a new name.
11:15:52 <cocreature> no it isn’t
11:15:56 <cocreature> but most of it is still up2date
11:17:27 <ReinH> Frenchiie: here are some good free resources https://github.com/bitemyapp/learnhaskell
11:17:40 <ReinH> It recommends some lecture notes, which tend to not be very wordy.
11:17:43 <Frenchiie> cocreature: k thanks
11:18:25 <ondrejs> ReinH: it was just an example, the function is more complicated than + and I its result is wrapped in a functor so I need to fmap the (... .~) over the result
11:18:45 <ReinH> ondrejs: why can't you just use %~?
11:19:23 <ReinH> x & y .~ (f (x ^. y)) = x & y %~ f
11:19:36 <ReinH> and I don't see any fmap in your original
11:20:56 <ReinH> Lens has the %~ operator for modify, you don't need to set (f . get)
11:21:06 <Frenchiie> ReinH: thanks!
11:21:18 <ondrejs> ReinH: it's not there, I simplified, my current code is (\x -> st & lens .~ x) <$> (handle event (st ^. lens))
11:22:24 <ondrejs> ReinH: I know %~ but not how to make it mapped over the `handle` function. I am reading about mapped know, but I am not yet sure if it will help me with that
11:22:26 <ReinH> There has to be a more lensy way to do that, but I'm not fluent in lens
11:22:52 <ReinH> glguy: paging glguy
11:23:10 <ondrejs> ReinH: yes, that's exactly my situation :-)
11:23:16 <ReinH> any time you use .~ to set the result of using ^., I have to assume you're doing it wrong. ;)
11:23:28 <ReinH> ondrejs: yes, I understand now :)
11:25:22 <ReinH> ondrejs: the danger with simplifying is that sometimes you omit necessary complexity. :)
11:25:37 <lyxia> ondrejs: lens (handle event)
11:27:10 <ReinH> honestly, complex lens usage is where I really wish Haskell had a good hole-driven development story.
11:27:32 <ReinH> If I give GHC enough help, it should be able to solve these things for me. It's just wiring together categorical stuff.
11:28:10 <lyxia> The operator is called (%%~) in lens, lens %%~ handle event, though it's just the identity function
11:28:41 <cocreature> (%%~) is great in that it has a lot of docs and then it tells you that it’s just id
11:28:46 <ReinH> heh
11:29:03 <ReinH> I bet GHC could figure out id.
11:29:07 <ReinH> is what I'm saying
11:33:57 <ondrejs> lyxia: `lens (handle event)` doesn't make sense to me (nor typechecks)
11:33:59 <glguy> ReinH: what's up?
11:34:09 <ReinH> glguy: I wanted your lens expertise.
11:34:23 <ReinH> Well, I wanted to point it at ondrejs's problem.
11:34:50 <ReinH> Think of this like a bat signal but for lens
11:35:39 <glguy> looks like lyxia got it
11:36:35 <glguy> to be more explicit, traverseOf can be used
11:37:36 <Tuplanolla> @let (>×<) = lensBatSignal
11:37:38 <lambdabot>  Defined.
11:39:39 <lyxia> ondrejs: what did you write around it and what is the type error
11:40:28 <ondrejs> lyxia: just fixed it now, did some silly mistake. Works like a charm now
11:41:27 <lyxia> ondrejs: lens %%~ handle event   is probably more idiomatic
11:41:44 <ondrejs> ReinH, lyxia, glguy: Thank you all, it really helped me
11:42:05 <ondrejs> lyxia: doesn't seem to be in microlense
11:43:53 <blackdog> ReinH: i do wonder why it's hard to say "this hole requires typeclasses X , Y and Z"
11:43:55 <lyxia> ondrejs: there's traverseOf which is the same with letters
11:44:25 <ondrejs> lyxia: I'm just looking at the funny implementation https://hackage.haskell.org/package/microlens-0.4.8.0/docs/src/Lens-Micro.html#traverseOf :-)
11:44:34 <glguy> The named operations are nice as you can work out what they do from knowledge of the non '-Of' suffixed version
11:45:10 <glguy> ondrejs: If you're using alternative lens packages, it's useful to mention that in your questions since those packages change some definitions and omit others
11:45:17 <glguy> (in this case it worked out)
11:49:15 <ondrejs> lyxia, glguy: do you know about some explanation for dummies why just `lens (handle event) state` works (given `handle event` returns something in a functor) or do I have to read some detailed lens tutorial from the ground up?
11:51:58 <ReinH> ondrejs: what is the type of handle event?
11:53:58 <ondrejs> ReinH: handle :: Event -> List -> SomeMonad, but never mind I just realized the type of lenses is just general enough to work with any functor
11:55:13 <Sh4rPEYE> Hey. I'm going through the chapter 25 in the Haskell Book and there is an exercise: Write an Applicative instance for a Compose newtype (which is Compose { getCompose :: f (g a) }), both f and g are Functors.
11:55:46 <Sh4rPEYE> I'm stuck right at pure. How can I write pure :: a -> Compose f g a, without adding any infomration?
11:55:59 <ondrejs> ReinH: and the reason they work with "ordinary stuff" is that (Const * r) is a functor... I'm having lens enlightenment day 
11:56:22 <Sh4rPEYE> It seems like I should choose some default functors f and g to use here... But that has to be wrong
11:56:32 <ReinH> Sh4rPEYE: what do you know about f and g?
11:56:54 <sproingie> pure is composition with identity
11:57:04 <monochrom> I am pretty sure you need to assume Applicative f and Applicative g.
11:57:14 <ReinH> That's why I'm asking Sh4rPEYE 
11:57:27 <Sh4rPEYE> Oh sorry, right. They're both applicatives as well
11:57:48 <ReinH> Which means you can use pure and (<*>) on both
11:57:53 <Sh4rPEYE> Oh, so I can go with their pure
11:57:59 <ReinH> Yes.
11:58:14 <Sh4rPEYE> Sure. Should've noticed before asking here... Thanks
11:58:20 <Unicorn_Princess> I have a few questions about https://github.com/data61/fp-course 1) in the Optional.hs module, am I supposed to only use the monad/applicative instances and helper functions, or can I pattern-match on the ADT constructors too?
11:58:24 <ReinH> :t pure . pure
11:58:25 <lambdabot> (Applicative f, Applicative f1) => a -> f1 (f a)
11:59:08 <Unicorn_Princess> 2) I notice all the tests and QuickCheck properties are commented out, but they run anyway when I run tests - what's up with that?
12:00:34 <ReinH> Unicorn_Princess: Yes, you can pattern match. And the tests use doctests, which parses comments for tests and then runs those tests.
12:01:27 <ReinH> Unicorn_Princess: The instances are written in terms of the functions you define earlier.
12:01:32 <ReinH> You can't use them to define those functions.
12:02:23 <ReinH> The goal of the exercise is to implement the instances by pattern matching.
12:02:46 <Unicorn_Princess> ReinH: thanks. tho in the case of Optional, the instances are already implemented :)
12:03:03 <ReinH> The instances are at the bottom.
12:03:10 <ReinH> They are implemented in terms of the functions at the top.
12:03:16 <ReinH> Your task is to write those functions.
12:03:43 <ReinH> If you don't write those functions, the instances will not work.
12:08:54 <Unicorn_Princess> hm. so I ran the tests successfully at first (they all failed, but they did run), but now doing 'cabal test' just gives the message that the tests are running, and spawns a ghc instance that takes up a full cpu core, but nothing happens
12:09:09 <ReinH> How did you implement the functions?
12:09:28 <ReinH> Did you try to use the instance methods?
12:10:04 <Unicorn_Princess> ReainH: I used the methods, and the tests passed. I also pattern matched them, yesterday, and they also passed
12:11:33 <ReinH> I thought you said the tests took up a full cpu core and nothing happens?
12:11:48 <ReinH> Do you understand that the instance methods are implemented using the functions?
12:12:02 <ReinH> And that if you try to implement the functions using the instance methods, you create an infinite loop?
12:12:16 <ReinH> > let x = x in x
12:12:20 <ReinH> You are doing exactly this.
12:12:22 <lambdabot>  mueval-core: Time limit exceeded
12:12:29 <Unicorn_Princess> oh, yes, you're right ^^
12:12:50 <ReinH> Yeah, that's why I've said it 4 times now.
12:12:57 <Unicorn_Princess> but then.. why did they pass the 1st time I did that...
12:13:07 <ReinH> I don't believe they did/
12:13:11 <ReinH> Since that is impossible.
12:13:39 <ReinH> I think it's more likely that you are misremembering, since that is possible.
12:13:42 <Unicorn_Princess> hm. I think maybe I didn't rebuild the solution
12:13:48 <Sh4rPEYE> ReinH: Any idea how to write `ap` for Compose? It is something like this f <*> (<*> a), but that obviously doesn't work. I somehow have to chain up two aps, but I have no clue about how to do that.
12:13:55 <Unicorn_Princess> anyway thanks a bunch
12:14:30 <shapr> debugging is a conversation between what I believe is true, and what is actually true.
12:14:49 <Sh4rPEYE> It is defined like this: (Compose f) <*> (Compose a) = undefined
12:15:15 <ReinH> shapr: indeed
12:15:30 <ReinH> Sh4rPEYE: Well, what do we know? We know the result needs to be wrapped back up in Compose.
12:15:34 <Sh4rPEYE> I know I have to somehow "get" the function from under two structures
12:15:47 <ReinH> So we have Compose f <*> Compose x = Compose ???
12:15:52 <Sh4rPEYE> Yes
12:16:00 <ReinH> We also have an f and an x, and the knowledge that f and g are Applicative.
12:16:05 <ReinH> So what can we do with f and x?
12:16:42 <ReinH> (f the type constructor is applicative, not f the value, of course)
12:16:44 <Sh4rPEYE> I somehow want to apply f to x two times, because f :: f (g (a -> b)) and x f ( g a)
12:17:15 <Sh4rPEYE> We can apply it, both f and g the type constructors, as they're both Applicative
12:19:28 <ReinH> So we want f <*> x, but we need to lift it by one level.
12:19:37 <ReinH> Yes?
12:20:14 <Sh4rPEYE> Lift what? Probably yes, asking just to be sure
12:20:43 <ReinH> :t (<*>)
12:20:44 <lambdabot> Applicative f => f (a -> b) -> f a -> f b
12:20:49 <Sh4rPEYE> Oh, yes
12:21:14 <Sh4rPEYE> We need f (g (a -> b)) ...
12:21:18 <ReinH> But we want (Applicative f, Applicative g) => f (g (a -> b)) -> f (g a) -> f (g b)
12:21:42 <ReinH> So we want to apply <*> under f, or iow lift <*> to apply to f's of g's
12:22:47 <Sh4rPEYE> Compose $ (<*>) <$> f <*> a
12:22:52 <Sh4rPEYE> :t Compose $ (<*>) <$> f <*> a
12:22:54 <lambdabot> error:
12:22:54 <lambdabot>     • Data constructor not in scope: Compose :: f0 (f1 b0) -> t
12:22:54 <lambdabot>     • Perhaps you meant variable ‘icompose’ (imported from Control.Lens)
12:23:03 <Sh4rPEYE> :t (<*>) <$> f <*> a
12:23:04 <lambdabot> error:
12:23:04 <lambdabot>     • Couldn't match expected type ‘f1 (f a0)’ with actual type ‘Expr’
12:23:04 <lambdabot>     • In the second argument of ‘(<*>)’, namely ‘a’
12:23:31 <Sh4rPEYE> Well, anyway, that should be it
12:24:05 <ReinH> Correct.
12:24:47 <Sh4rPEYE> I would have never though of that myself.
12:25:56 <ReinH> :t liftA2 (<*>)
12:25:57 <lambdabot> (Applicative f, Applicative f1) => f1 (f (a -> b)) -> f1 (f a) -> f1 (f b)
12:26:44 <ReinH> You know f :: f g (a -> b) and x : f (g a)
12:27:19 <ReinH> You have (<*>) :: g (a -> b) -> g a -> g b and you need to apply it under f.
12:27:48 <Sh4rPEYE> I know, I just... Didn't look at it in the right way. Now it seems trivial
12:28:00 <ReinH> It helps to write all the types out
12:29:11 <Sh4rPEYE> I haven't yet transitioned fully into the Haskell functional mindset
12:29:21 <ReinH> Well, that's why you're doing these exercises
12:29:37 <Sh4rPEYE> (yet I'm at chapter 25 now; maybe I'm doing something wrong)
12:30:11 <ReinH> It might help or hurt to look at it as a purely algebraic exercise of pushing symbols around.
12:30:53 <Sh4rPEYE> I do that sometimes already; it helps for writing binds tremendously
12:31:31 <ReinH> It is, in fact, a purely categorical exercise of making arrows line up. At least when the types fully specify the behavior.
12:31:48 <Sh4rPEYE> (though sometimes I catch myself thinking in terms of oversimplified metaphors... Like "I get that f from the box, give it its a, and put the b I get back into the old box")
12:32:26 <ReinH> Discarding those metaphors and looking at it as a game of symbol pushing helped me a lot.
12:32:38 <ReinH> The metaphors helped get the that point though.
12:32:48 <Sh4rPEYE> ReinH: Category Theory is on my to-read-about list right after this Book.
12:32:52 <ReinH> something something Wittgenstein's Ladder.
12:33:17 <ReinH> Category Theory is nothing more than the algebra of functions (or, rather, function-like things)
12:33:29 <ReinH> So I'm just specifying which algebraic symbol pushing game you're playing
12:33:44 <Sh4rPEYE> ReinH: Sounds pretty cool. 
12:34:41 <Sh4rPEYE> I'm sure each and every different function-ey thingy has its own souldcrushing name, though
12:34:53 <Sh4rPEYE> soulcrushing*
12:35:41 <ReinH> Category theory is a game played with id and (.) and the rules ensure that they behave the way you expect. It just lets you play that game with things other than functions.
12:36:06 <geekosaur> Sh4rPEYE, just wait, CT does the opposite
12:36:09 <Sh4rPEYE> I thought of doing some little side-project before diving into Category Theory, something simple like maze generator, or web scraper... Haskell Book will be enough for that sort of thing, right (plus a little learning here and there on the way)?
12:36:29 <geekosaur> things you never would think were related get called categories and talked about with the same category terms :)
12:36:31 <ReinH> Yes, the CT vocabulary is an embarrassment of riches.
12:36:35 <Unicorn_Princess> in the data61 course, if I run 'cabal test', it runs all the tests and they perform as expected. but if I run doctest -isrc -Wall -fno-warn-type-defaults src/Course/List.hs, it complains it can't find module Test.QuickCheck. I -can- load quickcheck from inside ghci though
12:36:41 <cocreature> Sh4rPEYE: sure, that should definitely be doable
12:37:07 <Unicorn_Princess> (that doctest command is what the data61 course recommends for testing a single module)
12:37:45 <cocreature> Unicorn_Princess: can you load it from plain ghci or only from "cabal repl"?
12:38:05 <Sh4rPEYE> I'll probably use some library to show the generated maze afterwards. Is there anything simple that would work? With Haskell I've always used only console.
12:38:08 <Unicorn_Princess> cocreature: I only tried from plain ghci. hm, hold on, I have an idea
12:38:26 <cocreature> Unicorn_Princess: hm, try "cabal exec -- doctest …" anyway
12:38:41 <cocreature> that would have made more sense if plain ghci didn’t work but it’s worth a shot either way
12:39:50 <ReinH> Sh4rPEYE: A friend of mine wrote a book on maze generation, fwiw http://www.jamisbuck.org/mazes/
12:40:20 <Unicorn_Princess> this course is a bit wonky in general - its cabal file refers to a Course/Id.hs, which no longer exists, but Course/ExactlyOne.hs isn't in the cabal file. so I had to remove one and add the other before it would compile :/
12:40:21 <cocreature> huh I didn’t know maze generation is something you can write hole books about
12:40:23 <ReinH> Lots of stuff is available for free on the blog. It's in Ruby but would be fun to port to Haskell, especially since you'll need to replace algorithms that use mutation with ones that use state or similar.
12:42:10 <ReinH> cocreature: I'm writing an entire talk on rotating a matrix by 90 degrees so.
12:42:24 <cocreature> ReinH: heh
12:42:49 <ReinH> It's a nice excuse to talk about a variety of CS topics, from group theory to Morton ordering.
12:43:15 <ReinH> And Okasaki's inverted quadtrees.
12:43:30 * cocreature starts singing the finite simple group song
12:43:33 <ReinH> And linear algebra. Turns out there are a lot of ways to rotate a matrix.
12:43:39 <ReinH> What is the finite simple group song?
12:43:46 <cocreature> https://www.youtube.com/watch?v=BipvGD-LCjU
12:43:54 <erisco> I am cultured!
12:43:59 <ReinH> oh yeah I remember this!
12:44:12 <cocreature> sry that was the first thing that came to my mind when you mentioned group theory :)
12:45:01 <erisco> generating puzzles I am actually finding to be a worthy challenge
12:45:23 <erisco> mazes would be right up that alley
12:46:07 <erisco> you need the puzzle to be satisfiable but generally with just one solution
12:46:12 <geekosaur> maze generation drove a fair bit of math development iirc
12:46:13 <erisco> and then you want it to be *interesting*
12:46:23 <monochrom> erisco: Was that an intended pun? :)
12:46:41 <orion> Anyone know a good library to use for displaying a command prompt?
12:46:42 <erisco> uh, yeah, totally on purpose
12:46:47 * erisco shifts his eyes
12:47:02 <ReinH> orion: haskeline
12:47:17 <monochrom> Yeah, ghci itself uses haskeline :)
12:48:24 <erisco> I found a similar challenge when I briefly did some game/graphics programming
12:48:46 <erisco> wrt "interesting" ... turning some algorithms into something visually interesting
12:49:23 <erisco> you know, like plumes of fire and smoke thrusted from a rocketship
12:49:31 <erisco> that is wonderfully sophisticated
12:50:15 <erisco> or just a few sprites you randomise and move about a bit, heh, but it is interesting to relate programs to visual perception
12:50:25 <erisco> or just the concept of "interesting" in the human sense
12:51:44 <orion> ReinH: Thanks.
12:54:55 <Sh4rPEYE> ReinH: That webpage is the one that inspired me to implement mazes in Haskell!
12:55:04 <erisco> I wonder if you could teach an AI to identify the algorithm used to generate a maze
12:56:07 <EvanR> erisco: i was looking at math to "accurately" render small letters, to get the most accurate render at low rez. Then i thought, most accurate doesnt tell you if its legible. Can you build an AI / algorithmic thing to find the most legible low rez rendering
12:56:37 <erisco> heh
12:56:51 <geekosaur> most legible to who?
12:56:56 <erisco> I suspect the proprietary algorithms actually know the pixel geometry of the panels
12:57:02 <geekosaur> (cf. the serif vs. sans battle)
12:57:09 <EvanR> right, each person may have different needs
12:57:14 <erisco> i.e. sub-pixel rendering is critical
12:57:15 <ReinH> Sh4rPEYE: heh :)
12:57:32 <erisco> either that or you use a bitmap font that is pixel aligned
12:57:52 <EvanR> but do proprietary algorithms know how messed up your eyes are from years of abuse on competitors text rendering system :)
12:58:05 <EvanR> so can compensate
12:58:50 <pikajude> why would they
12:58:50 <erisco> like Apple's text rendering compatibility mode for Windows users?
12:59:08 <EvanR> solution... read books
12:59:16 <erisco> like a Kindle? :P
12:59:21 <pikajude> yes
12:59:36 <erisco> at 300dpi you can't dell the difference, some such
12:59:44 <EvanR> maybe high rez displays render this line of thought moot
12:59:52 <erisco> anyways, I have a Kindle and it looks like paper to me
13:00:07 <erisco> the retina displays are also remarkably sharp
13:00:14 <pikajude> they're gorgeous
13:00:41 <erisco> you can also compensate by enlarging the text and sitting an extra couple feet back :P
13:00:57 <geekosaur> and they have full-color pixels in the works so subpixel rendering won't be a thing any more
13:01:04 <EvanR> this renders so much of GPU tech useless
13:01:24 <erisco> maybe they can sell you a pair of optics so you can keep your chair in the same spot
13:01:40 <bollu> @msg quchen yep, text to pull in prettyprint for colored error messages :)
13:01:40 <lambdabot> Not enough privileges
13:01:49 <bollu> @tell quchen yep, text to pull in prettyprint for colored error messages :)
13:01:49 <lambdabot> Consider it noted.
13:02:13 <EvanR> like maybe solid state drives make database tech useless
13:02:18 <erisco> EvanR, such as? certainly anti-aliasing becomes unimportant
13:02:37 <EvanR> theres a tower of things going on under the umbrella of antialiasing yeah
13:02:54 <erisco> not sure how much of that is specifically supported by the pipeline
13:03:30 <EvanR> there have been extensions over the years dedicated to it, if its not hardwired its still in microcode and software
13:03:42 <EvanR> all the way to font rendering software
13:04:33 <EvanR> it makes you wonder whats really important in the software world!
13:04:41 <EvanR> dirty rectangle stuff? obsolete, who cares
13:04:54 <erisco> 120Hz and g-sync, duh
13:04:55 <EvanR> shoulda done something else with my youth!
13:05:01 <erisco> oh, and 1ms latency
13:05:36 <erisco> I run a 4ms latency monitor because it is also IPS... a trade-off
13:06:13 <erisco> actually hadn't used one before and so I wanted to see what the deal was
13:06:16 <EvanR> anyway, AI to produce answers that are not just in some sense right but adapted to the users human cruddiness
13:06:19 <dolio> erisco: Apparently there's all kinds of work put into custom glyphs for low point sizes that don't look anything like the actual glyphs, but are mangled by font rendering engines into something that looks good for the limited number of pixels.
13:06:20 <zachk> erisco, IPS?
13:06:31 <dolio> But if you have a ton of pixels per inch, that no longer matters.
13:06:39 <geekosaur> SSDs don't make database tech useless; that to my mind falls under the same heading as '640k ought to be enough'
13:06:42 <erisco> zachk, yeah, opposed to TN
13:06:55 <zachk> TN?
13:07:08 <erisco> they're different technologies used in LCDs
13:07:25 <Welkin> 64k*
13:07:25 <EvanR> ah
13:07:42 <erisco> TN is cheaper and has less latency but IPS has better colour and better viewing angle
13:07:42 <EvanR> like you still want old database tech for the 1 petabytes of disk that you now have to augment your 1 terabyte SSD
13:08:12 <geekosaur> give it a couple years and they'll be using the database tech on the SSD and demanding something faster :)
13:08:43 <erisco> they actually need to push densities higher now for VR
13:08:59 <erisco> or figure out how to strap a larger screen onto your head
13:09:14 <Welkin> VR is too stupid
13:09:23 <erisco> nuh uh, you're stoopid
13:09:27 <Welkin> why not bypass your eyes entirely?
13:09:45 <EvanR> old man Welkin tells VR-wear kids to get off his lawn!
13:09:45 <Welkin> dreams are the future of "virtual" reality
13:09:51 <EvanR> wearing
13:09:52 <erisco> good point! I'll go talk to Dr. Bones right away
13:10:46 <EvanR> future? whos been popping pills for * years
13:11:36 <erisco> what I want to see is the continuous display
13:11:43 <erisco> direct vector rendering
13:12:13 <EvanR> my monitor is a smooth function from the real plane to sRGB color space
13:12:37 <EvanR> paid extra for the differentiability
13:13:19 <EvanR> dont say no one can notice
13:13:30 <Unicorn_Princess> again the data61 course: I noticed it gave a warning that ~/.cabal/bin wasn't in the PATH when I ran 'cabal install cabal-install'. thinking it might be the cause of how 'doctest ...' couldn't find QuickCheck, I added it to the PATH, logged out and back in, did cabal clean and re-ran all the build commands, and the warning was gone. but now 'cabal test' just gives a "course-0.1.4: installed package info from too old version of Cabal (
13:14:24 <erisco> is it worth it? been thinking of upgrading my Weierstrass
13:14:53 <Unicorn_Princess> *cabal build gives the error already, not cabal test
13:15:26 <sm> Unicorn_Princess: sounds like you're using a much newer cabal-install now, maybe "cabal clean" in the project will help
13:15:50 <Unicorn_Princess> sm: did that already, trying it again as we type
13:16:16 <sm> maybe ghc-pkg unregister course 
13:16:43 <Unicorn_Princess> what does that do? which parts are the command and which a description? :)
13:17:54 <sm> "ghc-pkg unregister course" <- ghc-pkg is a lower-level tool used by cabal, this would uninstall the course package forcing it to be reinstalled by cabal. If cabal clean doesn't help.
13:18:47 <sm> and if you're using a cabal sandbox (PROJECTDIR/.cabal-sandbox/ exists) it's a bit more complicated
13:19:32 <Unicorn_Princess> no need for the ghc-pkg thankfully - running cabal clean for the -second- time, fixed it :)
13:21:36 <Unicorn_Princess> 'doctest ...' and 'cabal exec -- doctest ...' both still fail to find Test.QuickCheck though :(
13:24:29 <sm> can you share the .cabal file ?
13:24:30 <cocreature> Unicorn_Princess: silly question but have you tried "cabal install QuickCheck"?
13:24:56 <sm> sometimes they have a test flag, and you need to cabal install -ftest --only-dep to get the libs for tests
13:26:26 <Unicorn_Princess> cocreature: just did, didn't help :(
13:27:37 <Unicorn_Princess> sm: the cabal file in question: https://github.com/data61/fp-course/blob/master/course.cabal (only Course.Id is commented out, and Course.ExactlyOne is added to that list)
13:28:36 <Unicorn_Princess> ok, so, since 'cabal test' seems to work, why don't I instead make that run only the tests I care about. but how?
13:30:29 <Unicorn_Princess> hm, 'seems to work' is correct - I only get errors from the List.hs module, but I -should- be getting errors from all the modules, no?
13:35:04 <sm> sorry, what's the problem again ?
13:35:57 <erisco> when you get too into writing code...
13:36:45 <Unicorn_Princess> 'cabal test' seems to only run the tests for List.hs, and 'doctest -isrc -Wall -fno-warn-type-defaults <filename.hs>' can't load QuickCheck
13:37:49 <sm> re the first: does the cabal test output indicate that it's processing all the hs files  ?
13:38:25 <sm> https://github.com/data61/fp-course/blob/master/test/doctests.hs#L19 looks like it should 
13:38:55 <Unicorn_Princess> sm: where would the cabal test output indicate that?
13:39:03 <sm> on your console
13:39:35 <sm> paste a transcript of your cabal test run, if you like (on a paste site)
13:40:26 <sm> if I'm not making sense, let me know
13:41:36 <Unicorn_Princess> here you go - the cabal test output: http://lpaste.net/356292
13:42:07 <sm> doesn't look good :)
13:42:14 <Unicorn_Princess> yes
13:42:35 <Unicorn_Princess> this course is a mess :/
13:43:47 <sm> the author is here, you could ask for support
13:45:28 <sm> as you say, it's processing only the first source file. I'm not sure why, something amiss in this loop perhaps ? https://github.com/data61/fp-course/blob/master/test/doctests.hs#L16,L26
13:47:05 <Unicorn_Princess> maybe. commenting out files from that preferred order seems to have no effect. I'll mess with the isSourceFile predicate to exclude List.hs, see what happens
13:47:07 <sm> it's because the doctest function exits the whole program, it's not designed to be called repeatedly
13:47:39 <sm> https://github.com/sol/doctest/blob/master/src/Run.hs#L70
13:49:39 <Unicorn_Princess> huh. well, excluding List.hs using the predicate got the Functor tests to run... and -then- the List test. bizarre
13:49:57 <sm> just FYI the problem space here is very large, so just trying random things is unlikely to work here - better to troubleshoot more systematically
13:50:02 <Unicorn_Princess> I think I'll try ignoring this problem, and just solve the List.hs exercises, and see what happens
13:51:14 <Unicorn_Princess> well, it wasn't -that- random - it's what it uses to feed the doctest function
13:51:59 <sm> ok. But notice how I worked down from cabal test output, to cabal file, to test suite defined in cabal file, to test suite source file, to doctest package, to doctest source
13:52:24 <sm> but sure, sometimes it's quicker to try a lot of stuff and see what it does
13:53:13 <Unicorn_Princess> unrelated-ish - is it necessary to run 'cabal build' before 'cabal test'?
13:53:24 <sm> no
14:10:05 <hsk3> Should I always use sequenceA instead of sequence?
14:10:05 <hsk3> After all, sequenceA is more general.
14:12:06 <lyxia> as you wish
14:12:31 <hsk3> ok..
14:16:14 <kadoban> Rarely matters, but ya if you're working with something that's only Applicative you don't have a choice, if you're doing a generic function sequenceA would probably be better if it can then be Applicative. Usually you have a concrete Monad though and then it doesn't matter.
14:17:11 <hsk3> kadoban but even if I have a monad, wouldn't it be a better practice to use sequenceA? I'm wondering why sequence even exists...
14:17:27 <kadoban> Historical reasons I believe (Applicative didn't exist)
14:17:29 <hsk3> ah ok, probably historical reasons as earlier Monad didn't inherit from Appl...
14:17:30 <hsk3> yeah
14:17:54 <kadoban> Not sure it's really better practice or not. Probably not a bad thing to do, sequenceA
14:18:33 <hsk3> yeah i kinda like sequenceA. make it clear it doesn't use anything in Monad
14:19:44 <monochrom> If you're using it on a concrete type, it probably doesn't matter. If you're writing a polymorphic thing, you already know to prefer sequenceA. :)
14:20:25 <monochrom> I do like the prospect of the young generation getting used to sequenceA and obsolete we old-geezers.
14:20:52 <glguy> When I was your age sequence only worked on lists!
14:20:59 <kadoban> Haha
14:21:18 <monochrom> I still use sequence most of the time, just because old habit doesn't die, not because I don't want to generalized.
14:21:29 <hsk3> :)
14:21:33 <monochrom> Oh haha there is that too.
14:21:49 <kadoban> Its name is better, sequenceA is a bit awkward, but oh well.
14:21:52 <hsk3> ok cool. i'll go for sequenceA. actually, why didn't they just move sequence into Applicative in GHC 8?
14:22:02 <geekosaur> now I'm thinking of a line from _Hellspark_
14:22:28 <glguy> and we walked uphill to get our type errors from the printer, and uphill back to our office
14:22:42 <monochrom> Yeah we're getting ready to make "return" a true synonym of "pure". Maybe as well do it to "sequence" too.
14:22:56 <kadoban> That would be nice, having sequence just generalized.
14:23:08 <pikajude> sequence isn't generalized already??
14:23:19 <kadoban> I guess that could technically break some code, but I can't imagine it's much.
14:23:20 <pikajude> oh wait, are you guys talking about [m a] -> m [a]
14:23:22 <monochrom> After all, even "length" got its generalization treatment.
14:23:23 <kadoban> Or could it even?
14:23:39 <monochrom> @type sequence
14:23:41 <lambdabot> (Monad m, Traversable t) => t (m a) -> m (t a)
14:24:03 <monochrom> So it did get liberated from []. But it still isn't liberated from Monad.
14:24:14 <pikajude> what should it be? applicative?
14:24:18 <hsk3> yeah
14:24:22 <pikajude> nice
14:24:30 <pikajude> @type sequenceA
14:24:31 <lambdabot> (Applicative f, Traversable t) => t (f a) -> f (t a)
14:24:36 <pikajude> just...just give us that one :(
14:25:12 <monochrom> I think maybe there was a mild objection that using >>= and using <*> may have different efficiencies (for some instances) so maybe you don't always want sequenceA.
14:26:37 <hsk3> monochrom shouldn't one always be able to make the <*> performance match the >>= performance?
14:26:42 <hsk3> if one has to ditch >>=
14:26:54 <monochrom> Yes, that's likely too.
14:27:04 <monochrom> I am not entirely sure either way.
14:27:09 <glguy> It isn't necessarily the case that the performance can be the same
14:29:58 <barcabouna> has anyone tried Haxl?
14:31:25 <hsk3> ok thanks for a useful discussion on sequence and sequenceA
14:34:29 <iqubic> Well, Linear Time Sorting with Divisable and Decidable sounds great, but is it actually used in the wild?
14:34:40 <iqubic> Or is it just a load of theory.
14:34:41 <iqubic> ??
14:34:46 <iqubic> That is the question.
14:35:23 <c_wraith> iqubic, you could always see if the library works..
14:35:25 <glguy> barcabouna: You get better answers with more targetted questions. Survey questions like that are more likely to be ignored
14:36:03 <iqubic> c_wraith: I'm not sure what library of Kmett's includes linear time sorting thogh
14:36:09 <iqubic> s/thogh/though
14:36:20 <c_wraith> @hackage discrimination
14:36:20 <lambdabot> http://hackage.haskell.org/package/discrimination
14:37:31 <c_wraith> note that the library is way easier to *use* than understand. :) 
14:39:04 <barcabouna> if anyone has tried Haxl, what is your experience with it? where does it excel and do you think it has had a big impact on your project?
14:39:14 <qqwy`> Good day! I was thinking today about functions like `take`, `drop`, `split`, `filter`, etc. In de Prelude (/Data.List) they have been provided for lists. Similar functions exist for Data.Sequence and Data.Set, but I am wondering why there seems to not be a general version that works on any of these types
14:39:26 <qqwy`> i.e. what kind of ADT is at work here behind the scenes?
14:39:54 <Unicorn_Princess> how can I limit the output of 'cabal test'? there's a lot of tests in each file, and finding the ones I care about in the output as I finish each exercise in the data61 course is bothersome
14:40:59 <Unicorn_Princess> using 'cabal test | egrep Filename.hs:<line nb>' for now, but it's kinda annoying
14:41:34 <c_wraith> qqwy`, you can find classes for things like that in the classy-prelude project. (or its dependencies) 
14:41:36 <geekosaur> qqwy`, these days there is for many of them. (Foldable and Traversable typeclasses)
14:41:52 <geekosaur> ...the problem with this is now you can get unexpected type errors
14:41:56 <glguy> Unicorn_Princess: You'd need to write your test file in such a way that it considered the arguments passed to it and ran the requested tests. Depending on what test framework you're using this might already be implemented
14:42:12 <c_wraith> geekosaur, or unexpected lack of type errors... 
14:42:23 <geekosaur> before, it always knew you were using a list. now it can get into situations where it can't figure out which collection to use, or it uses one you didn't expect
14:43:12 <geekosaur> that too, yes
14:43:12 <Unicorn_Princess> glguy: I'm working through https://github.com/data61/fp-course , and I don't think it does any such thing
14:44:45 <Unicorn_Princess> cabal test | egrep 'Filename.hs:[nb range][nb range][nb range]' will do for now I guess
14:46:36 <glguy> Unicorn_Princess: It uses doctest, and doctest has options
14:52:22 <iqubic> @hackage discriminant
14:52:22 <lambdabot> http://hackage.haskell.org/package/discriminant
14:53:22 <glguy> Unicorn_Princess: Oh, I just found "(Support for module and file targets has not been implemented yet.)"
14:53:28 <glguy> so you might be out of luck
14:57:06 <Unicorn_Princess> glguy: nah, all the tests are in the same module and file, so I'd be out of luck either way, but thanks :)
15:03:59 <sm> doctest needs some help
15:20:10 <qqwy`> c_wraith and geekosaur: Thank you for answering my question :-)
15:21:53 <ThreeFx> Hey #haskell, is there any way I can exploit uniplate/Control.Lens.Plated to transform my A
15:22:01 <ThreeFx> *DT into another structure
15:30:58 <mbw> I have a question about the two laws an instance of MonadTrans should satisfy: https://www.stackage.org/haddock/lts-8.18/transformers-0.5.2.0/Control-Monad-Trans-Class.html#t:MonadTrans . The documentation says "Instances should satisfy the following laws, which state that lift is a monad transformation" . Now I'm not a mathematician, but these laws look like a homomorphism to me, i.e. `lift` is supposed to 
15:31:04 <mbw> preserve the algebraic structure of monads. Is there a difference between "x homomorphism" and "x transformation"? Also, if you lifted to some monad that didn't satisfy the monad laws, but the implementation this particular MonadTrans instance was correct, would these laws be satisfied??
15:38:31 <jle`> mbw: homomorphism and transformatino are sort of used informally 
15:39:08 <jle`> homomorphism implies that some structure is being preserved, and in this case, yes, lift is a part of a monad homomorphism
15:39:15 <jle`> transformation is just a general/more vague term
15:39:27 <jle`> they basically use a generic term and then define what it means right there
15:39:48 <jle`> ignore the fact that "transformation" is an english word; they're not using any mathematical definition
15:41:16 <monochrom> You will find one very precise definition for "linear transformation", one very precise definition for "natural transformation", etc.
15:41:25 <mbw> jle`: Can the difference between the two terms be formalized? All I can find is people asking about linear transformations / vector space homomorphisms.
15:41:39 <monochrom> But don't expect one for "transformation" alone isolated out of context. There is none.
15:41:43 <jle`> transformation is not a term
15:41:49 <jle`> it's just a bunch of letters
15:42:00 <erisco> affine transformation
15:42:19 <jle`> if the familiarity of the word disturbs you, you can instead read it as "lift is a monad floopyboop"
15:42:30 <jle`> "and by floopyboop, we mean:  (etc.)"
15:42:40 <monochrom> Expect fairly arbitrary choice of names from mathematicians.
15:42:42 <mbw> haha ok
15:43:02 <mbw> These terms seem to be used like "canonical isomorphism".
15:43:23 <jle`> yes, the way they are using transformation there is not a reference to any mathematical concept
15:43:30 <jle`> they could have used any arbitrary word or string of letters
15:43:39 <jle`> they can do this, because they define exactly what they mean on the next line :)
15:43:55 <monochrom> Another example is how "order of a group" is completely unrelated to "total order".
15:44:27 <erisco> is that the number of objects? I can't remember
15:44:35 <mbw> However, if they had used a more precise term, maybe it would have given a glimpse at the motivation behind this particular choice of laws.
15:44:37 <monochrom> So if you take out the word "order" out of context and ask ##math, you won't even get consistent answers.
15:44:42 <monochrom> Yes.
15:45:09 <erisco> set cardinality, list length, group order, vector magnitude... am I missing anything?
15:45:10 <monochrom> Oh and don't forget "ordered pair".
15:45:41 <mbw> That one I found annoying...
15:46:05 <monochrom> I don't think vector magnitude fits there.
15:46:21 <monochrom> You want vector dimension there.
15:46:45 <erisco> I... suppose
15:46:49 <monochrom> Vector magntitude would be more along the line of measures and volumes.
15:47:07 <erisco> I was just going off the intuition of "size of"
15:47:34 <monochrom> Ah but the real number interval [0,1] has two "size"s.
15:47:48 <jle`> mbw: see http://hackage.haskell.org/package/mmorph-1.1.0/docs/Control-Monad-Morph.html for a more detailed treatment of what the class of transformation the monadTrans documentation refers to is all about
15:48:06 <jle`> gabriel gonzalez calls them "monad morphisms"
15:48:08 <monochrom> One size is 1 --- its measure; another is c --- its cardinality.
15:48:27 <monochrom> Would you also like to hear about Cantor's set? :)
15:48:54 <erisco> save it for my bedtime story
15:50:10 <mbw> Isn't "morphism" kind of vague as well? Except if "morphism ~ homomorphism", then the terminology would go hand in hand with "monoid morphisms".
15:50:52 <monochrom> We have nailed "morphism" to refer to the category theory notion. No vaguarity there.
15:51:03 <Cale> mbw: Yeah "morphism" is a synonym for "arrow" (in a category)
15:51:13 <monochrom> Did I mention to expect mathematicians to be arbitrary? :)
15:51:40 <monochrom> They nailed "morphism" but they didn't nail "transformation".
15:52:01 <Cale> and generally "homomorphism" means the same thing in a technical sense, though that carries more of the connotation that we're preserving some algebraic structure.
15:52:05 <monochrom> It does look like when they say "transformation" it is only because they want to over-glorify some function.
15:52:22 <Cale> Well, which sort of "transformation"?
15:52:31 <Cale> There are linear transformations, which are technically defined.
15:52:58 <monochrom> Linear transformation, natural transformation, affine transformation, Lorentz transformation.
15:53:19 <Cale> heh
15:53:44 <Cale> With linear and affine transformations, it's a synonym for "map" or "function"
15:54:03 <Cale> (you can replace the word with "map" or "function" and nobody will bat an eye)
15:54:11 <monochrom> Natural transformation is also a function, just doesn't fit set theory's kind of function.
15:54:33 <Cale> right, and while you can probably get away with "natural map"
15:54:38 <monochrom> I guess a pair of functions?
15:54:42 <Cale> "natural function" would have people look at you weird
15:55:58 <mbw> Huh, I always thought "map" was more general than "function", but apparently it's the other way around?
15:56:05 <Cale> and "Lorentz transformation" is a physics thing, I don't know if you could get away with swapping that one out for other words.
15:56:20 <Cale> mbw: They're pretty close to being synonyms.
15:56:45 <jared-w> Doesn't map describe what to do with functions? It would be odd to have that be more general than "function"
15:56:48 <monochrom> Lorentz's is a coordinate transformation. So you could discussion coordinate transformation instead.
15:56:51 <mbw> But clearly affine transformations are more general than linear ones.
15:57:00 <Cale> It's just, when you start doing that replacement in the middle of technical terms, you don't always get away with something which sounds okay to most people's ears.
15:57:08 <mbw> (At least the way I understand it...)
15:57:24 <Cale> mbw: That's correct
15:57:51 <Cale> But "affine transformation", "affine map", "affine function" all mean the exact same thing
15:58:01 <Cale> and are used interchangeably
15:58:06 <mbw> aaaah
15:58:23 <mbw> I thought you meant you could just replace "linear transformation" with "map".
15:58:34 <jared-w> Im just gonna use "affine-y thinge-y" in technical ilterature from now on as the second word can be inferred from context
15:59:03 <monochrom> There was a time "triple" could be inferred from context, too.
15:59:42 <monochrom> Said context would inform you that it's a functor, and it has mu :: a -> F a, and it has nu :: F (F a) -> F a
15:59:57 <Cale> mbw: Actually, you sometimes could get away with that, given appropriate context, or possibly with "transformation" too, but you generally couldn't get away with "function", because that word on its own tends to refer to a plain set function.
15:59:59 <monochrom> Hmm is that nu? Or is that eta?
16:00:37 <Cale> eta
16:00:46 <Cale> nu looks like ν
16:00:49 <Cale> η is eta
16:00:50 <monochrom> Yeah
16:01:08 <jared-w> that messes me up every time
16:01:41 <jared-w> (except when I was taking quantum mechanics because I was writing nu about 50 times a day)
16:02:01 <mbw> And even in quantum mechanics, people talk about the dirac delta "function".
16:02:22 <jared-w> We just do it to piss off the math majors ;)
16:02:36 <mbw> Though I'm not exactly sure in what way a distribution generalizes a function.
16:03:06 <jared-w> Whenever a math professor walked by, my physics professor made sure to emphasize the dirac function, regardless of how relevant it was to whatever we were talking about at the moment
16:05:02 <mbw> Regardless how much ridicule the dirac function resulted in historically, this particular hack was necessary I believe.
16:07:10 <Cale> Well, you can start with just understanding the way that measures generalise functions.
16:09:05 <mbw> Thanks for the hint.
16:09:34 <erisco> and rho is a p, so what can you do
16:09:54 <julianleviston> :t peek
16:09:55 <lambdabot> error: Variable not in scope: peek
16:10:14 <julianleviston> :t Control.Comonad.Store.Class.ComonadStore.peek
16:10:16 <lambdabot> error:
16:10:16 <lambdabot>     Not in scope: ‘Control.Comonad.Store.Class.ComonadStore.peek’
16:10:16 <lambdabot>     No module named ‘Control.Comonad.Store.Class.ComonadStore’ is imported.
16:10:21 <Cale> https://en.wikipedia.org/wiki/Radon%E2%80%93Nikodym_theorem
16:10:42 <Cale> There are some annoying technical conditions there
16:11:21 <Cale> But basically, if you have a measure μ which is absolutely continuous with respect to the Lebesgue measure λ, that is, λ(A) = 0 implies μ(A) = 0
16:11:40 <Cale> and some additional condition about sigma-finiteness,
16:12:46 <Cale> then you can express μ(A) as the usual Lebesgue integral on A, of some Lebesgue measurable function f
16:13:23 <Cale> and that's an ordinary nonnegative function on the reals then
16:13:57 <Cale> This f is called the Radon-Nikodym derivative of μ (with respect to λ)
16:14:51 <Cale> In general, there will also be measures which are not absolutely continuous: they might assign some weight to single points.
16:14:55 <mbw> I am pretty sure I won't get to apply this knowledge anytime soon, unfortunately.
16:15:20 <Cale> The Dirac delta can be explained as such a measure: it assigns measure 1 to any set containing 0, and measure 0 to anything else.
16:17:01 <Cale> This measure is not absolutely continuous with respect to the Lebesgue measure, as we usually have λ({0}) = 0, but by analogy with what goes on with Radon-Nikodym derivatives, you can understand possibly wanting to find some sort of function-like thing which behaves similarly.
16:17:48 <Cale> (of course, such a function doesn't really exist, but that's where the idea that Dirac delta is a "function" of some sort comes from)
16:18:15 <mbw> I wonder if Dirac knew all that
16:18:54 <Cale> He might not have explained it the same way, but he basically knew that he wanted something which "behaved a particular way when integrated"
16:19:29 <Cale> You can get away with more behaviours by just constructing a measure in the first place, than you can trying to come up with an actual function to integrate.
16:19:46 <orion> Cale: Are you in academia, and is this your expertise?
16:19:57 <Cale> Νo, I just studied this in undergrad
16:20:08 <Cale> I work as a Haskell programmer :)
16:20:19 <orion> Lucky.
16:20:45 <orion> What did you study in (under)grad?
16:20:56 <Cale> Pure mathematics
16:21:15 <jared-w> never woulda guessed /s
16:21:18 <Cale> I didn't go to grad school, but I did take a lot of cross-listed courses
16:21:34 <orion> If you could do it all over, would you still use pure math?
16:21:41 <orion> choose*
16:21:50 <Cale> Yes, I wouldn't trade that choice for anything
16:22:07 <Cale> It's such a nice starting point to understand so many other things
16:22:20 <jared-w> I'm glad I'm in CS right now but if I could redo stuff I would've traded my first 3 years of computer engineering for 3 years of pure math. Didn't help that I chose a university that had a poorly designed course
16:23:36 <mbw> I think I know where you're coming from. I study chemistry and I find my lack of mathematical background frustrating.
16:23:45 <orion> I studied biochemistry.
16:23:55 <orion> I kind of wish I had done pure math though.
16:24:52 <koala_man> I almost went pure path. I'm really glad I didn't.
16:24:58 <orion> koala_man: Why
16:25:59 <Cale> I went to the University of Waterloo for my BMath pure mathematics, and, having a Faculty of Mathematics there, from what I understand it can be quite different from what you get in most other places.
16:26:40 <Cale> The people doing CS there would get a BMath in CS :)
16:26:57 <Cale> (and as such, would end up with a very strong mathematics background)
16:27:10 <jared-w> oh man I'm jealous
16:27:25 <mbw> I don't know if I would've been able to get motivated to learn certain things if they would've been forced on me by a curriculum. It was actually Haskell that made me pick up certain mathematical topics that interested me. When I tried to learn about category theory, I noticed I was lacking in abstract algebra, during the study of which I then noticed I needed set theory, etc. etc. It was actually only via 
16:27:31 <mbw> the concept of currying I could make a gist of dual vector spaces...
16:27:42 <koala_man> orion: programming is my passion, and it helped me discover AI
16:27:44 <jared-w> My CS course here is much better than at the university I was at before I transferred. Still, I want to take a lot more math...
16:27:47 <Cale> I didn't actually take many CS courses though -- if you wanted to take anything past 2nd year CS, you needed special permission and you had to pay more, for some silly reason, so I didn't bother.
16:28:02 <Cale> I just did... all the pure math courses
16:28:29 <orion> "It was actually Haskell that made me pick up certain mathematical topics that interested me" -- That's exactly my experience.
16:28:38 <monochrom> Economics says not silly because demand is higher. :)
16:29:27 <Cale> mbw: In my case, it very much wasn't forced on me -- I became interested in pure mathematics while I was in highschool, from outside sources :)
16:29:46 <mbw> That sounds ominous
16:29:56 <Cale> One of which was Douglas Hofstadter's book "Gödel, Escher, Bach: An Eternal Golden Braid"
16:30:20 <jared-w> I really need to get around to reading that book...
16:31:25 <Cale> But also, I just found a handful of halfway-decent mathematics texts in the library, and started reading stuff on MathWorld/PlanetMath and struggled a bunch to piece anything together.
16:31:40 <mbw> In highschool I invested minimal effort in math -- kind of stupid in retrospective, but I originally intented to become a concert pianist :/
16:32:05 <monochrom> So for example U of Toronto did approve the CS department to deregulate its fees. Do not think that it is a rip off --- it means the department can actually hire more TAs. The difference shows.
16:33:08 <Cale> monochrom: In UWaterloo's case, I think the explanation was all the computing equipment :)
16:34:27 <Cale> monochrom: I wouldn't have thought it a rip-off, but it was enough of an inconvenience that I didn't bother continuing to take CS courses.
16:35:39 <mbw> Rip-off or not, leaving uni with a lot of dept is a decision you might not want to make.
16:37:29 <Cale> I thankfully left uni with only about $16k CDN in interest-free debt, and that got covered pretty quickly by my first job.
16:38:21 <erisco> 6 mo interest free
16:39:20 <orion> Cale: How did you land a Haskell job?
16:39:27 <Cale> Here you can get arbitrarily long extensions on interest relief
16:39:31 <Cale> So long as you're not working
16:39:42 <mbw> Thankfully I don't have to pay to study in Germany. On the other hand, I can't get one of those fancy Haskell jobs either, lacking formal background :(
16:40:07 <Cale> orion: By hanging out in #math and #haskell and teaching people about math and Haskell until some of them started companies
16:40:29 <erisco> can you? guess I didn't look that deep
16:40:43 <mbw> And there I thought the people on #haskell were good samaritans.
16:41:00 <koala_man> mbw: but you probably have to provide your own room and board for the duration?
16:41:20 <erisco> well we can't all be as impressive as Cale
16:41:24 <Cale> erisco: Yeah, you basically just call them up and explain that you have no or little income, and they will send you a form, and you fill it out and send it back and get another 6 months.
16:42:18 <erisco> seems like a lot of administrative overhead to extract a few dollars of interest then
16:42:31 <monochrom> No! No interest!
16:42:49 <Cale> mbw: In my experience with the hiring process, formal background is much less important than if, say, you have a bunch of nice looking code on github or somewhere that you can show.
16:42:50 <mbw> koala_man: You do. Though I know of many people who stay with their parents.
16:43:41 <orion> Cale: What about stackoverflow?
16:44:00 <koala_man> Norway has the same system, and I know someone who worked there. It was apparently pretty nice. They were hemorrhaging money by design, and had plenty of authority to pause and postpone interests and payments
16:44:12 <Cale> I dunno, I haven't gone and looked for people's stackoverflow answers, but if someone mentioned it, I might have a look :)
16:48:09 <mbw> Cale: So do people generally just advertise themselves via toy libraries, or are we talking about genius contributions to the linux kernel? After all, there's only so much spare time, and I was always wondering how much people realistically invest on this. And at what point are you done learning the basics and can get started on larger projects... From my experience with C++ and now Haskell, it feels like 
16:48:15 <mbw> never.
16:49:27 <orion> I feel like when you get a job you get trapped fighting fires and never actually growing.
16:49:56 <orion> I don't know how to justify to management that I need time off to study category theory.
16:49:57 <mbw> Then on the other hand, you probably can't generalize these kind of things. Wasn't the guy who invented homebrew reject by Google because he wasn't able to invert a binary tree in the interview?
16:50:12 <orion> mbw: Where did you hear that?
16:51:01 <mbw> I think I read about it a year ago, it was an angry twitter post by the guy himself I think. But hey, it might be a hoax, you never know.
16:51:27 <orion> https://twitter.com/mxcl/status/608682016205344768
16:52:08 <mbw> Seems that's it.
16:55:04 <Cale> mbw: Well, I don't think it has to be anything too genius, but it helps if the code is tasteful. The important thing is the extent to which you can convince people that you're not going to write a bunch of shitty code -- or at least, that you'll pick up how to avoid doing so reasonably quickly.
16:55:48 <Cale> mbw: Yeah, I can only speak for the hiring processes I was let in on :)
17:00:16 <monochrom> Wait, what is "invert a binary tree"? :S
17:00:29 <mbw> Cale: What did the transition from "learning basics" to "doing real-world stuff", like contributing to open-source projects for instance, look like for you? That may probably be sound stupid, but if you don't study CS or anything related and don't know people who can show you the ropes, that is an actual problem.
17:00:39 <monochrom> I am teaching a data structure course, and I don't even know what that means :)
17:01:01 <monochrom> (Can it put it on my midterm?)
17:01:02 <mbw> toTree . reverse . toList?
17:01:12 <monochrom> Ah.
17:01:39 <monochrom> Hmm maybe I should put it on the midterm!
17:01:47 <mbw> Or maybe you build the CoTree.
17:02:05 <monochrom> And add the remark "This was a Google interview question! Also the Homebrew guy flunked this one!"
17:02:09 <Cale> mbw: For me, it was weird...
17:02:41 <Cale> mbw: I was in university still at the time, and making lots of contributions to the old Haskell wiki (the first one, which was running moinmoin)
17:03:21 <Cale> and someone contacted me there about being a research assistant for the summer -- they were looking to hire someone to work on Haskell code. This was way back in 2004 :)
17:03:23 <hpc> slightly off topic: now that we have discovered covfefe, has anyone ever considered what a theoretical vfefe might look like?
17:03:55 <orion> meme magic.
17:04:07 <glguy> Let's nip that early
17:04:15 <Cale> mbw: and so I got hired to work on this compiler for a special purpose language for signal and image processing, written in Haskell.
17:09:14 <orion> Cale: At when point did you feel confident in your ability to teach?
17:09:20 <Cale> mbw: and I hadn't written anything larger than toy projects at the time -- the most complicated thing I'd built was a thing where you could type in a chord either by giving its name, or the individual notes, and it would list off the chords which could be obtained by either deleting notes or reinterpreting which note was the root
17:09:20 <mbw> Cale: The term "winning the lottery" would probably not adequate for this kind of thing, since you did invest time and work for this. Still, it's probably "untypical" I guess?
17:09:50 <Cale> mbw: Yeah, I've been in the Haskell world longer than most people, it's safe to say :)
17:10:06 <Cale> When I first came to this channel, there were something like 20 people here
17:10:41 <orion> Cale: People ask questions here and I am afraid to answer for fear of being wrong. Did you feel the same way?
17:10:42 <Cale> (and yet Haskell had already been around for a decade when I started picking it up!)
17:11:09 <Cale> orion: For a long time while I was learning, I just lurked and absorbed things by osmosis
17:12:26 <Cale> orion: But I like being helpful when I can be, and explaining things interactively to people is also a really good way to polish up your own understanding.
17:13:38 <mbw> I recently picked up a book about Miranda by Ralf Hinze. If you skip over the code listings inside, it looks a lot like Haskell. Despite these roots, Haskell has probably changed a lot since its inception.
17:13:44 <barcabouna> does haskell have a design faq? (python -> https://docs.python.org/3.6/faq/design.html)
17:14:03 <barcabouna> specifically im interested in some things such as why lists are linked and not arrays, but im also interested in the rest
17:14:53 <Cale> barcabouna: Well, lists are not arrays because we have arrays too, and lists are more control structures than they are data structures
17:15:18 <monochrom> Haskell doesn't have a design FAQ.
17:15:30 <barcabouna> too bad that was a good read
17:15:45 <Cale> barcabouna: Lists can be lazy: there might only be one element of a list active in memory at any point in time, and you might walk along it forever
17:15:52 <monochrom> But there are lengthy articles, each expounding on only one front.
17:15:58 <Cale> > [1..]
17:16:00 <lambdabot>  [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,...
17:16:20 <monochrom> For example there is a 20-page article recounting why they chose laziness.
17:16:26 <barcabouna> Cale: that doesn't say anything about the fact it couldn't be an array
17:16:39 <Cale> barcabouna: Arrays exist in memory all at once
17:16:48 <barcabouna> monochrom: i think i have one of those. kinda lengthy though
17:16:51 <mbw> barcabouna: I don't know about Python, but you see languages advertised as being a "replacement for x" or "better x" everywhere. For instance Rust as a replacement for C++, C++ as a better C. Haskell just is.
17:16:58 <barcabouna> Cale: lists do as well
17:17:05 <Cale> barcabouna: No, that's what I'm saying
17:17:35 <barcabouna> i'm not asking about lazy lists or how the lists are to the user
17:17:40 <Eduard_Munteanu> I sometimes wonder if conduits are a nicer approach than lazy lists.
17:17:41 <barcabouna> im asking low-level implementation
17:17:42 <Cale> barcabouna: As we run through each element of [1..] there for printing, it becomes garbage that can be collected
17:17:59 <barcabouna> like why do they need to be linked lists and not arrays with O(1) access?
17:18:02 <Cale> barcabouna: and the subsequent elements of the list aren't computed yet -- there's simply an expression in memory waiting to compute the future ones
17:18:16 <monochrom> The Haskell Report itself explains a few decisions (but not all). There are 3 paragraphs explaining why they have the monomorphism restriction, one blurb why there is seq.
17:18:28 <Cale> I am talking about low level details (I think!)
17:19:06 <monochrom> I certify that Cale is talking low level.
17:19:10 <Cale> barcabouna: You can do fancier things, like having a chunk of n elements be an array, and basically have a lazy list of such chunks. That's what the Vector library does.
17:19:22 <barcabouna> Cale:  im sorry it is still unclear why linked lists are better, in this example
17:19:28 <monochrom> (Now who certifies me? Haha.)
17:19:29 <Cale> barcabouna: Not better.
17:19:37 <Cale> Just different. They serve different purposes.
17:19:41 <Cale> Lists are like loops.
17:19:51 <Cale> Just as a loop can either not occur (because its precondition isn't met)
17:20:02 <Cale> or consists of a single iteration followed by another loop
17:20:14 <Cale> A list is either the empty list []
17:20:22 <Cale> or consists of a single element followed by another list (x:xs)
17:20:41 <Cale> Lists basically are our loops
17:20:55 <mbw> barcabouna: If you chose to work with lists like they were arrays, i.e. do a lot of linear algebra for example with index-based algorithms, you would choose an array. But if lists were implemented with arrays, the way they are typically used in producer <-> consumer relationships, that would be pretty inefficient. Even with C++ vectors, the push_back() operation runs only in armortized constant time and 
17:20:57 <Cale> By putting things in a list, you are saying "I am going to iterate over these things in this particular order"
17:21:01 <mbw> might have to reallocate and copy in linear time if the underlying buffer is full.
17:21:07 <Cale> and a list is the best structure for arranging this
17:21:28 <barcabouna> mbw: what operations specifically would be inefficient?
17:21:48 <barcabouna> im sorry i'm not understanding what really is worse in arrays
17:21:59 <barcabouna> well, worse for haskell's default lazy list type
17:22:09 <Cale> barcabouna: consider adding an element to the beginning of an array
17:22:17 <barcabouna> ok
17:22:21 <barcabouna> O(n)
17:22:25 <barcabouna> O(1) for lists
17:22:30 <mbw> Given a naive implementation even something like appending to the front, (x:xs). If safed in consequtive memory, you would either rely on there being a larger buffer available, or you would have to allocate a larger array.
17:22:42 <mbw> *saved
17:22:59 <barcabouna> but adding an element to the end of an array can be O(1) in the long run
17:23:17 <barcabouna> so if the reason is becaus lists get often added 1 element the same is true for arrays
17:23:32 <Cale> > let primes = 2 : filter isPrime [3..]; isPrime n = all (\p -> n `mod` p /= 0) (takeWhile (\p -> p * p <= n) primes) in primes
17:23:33 <lambdabot>  [2,3,5,7,11,13,17,19,23,29,31,37,41,43,47,53,59,61,67,71,73,79,83,89,97,101,...
17:23:43 <Cale> ^^ how would you write this with an array?
17:24:17 <barcabouna> Cale: how would you write it in a list? that's lazy so i guess it's one element either way
17:24:24 <Cale> I'm not saying it can't be done, but you're going to need to arrange something special in order to approximate an infinite list
17:24:31 <Cale> That is a list
17:24:37 <Cale> It's an infinite list of primes
17:24:42 <mbw> Sure but that's not the whole story. What if elements are very large in size, like bitmaps or something. Then things can become kind of risky up to the point where "adding 1 more" might result in an out-of-memory exception, if your underlying buffer grows exponentially.
17:25:18 <Cale> barcabouna: takeWhile is another thing which makes a whole lot more sense for lists than for arrays
17:25:28 <Cale> Or filter
17:25:46 <Eduard_Munteanu> Assuming unboxed arrays, I suppose.
17:25:51 <Cale> filter is weird for arrays, because you aren't going to know how many elements you have until you've traversed the whole array
17:26:01 <Eduard_Munteanu> Boxed arrays just store pointers.
17:26:02 <Cale> So you can't allocate the new array until you've already done all the work
17:26:08 <barcabouna> mbw: i don't follow
17:26:24 <barcabouna> so if an element is very large, compare both data structures adding it
17:26:31 <Cale> With lists, filter might only have one element of the input array be live in memory at a time
17:26:37 <Cale> and similarly for the output list
17:26:49 <Cale> er, I misspoke
17:26:59 <Cale> With lists, filter might only have one element of the input *list* be live in memory at a time
17:27:27 <Cale> > filter even [1..]
17:27:28 <lambdabot>  [2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,50,52,5...
17:27:52 <Cale> This doesn't pull all the elements of [1..] into memory at the same time, and then begin figuring out which of them are even :)
17:28:01 <Cale> (it couldn't, because that list is infinitely long!)
17:28:19 <barcabouna> Okay but you can have a lazy array as well Cale
17:28:35 <Cale> You can typically have an array which is lazy in the individual elements
17:28:52 <Cale> i.e. each element is a pointer to code
17:28:58 <Cale> But it's harder to have an array which is lazy in its structure
17:29:11 <mbw> barcabouna: I was comparing the cons operator (:), which appends an element to the front of a list, and a C++ vector, which (assuming you come from an imperative background) uses push_back to append an element to the end. The push_back() is armortized constant only because of the reallocation strategy, which might for example allocate a new buffer of double the size if necessary. If it would allocate a 
17:29:12 <Cale> You can have a data structure which is effectively a lazy list of arrays
17:29:15 <barcabouna> i don't follow
17:29:17 <mbw> buffer of size (n+1), you would be having a bad time...
17:29:56 <Cale> barcabouna: An "array" is a data structure which is allocated as a contiguous chunk of memory, which is what lets us index into it quickly.
17:30:07 <barcabouna> linked lists are like this [] <-> [] <-> [] 
17:30:07 <barcabouna> damn connection
17:30:11 <barcabouna> linked lists are like this [] <-> [] <-> [] 
17:30:17 <barcabouna> arrays are like this [][][][]
17:30:39 <Cale> heh, yes, roughly
17:30:43 <Cale> But in Haskell, it can be even weirder
17:30:52 <Cale> because the links don't necessarily immediately go to a list cell
17:31:04 <Cale> they go to *code* which returns a list cell when entered
17:31:35 <barcabouna> mbw: that's exactly the same thing done in python. allocation strategy is better. alsoo they don't use an array of objects but of pointers to objects so moving stuff around is faster
17:31:37 <mbw> > print (take 3 [1,2,3,undefined])
17:31:39 <lambdabot>  <IO ()>
17:31:57 <Cale> (and the first time the code is run, it replaces the pointer to code with a pointer to a shorter bit of code which immediately returns the result which was computed the first time)
17:32:05 <barcabouna> but appends are almost O(1). and since access is very frequent in imperative programs because of modification then it must be O(1) ergo arrays
17:32:16 <barcabouna> im lookiung for a similar explanation but for lists in haskell
17:32:39 <Eduard_Munteanu> Here's a lazy "array" version of [1..]: f = id
17:32:40 <Cale> barcabouna: okay, so recall that undefined is a thing which if it gets evaluated, the program dies with an exception
17:32:52 <robertkennedy> How many hours do you think it would take to aim lambdabot at a company XMPP chatroom?
17:32:53 <Cale> > undefined
17:32:55 <lambdabot>  *Exception: Prelude.undefined
17:33:31 <geekosaur> last time someone tried to generalize lambdabot to not-irc it failed pretty badly, iirc
17:33:35 <barcabouna> Cale: you're talking alot about laziness but i don't understand yet what it has to do with the difference in data structure
17:33:40 * dramforever got confused with FGL
17:33:42 <Cale> barcabouna: We can use this to experiment and find out what parts of an expression get evaluated
17:33:48 <geekosaur> in theory irc is just a plugin. in practice too much of irc is assumed all through it :/
17:34:00 <barcabouna> right now laziness is like this for me: [code1][code2] vs [code1]<->[code2]
17:34:00 <Cale> barcabouna: Laziness is the main difference here, it's the thing which we most need to focus on
17:34:03 <monochrom> Oh lambdabot on the matrix. Now there is a thought. :)
17:34:16 <geekosaur> and half of the irc-specific functionality is in the core; the irc plugin ends up being mostly auth
17:34:26 <Cale> > case 1 : undefined of (x:xs) -> x + 1
17:34:28 <lambdabot>  2
17:34:37 <Cale> ^^ note how we didn't evaluate the undefined here
17:34:38 <Eduard_Munteanu> XMPP probably supports some IRC interoperation with gateways.
17:34:57 <Cale> So, even if undefined was some complicated expression for an expensive list to compute
17:35:08 <Cale> that expression would not have been evaluated at all
17:35:27 <Cale> With arrays, tricks like that can't easily be pulled
17:35:32 <Eduard_Munteanu> So you could set it up on an IRC server and connect it with your XMPP server.
17:35:46 <barcabouna> can you make the same example with arrays?
17:35:48 <robertkennedy> Maybe... The XMPP backends at the moment are maybe unmaintained, I was hoping it'd be mostly making sure those compiled
17:36:14 <Cale> barcabouna: I can't even do it because I'd need an easy way to add an element to the beginning of another array, and that operation isn't even in the library.
17:36:16 <glguy> Ooh, fixed another bug in the vim syntax highlighting, it didn't know about spaces allowed between backticks and the identifier being turned into an operator
17:36:33 <Cale> barcabouna: But presumably such an operation would involve *looking at* the second array
17:36:43 <Cale> barcabouna: In order at least to figure out how big it is
17:36:46 <barcabouna> Cale: well pretend you have an array instead of a list but you can still do the same operations 
17:37:04 <dramforever> Then nothing would be different
17:37:06 <mbw> > listArray (0,2) [0,1,undefined] ! 0
17:37:08 <lambdabot>  0
17:37:08 <dramforever> Cale, barcabouna: This can be useful https://stevekrouse.github.io/hs.js/
17:37:16 <robertkennedy> But marshalling XMPP <-> IRC and running lambdabot at the IRC handle is clever, I wouldn't've thought of that
17:37:20 <mbw> (However, that's probably a boxed one)
17:37:21 <barcabouna> also i dont understand a lot the example
17:37:32 <barcabouna> looking at an array size is O(1) if you cache the size
17:37:33 <Cale> > listArray (0,2) (0 : undefined) ! 0
17:37:35 <lambdabot>  *Exception: Prelude.undefined
17:37:41 <Cale> ^^ this doesn't work
17:38:14 <Cale> I can't be lazy about the actual allocation of the array elements
17:39:04 <barcabouna> dramforever: that mentions nothing about lists or arrays only map/fold which are higher level operations
17:39:17 <Cale> That whole chunk of memory needs to get allocated all at once, right at the time of construction of the array
17:39:33 <Cale> and if we're talking about lists, we might not know how long the list is
17:39:51 <Cale> whereas with an array, you *must* know, up front, how many things you're going to have
17:40:00 <dramforever> barcabouna: You can write your own expressions and see how they evaluate
17:40:06 <Cale> With a list, it can be up to the whim of the computation we're doing to generate the list
17:40:14 <Cale> I can define a list of all valid chess positions
17:40:22 <barcabouna> dramforever: very useful for understanding laziness thanks
17:40:23 <Cale> and I have no idea how many chess positions there are
17:40:54 <Cale> (well, I have *some* idea, but only approximately -- and it's approximately too many to fit in memory)
17:41:11 <barcabouna> Cale: so you're saying basically because arrays have to be contiguous memory we can't add stuff to them
17:41:24 <robertkennedy> Unrelated: if I run something like `f ys xs = ys `deepseq` ys ++ xs`, the computed elements of `ys` will not deallocate, correct?
17:41:29 <Cale> We can't dynamically construct them as we find new elements that should be in there, one at a time
17:41:51 <dramforever> barcabouna: Have you heard of something called a 'stream'?
17:41:55 <barcabouna> but that is not really true, like i said earlier you can allocate new contiguous memory in chunks and optimize further by allocating it for pointers only
17:41:58 <Cale> barcabouna: You're a python guy -- lists are basically like generators, moreso than arrays
17:41:59 <barcabouna> dramforever: i have in iris
17:42:08 <dramforever> Ooh python
17:42:17 <barcabouna> Cale: generators are linked lists? i did not know that
17:42:28 <barcabouna> i thought they were like objects/classes whatever
17:42:34 <Cale> barcabouna: Well lists in Haskell don't behave like plain linked lists
17:42:34 <dramforever> You know with an iter(...) you can get the next element one by one right?
17:42:48 <barcabouna> like 1 fixed variable almost
17:42:54 <monochrom> Generators are an OOP emulation of lazy lists. But I'm speaking pedantically.
17:43:02 <Cale> barcabouna: They resemble linked lists, but what actually takes place in the face of lazy evaluation and garbage collection is rather different most of the time.
17:43:20 <monochrom> But there is truth in: Lazy lists are what generators dream of at night.
17:43:29 <Cale> yeah
17:43:30 <dramforever> Purely functional generators
17:43:35 <Cale> It's not exactly the same thing
17:43:48 <Cale> Generators in python can be attached to code which has effects as it runs
17:43:51 <barcabouna> Cale: ok so when evaluated lists in haskell are linked lists, when not evaluated they're basically a variable
17:44:00 <barcabouna> but say i want to do some slicing, being a python guy
17:44:01 <Cale> whereas lists are pure values whose evaluation doesn't have side effects
17:44:08 <barcabouna> so i say s[50:80]
17:44:17 <Cale> So you drop 50, and take 30
17:44:21 <Cale> or some such
17:44:23 <mbw> barcabouna: When I started with Haskell, I found this simple wordcount implementation ("wc -l") impressive: "main = print . length . words =<< getContents". This splits an input string at newline characters, resulting in a list of lists (the naive "String" type is a type synonym for [Char]), then determines its length and prints it. If you call this with a large input file for example, the program doesn't 
17:44:27 <barcabouna> and that is going to be very costly in a linked list once it gets evaluated
17:44:29 <mbw> need to copy everything, like working with a stream.
17:44:34 <Cale> barcabouna: why costly?
17:44:44 <Cale> barcabouna: It involves skipping a bunch of elements, sure
17:44:49 <dramforever> It is pretty costly tbh
17:44:57 <Cale> But imagine doing that to a python generator
17:45:01 <Cale> it would be similar in cost
17:45:06 <monochrom> It is costly or not depending on why you're writing that kind of code.
17:45:16 <Cale> You have to generate 30 elements which you're going to throw away
17:45:22 <Cale> or 50 rather
17:45:28 <Cale> and then 30 elements which you keep
17:45:43 <monochrom> If you can solve an NP-complete problem in polynomial time by merely writing code like that, not costly at all :)
17:45:45 <Cale> and then there might be arbitrarily more elements, and those will never get computed
17:46:06 <barcabouna> mbw: that's a cool wc -l implementation
17:47:38 <Cale> barcabouna: Does that help?
17:47:52 <barcabouna> it does not
17:47:54 <Cale> barcabouna: By all means, you should not use lists in cases where you need to be able to randomly access elements
17:47:58 <barcabouna> you spoke a lot about laziness
17:48:12 <barcabouna> saying that things are not evaluated yet etc
17:48:14 <Cale> barcabouna: By constructing a list in Haskell, you are saying "please iterate over these things"
17:48:14 <robertkennedy> barcabouna: I've been meaning to ask a pythoner this: if I have something like `def f(dictionary): {- something that destroys the dictionary -}; def g(d): return '~'.join(d.keys()), f(d)`, how can I get that first return of f to force? ATM the best I have is passing a .copy() of d to f. 
17:48:17 <dramforever> barcabouna: I wonder if this helps: you see that hs.js?
17:48:18 <barcabouna> im saying when things are already evaluated
17:48:29 <barcabouna> why have linked lists and not arrays?
17:48:37 <Cale> barcabouna: Okay, here's another thing
17:48:43 <dramforever> Ooh
17:48:57 <Cale> barcabouna: Suppose I'm done with the first 1000 elements of a list -- finished processing them already
17:49:02 <dramforever> Because they are pretty simple?
17:49:20 <Cale> barcabouna: I can garbage collect those
17:49:31 <barcabouna> robertkennedy: that code is hard to read
17:49:41 <Cale> barcabouna: Suppose I'm done with the first 1000 elements of an array -- finished processing them. I can't garbage collect anything.
17:49:51 <Cale> (supposing I still need the rest of the array)
17:50:11 <Cale> Because there's no way for the runtime system to know that I'll never need the initial elements of the array again
17:50:13 <barcabouna> def f(dictionary): "destroy" dictionary
17:50:19 <Cale> because I can randomly access it
17:50:20 <barcabouna> def g(d): return '~'.join(d.keys()), f(d)
17:50:34 <robertkennedy> barcabouna: yeah, sorry, python is pretty whitespace dependent. I'm generally asking if you know how to force strictness in Python. 
17:50:42 <dramforever> @let myList = [1,2,3,4,5,6]
17:50:43 <lambdabot>  Defined.
17:50:52 <dramforever> > 4 : myList
17:50:54 <lambdabot>  [4,1,2,3,4,5,6]
17:50:58 <dramforever> > 9 : myList
17:51:00 <lambdabot>  [9,1,2,3,4,5,6]
17:51:35 <dramforever> Those two cons cells' tails both point to myList
17:51:36 <barcabouna> robertkennedy: what do you mean force strictness? and what is "destroy" the dictionary?
17:51:46 <robertkennedy> An example f (when I used it) was a "power dictionary", where I took set intersections on every keyset in a dictionary by recursively calling f with d.pop()
17:51:50 <Cale> barcabouna: With lists, I might have a list of a few billion elements, and I'm just lazily walking through it, computing elements one at a time as I need them, and the ones I've already used, so long as I don't retain references to them, might be picked up by the garbage collector
17:52:16 <Cale> barcabouna: If I try to allocate an array of a few billion elements, I'm going to need a few gigabytes of memory all at once
17:52:31 <Cale> and then that's all going to be resident until I'm done with the whole array
17:53:05 <robertkennedy> The problem was that the `'~'.join(d.keys())` would only ever have the remaining key in it
17:53:37 <Cale> Now, if I'm going to have to walk through the list a whole bunch of times, and it's going to need to stay in memory, the array might have an advantage here
17:53:53 <Cale> Lists are almost an order of magnitude more overhead if they're retained in memory
17:54:06 <barcabouna> Cale: python is a garbage collected langauge. if you array was lazy and also immutable and you don't reuse it after generating the first 30 elements i guess it could start deallocating all of that space just the same if you had a pointer array
17:54:13 <Cale> But they can be several orders of magnitude less overhead if not.
17:54:16 <mbw> barcabouna: The prototypical example would be "for [1..n-1] $ do ...", where the iteration space space is described by a list. An explicit tail-recursive function might be more efficient, but using a lazy list will be easier and probably be just good enough.
17:54:33 <Cale> barcabouna: No it can't, because it can't prove you won't do array[0]
17:54:46 <mbw> (Well what accounts for a prototypical example is a matter of perspective I guess...)
17:55:03 <Cale> barcabouna: There's no way (without ridiculous feats of heroism) to know what indices of the array you'll need in the future
17:55:15 <Cale> In general, that's equivalent to the halting problem
17:55:38 <Cale> (it might be solved in some cases, but it's not so easy)
17:55:41 <Welkin> barcabouna: an array in python is not a simple C array
17:55:47 <dramforever> Anybody here good with fgl? How can I get the edge (u, v, label) from a graph given (u, v)?
17:55:51 <barcabouna> robertkennedy: im sorry i still don't get the issue. what do you even want to do?
17:56:02 <Welkin> it's a dynamically allocated array (and may even be implemented using a hash table for all we know)
17:57:29 <robertkennedy> Example f: `def f(d): if d == {} then return () else {k,s = d.pop(); yield [k],v; for kPrime, vPrime in f(d): yield kPrime, vPrime; yield [k] + kPrime, v.intersection(vPrime);`
17:57:39 <monochrom> I am beginning to suspect that Python's is a linked list underneath and it can't free up a prefix that won't be used.
17:57:42 <barcabouna> im sorry but none of the examples exlain plainly the exact flaw between the 2 data structures. like access times and such and cases where one is clearly flawed
17:57:54 <Cale> barcabouna: :(
17:58:09 <Cale> barcabouna: I gave you an example where the difference was gigabytes of memory in one direction or the other
17:58:14 <barcabouna> Cale has cited an example of walking a list talking about how the array needs to absolutely stay in memory. why is that? why can't i drop part of the array from memory?
17:58:15 <Cale> depending on access pattern
17:58:25 <robertkennedy> That might compile; might need another yield to kick it off
17:58:33 <barcabouna> remember it's a Lazy Array
17:58:34 <Cale> barcabouna: because you can access the elements randomly
17:58:49 <Cale> barcabouna: So there's no way to know that you won't index into it near the start in the future
17:58:57 <barcabouna> you cant eaccess elements randomly in a linked list?
17:59:00 <Cale> If you have a reference to the array, you can easily get *any* of the elements
17:59:04 <Cale> correct
17:59:05 <Welkin> a standard Array is a pre-allocated block of contiguous memory, whereas a standard List (singly-linked) is a *linked* data structure of dynamically allocated memory (non-contiguous)
17:59:18 <Cale> You can only access the first element of the list, and the tail, which is another list.
17:59:29 <erisco> singly linked list, mind you
17:59:30 <monochrom> barcabouna, with an array in Haskell, the allocation granularity is the whole array block. Garbage collector knows one pointer to the whole block but nothing more.
17:59:31 <Cale> and then if you have the tail, but not the original list
17:59:36 <dramforever> barcabouna: If you want to access a linked list, you will need to keep holding on to the head
17:59:37 <Cale> you can't get back to the original list
18:00:01 <Cale> So the original list may be deallocated, if you have no reference to it
18:00:01 <robertkennedy> Anyway, if you run g(d), there is (to my knowledge) no way to make `'~'.join(d.keys())` not be affected by f. I think that is what Haskell is solving. 
18:00:04 <monochrom> In fact garbage collector doesn't even know it stands for an array, much less who are the future indexes you will use.
18:00:06 <dramforever> But if you *only* needs the tail, you don't need to hold on to the head
18:00:09 <barcabouna> robertkennedy: i will gladly read any code you post in a paste
18:00:20 <barcabouna> but reading that python code one after the other is very hard
18:00:22 <robertkennedy> Omw
18:00:33 <barcabouna> honestly
18:00:44 <Welkin> it doesn't help that in python, they call dynamic arrays "lists"
18:01:00 <barcabouna> you have a whole function with lots of actions inside on one line
18:01:24 <monochrom> Whereas with Haskell list, garbage collect knows every pointer to every individual cons cell. It can free up cells at that fine level.
18:01:26 <Welkin> it also doesn't help that so many examples for newcomers to haskell use that stupid list indexing operator (!!)
18:01:42 <Welkin> why does ti even exist?
18:01:54 <Cale> Yeah, (!!) shouldn't exist
18:02:24 <mbw> I always hear that sound from metal gear solid when I see it
18:02:30 <monochrom> (!!) is probably alongside with head and tail in that regard, i.e., some people wanted it in Prelude because Lisp has them.
18:02:31 <Cale> There are some use cases for it, but most of them would be better off with something that at least produced a Maybe result
18:02:37 <Cale> yeah
18:02:37 <dramforever> To be fair, a singly linked list is a pretty bad design choice, *data-structure-wise*
18:02:38 <MarcelineVQ> who's footprints are these...
18:02:52 <monochrom> My conclusion is that Lisp is the one that shouldn't exist. :D
18:02:57 <Welkin> dramforever: it depends on what you are doing
18:02:57 <Cale> dramforever: But the point is that lists are not so much data structures in Haskell as they are control structures :)
18:03:06 <Welkin> a singly-linked list can be just what you need
18:03:07 <dramforever> Exactly
18:03:35 <Cale> barcabouna: Why does python have for loops?
18:03:49 <dramforever> Welkin: and {{type String = [Char]}} doesn't help at all
18:03:57 <Welkin> monochrom: yeah, because lisp isn't even functional™
18:04:12 <Welkin> dramforever: it makes sense
18:04:17 <Welkin> we have Text and ByteString as well
18:04:22 <Welkin> it depends on what you need though
18:04:26 <Welkin> they all have their use cases
18:04:55 <Cale> barcabouna: The answer to that is pretty much the same as the answer to the question of why Haskell places so much emphasis on lists.
18:04:59 <erisco> I tried to get into lisp but couldn't find the syntax
18:05:01 <dramforever> Using libraries for everything, yeah so C++
18:05:18 <Cale> Lists are just a nice way to express iteration in Haskell.
18:05:26 <dramforever> Using libraries instead of standard builtin stuff for everything, yeah so C++
18:05:41 <Welkin> dramforever: I prefer it that way
18:05:42 <monochrom> Haha erisco
18:05:43 <Welkin> no built-ins
18:06:14 <Cale> erisco: You joke, but there's a lot more of it than people are usually willing to admit
18:06:26 <dramforever> Welkin: I do too
18:06:33 <Cale> If you include all the special forms and such.
18:06:37 <dramforever> But it's definitely not helping
18:07:09 <monochrom> It is usually unimportant what's built-in and what's in Prelude.
18:07:28 <Welkin> lisp is strange in that regard. It's a fun idea, and you get excited about the thought of using it, but then you sit down to actually use it, and that feeling vanishes in a sea of parentheses and archaic function names
18:08:08 <monochrom> But it is a sign of first-class-everything if you can afford to put more things out of the built-in bin and into the Prelude bin.
18:08:17 <julianleviston> can someone help me to get going with the contexts function from Control.Lens.Plated ?
18:08:27 <barcabouna> robertkennedy: why are you using generators? 
18:08:51 <julianleviston> specifically, I don’t know how to use a context to do something like, say, return the original data structure modfiied
18:08:58 <barcabouna> it hasnt got much to do with normal dictionaries
18:08:59 <monochrom> Case in point: Our && can afford to be in a library. A lot of languages simply can't. That is remarkable.
18:09:56 <dramforever> something something Scala something
18:10:56 <monochrom> We do strangely have a built-in if-then-else which doesn't have to be built-in, apart from the exact grammar.
18:11:09 <Welkin> and it shouldn't be used o.o
18:11:19 <Welkin> that's another one of those things that beginners abuse
18:11:27 <monochrom> Ah but I'm beginning to like multiway-if :)
18:12:21 <monochrom> Another strange part is that the empty list symbol [] and the cons symbol : are reserved words too. You just can't write "data X a = [] | a : X a"
18:12:58 <Welkin> and : and :: are swapped
18:12:58 <julianleviston> I can use pos :: :: ComonadStore s w => w a -> s to get the value out of a Context, but I really don’t understand how to use peek :: ComonadStore s w => s -> w a -> a to do adjusting. 
18:12:58 <monochrom> [] being reserved is understandable, but I don't actually understand why : is too, it seems to me there is no need.
18:13:27 <Welkin> which idris fixed (and elm too), but purescript kept the same
18:13:29 <barcabouna> Cale: so linked lists are a great match for for loops. arrays are equally good from a performance standpoing
18:13:42 <Cale> barcabouna: But not in the same contexts
18:13:44 <Welkin> barcabouna: no, not really
18:13:50 <geekosaur> I suspect just because, if you've made one list constructor a reserved construct, it's easier to do the same to the other
18:13:52 <barcabouna> provide some examples?
18:13:54 <Welkin> it always depends on your exact needs
18:14:06 <barcabouna> like you're a language designer
18:14:06 <Welkin> arrays and lists have their uses
18:14:09 <monochrom> Anyway "data X a = Nil | a :> X a" is user-definable. Furthermore, GHC lets you add rewrite rules so your homebrew list can enjoy as much fusion as "built-in" list.
18:14:10 <Cale> barcabouna: Arrays have their place, but it's almost never the case that you wouldn't care whether you had a list or an array
18:14:11 <Welkin> one is not "better" than the other
18:14:11 <geekosaur> than to try to get them to work togeher when on eis fish and the other fowl
18:14:12 <mbw> I would love it to be able to define operators like (<,>) a a = ...
18:14:19 <Welkin> if one was better, the other would not exist
18:14:34 <barcabouna> and you choose everything about how haskell looks to the user. noew you need to chooose how to implement "lazy lists" in C. and you choose linked lists instead of arrays. why?
18:14:35 <mbw> *a b
18:14:36 <Cale> barcabouna: They're almost never equivalent unless you're doing something very cheap -- in most cases one is vastly superior to the other
18:14:41 <monochrom> Testimony of how more things are first-class in Haskell than other languages.
18:14:59 <Cale> barcabouna: and it goes both ways -- sometimes lists are *way* better than arrays
18:15:35 <barcabouna> Welkin: this is not a quest for which one is better. this is why haskell chose linked lists. it's a design question
18:15:36 <Cale> and sometimes arrays are *way* better than lists
18:15:48 <Welkin> this is why we have so many data structure: arrays (of all flavors), lists (of all flavors), trees (of all flavors), graphs, sequences, maps/hashmaps, sets, etc
18:15:58 <barcabouna> because linked lists have a lot of downsides. like slicing
18:16:11 <Welkin> barcabouna: I don't understand the question
18:16:15 <geekosaur> we have arrays also. twice even (Array and Vector)
18:16:17 <Welkin> how did haskell "choose" linked lists?
18:16:23 <Welkin> you don't have to use them if you don't want to
18:16:24 <geekosaur> but in functional programming, linked lists are basic iteration
18:16:51 <barcabouna> No guys i understand
18:16:57 <barcabouna> i don't have to use linked lists in haskell
18:16:58 <Cale> barcabouna: If I have an array, and I want to iterate over a bunch of its elements, I'll probably express that by turning part of the array into a list.
18:16:59 <barcabouna> that's not the question
18:17:08 <Cale> (or at least, I have that option)
18:17:08 <jared-w> Also, there's two different things about linked lists (to me)
18:17:23 <jared-w> there's linked lists as a data structure and then there's linked lists as a /concept/
18:17:30 <geekosaur> did you have an alternative for the use case?
18:17:37 <barcabouna> why did haskell choose to have linked lists as the more basic type? [1,2,3] is a linked list in haskell but an array of pointers in python
18:17:38 <jared-w> Just like you can have an iterator object in Java or talk about iteration as a concept
18:17:54 <Cale> barcabouna: Because we use lists to express iteration.
18:18:01 <geekosaur> to the extent that *that* is a question, early functional programming was very heavily influenced by lisp and scheme, where cons-lists are a fundamental type
18:18:08 <mbw> barcabouna: You can even use vectors like lists, such that with the -XOverloadedLists extension, [1,2,3,4,5] could be of type Vector Int. It's not like using any other data structure was immediately annoying.
18:18:15 <Cale> barcabouna: and iteration is so damn common throughout programming, that it's pretty important to have a good way to express it
18:18:30 <barcabouna> Cale: that has nothing to do with design choices
18:18:37 <dramforever> ???
18:18:40 <Cale> It has everything to do with this design choice
18:18:41 <Welkin> barcabouna: python's "lists" (better called arrays) are not actually standard arrays, they are dynamic arrays
18:18:45 <barcabouna> how you express iteration doesn't mean anything about how haskell implements it
18:18:50 <barcabouna> you can express it how you wish
18:18:54 <monochrom> Ironically, I think there is a sense of "basic" such that Haskell list is less basic than array.
18:19:02 <Welkin> conceptually, you can treat arrays and lists the same, but if you consider their implementation, they can be very different
18:19:05 <Cale> barcabouna: The language is opinionated about how you express things.
18:19:08 <monochrom> The sense is this: array has to be built-in, list doesn't.
18:19:37 <Cale> barcabouna: Arrays are unsuitable to express many kinds of iteration, because, well, for one, they can't be infinitely large.
18:19:48 <Cale> and secondly, they have to be allocated all up-front
18:19:58 <Welkin> a list is a form of the free monad :D
18:20:01 <Welkin> lol
18:20:21 <barcabouna> monochrom: built-in?
18:20:27 <Cale> So you have to know how many steps your loop is going to have if you want to turn it into an array
18:20:32 <Welkin> Cale is right about lists being an easy way to represent recursion
18:20:34 <barcabouna> Cale: neither can lists be infinitely large
18:20:36 <monochrom> Actually where can I read on how Python implements its "array"/"list"?
18:20:39 <Welkin> lists, trees, etc
18:20:40 <Cale> barcabouna: Incorrect.
18:20:41 <barcabouna> lists as a datatype
18:20:42 <Welkin> any linked structure
18:20:44 <Cale> barcabouna: Maybe you missed that.
18:20:47 <Welkin> arrays are not linked structires
18:20:51 <Cale> barcabouna: But lists in Haskell can be infinite.
18:20:58 <Cale> (and *often* are)
18:21:00 <barcabouna> i'm talking about hardware
18:21:06 <barcabouna> when i say lists, im talking about C
18:21:08 <monochrom> I do feel that everything we've said about Python is unverified someone-made-that-up folklore.
18:21:15 <Cale> barcabouna: I'm talking about Haskell, I don't know what the heck you're talking about.
18:21:16 <barcabouna> i think you're confusing my list with haskell's list
18:21:24 <barcabouna> C lists are not infinite
18:21:31 <Cale> okay
18:21:32 <barcabouna> you may not have [1...] in C
18:21:36 <barcabouna> you may in haskell
18:21:36 <Cale> Haskell lists are not C lists
18:21:38 <dramforever> wait a sec
18:21:38 <barcabouna> but haskell has C
18:21:44 <barcabouna> so im saying, what does haskell do?
18:21:48 <Welkin> monochrom: something about comparing numpy (which uses C arrays) to python's lists/arrays
18:21:53 <dramforever> Why does Haskell have C?
18:21:56 <barcabouna> why does haskell use C linked lists and not C array
18:22:04 <mbw> wat
18:22:05 <Welkin> lol what?
18:22:05 <Cale> barcabouna: It doesn't use C linked lists
18:22:09 <barcabouna> dramforever: well i dunno what haskell has but it's running on a turing machine
18:22:13 <Welkin> there is no such thing as a C linked list
18:22:19 <Welkin> you can implement lists however you want
18:22:20 <barcabouna> so in the end it's gonna be LInked List or Array
18:22:25 <Cale> barcabouna: Haskell linked lists are *very* different from C linked lists -- they only look sort of similar on a surface level
18:22:42 <Cale> barcabouna: Because instead of having a pointer to an element and a pointer to another list cell
18:22:49 <jared-w> barcabouna: would it help you if I said that the compiler often transforms Haskell's linked list into whatever the hell makes sense?
18:22:57 <Cale> barcabouna: You have a pointer to *code* for an element, and a pointer to *code* for another list
18:23:08 <dramforever> Maybe you call that C linked list?
18:23:28 <jared-w> For instance, GHC will transform something like "foldr (+) [1..50]" into a primitive assembly raw loop with no "list" whatsoever in the machine code
18:23:29 <Cale> (you have a thunk)
18:23:34 <barcabouna> jared-w: very much it's the whole point of this discussion
18:23:43 <barcabouna> i was under the belief haskell used linked lists
18:23:50 <Welkin> o.o
18:23:50 <barcabouna> ergo doing [...] !! 50 is slow
18:23:54 <dramforever> It does
18:24:04 <barcabouna> but if you're saying it doesnt then im wrong about that
18:24:07 <Welkin> it uses linked lists if you use List in your code
18:24:11 <Welkin> otherwise, no
18:24:17 <dramforever> But only if you squint reeeeealy hard
18:24:28 <barcabouna> ok
18:24:31 <barcabouna> forget haskell for a second
18:24:42 <barcabouna> haskell doesnt exist. it hasnt been made yet.
18:24:52 <barcabouna> (btw this would be easier if someone here was like a core developer)
18:24:53 <Clint> :-O
18:24:56 <mbw> I miss haskell
18:25:02 <Welkin> maybe you are asking the wrong question, or are asking it in a way that none of us understand
18:25:05 <dramforever> barcabouna: You mean, like 'before' haskell?
18:25:06 <monochrom> wtf is "core developer"?
18:25:09 <barcabouna> so you need to make a language. it is a lazy language. and that language can store stuff of course
18:25:20 <barcabouna> monochrom: a person that pushes commits to ghc
18:25:35 <Cale> barcabouna: https://www.cs.kent.ac.uk/people/staff/dat/miranda/whyfp90.pdf
18:25:41 <Welkin> B.H. (Before Haskell)
18:25:43 <barcabouna> so you need to make a language and this language has a concept of lists. what data structure do you choose?
18:25:46 <Welkin> A.H. After Haskell
18:25:51 <monochrom> OK. But they would tell you what Cale and jared-w has been telling you.
18:25:54 <Cale> ^^ that paper is historical, and may help you to understand the history of how Haskell became this way
18:26:34 <Cale> It explains via examples some of the cool things that lazy evaluation and lazy data structures can do, and what the researchers who constructed Haskell in the first place were interested in examining.
18:26:39 <Welkin> barcabouna: do you understand what a thunk is?
18:26:53 <Welkin> that may be the root of this question
18:27:03 <monochrom> Reason being the GHC developers have long written their answer down in the STG paper and by now a lot of people here have read it.
18:27:12 <barcabouna> Cale: that paper has not a single word regarding linked list or array
18:27:12 <jared-w> If I were to make a new functional language right now
18:27:14 <Cale> (The title of the paper is "why functional programming matters", but it is very much aligned with the side of lazy functional programming)
18:27:28 <Cale> barcabouna: It does plenty of stuff with lists
18:27:30 <jared-w> I would use a linked list
18:27:30 <dramforever> barcabouna: Well, Lisp got the simple one: cons cell
18:27:43 <barcabouna> Cale: i actually have that paper somewhere i need to read iit XD
18:27:51 <jared-w> and then the compiler would translate that linked list into whatever was efficient for how I was using it
18:28:00 <jared-w> *coughjustlikeHaskelldoesnow*
18:28:01 <Welkin> barcabouna: can you restate your original question?
18:28:11 <robertkennedy> T
18:28:11 <Cale> barcabouna: Actually, let's take another approach
18:28:16 <dramforever> No wait, my answer is:
18:28:25 <Cale> barcabouna: Let's re-examine how algebraic data types work
18:28:31 <dramforever> My answer is: both
18:28:34 <barcabouna> jared-w: i also read some quote about peyon saying that future haskell will be non-lazy by default
18:28:40 <Cale> and then we'll look at list as a mere application of that
18:28:46 <monochrom> If you are to roll back history to when Haskell did not exist, and you try to invent a lazy language, you will decide that lazy cons list will nicely stand for for-loops. It's a logical conclusion. It's what Cale has been saying. If only you stopped denying.
18:28:51 <jared-w> barcabouna: that quote is often interpreted however someone wants to interpret it
18:28:52 <dramforever> Linked list, array, hybrid chunks/balanced binary trees
18:29:05 <barcabouna> jared-w: haskell does not do that
18:29:29 <dramforever> I thought SPJ meant that lazy is a means not an end
18:29:29 <Cale> barcabouna: peyon?
18:29:30 <jared-w> iirc Peyon himself mentioned that what he was getting at is that "the next major FP language that hits the mainstream consciousness will be non-lazy by default because most people suck at thinking in a lazy manner"
18:29:33 <barcabouna> you have previously confirmed that [1..] always takes a long time
18:29:38 <Cale> Who is Peyon?
18:29:39 <Welkin> simon peyote jones :P
18:29:41 <barcabouna> [1..] !! 50
18:29:48 <Cale> Simon Peyton Jones?
18:29:50 <jared-w> s/peyon/SPJ/
18:29:52 <Welkin> shh
18:30:02 <Welkin> haha
18:30:08 <jared-w> :t (!!)
18:30:09 <lambdabot> [a] -> Int -> a
18:30:18 * geekosaur notes that list fusion *is* a thing in ghc
18:30:20 <robertkennedy> @barcabouna lpaste.net/356298
18:30:20 <lambdabot> Unknown command, try @list
18:30:22 <Cale> He's said some stuff that was mildly anti-laziness, in the effort to emphasise how important purity is.
18:30:36 <dramforever> I think there's a dilemma here
18:30:37 <Cale> But I think he understands very well how important lazy evaluation is too :)
18:30:49 <dramforever> You can put all sorts of data structures into the standard library
18:30:50 <Welkin> spj says a lot of things as a joke
18:30:52 <dramforever> and nobody will use them
18:30:52 <jared-w> barcabouna: Firstly, haskell /does/ indeed transform a linked list into whatever it wants to when it compiles
18:30:54 <robertkennedy> barcabouna: lpaste.net/356298
18:30:57 <barcabouna> monochrom: im not denying anything but what you said is just silly. so you're designing a language and the bare metal data structure is going to be a linked list because "it's nice" ? it's gotta be ideal for your use case as well, performant!
18:31:01 <Welkin> he also said "haskell is a useless language"
18:31:11 <geekosaur> it's relatively easy to detect when a bunch of laxy linked list operations can be compressed into a pipeline without ever generating the list
18:31:12 <Cale> Welkin: Yeah, there are a lot of tongue in cheek things which he says and people don't get the joke
18:31:15 <geekosaur> *lazy
18:31:32 <dramforever> barcabouna: then what you *really* need is a nice way to express all sorts of data structures
18:31:35 <jared-w> barcabouna: ok, do you know anything about C++?
18:31:38 <Cale> barcabouna: We're not just going to have lists
18:31:40 <dramforever> Because *that*'s how you get performant
18:31:45 <Cale> barcabouna: We're going to have algebraic data types
18:31:53 <Cale> barcabouna: and lists will just be one consequence of that
18:32:01 <dramforever> No fgl help for me :(
18:32:17 <Welkin> dramforever: c++ is performant? I thought it was horribly slow. Or maybe I'm thinking of c template metaprogramming and boost
18:32:25 <Cale> dramforever: Sorry, I looked, the best thing I found was lneighbours
18:32:36 <Cale> dramforever: but that kinda sucks
18:32:40 <barcabouna> jared-w: i have used C extensively
18:32:43 <barcabouna> not C++
18:32:47 <barcabouna> but i know some things
18:32:49 <dramforever> Cale: :(
18:33:00 <dramforever> I guess I need to roll my own graph thingy
18:33:03 <systemfault> Welkin: C++ is ultra-fast at runtime, slow at compile time :)
18:33:04 <barcabouna> Cale:  i understand you're going to have all datatypes
18:33:16 <barcabouna> i'm saying the default datatype for packed stuff
18:33:29 <jared-w> Do you understand how C++ is able to allow things like a vector<int> for "free" (aka you pay no performance penalty for abstraction)?
18:33:39 <barcabouna> Welkin: c++ terribly slow?
18:33:49 <Cale> dramforever: I usually just use something like Map Vertex (Set Vertex) or some adjustment of that to include whatever labels I need
18:34:08 <dramforever> No wonder no fgl help for me :(
18:34:25 <Welkin> systemfault: yeah, that's what I meant, the transformations/compilation
18:34:25 <Cale> barcabouna: There won't be a default data type. You know what, screw arrays, they're not what interests us right now.
18:34:25 <barcabouna> jared-w: i have never looked into that
18:34:43 <Cale> barcabouna: We'll make a language, and it'll have pattern matching and lazy recursive data structures
18:34:45 <jared-w> maybe you should. It might help you understand how linked lists in Haskell are a "variable cost abstraction"
18:34:52 <Cale> and that'll be good, we can write lots of papers
18:34:53 <dramforever> Cale: thank you for trying to help, anyway
18:35:05 <barcabouna> jared-w: maybe you could tell me though
18:35:06 <Cale> (and it's 1990 right now btw)
18:35:13 <jared-w> (that is, sometimes it compiles down to raw assembly loops. Sometimes it compiles down into a linked list with pointers. Sometimes it compiles down into somewhere in-between)
18:35:18 <Cale> Or possibly 1985 or something
18:35:26 <Cale> haha
18:35:34 <Welkin> Cale: why not 1600?
18:35:52 <barcabouna> Cale: default syntax for [1,2,3] is linked list which clearly states the design choice here was preference for linked list
18:35:55 <Cale> barcabouna: When Miranda and Haskell were starting out, the goal was to explore what could be done with these ideas involving pattern matching and lazy evaluation.
18:36:18 <barcabouna> the same goes for strings, preference for linked list
18:36:25 <barcabouna> so i just wanna ask a core developer why
18:36:30 <barcabouna> why did they choose linked list
18:36:35 <Welkin> barcabouna: but we have Text and ByteString
18:36:37 <barcabouna> they couldve chosen array
18:36:37 <jared-w> barcabouna: C++ has a concept of zero-cost abstraction. It comes from the idea that if you tell the compiler exactly how to rewrite something down into efficient code, you can write at a higher level. For example:  for (item : container) { print(item) } can be transformed into efficient assembly if the compiler is told about how to iterate a container
18:36:42 <Cale> barcabouna: Right, well, if you go back far enough, they only had algebraic data types, which means no arrays
18:36:48 <barcabouna> Welkin: we do. but the default is linked list
18:36:59 <Welkin> barcabouna: there is no default
18:36:59 <Welkin> o.o
18:37:01 <Cale> barcabouna: but you can define lists as an algebraic data type
18:37:01 <barcabouna> and in python the default is array. they coulve chosen linked list but did not
18:37:06 <Welkin> String is not more a default than Text
18:37:13 <systemfault> https://haskell.godbolt.org/ is fun for seeing what compiled Haskell code looks like.
18:37:21 <barcabouna> Welkin: is Text in the Prelude?
18:37:23 <dramforever> :t "What are you talking about, Welkin?"
18:37:24 <lambdabot> [Char]
18:37:25 <Cale> barcabouna: But you might also ask why the "default" as it were stayed that way
18:37:39 <Cale> barcabouna: and the answer is that an entire style of writing programs is based on this decision
18:37:46 <jared-w> barcabouna: you're really stuck on the implementation of things. I think it's coming from how, in C, the code you write is the code that you get out of the compiler. Functional languages are far more declarative and expressive than that
18:38:30 <Welkin> it's likely that if you are writing in a language as high-level as haskell, you don't care about cpu cycles or kb of memory o.o
18:38:42 <Welkin> so why even bother thinking about it
18:38:43 <jared-w> Try to disconnect your mental need for a 1 : 1 correspondance of written code to compiled execution and you'll find it easier to believe that the "linked list" isn't really even a linked list
18:38:58 <Cale> It is *kind of* a linked list
18:39:04 <Cale> It's just that the links are really strange
18:39:06 <Cale> because they're code
18:39:29 <barcabouna> Welkin: why bother is because Strings are horribly slow because they're linked lists, and if i do take 50 arr i get the same thing
18:39:36 <barcabouna> but that's no reason
18:39:39 <barcabouna> i just wondered why
18:39:40 <Welkin> barcabouna: Strings are not that slow
18:39:43 <jared-w> Like, when I write something like "foldr (+) [1..50]" I'm not telling the compiler "make a linked list. Iterate over it. Sum everything. Output the total sum". I'm telling the compiler "yo so I want the total sum of the numbers 1 through 50. Idc how you do it, just make it work"
18:39:46 <geekosaur> especially when fused
18:39:47 <Welkin> I hear this all the time, but it's not true
18:39:55 <geekosaur> also a short String is far more optimal than a short Text
18:40:12 <Cale> Welkin: you're talking to the guy who was upset that "yes" wasn't outputting several GiB/s :)
18:40:18 <Cale> (to /dev/null)
18:40:19 <Welkin> o.o
18:40:45 * geekosaur is wondering of some of this is 'move fast, break stuff' mentality
18:40:55 <jared-w> To be fair, the C version can hit 120+ GiB/s if you mess around with some stuff...
18:41:01 <julianleviston> I’ve often wondered why people often use byte strings… do they mean to use Text?
18:41:02 <geekosaur> or maybe just 'I dont care about 20 years of existing code, everything must cater to my right now'
18:41:12 <Cale> jared-w: That sounds an order of magnitude more than I've ever seen
18:41:17 <Cale> jared-w: But maybe :)
18:41:30 <geekosaur> julianleviston, often it's because both disk files and network streams are octet strings / ByteStrings
18:41:30 <MarcelineVQ> you guys are full of beans today
18:41:45 <jared-w> Cale: what they did was modified the programs to output entire cache pages worth of yes at the same time. Basically reducing the output to "how fast are my CPU registers?"
18:41:52 <geekosaur> and you're forced to deal with encoding issues, especially with network streams where the encoding can chnage dynamically
18:41:53 <julianleviston> geekosaur: makes sense I guess.
18:41:57 <barcabouna> is there someone who absolutely knows i can maybe ping?
18:42:03 <Cale> I know
18:42:05 <barcabouna> i'm curious about this
18:42:07 <Cale> What do you want to know?
18:42:13 <jared-w> julianleviston: If people use ByteStrings, it SHOULD mean that they can make zero assumptions about the underlying data (eg it's not guraranteed to be UTF8, or a "string" or "text")
18:42:24 <Welkin> barcabouna: if they are willing to sit with you for 3 hours until you finally accept their answer or they give up
18:42:24 <julianleviston> jared-w: right… 
18:42:27 <jared-w> Most of the time, untortunately, people use ByteStrings because they're stupid
18:42:47 <barcabouna> Welkin: that won't have to happen because they probably know the exact reason they chose linked list
18:42:50 <Welkin> jared-w: or it's just a binary blob
18:42:59 <mbw> ByteStrings sound fast. People crap their pants prematurely.
18:43:00 <barcabouna> i don't need to accept an answer
18:43:10 <Welkin> lol mbw 
18:43:10 <julianleviston> jared-w: is it because it has the word “String” in it? ;-)
18:43:11 <barcabouna> i asked for the reason
18:43:13 <Welkin> attoparsec
18:43:39 <Cale> barcabouna: It's really important that you understand the difference between lists in Haskell and the linked lists you might've implemented in C
18:43:52 <Cale> barcabouna: They're structurally similar, but not the same.
18:43:58 <Welkin> Cale: maybe you should teach barcabouna Scheme?
18:44:20 <barcabouna> what do lists become in haskell, apart from generators when they're being used lazily
18:44:22 <barcabouna> for example
18:44:25 <jared-w> the tl;dr is that lists in C is a list. Lists in Haskell are a concept. Sure, they're sometimes *actually* lists, but really they're just a concept...
18:44:33 <barcabouna> say i do a = [1..10]. that's not yet a linked list
18:44:37 <barcabouna> if i do print a
18:44:40 <Cale> https://www.dcc.fc.up.pt/~pbv/aulas/linguagens/peytonjones92implementing.pdf -- this is an old paper, and it's a bit out of date, but it's a good place to get a handle on how to actually implement Haskell in hardware
18:44:43 <barcabouna> then it's evaluated and a linked list
18:44:48 <jared-w> barcabouna: I already gave you an example of a list that transforms into a raw assembly loop
18:45:03 <Cale> If you do print a, and a is not used anywhere else, then something interesting happens.
18:45:06 <barcabouna> jared-w: when?
18:45:14 <barcabouna> Cale: what if you use it again
18:45:15 <jared-w> like twice. "foldr (+) [1..50]"
18:45:16 <Cale> [1..10] is evaluated, and produces 1 : [2..10]
18:45:41 <Cale> barcabouna: Well, if you *do* use it again, then the list remains in memory in an already-evaluated state
18:45:42 <barcabouna> see im not concerned with lazy power right now
18:45:46 <barcabouna> im the core haskell developer
18:45:49 <julianleviston> can anyone tell me how I would modify a Context in contexts :: Plated a => a -> [Context a a a] from https://hackage.haskell.org/package/lens-4.15.2/docs/Control-Lens-Plated.html ? that is, I have some recursive syntax that I’d like to be able to pick any point of, and adjust with a function
18:45:51 <barcabouna> i know lazy power avoids memory
18:45:55 <Welkin> o.o
18:45:58 <Welkin> lol what
18:46:02 <barcabouna> im concerned with being forced to use an evaluated list
18:46:11 <barcabouna> so i have this list, it has been forced to become un-lazy
18:46:16 <barcabouna> why is it a linked list?
18:46:17 <Welkin> 21:45 < barcabouna> im the core haskell developer <-- what does that mean?
18:46:36 <barcabouna> Welkin: it means try to pretend i am the developer of a lazy language that is haskell
18:46:38 <jared-w> Welkin: he's trying to walk through why we use linked lists from the perspective of "assume I'm implementing haskell from scratch"
18:46:44 <barcabouna> yes
18:46:48 <Cale> barcabouna: If you evaluate something which is bound to a variable, then it will remain evaluated so long as that variable stays in scope.
18:46:56 <jared-w> But without actually reading the 50 papers we linked that explain this in extreme detail...
18:47:11 <Cale> barcabouna: That's part of what lazy evaluation means
18:47:12 <barcabouna> jared-w: that is why i asked for a faq in the first place
18:47:22 <barcabouna> also none of the links talked about arrays/linked list
18:47:36 <barcabouna> it's not liike im not reading everything
18:47:46 <Cale> https://www.dcc.fc.up.pt/~pbv/aulas/linguagens/peytonjones92implementing.pdf -- start with this. It doesn't talk about lists, it talks about even more fundamental details
18:47:55 <barcabouna> Cale: ok it will remain evaluated
18:47:59 <barcabouna> now on to my question
18:48:08 <jared-w> barcabouna: https://haskell.godbolt.org/ look at this really quick
18:48:11 <barcabouna> and this is why it's a question for a core developer so we don't have to guess..
18:48:19 <barcabouna> is why evaluated lists are linked
18:48:21 <barcabouna> ?
18:48:43 <julianleviston> barcabouna: what do you mean when **you** say linked?
18:48:44 <jared-w> It's the assembly code for a sum over array using a list in haskell
18:49:02 <jared-w> implemented using worst case primitive recursion. Worst possible way to implement it
18:49:12 <barcabouna> Cale: im sure that's an interesting paper but my question can be answered in one or 2 phrases
18:49:16 <eschnett> where there is the actual addition happening?
18:49:29 <Cale> barcabouna: Because we're not about to waste time retraversing them and they might only be partly evaluated
18:49:30 <julianleviston> barcabouna: what are you meaning by “linked” when you say linked?
18:49:32 <jared-w> eschnett: you can do primitive addition purely through move instructions 
18:49:43 <jared-w> eschnett: welcome to x86. Black magic is just the beginning :p
18:49:44 <Cale> barcabouna: The minimum amount of evaluation which can happen is for the list to be put into weak head-normal form
18:49:59 <Cale> An expression is in weak head-normal form if:
18:50:17 <Cale> 1) It is a data constructor applied to some expressions
18:50:20 <barcabouna> jared-w: look at 200 lines of asm "real quick"?
18:50:21 <Cale> 2) It is a lambda
18:50:46 <Cale> barcabouna: When we put a list into WHNF, we end up with something of the form []
18:50:46 <jared-w> barcabouna: the website highlights in greek the relevant stuff (the implementation of the actual function)
18:50:50 <Cale> or something of the form (x:xs)
18:51:12 <Cale> i.e. one of the data constructors of the list type, and if it's (:), then the (:) will be applied to some arguments
18:51:44 <Cale> In memory, the data constructors will consist of an integer tag, together with an appropriately sized array of pointers to code for the argument thunks
18:52:11 <monochrom> jared-w: Could you remind me which instruction does addition via moving? Thanks.
18:52:18 <barcabouna> julianleviston: [] <--pointer--> [] <---pointer---> []
18:52:32 <monochrom> To be honest I think I saw it, in GHC-generated asm no less!
18:52:51 <barcabouna> jared-w: ah yeah i read about that, awesome talk. defcon right?
18:52:55 <monochrom> Was totally horrified that it's so CISC-style.
18:53:08 <Cale> Those thunks are code which when entered, will first rewrite the pointer to point at a black hole (which will throw an exception indicating the program is in an infinite loop if entered), then they will compute the value of the expression being represented (to WHNF), and then they will rewrite the pointer to point at code which immediately returns the result (and which is adjacent to the actual value in memory)
18:53:28 <kadoban> x86 or whatever it's called nowadays is only RISC in its microcode I think.
18:53:35 <Cale> before returning it
18:53:40 <Cale> clear?
18:53:59 <Cale> That's what a thunk is -- and they show up whenever you have a bound variable
18:54:08 <Cale> This mechanism is what allows for two things:
18:54:27 <Cale> 1) Lazy evaluation, i.e. these represent possibly-not-yet-evaluated expressions at runtime
18:54:40 <Cale> 2) Polymorphism -- if everything is a pointer to code, it's all the same size in memory
18:55:39 <eschnett> jared-w: i’m not sure ghc is using mov instructions for everything
18:55:40 <julianleviston> barcabouna: ah ok. I thought it was more like: onething, pointerTo 2 …. twoThing, pointerTo 3…. threeThing, pointerTo4 but maybe we mean the same thing
18:56:03 <Cale> If this sounds expensive, imagine how expensive it was in the 1980's
18:56:07 <monochrom> Oh actually the one I saw was not so much "move" as "load effective address".
18:56:40 <monochrom> "compute the address of my_array[my_index] please?"  Now you get your addition.
18:57:05 <Cale> barcabouna: It's this rewriting of the pointer to code to be a pointer to a smaller bit of code which immediately returns the computed result which is what we mean when we say that stuff "remains evaluated"
18:57:10 <barcabouna> julianleviston: that's exactly what i mean
18:57:26 <barcabouna> [onething] ---pointer---> [twothing]
18:57:31 <barcabouna> only i used a doubly linked list
18:57:34 <barcabouna> cause why not
18:57:42 <barcabouna> but ok if its' a single linked list
18:57:44 <Cale> barcabouna: There are some alternate approaches to implementing laziness. We could also use a boolean flag, and then either a pointer to code, or a pointer to the computed result
18:58:05 <barcabouna> Cale: ok that's not an issue
18:58:06 <Cale> barcabouna: but it turns out that testing the boolean flag all the time ends up being worse than just always jumping into the code
18:58:21 <barcabouna> hmmm
18:58:48 <barcabouna> ah ok 
18:58:49 <Cale> (however, there are cases where it's convenient, so in modern ghc, there's also pointer tagging)
18:58:51 <barcabouna> this makes some sense
18:59:01 <barcabouna> though i really have a hard time following all of you
18:59:06 <barcabouna> this is the first answer ive gotten
18:59:08 <barcabouna> so you're saying
18:59:19 <barcabouna> - our language is 99% of the time lazy
18:59:22 <jared-w> eschnett: you're right, it's not. I was thinking of something else when I said you can implement addition with move. I got a few wires crossed by accident when I was recalling the low-level computing class I took last quarter
18:59:36 <julianleviston> barcabouna: yeah, singly-linked
18:59:44 <Cale> Pointers are aligned to multiples of 4 or 8 and if the thing is already evaluated, the low order bits get reused to store the tag for which constructor it evaluated to, so you can find out just by masking them off
19:00:00 <barcabouna> - we could use either array or linked list for thunks. if thunks were pieces of code and we had an array we would have to count up the index to the end. to do this we would have to check we're not out of bounds
19:00:18 <barcabouna> - if we use a linked list then we don't have to check we're out of bounds and just keep jumping until the end
19:00:20 <barcabouna> ...but
19:00:24 <julianleviston> barcabouna: you kinda get to choose if it’s lazy, actually. You can force strictness
19:00:25 <barcabouna> what is the end?
19:00:35 <barcabouna> don't we have to check if the end is a null pointer?
19:00:54 <Cale> barcabouna: also, yeah, that's true -- GHC doesn't always strictly adhere to lazy evaluation -- in fact, 99% is pushing it. There's a lot of stuff GHC does to analyse when it will certainly need the result of an evaluation eventually and do the evaluation sooner
19:01:07 <Cale> the end of the list is another integer tag
19:01:51 <barcabouna> julianleviston: yeah but we're designing the language and we have lazy-by-default so it makes sense to make that a priority
19:02:03 <barcabouna> if it was strict i think linked lists would not be default
19:02:04 <Cale> Like, if it's a cons-cell, you'll have a 1 in memory, followed by the two pointers to code for the element and tail
19:02:12 <Cale> and if it's a nil, you'll have a 0
19:02:37 <barcabouna> Cale:  maybe there doesn't need to be a check for null pointer because of that special rewriting technique
19:02:53 <Cale> In general, for an arbitrary algebraic data type, each of the constructors will have its own integer tag
19:03:00 <barcabouna> maybe everytime a thunk is added the last pointer is updated to have a code return section or something
19:03:07 <Cale> barcabouna: well, it can't be a null pointer, because stuff is going to jump there
19:03:19 <Cale> and so you'd start trying to execute code at location 0
19:03:24 <Cale> which would cause a segfault
19:03:31 <codygman> I've found myself recently needing a "lookup" that is fuzzy and have a solution, but was wondering if there might be a better one. I'm using find as such: find ((`any` ["needle1","needle2"]) . flip isInfixOf) ["xneedle1x"]
19:03:51 <Cale> barcabouna: oh, I suppose it could if you were going to do an additional test
19:04:05 <Cale> barcabouna: but yeah, we don't want to do that test, and it wouldn't make sense for many other data types
19:04:08 <codygman> or if you prefer: find (\s -> any (`isInfixOf` s) ["needle1","needle2"] ) ["xneedle1x"]
19:04:11 <eschnett> jared-w: ah, it’s this line: “jmp base_GHC.Num_+_info”
19:04:14 <barcabouna> [thunk] ---> [thunk] --> .... [last thunk]
19:04:15 <Cale> barcabouna: What I'm talking about here actually doesn't only apply to lists
19:04:23 <Cale> barcabouna: this is how all algebraic data types work
19:04:33 <jared-w> eschnett: yup, that's the one. I should've pointed that out earlier, m'bad
19:04:34 <barcabouna> [last thunk] --> ?
19:04:35 <Cale> You have an integer tag
19:04:48 <barcabouna> how do you know when you're out of thunks
19:04:51 <Cale> and that's right next to thunks for each of the arguments of the constructor
19:05:03 <Cale> and you know how many arguments because type system
19:05:29 <Cale> So, e.g. a cons cell would look like
19:05:52 <Cale> [1][pointer to code for first element of list][pointer to code for rest of list]
19:06:03 <Cale> and the empty list would just look like
19:06:04 <Cale> [0]
19:06:24 <jared-w> eschnett: replace the function with "sumOverArray xs = foldr (+) 0 xs" to see how the compiler changes the code
19:06:38 <barcabouna> Cale: what is [][][] ?
19:06:46 <barcabouna> is it a thunk or the whole thunk linked list?
19:06:47 <Cale> each of those is a word in memory
19:06:58 <barcabouna> ok so it's a small array
19:07:00 <barcabouna> it's a thunk
19:07:01 <Cale> and this is just one (x:xs)
19:07:06 <Cale> not a whole list
19:07:22 <barcabouna> so every thunk has a pointer to the beginning of the list
19:07:23 <Cale> the whole list is going to be spread throughout memory, and is made up of bits of code and these data structures
19:07:24 <barcabouna> alright
19:07:31 <Cale> nono
19:07:43 <Cale> Sorry, each cons cell is the beginning of a particular list :)
19:07:43 <barcabouna> aaahh
19:07:58 <barcabouna> earch thunk has a pointer to its own code and a pointer to the rest of the list
19:08:01 <barcabouna> ok ok
19:08:06 <barcabouna> so the thing is this Cale
19:08:07 <Cale> each cons cell does
19:08:10 <Cale> not each thunk
19:08:28 <barcabouna> the reason linked list can be better for this lazy language is because instead of bounds checking an array we keep jumping code
19:08:34 <Cale> A thunk is just a pointer to code and possibly data, which is used with a particular convention
19:08:37 <barcabouna> that is the only reason we have thought of so far
19:08:57 <barcabouna> so instead of making a check we just jump
19:09:11 <barcabouna> it's debatable how much improvement that is but maybe it is a lot i have never tried to compare
19:09:20 <Cale> and the idea is that this is how we represent a possibly unevaluated expression at runtime
19:09:29 <barcabouna> but in order for this to happen
19:09:33 <Cale> If the thunk hasn't yet been evaluated, the data is not yet available, and it's just a pointer to code
19:09:34 <barcabouna> we need to never make a check
19:09:39 <barcabouna> except the end
19:09:46 <Cale> The first time we enter that code, we do the following
19:09:52 <barcabouna> so how do we know when we've reached the end of the linked list of ponters?
19:09:59 <Cale> 1) we rewrite the thunk pointer to point at a black hole address
19:10:17 <Cale> (because if it's re-entered, our program is definitely in an infinite loop, so might as well break out)
19:10:20 <barcabouna> because if the answer is "at the end of the list of pointers is a pointer back to some logic that will take us back into program logic"
19:10:23 <barcabouna> then ok
19:10:25 <Cale> 2) we compute the value of the expression
19:10:30 <barcabouna> if the answer is a null pointer then not ok
19:10:42 <Cale> 3) We then rewrite the pointer to point at code which will return the result immediately
19:10:50 <Cale> 4) We return a pointer to the result.
19:11:09 <Cale> and the result will be one of these integer constructor tags followed by thunks for the arguments to the constructor
19:11:23 <Cale> good?
19:11:27 <Cale> That's what a thunk is
19:11:33 <Cale> at least in GHC
19:11:49 <Cale> "Thunk" is the general term for any data structure used to represent an expression at runtime.
19:12:06 <barcabouna> this code jumpoing technique is pretty good
19:12:20 <barcabouna> what lies in the last chunk?
19:12:25 <barcabouna> thunk
19:12:36 <Cale> So, eventually, we'll reach the end of the list
19:12:52 <Cale> and the thing which gets returned will be a pointer to an integer constructor tag as usual
19:12:57 <Cale> but that constructor tag will be 0
19:13:04 <barcabouna> so?
19:13:07 <Cale> and so we'll know it's a representation for [] and not (x:xs)
19:13:14 <Cale> and so there won't be arguments to match
19:13:19 <barcabouna> then there is an issue 
19:13:31 <Cale> Consider a different data type for a moment
19:13:42 <Cale> data Tree a = Leaf a | Branch (Tree a) (Tree a)
19:13:50 <Cale> Here, we'll always have at least one argument
19:13:57 <barcabouna> ok
19:14:23 <Cale> Either we'll get the integer tag 0, with one thunk for the value of type a
19:14:23 <Cale> Or we'll get 1, with two thunks for the subtrees
19:14:39 <barcabouna> ok
19:14:41 <Cale> If there were more data constructors, we'd use larger integer tags
19:14:49 <Cale> Lists are basically defined like:
19:14:55 <Cale> data List a = Nil | Cons a (List a)
19:15:04 <barcabouna> yes
19:15:07 <barcabouna> what is Nil
19:15:42 <Cale> The empty list
19:15:51 <barcabouna> Cale: you see if Nil is 0 we will need to check if it is zero at eatch linked list step. if this happens, then nothing is gained in comparison to array bounds checking
19:15:57 <barcabouna> but
19:15:59 <barcabouna> i envision this
19:16:06 <barcabouna> if Nil is actually a pointer back to some code
19:16:11 <Cale> You can't actually write Nil, but if you wanted to define your own list data type which is the same as the one which has built-in syntax, you could do it like that
19:16:12 <barcabouna> then we will have no issue
19:16:27 <Cale> Well, Nil itself won't be a pointer to code
19:16:45 <Cale> It'll be an integer tag (0) and since it has no arguments, there are no further pointers to code which come with it
19:17:08 <barcabouna> so we always check the integer tag
19:17:16 <Cale> That's what pattern matching does
19:17:16 <barcabouna> before jumping code
19:17:20 <Cale> It looks at the integer tag
19:17:30 <Cale> and then binds the variables in the pattern to the thunks
19:17:31 <spehn> word of the day: circumlocution
19:17:46 <Cale> e.g. when you have a function declaration like
19:17:49 <Cale> f [] = ...
19:17:52 <Cale> f (x:xs) = ...
19:18:05 <Cale> What will happen when f is applied to some expression for a list
19:18:12 <Cale> is that the thunk for that expression will be entered
19:18:22 <Cale> and it will eventually return a pointer to an integer tag
19:18:27 <Cale> telling us which pattern matches
19:18:40 <Cale> and then if that integer tag is 1, i.e. it's a cons cell
19:18:50 <Cale> then x will be bound to the first thunk following that 1
19:18:54 <Cale> and xs will be bound to the next
19:19:12 <Cale> and so the evaluation of x and xs can wait indefinitely until we need to examine them further
19:19:35 <Cale> if the integer tag is 0, then the [] pattern matches
19:19:41 <Cale> and we don't bind any variables
19:19:51 <barcabouna> im not sure we have it then
19:19:55 <barcabouna> i certainly don't
19:20:10 <Cale> Okay, what's still confusing?
19:20:23 <barcabouna> what makes a linked list a more performant data structure for this sort of evaluation?
19:20:36 <barcabouna> i thought it was because there was no bounds chcking
19:20:45 <Cale> Okay, so a lot of functions are recursive, right
19:20:47 <Cale> ?
19:20:49 <barcabouna> yes
19:21:00 <Cale> We expect that f will be applied somehow to xs
19:21:00 <barcabouna> at least in functional world
19:21:19 <Cale> So we might emit some part of our result and proceed with f xs
19:21:26 <Cale> let's just imagine we're writing map
19:21:29 <Cale> map f [] = []
19:21:34 <Cale> map f (x:xs) = f x : map f xs
19:21:36 <barcabouna> yeah 
19:21:45 <barcabouna> write [do f on x] and proceed
19:21:51 <Cale> So, in this case, we're able to compute our cons cell for the resulting list
19:21:58 <Cale> and it will be applied to two thunks
19:22:00 <Cale> one for f x
19:22:03 <Cale> and the other for map f xs
19:22:12 <Cale> We will return *immediately*
19:22:23 <barcabouna> so write [do f on x] ---> [do map f on xs]
19:22:32 <Cale> because the thing which wanted to do this evaluation is probably trying to match the result against [] or (t:ts)
19:22:40 <Cale> write
19:22:42 <barcabouna> and then we return immediatly ok
19:22:46 <Cale> [1] [f x] [map f xs]
19:23:00 <Cale> where each of the things in brackets is one machine word
19:23:15 <Cale> (and the latter two are pointers to code, the first is a physical integer)
19:23:20 <barcabouna> ok
19:23:34 <barcabouna> and this is one thunk
19:23:56 <barcabouna> (also what's the 1?)
19:24:06 <Cale> the integer tag which tells us it's a (:) rather than a []
19:24:06 <barcabouna> so do we write this thunk in a linked list or in an array?
19:24:29 <Cale> You can think of this as an array
19:24:33 <Cale> these are next to each other in memory
19:24:35 <barcabouna> ok so the integer tag tells us which is the last thunk
19:24:43 <Cale> (and also next to the easy code which immediately returns this)
19:24:46 <barcabouna> if this was an array we wouldn't even need that extra tag
19:24:57 <barcabouna> for lists. for trees i dunno
19:24:59 <Cale> which is the last list cell
19:25:01 <Cale> rather
19:25:21 <Cale> The integer tag tells us which of the data constructors of the type we have
19:25:23 <barcabouna> no no
19:25:27 <Cale> So if I make up some data type like
19:25:30 <barcabouna> do we put these 3 words in an array slow
19:25:34 <barcabouna> or in a linked list slot
19:25:40 <barcabouna> not are these 3 words in a linked list
19:25:46 <barcabouna> i know they're contiguous memory
19:26:06 <Cale> Well, the "linked list" is very tenuously held together in memory by these pointers to code
19:26:14 <Cale> It's not an honest C linked list
19:26:39 <barcabouna> well all these thunks are linked are they not?
19:26:45 <Cale> uh
19:26:55 <barcabouna> or are they an array?
19:26:59 <Cale> Well, the thunk for a cons cell, when you evaluate it
19:27:05 <Cale> gives you a pointer to the array
19:27:14 <barcabouna> yeah one thunk is an array
19:27:16 <Cale> [1] [thunk: x] [thunk: xs]
19:27:18 <barcabouna> the whole list of thunk
19:27:20 <Cale> nono
19:27:24 <Cale> the thunk is a pointer to code
19:27:31 <Cale> this array is something which is not a thunk
19:27:35 <barcabouna> pretend you have many thunks
19:27:39 <barcabouna> where do you store them?
19:27:44 <Cale> (but it contains thunks)
19:28:03 <Cale> okay?
19:28:22 <barcabouna> ah ok i was under a different impression
19:28:25 <barcabouna> so those are 3 thunks?
19:28:27 <barcabouna> actually 2
19:28:42 <barcabouna> [int][thunk][thunk]
19:28:49 <barcabouna> this triplet
19:29:01 <barcabouna> does it get linked with something else? 
19:29:10 <Cale> In general, there will be an integer tag, and then however many thunks make sense for that data constructor
19:29:17 <Cale> (it will match the number of arguments of the data constructor)
19:29:40 <barcabouna> say i have evaluated a whole lazy list and i don't want to throw it away
19:29:44 <Cale> So since (:) takes two arguments, you get 2 thunks
19:29:48 <barcabouna> what thunks do i have?
19:29:51 <Cale> and [] takes 0, so you get none
19:30:14 <Cale> With (:), you get the thunk for the first element of the list, and then the thunk for the rest of the list
19:30:29 <barcabouna> ok 
19:30:39 <Cale> If the rest of the list is the empty list, when you enter its thunk, you will eventually get back a pointer to the integer tag 0
19:30:49 <barcabouna> ok
19:31:13 <barcabouna> so in an ideal context we only ever have at most 3 words occupied 
19:31:15 <Cale> If it's a nonempty list, when you enter the thunk, you'll get back a pointer to the integer tag 1, and then two more thunks
19:31:17 <barcabouna> (in this case)
19:31:20 <Cale> yeah
19:31:30 <julianleviston> barcabouna: oh cool! I didn’t realise you were designing a language
19:31:35 <barcabouna> as long as this is lazy
19:31:35 <Cale> When we're done with any one of these list cells, it might be thrown away
19:31:42 <Cale> yeah
19:32:11 <Cale> Now, if we retain a pointer to the first cons cell, then we'll have no choice but to keep all this allocated
19:32:23 <barcabouna> exactly
19:32:27 <Cale> and that will use much more memory than an array would, and traversing it would be much slower
19:32:28 <barcabouna> julianleviston: ?
19:32:40 <barcabouna> Cale: exactly
19:32:49 <julianleviston> barcabouna: you said you were designing the language. All good.
19:32:49 <barcabouna> i see you're following me
19:32:52 <Cale> However, we can do lots of really cool stuff
19:32:57 <barcabouna> julianleviston: when?
19:33:07 <barcabouna> i said i wrote an interpreter some time ago
19:33:14 <Cale> Often, intermediate lists will almost not be in memory at all
19:33:16 <barcabouna> and im interested in researching aoutomated parallelism
19:33:23 <julianleviston> barcabouna: 12:01 barcabouna: julianleviston: yeah but we're designing the language and we have lazy-by-default so it makes sense to make that a priority
19:33:24 <barcabouna> never that im designing a language
19:33:40 <Cale> Or perhaps only one cell at a time will be allocated, along with its thunks
19:33:41 <julianleviston> barcabouna: oh sorry, my bad. I misunderstood, probably.
19:33:49 <Cale> and everything else will be garbage collected
19:33:56 <barcabouna> julianleviston: that was like an expression. we use them often in italian to try and relate with who we're speaking
19:34:15 <Cale> barcabouna: does that make sense?
19:34:33 <Cale> There are two disadvantages to arrays:
19:34:54 <barcabouna> Cale: yeah it makes sense
19:35:09 <barcabouna> 95% of the time we don't expect to keep evaluated stuff in memory
19:35:11 <Cale> 1) We have to allocate all of their memory at once, can't incrementally allocate one cell at a time, because that gives up on being able to do random access quickly
19:35:51 <barcabouna> but what if we do have to keep evaluated stuff in memory and we end up with a non-lazy 100-item data structure. why do we choose linked list?
19:35:55 <Cale> 2) We can never garbage collect the start of an array if we're done with processing it, but still need the later part, because the code always has the option of indexing the earlier elements
19:36:10 <Cale> barcabouna: Because the programmer was lazy and made a bad decision
19:36:22 <Cale> well, 100 items is pretty small, you might just not care
19:36:35 <barcabouna> ok so maybe ...
19:36:42 <barcabouna> see you left out the pointer part
19:36:45 <barcabouna> like python does
19:36:48 <barcabouna> but even then
19:37:01 <barcabouna> if you're going to be deleting from the head...you'd still have to rearrange all pointers
19:37:10 <barcabouna> so maybe that is the reason
19:37:11 <Cale> What do you mean?
19:37:17 <Cale> You mean with arrays?
19:37:21 <barcabouna> yeah
19:37:26 <barcabouna> not arrays of objects
19:37:35 <barcabouna> array of pointers to objects
19:37:36 <Cale> Well, to know that you can delete the early part of an array because you're done with it
19:37:44 <Cale> you need some way to scrutinize what's in memory
19:37:51 <Cale> and prove that it'll never again be needed
19:38:04 <barcabouna> in theory garbage collecting the single objects is still O(1) though
19:38:09 <Cale> But because there's a function which can take a pointer to the array and an index, and just look up that index
19:38:12 <Cale> it's hard to prove that
19:38:13 <barcabouna> but updating that list isn't 
19:38:17 <EvanR> the free operation isnt necessarily cheap
19:38:44 <barcabouna> but if you're actually popping stuff from the head why do you even have a whole allocated array?
19:38:48 <EvanR> mark and sweep can be much more efficient bulk-wise
19:38:50 <Cale> You also won't easily be able to prove that the *elements* of the array won't be needed again, for the same reason
19:38:53 <monochrom> "single object" is in contradiction with "but can't you free up 1/3 of it?"
19:40:07 <Cale> Now, if we had an array data type which, say, had a slice operator built in at a low level, that could be implemented by a pointer to the original array with the slice indices, but the GC *knew* about this, then maybe the GC could reallocate the slice, if that was your last reference to the original array
19:40:18 <Cale> But that's fancy, and I don't know that any language implementation actually does that.
19:40:27 <barcabouna> monochrom: we have an array of pointers though
19:40:37 <barcabouna> not a single array of objects
19:40:52 <barcabouna> so we can sweep up the thunks just as quickly
19:40:59 <barcabouna> the only part that is left is the pointers
19:41:05 <Cale> thunks are pointers
19:41:06 <monochrom> The array is the single object I'm talking about. I don't care that you don't even put things inside.
19:41:17 <barcabouna> but if we're throwing away everythings as soon as we evaluate it then we don't even need awhole array
19:41:49 <Cale> barcabouna: so, choosing the right data structure is important
19:42:09 <Cale> lists are valuable for this kind of streaming access pattern which allows us to iterate over the elements
19:42:20 <julianleviston> What is an ωCPO ?
19:42:21 <monochrom> Actually nevermind me. GC doesn't care, that's the important part.
19:42:57 <Cale> which is very very commonly what you want, but it's weirdly in cases where you're not really thinking of the list as some sort of resident data structure so much as just a way of expressing a loop over a bunch of things
19:43:30 <julianleviston> Ah I think I found out, kinda: https://en.wikipedia.org/wiki/Complete_partial_order
19:43:31 <Cale> and it's possibly hard to imagine all these cases if you haven't been writing Haskell code for a long while
19:43:40 <barcabouna> hmm
19:44:07 <Cale> But we have lots of idioms of expression related to the processing of lists
19:44:27 <Cale> and they don't usually translate very well to arrays
19:44:52 <Cale> and also!
19:44:55 <Cale> There are rewrite rules
19:45:15 <Cale> which transform composites of functions on lists to eliminate the construction of intermediate lists altogether
19:45:18 <Cale> for example
19:45:23 <Cale> if you write  map f (map g xs)
19:45:39 <Cale> the compiler will rewrite that to (something equivalent to)  map (f . g) xs
19:45:50 <barcabouna> so no arrays because nobody wants people to slice stuff and access direct indexes?
19:45:52 <Cale> and the list  map g xs  will never actually exists
19:45:53 <Cale> -s
19:46:09 <Cale> Well, there are a lot of cases where you don't care about slicing and random access
19:46:11 <EvanR> but the list might not exist anyway
19:46:15 <Cale> If you do, then by all means, use arrays
19:46:20 <barcabouna> yeah there are 
19:46:26 <Cale> (or Vector, which is a really nice array library)
19:46:26 <barcabouna> ok yeah
19:46:50 <Cale> Or Map
19:46:51 <Cale> :)
19:47:07 <Cale> Actually, you can go a long way not caring about anything but Map and Set and lists
19:47:16 <kadoban> I'm quite a bit fan of Data.Array. It's insanely nice being able to have so much freedom of indexing strategies.
19:47:23 <monochrom> I use the vector library all the time.
19:47:24 <Cale> oh, yeah
19:47:27 <kadoban> I miss that every time I don't use haskell.
19:47:30 <barcabouna> so basically linked lists are used for lazy [1,2,3] because developers never expect people to not use recursion (or similar patterns) and they expect to be able to throw away stuff with O(1)
19:47:34 <Cale> That's one thing I wish Vector carried along from the old array library
19:47:41 <Cale> Ix is damn cool
19:47:52 <Cale> At least, I was impressed by it as a beginner
19:48:02 <Cale> "Why don't all languages do this?" I thought
19:48:08 <kadoban> I know, right?
19:48:10 <Cale> and then everyone proceeded to ignore it :)
19:48:25 <kadoban> :(
19:48:33 <barcabouna> Cale: i hear you. in python dictionaries are a god send. hash maps are my favorite data structure
19:49:25 <monochrom> Pascal (probably Algol too) was a bit flexible like that. You can ask for "array(3..10)" or similar syntax.
19:49:34 <barcabouna> Ix?
19:49:47 <monochrom> And I forgot whether the index type could even be user-defined enumeration type.
19:50:01 <monochrom> Probably Ada too.
19:50:21 <kadoban> It lets you use 1-based arrays or 0-based arrays or 'C' based arrays, or 3 dimensional arrays starting with the first 3 primes for each coordinate or whatever the hell you want.
19:50:25 <kadoban> (Ix)
19:50:32 <monochrom> Basically all the mainstream languages avoid this power because the mass is too dumb?
19:51:04 <barcabouna> what?
19:51:06 <kadoban> I guess because it implies an extra math operation per lookup, maybe, and that's too much? I dunno, it's weird to me.
19:51:23 <barcabouna> primes for each coordinate?
19:51:32 <barcabouna> so it's a multidimensional array
19:52:23 <kadoban> The last one was a bit facetious. But mostly it's just an array that you get to choose how you index into.
19:52:34 <barcabouna> how does it work? hackage says it's used for indexing
19:52:52 <monochrom> I don't think it's about performance. Basically every array lookup incurs extra math operation already, unless the element type is single-byte.
19:53:13 <monochrom> In other words, address of a[i] is already a + i*4 or something.
19:53:33 <monochrom> Plus! The optimizer people already knew how to mitigate this 40 years ago.
19:53:46 <kadoban> I think it's about conceptual performance. So much in computing seems to come down to nobody wanting to do anythnig that even *looks* like it could be extra work for the computer, regardless of how small or nonexistant it actually is in practice.
19:54:43 <kadoban> Otherwise I just have no explanation. Maybe historical reasons? *shrug*
19:55:34 <monochrom> If you have "for i=1 to 10 ... a[i] ...", they have a "move the loop invariant outside" optimization, and maybe plus a couple of simple constant folding, that compiles your loop to: "p := &a; for i=1 to 10 ....  p ... p:=p+4". No extra math operation at all.
19:58:47 <monochrom> At the end, optimizing "array(3..10)" is no more work than optimizing vanilla Fortran arrays. It is not rocket science.
19:59:43 <monochrom> To be sure, Haskell doesn't quite do that optimization.
20:00:02 <monochrom> But your professional Ada compiler likely does it.
20:01:40 <Cale> It would be nice to have an array library which combines all the nice things about Vector with an Ix or Ix-like mechanism, and then makes sure that some of these optimisations actually happen.
20:02:05 <monochrom> I think the history angle is that to a large extent people looked upon C rather than Algol as their role model.
20:02:37 <kadoban> Cale: I need to figure out sometime what Vector does better
20:02:44 <Cale> kadoban: Stream fusion
20:02:50 <monochrom> Irrational, but recall that once upon a time C had this commercial success glamour that for example today's Java and C# have.
20:02:56 <kadoban> Ah. Array isn't so good at that?
20:03:02 <Cale> It does none of it
20:03:11 <Cale> and there aren't so many collective operations on Array anyway
20:03:41 <julianleviston> monochrom: I like that… “commercial success glamour” :-)
20:03:54 <kadoban> Ah, I see. I'll have to think about what that would mean for my uses. I should just use vector for a while and see
20:04:05 <kadoban> monochrom: Yeah, that's always seemed unfortunate to me.
20:04:10 <Cale> It also kills me that ByteString doesn't still have stream fusion
20:04:37 <jared-w> really? It doesn't? You'd think that /bytes/ would be the first to be fused...
20:04:41 <mnoonan_> :t forever
20:04:42 <lambdabot> Applicative f => f a -> f b
20:04:48 <mnoonan_> ^ what’s the deal with the ‘b’ there?
20:04:53 <monochrom> PL/I almost could be that role model. It was for a while, with IBM behind it and whatnot, before C stole the thunder. But PL/I was really poorly designed, people couldn't really stand it, despite their love of IBM.
20:04:58 <Cale> jared-w: It did when first released
20:05:00 <kadoban> What actually goes into implement fusiony stuff? Is it just a bunch of rewrite rules, or is there more to it?
20:05:05 <Cale> But the code for doing that hasn't been maintained
20:05:11 <jared-w> aww, that sucks
20:05:58 <Cale> kadoban: It's careful rewrite rules and conversions to/from a data type which lets you represent many collective operations without requiring recursion.
20:06:16 <kadoban> monochrom: An oldschool coder I worked with for a while once got into a PL/I rant. It seemed really raw even after the passage of time xD
20:06:37 <jared-w> PL/I?
20:07:22 <Cale> https://pdfs.semanticscholar.org/64d2/a65a7d559f9b05570fb0fea8bb4cccd83ae2.pdf -- good paper about stream fusion
20:07:26 <monochrom> When I was in school, the prof used to joke about how "if if=then then then=else else else=if" was legal in PL/I
20:07:52 <kadoban> Cale: Thanks, I'll take a peek
20:08:01 <kadoban> monochrom: Nice
20:08:22 <jared-w> That's some next level beautiful right there
20:08:36 <monochrom> Even Sun and Microsoft don't dare to do that when it's their turns :)
20:08:59 <Cale> kadoban: That was the one for lists... there was an earlier one for ByteString...
20:09:25 <juri_> 1e1 ~= 10e0, right?
20:09:26 <kadoban> I'm sure I'll learn from any of them. My knowledge on the subject is obviously extremely lacking
20:09:37 <Cale> http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=3C72D6AF8D98382AF32F612695EB2219?doi=10.1.1.90.3166&rep=rep1&type=pdf
20:09:41 <Cale> yeah, this thing :)
20:09:43 <monochrom> In principle, nothing says you can't bring the same stream fusion rewrite rules to Data.Array. But what Aesop said about the mouse community planning to tie a bell to the cat.
20:09:59 <jared-w> juri_: they are exactly the same
20:10:01 <Cale> that one might even be easier to browse through :)
20:10:04 <Cale> it's the same idea
20:10:08 <kadoban> Cool
20:10:11 <juri_> jared-w: thanks. :)
20:10:11 <monochrom> > 10e1 
20:10:13 <lambdabot>  100.0
20:10:14 <skiddieproof> Why does (fix (\x -> x) 3) = 6, and (fix (\x -> x) 12) = 132. What is fix doing here?
20:10:14 <monochrom> err
20:10:16 <monochrom> > 10e0
20:10:18 <lambdabot>  10.0
20:10:22 <monochrom> > 1e1
20:10:25 <lambdabot>  10.0
20:10:27 <monochrom> Yes!
20:10:32 <Cale> plus, you get to watch Haskell from 2007 beating simple C code :)
20:11:19 <jared-w> inb4 "but C is so fast bro"
20:11:21 <monochrom> > fix (\x -> x) 3
20:11:21 <julianleviston> What would the expression “when writing functions ∈ A* -> B” mean? (context is the bananas , etc. recursion schemes paper)
20:11:27 <lambdabot>  mueval-core: Time limit exceeded
20:11:34 <monochrom> Cannot reproduce.
20:12:19 <monochrom> Perhaps A* means [A]?
20:12:22 <julianleviston> Like… is A -> B considered a set? or is this category theory stuff?
20:12:22 <Cale> julianleviston: I'm going to guess that A* is lists of elements of type A
20:12:28 <kadoban> Functions of that type maybe? I'm not sure what A* means
20:12:32 <monochrom> A set.
20:12:33 <julianleviston> yeah I think A* means [A] in haskell (A* is Squiggol)
20:12:56 <monochrom> I think it's safe to read it as "when writing functions of type [A]->B"
20:12:59 <Cale> and ∈ means "element of"
20:13:03 <julianleviston> Ah okay...
20:13:04 <Cale> so yeah, "of type"
20:13:05 <jared-w> Someone really needs to rewrite that paper using programming syntax
20:13:25 <julianleviston> Cale: yeah, the element of / member of symbol threw me… I guess it’s a category?
20:13:44 <monochrom> But if you pull yourself away from specific programming languages, sometime's it's OK to use sets instead of types.
20:13:50 <Cale> julianleviston: You're just thinking of the type as the set of things of that type
20:13:50 <julianleviston> jared-w: well I think it is, it’s just using a language that no one has used in a while so it’s difficult to find details of what the syntax is
20:14:17 <jared-w> julianleviston: fair. I would take "a lookup table of wtf the syntax does" as a nice consolation prize
20:14:32 <julianleviston> Cale: ahh so [Int] -> String would be (member) of [A] -> B ?
20:14:42 <julianleviston> jared-w: totally agreed!
20:14:49 <monochrom> Like "3 ∈ ℕ" instead of "3 :: Natural"
20:15:51 <julianleviston> Thanks
20:16:15 <kadoban> skiddieproof: Out of curiosity, where'd you get those? I was all excited until they didn't seem to work.
20:17:23 <juri_> > 1e1
20:17:25 <lambdabot>  10.0
20:17:27 <skiddieproof> kadoban: I am using intero on emacs, windows 10
20:17:30 <juri_> > 1e+1
20:17:32 <lambdabot>  10.0
20:17:42 <juri_> > 10e-0
20:17:44 <lambdabot>  10.0
20:17:48 <kadoban> skiddieproof: Oh and that's the outputs you get? Hmm.
20:17:49 <juri_> > 10.0e-0
20:17:51 <lambdabot>  10.0
20:18:30 <monochrom> Windows 10 can find non-least fixed points!
20:19:04 <monochrom> More seriously I doubt that it has anything to do with intero, emacs, or Windows 10.
20:19:44 <monochrom> You very likely have loaded some code that defines another fix.
20:19:58 <kadoban> Is there some Num instance that'd make that true? Kind of confused how that would be. Oh ... that's a more sane idea.
20:20:56 <monochrom> @type fix (\x -> x)
20:20:58 <lambdabot> a
20:21:07 <monochrom> @type fix (\x -> x) 3
20:21:08 <lambdabot> t
20:21:24 <kadoban> Hmm, that's a good point.
20:22:50 <skiddieproof> monochrom: reloaded emacs and now I get 'variable not in scope: fix :: (t0 -> t0) -> Integer -> t
20:23:03 <julianleviston> So how would I read ⨁ ∈ A || B -> B ?
20:23:18 <jared-w> ahh, yup. You had a different fix loaded up somehow, skiddieproof 
20:23:32 <kadoban> A || B is maybe a sum type of A and B ?
20:23:38 <monochrom> (⨁) :: Either A B -> B, maybe? I am not entirely sure.
20:23:46 <julianleviston> That’s wht I’m not sure about… but yeah fair enough
20:24:00 <jared-w> julianleviston: to be quite honeset, any question you ask us about this paper is, 90% chance, going to result in wild guessing
20:24:16 <kadoban> At least it's vaguely consistent guessing so far.
20:24:17 <julianleviston> it’s talking about list-catas, so fold here, I’m pretty sure.
20:24:40 <jared-w> It's known as one of the more impetnerable papers. What we really need is a symbol "decoding" somewhere
20:24:49 <julianleviston> ohhhh
20:25:20 <julianleviston> I think it’s probably the fact that in Haskell a -> b means a and b can be the same or different types, but in this language, it’s explicitly saying “hey you can use an a or a b”
20:25:36 <julianleviston> that’s cool...
20:26:11 <jared-w> seems weird to use an arrow as an or... 
20:26:23 <julianleviston> I dont think the arrow was the or.
20:26:48 <julianleviston> it means (+) is a function from type a to b, or from type b to b (I think)
20:26:58 <julianleviston> yeah sorry for the questions, but your answers are really helpful
20:27:12 <jared-w> np, I'm finding it entertaining at the least :p
20:27:24 <monochrom> No, ⨁ is unlikely to be a unary function name. The type is unlikely the unary Either A B -> B.
20:27:46 <julianleviston> it seems to be a binary function to me.
20:27:55 <kadoban> Is that somehow  :: A -> B -> B  ?
20:28:03 <monochrom> or (A,B) -> B
20:28:06 <julianleviston> kadoban: yeah that’s what I reckon…
20:28:15 <kadoban> Oh, yeah (A, B) -> B sounds saner
20:28:34 <julianleviston> but in squiggol, a and b means “they have to be different” whereas in haskell it doesn't.
20:28:39 <julianleviston> (is my assumption)
20:29:17 <julianleviston> this confused me at the very beginning of learning haskell, TBH. I always assumed things like const would **require** the types to be different
20:29:18 <julianleviston> :t const
20:29:19 <lambdabot> a -> b -> a
20:29:23 <julianleviston> :t const “a”
20:29:24 <lambdabot> error: lexical error at character '\8220'
20:29:28 <julianleviston> :t const 1
20:29:29 <lambdabot> Num a => b -> a
20:29:44 <monochrom> I don't think squiggol requires a and b to be different either.
20:29:46 <Cale> julianleviston: I... I don't think that's correct.
20:29:49 <julianleviston> so obvliously I can’t pass a Num a => a into there, right? lol (I know I can, it’s just me at the beginning of learning haskell’s thoughts)
20:29:53 <Cale> Yeah, they shouldn't need to be different
20:29:55 <julianleviston> Oh
20:30:33 <julianleviston> Then i’m stumped as to what (+) (member) A || B -> B means in page 3 of this paper. (sorry for the terrible nomenclature)
20:32:19 <Cale> A || B is the type of pairs, in Haskell, it'd be (a,b)
20:32:37 <monochrom> \∩/
20:32:56 <Cale> :t foldr
20:32:57 <lambdabot> Foldable t => (a -> b -> b) -> b -> t a -> b
20:33:00 <julianleviston> that doesn’t make much sense for their haskell translation which is h = foldr b (+)
20:33:03 <Cale> lol, Foldable
20:33:11 <Cale> See the function (a -> b -> b) ?
20:33:17 <julianleviston> I understand fold
20:33:20 <Cale> If you uncurry that, you get ((a,b) -> b)
20:33:24 <julianleviston> yeah I know
20:33:31 <Cale> which in their notation would be A || B -> B
20:33:39 <julianleviston> I’m just trying to work out if they’re typing in squiggol or category theory notation at that point
20:33:50 <julianleviston> interesting. That doesn’t hold for the rest of their exlanation at that point tho IMO
20:33:57 <julianleviston> explanation*
20:33:58 <Cale> It doesn't?
20:34:31 <julianleviston> unless the folding function for foldr in haskell takes pairs, which it doesn’t.
20:34:32 <Cale> Note that the arguments to foldr are backward from how they are in the Prelude
20:35:02 <julianleviston> oh, is the Bird & Wadler paper not about Haskell? Damn I should look it up.
20:35:03 <julianleviston> thanks
20:35:05 <dresuer> Heil Erlang
20:35:06 <monochrom> Haskell chose to make it A->B->B. But (A,B)->B works in another language. In fact I think SML does that.
20:35:27 <julianleviston> oh it’s a book lol
20:35:51 <Cale> julianleviston: It might be written using Haskell or Miranda, but these things are super early, and conventions like that weren't pinned down so well yet
20:35:57 <mrkgnao> having a LaTeX plugin active in IRC really makes #haskell come alive in weird and wonderful ways
20:36:03 <mrkgnao> (#haskell-lens more so)
20:36:06 <julianleviston> it’s this: https://usi-pl.github.io/lc/sp-2015/doc/Bird_Wadler.%20Introduction%20to%20Functional%20Programming.1ed.pdf
20:36:29 <mrkgnao> for instance, ₂ is a subscript 2
20:36:30 <monochrom> And then there is PVS which uses the notation "[A,B]->B". And it is a type synonym of "A->B->B".
20:36:45 <Cale> julianleviston: I actually rather like that order, just for the fact that when we write the list data type, we tend to write Nil first, and similarly when we're pattern matching
20:36:45 <julianleviston> hm… they seem to have got it wrong
20:37:40 <julianleviston> section 3.5 of the bird/wadler book talks about folding and uses standard haskell syntax
20:37:54 <Cale> yeah
20:37:57 <Cale> looks like it
20:38:02 <Cale> It's just argument order though :)
20:38:07 <julianleviston> yeah
20:38:18 <julianleviston> and argument type ...
20:38:35 <julianleviston> (ie not a tuple to a value, but two curried functions to a value)
20:39:40 <julianleviston> (if the || actually means 2-tuple)
20:39:53 <julianleviston> in any case, I can infer what’s meant. Thanks again :)
20:40:47 <julianleviston> Cale: I think you’re right, by the way, because the second case of “h” (which is the list-cata they’re talking about here) takes a Cons which is using a pair, so thanks)
20:43:41 <julianleviston> all this so I can hopefully undertand Lens.contexts and Lens.para better. I’m still a bit clueless as to what a Context is. It seems to be a ComonadStore, so I guess I better look into what exactly that is, too
20:48:52 <julianleviston> In the package https://hackage.haskell.org/package/recursion-schemes-5.0.1/docs/Data-Functor-Foldable.html is the Base t a -> a part that I see in the type sig of cata… is that the genericised fix point function for whatever data structure is being used in the cata?
20:51:21 <julianleviston> Ah I don’t really undertand that well enough. I’ll keep reading the paper :)
20:52:55 <julianleviston> Then I have to read this and understand it properly: http://blog.sumtypeofway.com/an-introduction-to-recursion-schemes/
20:57:09 <jared-w> Honestly I would read that first
20:57:53 <julianleviston> jared-w: I have.
20:57:54 <jared-w> https://en.wikipedia.org/wiki/Bird%E2%80%93Meertens_formalism ooooh
20:58:09 <jared-w> of course wikipedia has a lookup for all the notation... *sigh* should've looked there first lol
20:58:31 <julianleviston> jared-w:  it does? I’ve seen that page before and I didn’t find it enlightening
20:58:53 <jared-w> oh, actually that page is pretty useless, I didn't look super hard at it
20:59:08 <julianleviston> yes
20:59:24 <julianleviston> it’s definitely helpful to keep those things in mind
21:01:19 <jared-w> http://research.nii.ac.jp/~hu/pub/teach/msp11/BMF.pdf seen this?
21:01:43 <julianleviston> nope.
21:02:05 <julianleviston> that’s great. thanks! added it to my collection of things to read next. :) very useful
21:03:12 <jared-w> https://www.researchgate.net/publication/2631000_An_Exploration_of_the_Bird-Meertens_Formalism what about this?
21:03:33 <julianleviston> shoudl rpobably read the books referenced in that wikipedia article too
21:04:47 <julianleviston> no I haven’t seen that
21:04:57 <julianleviston> so much to learn
21:05:40 <julianleviston> thank you so much by the way!
21:06:30 <jared-w> np, I just googled "bird-meertens formalism" and clicked all the links on the first page
21:06:59 <julianleviston> yeah, I thought so :) you’re google-fu is 1000 times better than mine :)
21:07:15 <jared-w> sure :p
21:07:33 <julianleviston> should have linked me to “allow me to google that for you” lol :) I fail.
21:07:47 <barcabouna> robertkennedy: try something along the lines of this [[{x[j][0]:x[j][1]} for j in range(i, len(x))]+[{x[i][0]:x[i][1]}] for i in range(len(x))] where x is list(dict.items())
21:08:17 <barcabouna> you will not want to do what you are doing in ain imperative language because you will have to keep copying a mutable data type
21:08:30 <barcabouna> so you need to iterate over it the normal way
21:08:45 <barcabouna> but you cannot iterate in a for each so you may use indexes
21:09:14 <barcabouna> you cannot do that realiable until you transform the dictionary to a list 
21:09:22 <jared-w> barcabouna: what did robertkennedy want? I don't have the question in my scrollback history
21:09:34 <barcabouna> this is a basic recursion to iteration btw
21:09:42 <barcabouna> stop thinking about this like haskell
21:10:13 <barcabouna> jared-w: my code does not do what he wants but it is along the lines of what he wants he just needs to tweak it
21:10:29 <jared-w> It would help if I knew what he wanted. Can you re-state it?
21:11:16 <barcabouna> i think he wanted to take a dictionary and print the first item and then print recursively all the items of this function twice
21:11:38 <barcabouna> not twice the same but twice modified
21:11:43 <julianleviston> jared-w: so this || thing doesn’t seem to appear in any of these documents interestingly. I wonder if it’s set theory notation or cat theory notation I’m unaware of
21:11:58 <barcabouna> like go from 1 to 10, then go from 2 to 10, then 3 to 10 etc
21:12:10 <barcabouna> so O(n^2) ?
21:12:31 <jared-w> If you don't know what he wanted, how are you giving him code to show him how to do what he wants? :p
21:12:52 <julianleviston> jared-w: neither does the A* style of notation, unless * is the map (operation) notation, but to be applied to a type seems somewhat odd.
21:12:59 <julianleviston> I’ll just read on :)
21:13:01 <barcabouna> i think i understand what he was trying to do
21:13:18 <barcabouna> if he tries using indexes he will have fast code that does what he wants
21:14:54 <jared-w> Do you have a copy of what he actually wrote down as his question? That's really what I'm missing here
21:23:52 <dmwit> julianleviston: Where do you see ||? Sometimes that is used for logical or, and sometimes for string concatenation.
21:24:36 <barcabouna> lpaste.net/356298
21:24:40 <julianleviston> dmwit: page 3 of the paper bananas, lenses, (etc)
21:25:52 <dmwit> Oh, they use lots of nonstandard notation in that paper (as part of a sort of elaborate joke, I think).
21:26:00 <julianleviston> dmwit: I know what it means thanks to Cale but it all seemed a bit non-specific in the paper. I can infer it definitely means “a value of type (A,B)” if I was to use Haskell syntax, because they’re using pairs as function arguments, and this is the folding function of a cata.
21:26:23 <dmwit> Right.
21:30:29 <julianleviston> dmwit: finding it interesting trying to reconcile this paper with the definition of cata from Ed’s recursion-scheme library I’m sure it’ll become clearer once I understand Fix much better and read more… but yeah :) kinda sucks that I have to spend hours and hours learning this stuff to understand what a couple of functions because the docs are a bit light on / not tailored to people without the background of th
21:30:29 <julianleviston> author. 
21:30:56 <julianleviston> dmwit: specifically, the contexts, cosmos and para functions in here https://hackage.haskell.org/package/lens-4.15.2/docs/Control-Lens-Plated.html
21:31:29 <julianleviston> but it’s actually really good for my general understanding anyway, so :) yay :)
21:47:52 <barcabouna> robertkennedy: nvm you do not need indexes but you will have to check the inner loop value is not the same as the outer loop. also the simple way to do this is to store values as you yield them and then yield the store and then reyield every value in the store. 
21:47:59 <barcabouna> i otherwise suggest you look into itertools
21:48:30 <barcabouna> there is probably some clever way of looping forever but this thing is so recursive i have trouble seeing hte pattern
21:50:04 <threshold> is there an equivalent of LanguageDef for MegaParsec?
22:07:22 <mbw> I have a question about transformers. Say you're inside a function "fun :: [Int] -> IO Int" or something and inside this function, you want to make use of some "Maybe logic". You can write something like "r <- runMaybeT $ do ..." and now you're inside a closure of type MaybeT IO Int. However, how do you make use of functions of type "Maybe a" inside this do block idiomatically? I noted that "MaybeT . return 
22:07:28 <mbw> :: Monad m => Maybe a -> MaybeT m a", but this feels kind of weird, or rather I don't know if this is what people usually write or not. Am I missing some standard function here?
22:08:51 <mbw> Is it clear or should I provide an example?
22:15:31 <mbw> Everyone's gone to the movies it seems.
22:26:30 <julianleviston> mbw: why does the maybe logic have to be in IO? can’t it just be a simple expression? Or do you want to do some maybe thing that uses the IO stuff?
22:27:23 <julianleviston> mbw: … in which case, still, there’s nothing stopping you from grabbing the IO value out and stitching that into your Maybe expression. I guess it depends what you want the Maybe to be doing.
22:29:06 <mbw> julianleviston: This is for pedagogical purposes only, i.e. revisiting transformers in general.
22:30:33 <mbw> julianleviston: That is do say, there are propably situation where you might need nested do blocks, but I don't have anything realistic in mind. The Maybe type was just a contrived example.
22:31:46 <mbw> For instance, something like this https://mmhaskell.com/blog/2017/3/6/making-sense-of-multiple-monads
22:34:25 <mbw> From what I can see, most beginner examples seem to rely on you being able to just change the type signature.
22:43:49 <mbw> Ok I think the problem was just that I tried to come up with too stupid of an example. This isn't hard at all.
22:45:24 <rE-BoOt>  How to access ##security?
22:45:49 <rE-BoOt>  It says: Cannot join channel (+r) - you need to be identified with services
22:46:05 <rE-BoOt>  Whenever I try to join it
22:46:13 <glguy> rE-BoOt: This isn't an IRC support channel, it's for discussing the Haskell programming language
22:55:18 <cocreature> mbw: hoistMaybe from Control.Error.Util does that
22:57:04 <mbw> That's Gonzalez's errors package, right?
22:57:08 <cocreature> yep
22:57:58 <mbw> I guess these are the kinds of "small papercuts" he mentioned.
22:58:10 <cocreature> not sure what you’re referring to
22:58:32 <mbw> It's a citation from the errors documentation, or the corresponding post on haskellforall
22:58:37 <cocreature> ah ok
22:58:40 <cocreature> :t MaybeT . pure
22:58:41 <lambdabot> error:
22:58:41 <lambdabot>     • Data constructor not in scope: MaybeT :: f0 a -> c
22:58:41 <lambdabot>     • Perhaps you meant variable ‘maybe’ (imported from Data.Maybe)
22:58:53 <cocreature> :t Control.Monad.Trans.Maybe.MaybeT . pure
22:58:54 <lambdabot> Applicative m => Maybe a -> Control.Monad.Trans.Maybe.MaybeT m a
22:58:58 <cocreature> that’s also not too bad
22:59:39 <mbw> What is a realistic scenario for all these map* functions?
22:59:58 <iqubic> cocreatur: pure for that is just "Just $ pure m"
23:00:31 <iqubic> mbw: Like mapMaybeM and such? I have no idea
23:00:36 <cocreature> iqubic: so?
23:00:38 <mbw> Yeah those
23:00:48 <mbw> mapMaybeT
23:00:54 <mbw> etc.
23:00:58 <iqubic> cocreature: You were pointing out the types of a function
23:01:21 <cocreature> iqubic: right but the pure in that function is definitely not Just . pure
23:01:27 <iqubic> mbw: what other map functions are there?
23:01:34 <iqubic> cocreature: it isn't?
23:01:35 <mbw> And wouldn't flip mapMaybeT be more convenient to use...
23:01:52 <iqubic> you're not using maybeT's pure?
23:01:56 <cocreature> iqubic: it’s the pure from the Applicative instance of m
23:02:01 <iqubic> :t mapMaybeT
23:02:03 <lambdabot> error:
23:02:03 <lambdabot>     • Variable not in scope: mapMaybeT
23:02:03 <lambdabot>     • Perhaps you meant one of these:
23:02:24 <cocreature> mbw: the mmorph package contains generalizations of mapMaybeT and also explains some of the usecases
23:02:38 <iqubic> Oh. So why does it have the type of Maybe a -> MaybeT m a then?
23:02:48 <mbw> cocreature: Ah that's great.
23:03:03 <cocreature> mbw: basically it helps if you have some code that you can’t change which is written against a concrete transformer stack but you need to embed it in a different one
23:03:27 <mbw> So... transformer marshalling? :)
23:03:30 <cocreature> iqubic: in this case pure :: Maybe a -> m (Maybe a) and then the MaybeT constructors wraps that m (Maybe a) and gives you back a MaybeT m a
23:04:42 <iqubic> :t Control.Monad.Trans.Maybe.MaybeT
23:04:44 <lambdabot> m (Maybe a) -> Control.Monad.Trans.Maybe.MaybeT m a
23:04:53 <iqubic> Ah. I see how that works.
23:05:22 <iqubic> so it uses m's pure function in MaybeT . pure
23:07:28 <mbw> iqubic: If you consider newtype MaybeT m a = MaybeT { runMaybeT :: m (Maybe a) }, it should be clear. You just lift the Maybe and wrap it.
23:08:34 <iqubic> mbw: I figured it out already.
23:35:03 <cocreature> merijn: is there some track ticket for the ADOPT pragma you talked about here a while back? I hacked together a quick prototype but I’m too lazy to get into endless bikeshedding discussions so I’m hoping those have already happened somewhere
23:36:06 <merijn> cocreature: No, I didn't make one yet since I didn't have any time to work on it anyway
23:36:18 <cocreature> merijn: meh, I guess I’ll have to do that then
23:36:38 <merijn> cocreature: I was simply hoping that the very desirable functionality and witty name would preempt most bikeshedding :p
23:37:16 <cocreature> I’ll see how it goes. I guess I can always drop it if the bikeshedding becomes to annoying
23:37:46 <merijn> cocreature: I now a bunch of vocal/active people on the mailing list want it, so that helps
23:38:01 <merijn> And since it's a pragma I think there's limit room for complaints from the usual crowd
23:38:38 <cocreature> merijn: also just to be clear, you only expect this to silence warnings about orphans for that instance right? there is not some more complex semantic part that I’m missing here
23:39:34 <merijn> cocreature: Right, just for a single instance. I dislike the fact that -fno-warn-orphans disables them in the entire module, forcing people to move them into separate modules etc. I always want the ability to fix/silence warnings per occurence :)
23:42:03 <cocreature> merijn: one thing that I’m wondering is if it really makes sense to have something specific for -fno-warn-orphans or if we shouldn’t have a more general mechanism for controlling the scope of warnings
23:42:21 <merijn> cocreature: Maybe, but I wouldn't know how that should/would work
23:42:28 <cocreature> yeah me neither :)
23:42:43 <merijn> Most other warnings can be silenced in other ways (explicit signatures, explicit binds, etc.)
23:43:08 <cocreature> hm good point
23:43:37 <cocreature> well I’ll try to make a trac ticket sometime this weekend and I’ll see where it goes
23:44:52 <LiaoTao> Is there any way to perform interruptable parallel computation?
23:46:05 <merijn> LiaoTao: Do you plan to resume the computation? Or do you just wanna interrupt it
23:46:06 <LiaoTao> Say I want to call expensiveFunction in a separate thread whenever an event occurs, but if another event occurs any ongoing calculation in expensiveFunction should be interrupted and started over
23:46:20 <merijn> LiaoTao: Yes, have a look at Control.Exception.throwTo
23:46:23 <LiaoTao> Only interruption
23:46:32 <LiaoTao> merijn: Okay, thanks!
23:46:52 <merijn> LiaoTao: Actually, maybe better to use async, which wraps this sorta thing and deals with thread cleanup too
23:47:04 <LiaoTao> ?
23:47:15 <merijn> The async library
23:47:17 <LiaoTao> async package?
23:47:18 <merijn> @hackage async
23:47:18 <lambdabot> http://hackage.haskell.org/package/async
23:47:29 <ReinH> @google parallel and concurrent programming in haskell
23:47:31 <lambdabot> Plugin `search' failed with: connect: does not exist (No route to host)
23:47:31 <LiaoTao> I'll check it out
23:47:31 <merijn> There's a "cancel" operation in it
23:47:37 <ReinH> Search is still broken :/
23:48:07 <merijn> ReinH: Wild guess: It assumes connecting over IPv4 but is getting an IPv6 address back
23:48:18 <ReinH> wah wah
23:48:32 <ReinH> LiaoTao: Anyway, Simon Marlow wrote a free book that you should read.
23:48:38 <merijn> Agreed
23:48:53 <merijn> @where pch
23:48:53 <lambdabot> I know nothing about pch.
23:48:54 <LiaoTao> ReinH: I think I have it
23:48:55 <merijn> hmm
23:48:56 <LiaoTao> Just haven't read it
23:49:13 <LiaoTao> Async looks perfect though
23:49:34 <merijn> LiaoTao: It's one of the areas where things that are really difficult in other languages are super-easy in Haskell :)
23:49:47 <LiaoTao> Oh I noticed
23:49:51 <LiaoTao> parMap is amazing
23:49:52 <ReinH> You might want to have some transaction or atomicity guarantee in addition to just throwing an asyc exception, so you might want to look into exception masking, which is covered in the book.
23:50:30 <LiaoTao> ReinH: It's fine - the results are used only through GLUT events which are synchronized anyway
23:50:33 <LiaoTao> (I think)
23:50:44 <LiaoTao> But I'll definitely check it out
23:53:44 <julianleviston> http://chimera.labs.oreilly.com/books/1230000000929
23:54:36 <cocreature> that book is worth a read just for the chapter on exceptions even if you’re not doing any kind of parallel or concurrent programming
23:55:10 <LiaoTao> Will do! :) You guys are great.
23:55:22 <julianleviston> or the discussion on evaluation at the beginning. It’s a pretty great read, too
23:55:23 <LiaoTao> Best programming language channel on Freenode.
23:55:51 <ongy> just freenode? :(
23:56:09 <LiaoTao> ongy: I'd say multiverse, but the Gods would punish me
23:59:43 <iqubic> is glguy around?
