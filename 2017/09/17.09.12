00:50:18 <asturtz> Hey guys, what's the best way to found a Double to some level of precision?
00:51:00 <ongy> found a double?
00:51:11 <asturtz> round, sorry
00:51:45 <mauke> that doesn't sound like a good idea
00:51:54 <[exa]> asturtz: multiply/standard round/divide back ? but rounding is always bad.
00:52:05 <mauke> are you talking about decimal places?
00:52:20 <dminuoso> Why would you possibly want this. :"
00:52:21 <pacak> :t realToFrac :: Double -> Float
00:52:22 <lambdabot> Double -> Float
00:52:58 <asturtz> I mean technically yes, it's not for real computation, just ghci
00:53:07 <pacak> asturtz: There's not much sense in doing so...
00:54:35 <nshepperd_> I think asturtz means rounding for display
00:54:56 <asturtz> Yeah
00:55:02 <mauke> :t showFFloat
00:55:03 <lambdabot> RealFloat a => Maybe Int -> a -> ShowS
00:55:09 <pacak> :t take 4 . show
00:55:10 <lambdabot> Show a => a -> [Char]
00:55:15 <mauke> > showFFloat (Just 5) pi ""
00:55:17 <lambdabot>  "3.14159"
00:55:23 <mauke> not really rounding, though
00:55:44 <pacak> > take 4 $ show pi
00:55:46 <angerman> Hey cocreature, do you have a sample of your lrucache with STM?
00:55:46 <lambdabot>  "3.14"
00:55:54 <mauke> > showFFloat (Just 4) pi ""
00:55:56 <lambdabot>  "3.1416"
00:56:00 <nshepperd_> printf does the trick for me
00:56:20 <mauke> > printf "%.f" pi :: String
00:56:22 <lambdabot>  "3"
00:56:27 <mauke> > printf "%.4f" pi :: String
00:56:29 <lambdabot>  "3.1416"
00:56:33 <mauke> that's what I meant
00:56:33 * pacak prefers to see all the digits
00:56:34 <asturtz> showFFloat looks like it does the job
00:56:49 <ski> > showFFloat (Just 0) 0.5 ""
00:56:51 <lambdabot>  "0"
00:56:51 <ski> > showFFloat (Just 0) 1.5 ""
00:56:53 <lambdabot>  "2"
00:57:00 <asturtz> Thanks guys
00:57:15 <nshepperd_> Bankers rounding eh
00:57:24 <nshepperd_> Very sensible
01:03:44 <olligobber> > show pi
01:03:47 <lambdabot>  "3.141592653589793"
01:03:50 <olligobber> aww
01:04:16 <ongy> :t pi
01:04:17 <lambdabot> Floating a => a
01:04:41 <olligobber> intersting
01:04:52 <ongy> @src pi
01:04:52 <lambdabot> Source not found. BOB says:  You seem to have forgotten your passwd, enter another!
01:05:08 <opqdonut> it's part of the Floating type class IIRC
01:05:25 <ongy> @src Floating
01:05:25 <lambdabot> class (Fractional a) => Floating a where
01:05:25 <lambdabot>     pi                  :: a
01:05:25 <lambdabot>     exp, log, sqrt      :: a -> a
01:05:25 <lambdabot>     sin, cos, tan       :: a -> a
01:05:25 <lambdabot>     asin, acos, atan    :: a -> a
01:05:27 <lambdabot> [3 @more lines]
01:05:28 <opqdonut> yeah
01:05:28 <ongy> oh it is
01:06:00 <ongy> all of those are kind of fancy for a Floating typeclass
01:06:02 <olligobber> I guess it's needed in definitions of sin and cos too
01:06:27 <olligobber> ongy, Floating is more specific than Fractional though
01:07:40 <ongy> https://en.wikipedia.org/wiki/X86_instruction_listings#Added_with_80387 doubt it. at least on x86
01:13:15 <ertes-w> o
01:15:14 <dmwit> olligobber: What were you hoping `show pi` would do, out of curiosity? (Why the "aww"?)
01:15:28 <olligobber> dmwit, more decimal places
01:15:36 <dmwit> > pi :: CReal
01:15:38 <lambdabot>  3.1415926535897932384626433832795028841972
01:15:43 <[exa]> awwwwwwwwwwww
01:16:08 <olligobber> nice
01:16:11 <dmwit> > showCReal 100 pi
01:16:13 <lambdabot>  "3.1415926535897932384626433832795028841971693993751058209749445923078164062...
01:16:17 <pacak> ᕕ-ᐛ-ᕗ
01:16:34 <olligobber> the first 42dp are correct
01:20:27 <cocreature> angerman: not sure I understand the question. are you looking for a way to cache STM operations?
01:21:16 <ertes-w> > showCReal 1000 (acos (-1)) == showCReal 1000 pi
01:21:18 <lambdabot>  True
01:21:41 <angerman> cocreature: rather Data.LruCache.STM I guess
01:23:04 <cocreature> angerman: I’ll happily accept a PR to add that :) it’s basically just copying Data.LruCache.IO and replace IORef by TVar
01:23:26 <angerman> cocreature: hehe, yes. I was just wondering if you had one laying around. :-)
01:23:30 <cocreature> nope
01:23:57 <angerman> I'll try IO for now, and see if I end up needing STM. If so I'll open a PR.
01:24:17 <cocreature> caching STM feels kind of weird but I guess it can make sense
01:24:21 <angerman> Looks like I much more need a working Docker installation. 
01:25:12 <angerman> really high time to get cross compiling macOS -> linux working without docker.
01:27:00 <ertes-w> angerman: i don't like go as a language, but i must say that it has inspired a type of application design that pays very close attention to UX…  just install and start docker, and it's pretty much ready to go
01:27:58 <angerman> ertes-w: I'm kind of ok with docker... but when it just freezes because it's FS layer chokes on something... I'm a bit annoyed :)
01:28:38 <angerman> ertes-w: and knowing how close we are with proper cross compilation, this is the point where I sometimes wonder if I should rather pour a hour into fixing cross compilation right now, or try and make docker work.
01:29:00 <ertes-w> angerman: i don't use its default FS implementation…  you can tell it to use btrfs subvolumes or a ZFS pool instead, which works much better
01:29:44 <angerman> ertes-w: the key for me right now is (and I'm using it purely to build for a different architecture) to map my local file system into the container build there and leave the artifacts on my filesystem.
01:30:05 <angerman> I guess, one could arguably say, I'm abusing docker for something it's not intended for, ... I guess.
01:31:57 <ertes-w> not sure i understand…  you're using it as a cross-compilation helper?
01:32:30 <angerman> yes. 
01:32:59 <angerman> ertes-w: I'm mapping the current directory into the container, and run the container with the same environment the production system is. 
01:33:19 <ongy> sounds reasonable to me
01:33:20 <cocreature> tbh, I’m not surprised that’s a bit fragile on macOS
01:33:31 <angerman> [...]
01:33:33 <ertes-w> angerman: if this is just about a virtual filesystem, you can get away without the complexity of docker…  just use unshare(1) to get an isolated mount namespace, then you can bind-mount and pivot_root freely
01:33:53 <ongy> on macOS?
01:34:03 <ertes-w> err, that works only on linux
01:34:19 <ertes-w> wrong direction i guess =)
01:34:25 <angerman> yea. Everything is pointing to use linux... I just refuse to :)
01:34:38 <ertes-w> docker can run linux containers on mac?
01:34:43 <ongy> that's your fault :)
01:34:50 <angerman> nix on macOS is... meh. docker on macOS is ... meh.
01:34:53 <ongy> afaik docker has full VM backends for usecases like that
01:35:04 <ertes-w> ah
01:35:04 <angerman> ertes-w: yes it can.
01:35:11 <ertes-w> wow, that sounds expensive
01:35:59 <cocreature> sure it’s expensive but it’s nice for macOS devs that deploy to linux
01:36:16 <angerman> *without a proper cross compilation story :)
01:36:21 <ongy> about as expensive as an kvm emulation on linux
01:36:45 <ertes-w> do you have to cross-compile?  if the target is linux, you might just as well build there
01:36:53 <ongy> unless we are talking actual money, then it's the price of the mac
01:37:06 <angerman> ertes-w: I'd rather not build on the production machine.
01:37:35 <angerman> looks like docker just needed a few restarts in succession to get back on track.
01:37:48 <ertes-w> if i were a mac user, i would probably set up a linux VM with SSH access and a nix installation
01:38:13 <ertes-w> the boring kind of VM, not docker =)
01:38:24 <angerman> ertes-w: well... at that point you are not using mac anymore, as your tooling will live in the vm as well.
01:38:37 <angerman> ertes-w: and, see, I'd like to use my mac emacs :)
01:38:45 <ongy> sshfs...
01:38:46 <ertes-w> angerman: you build for linux exclusively?
01:38:48 <angerman> it even has "mac" in it's name.
01:38:57 <angerman> ertes-w: no. I build for linux, android and iOS.
01:39:03 <ertes-w> angerman: i assumed that you do your testing locally, and then (try to) cross-compile for linux
01:39:08 <angerman> the latter two are more of an experimental nature.
01:41:00 <angerman> ertes-w: and yes, I test locally. But I do need to compile for linux to deploy. And I don't want to compile on the production machine.
01:41:28 <ertes-w> angerman: that's what i mean: do you development locally, then build on the linux VM for deployment
01:41:36 <ertes-w> s/do you/you do/
01:42:00 <angerman> yes. that's exactly what I do. Where did we get lost in translation?
01:42:30 <ertes-w> cross compilation was mentioned…  what i'm proposing is not cross, but native
01:42:47 <angerman> where linux VM is docker for me.
01:43:47 <angerman> I just skip the step of `git pull; build; push artifacts somewhere` and just do the `build` step in the VM, with the local filesystem mounted.
01:45:34 <jakub> i am completely new to template haskell and all the resources online confuse me a bit, how could define a fixed type variable (say Int) with a name given by String?
01:46:05 <angerman> jakub: `foo = 1`?
01:48:47 <pacak> jakub: https://hackage.haskell.org/package/template-haskell-2.11.0.0/docs/Language-Haskell-TH-Syntax.html
01:49:28 <pacak> jakub: You want FunD, NormalB, LitE and IntegerL
01:49:40 <pacak> Alternatively you can use something like [| $foo = 1 |]
01:54:17 <jakub> pacak: thanks, i must admit I still get lost in the rather cryptic naming and syntax :/, in reality I have (as :: [a]) nad (f :: a -> String) and would like to do something like map (\a -> bind (f a) to a) as
01:54:28 <jakub> *and
01:56:20 <pacak> There are two sides to TH - you can either construct whatever AST you want manually or use quotes. For AST you want to have a [Dec]  on the top level.
01:56:35 <pacak> Try experimenting
01:56:59 <pacak> -ddump-splices is your friend
01:57:27 <jakub> pacak: i will, but i must say its one of the ugliest parts of haskell i've seen so fat
01:57:30 <jakub> *far
01:58:03 <pacak> You haven't seen ghc internals yet :)
01:58:26 <pacak> TH AST is just a shadow of whatever madness sits inside...
01:58:33 <jakub> pacak: yeah, that's right
01:58:34 <pacak> (or so they say)
01:58:52 <pacak> They both not as bad.
02:00:29 <jakub> i would welcome some explanation behind how any TH thing gets injected into the rest of haskell, because as of now I really don't know what top-level expressions can I expect to produce anything... really low accessibility
02:00:35 <angerman> > runQ [d| foo = 1 |]
02:00:37 <lambdabot>  <hint>:1:14: error:
02:00:37 <lambdabot>      parse error on input ‘=’
02:00:37 <lambdabot>      Perhaps you need a 'let' in a 'do' block?
02:01:27 <MarcelineVQ> hmm, it got pretty far
02:01:30 <angerman> jakub: it's two parts. One is (usually via quotation) to construct an expression. The other is via splicing $() to inject that expression where you want it.
02:01:46 <haskellwelp>  I'm  trying to count the number of times a type of character appears seperate from other types in a string. For example 135KK533KK45KK1. Would return 4 as numbers appeared 4 times. But it's not possible to write while loops or lookahead stuff etc in haskell. How would I go about it?
02:02:42 <ventonegro> haskellwelp: A string is a list of characters. Walk the list and count them
02:03:15 <angerman> jakub: You usually have a function that returns some `Q a`. (e.g. `f :: Int -> Q Decs`) and then you splice that somewhere e.g. `$(f 1)`.
02:03:16 <haskellwelp> ventonegro: I'm not just looking for the number of characters
02:03:47 <ski> @let while keepLooping step start = until (not . keepLooping) step start
02:03:48 <lambdabot>  Defined.
02:03:50 <cocreature> haskellwelp: what do you mean by “separate from other types in a string”?
02:03:52 <ventonegro> haskellwelp: I meant count the ones you want
02:03:55 <ski> that's a `while' for you
02:04:00 <jakub> angerman: that explains little unless you know what exact quotation is available, what types do the expressions have? does ghc pass through the file and on anything of some TH type do something with the expression? what is that type? so far I have seen too many constructions work and yet they seemed to share 0 syntax
02:04:05 <angerman> say `f i = [d| foo = i |]`
02:04:14 <kahlil29> cocreature: I'm still having problem with that Opaleye query.  I'm passing 
02:04:21 <ski> > span isDigit "135KK533KK45KK1"
02:04:23 <lambdabot>  ("135","KK533KK45KK1")
02:04:36 <cocreature> > (length . filter isDigit) "135KK533KK45KK1"
02:04:38 <lambdabot>  9
02:04:47 <ski> cocreature, not what they want
02:04:59 <cocreature> oh yeah I finally understood the question :)
02:05:02 <haskellwelp> ventonegro: I don't just want to count the type but how many times they appear in isolation. So for 135KK533KK45KK1 I want to pick out "135" "533" "45" and "1! i.e. total number of instances is 4
02:05:20 <ventonegro> haskellwelp: Oh, I see
02:05:23 <angerman> jakub: well. by default you have [||], and [d||] for declarations, and [p||] for patterns, I believe.
02:05:25 <cocreature> kahlil29: I can’t help you if you don’t show us the code and the error message
02:05:40 <ongy> > groupBy isDigit "135KK533KK45KK1" -- probably a good start
02:05:42 <lambdabot>  error:
02:05:42 <lambdabot>      • Couldn't match type ‘Bool’ with ‘Char -> Bool’
02:05:42 <lambdabot>        Expected type: Char -> Char -> Bool
02:05:52 <kahlil29> I'm passing 'asc(\r -> r ^. status)' to my function as an argument and I'm keeping the type of the parameter in function definition as 'Order a' as per Opaleye docs 
02:05:54 <ski> haskellwelp : you can do it with `span' and `break'. or explicitly using recursion. or with `until' or `while' if you really must
02:05:55 <cocreature> > wordsBy (not . isDigit) "135KK533KK45KK1"
02:05:57 <lambdabot>  ["135","533","45","1"]
02:06:05 <kahlil29> okay, paste incoming. I'll paste all the correct stuff this time 
02:06:23 <jakub> angerman: thx, where can i learn about those?
02:06:47 <angerman> jakub: e.g. `runQ $ f 3` with `f i = [d| foo = i |]` would give you `[ValD (VarP foo_1) (NormalB (LitE (IntegerL 3))) []]`.
02:07:01 <cocreature> > (length . wordsBy (not . isDigit)) "135KK533KK45KK1" -- haskellwelp 
02:07:02 <jakub> because so far all TH doc I have seen jump straight into examples, which honestly doesn't work one bit for me
02:07:03 <lambdabot>  4
02:07:22 <angerman> jakub: now you could construct the `[ValD ...]` yourself. But because that's rather painful, you usually go the route via the quasi quotation.
02:07:43 <haskellwelp> cocreature: sorry but I don't understand. What is that?
02:07:53 <cocreature> haskellwelp: isn’t that what you’re looking for?
02:08:01 <haskellwelp> cocreature: I don't know what it is
02:08:09 * ski imagines haskellwelp is doing this as an exercise, in order to bettr learn about how to do recursion
02:08:11 <cocreature> haskellwelp: do you know "words"?
02:08:24 <haskellwelp> cocreature: sure it's an English word...
02:08:30 <cocreature> no the haskell function
02:08:45 <cocreature> > words "abc def ghi"
02:08:47 <lambdabot>  ["abc","def","ghi"]
02:08:52 <ski> haskellwelp : yes ?
02:09:02 <haskellwelp> ski that's correct :)
02:09:26 <ski> so, then it's probably not that enlightening to use `wordsBy' here
02:09:58 <ski> haskellwelp : try one of my suggestions ?
02:10:43 <haskellwelp> I mean i've tried using recursion but only thing I can think of is to have an infinite number of if then cases lol
02:10:56 <ski> haskellwelp : btw, neither of "it's not possible to write while loops or lookahead stuff etc in haskell" is true, you can do both
02:11:10 <ski> (but you don't *need* to do either, here)
02:11:31 <ski> (you could use look-ahead if you wanted to)
02:11:42 <ventonegro> haskellwelp: If you recurse keep an accumulator (for the number of substrings already seen) and a Bool (to tell you whether you are inside a substring)
02:12:12 <ski> an accumulator isn't essential here
02:12:16 <cocreature> tbh, even if you’re doing this as an exercise I would recommend implementing "wordsBy" first
02:12:23 <cocreature> and make that the exercise
02:12:43 <ventonegro> haskellwelp: myFunc = go 0 False where go acc inString xs = ...
02:12:47 <ski> instead of `Bool', one can use two mutually recursive functions, to keep track of the state (as in Finite State Automaton/Machine)
02:13:26 <ventonegro> ski: Maybe start simple? I'm sure you can thing of many sofisticated ways of writing this
02:13:47 <ski> yes
02:14:15 <ski> the last mentioned approaches all seem about as simple to me
02:15:13 <ventonegro> ski: Except you are not they
02:15:25 <ski> it
02:15:56 <ski> it's true that some of them might feel a bit more simple than the others, depending on how one's currently thinking
02:16:09 <kahlil29> http://lpaste.net/358366  the changed code is here  
02:16:10 <kahlil29> cocreature:
02:16:28 <ski> but i'm not convinced a newbie would necessarily find the direct approach less simple than the accumulating one
02:16:59 <ski> (or find the mutual recursion one less simple than the `Bool' one)
02:17:46 <haskellwelp> Thanks guys. I don't know what any one thing is so I'm looking everything up 
02:17:57 <ski> haskellwelp : so, i suggest you try what appears to be most natural to you
02:18:31 <ski> it may help to consider how you'd do it, "if you're a computer, following instructions"
02:20:08 <ski> you'd probably start by checking the first element (you can think of it as a card in a deck of cards, if that helps)
02:20:18 <ski> then, what'd you do ?
02:21:15 <haskellwelp> The difficulty is handling the fact that the isolated parts can be any number of characters
02:21:30 <ski> yes
02:22:13 <cocreature> kahlil29: "Order a" is too general. it should probably be "Order JobPGR" since that seems to be what you’re ordering.
02:22:41 <ski> so it seems that you need to distinguish when you're just seeing more of the same, from when you start to see something different
02:23:03 <ski> (look-ahead would be one possible way to do this, i think, should you want to try that)
02:23:10 <kahlil29> cocreature: had tried that earlier and it was still giving errors. Tried it now and it's compiling. so frustrating. Anyway, happy that it's compiling. Thanks for your help . Will try running th query now 
02:23:10 <cocreature> kahlil29: you’re probably also going to need an explicit type annotation on "r ^. status". I don’t know enough about opaleye here but "Column Status" could work
02:23:46 <kahlil29> I had tried Column status some time back as well.... will fiddle around and try again
02:33:36 <jakub> angerman: because Int was just an example, if i need template that works for a different type, it needs to be an instance of Lift or else I am out of luck? seems like I am out of luck...
02:34:33 <angerman> jakub: what exactly was it you needed?
02:36:20 <jakub> angerman: i have a list of things (my custom type, rather complex), each thing internally has a name, i want to bind each of the values to a variable with the appropriate name (and a modified variant of the value to name')
02:37:10 <angerman> jakub: so you want to generate declarations from a list of things?
02:37:23 <jakub> angerman: yes
02:37:37 <angerman> jakub: let's assume your [T], can be converted into [(String,T)]?
02:37:55 <jakub> angerman: thats right
02:38:39 <angerman> and you basically want to generate `$x :: T` for each element
02:39:02 <dxtr> Our JIRA is a gold mine
02:39:10 <dxtr> One issue is literally "I would like a widget that we can use on the site"
02:39:17 <dxtr> Sigh..
02:39:23 <jakub> angerman: probably (though the syntax $x did raise complaints from ghc when I tried it)
02:40:49 <[exa]> dxtr: invalid wontfix needinfo closed
02:48:37 <angerman> jakub: I believe you want something like: mkNamedVal n v = let n' = pure (VarP $ mkName "foo") in [d| $(n') = v |]
02:48:55 <jophish> http://nautilus.cs.miyazaki-u.ac.jp/~skata/MagicHaskeller.html
02:49:38 <angerman> jakub: that will turn `mkNamedVal "foo" 1` into `[ValD (VarP foo) (NormalB (LitE (IntegerL 1))) []]`
02:49:44 <jakub> angerman: i tried something similar, got complaints about Lift instances
02:49:56 <phaazon> is there a way to push the latest docker image?
02:51:08 <phaazon> anyone to accept this https://github.com/freebroccolo/docker-haskell/pull/61 ?
02:51:30 <ski> jophish : looks fun
02:55:08 <haskellwelp> Why can't I call function 2 from function1 like this? http://lpaste.net/358373
02:55:10 <angerman> jakub: not sure... but https://gist.github.com/b7e6aa4334c628e9bd0ed1fb183ddabb gives me the expected result?
02:55:51 <angerman> not, that I would advocate TH. But it does do funky stuff like this.
02:57:10 <jakub> angerman: thx i will investigate
02:57:43 <jophish> haskellwelp: try putting `pure []` (at the top level) after the splice
02:58:05 <jophish> I can't remember the issue number
02:59:01 <jakub> angerman: one thing, your example hardcodes Int, other types might fail
02:59:19 <angerman> well they would need show for sure. 
03:00:02 <jakub> angerman: i am more concerned about other classes like Lift
03:00:02 <jophish> haskellwelp: the purpose is to split the dependency group
03:00:25 <jophish> haskellwelp: https://ghc.haskell.org/trac/ghc/ticket/9813 might be it
03:01:03 <angerman> jakub: ahh. I see what you mean.
03:01:15 <ventonegro> haskellwelp: Friendly suggestion here, do with it what you want. You are struggling with basic syntax, so you should first get acquainted with it before attacking problems like this one with strings.
03:02:31 <jophish> oops, disregard all my replies haskellwelp!!!
03:02:38 <jophish> I was speaking nonsense!
03:02:42 * ski was wondering ..
03:02:48 <ventonegro> haskellwelp: https://www.seas.upenn.edu/~cis194/spring13/lectures.html
03:03:18 <kahlil29> cocreature: It worked, thanks a bunch
03:03:40 <haskellwelp> Thanks jophish 
03:04:04 <haskellwelp> And thanks a lot ventonegro you're definitely right, I think it's best learned in conjunction though?
03:04:04 <jophish> heheh, I think I need to get my morning coffee
03:04:26 <ventonegro> haskellwelp: Go at least here first: https://www.seas.upenn.edu/~cis194/spring13/lectures/01-intro.html
03:04:41 * ski isn't sure what haskellwelp is trying to accomplish, in that paste
03:05:03 <ventonegro> haskellwelp: Then your questions will make more sense
03:07:48 <angerman> jakub: does https://gist.github.com/a6a9e623d270ee7bc7e38b2e4d3363db help?
03:13:03 <jakub> angerman: thx i figured that from first gist but i hate the prospect of deriving lift for all my myriad of types
03:14:29 <angerman> Well, you can of course write it all out by hand if you prefer.
03:14:50 <angerman> But the logic needs to be somewhere.
03:16:13 <jakub> angerman: write it by hand is where i get lost
03:17:08 <angerman> There is some dump-derive flag or so. Maybe look at what ghc derives for you?
03:35:07 <rifkin> i can't find WinHugs-Sep2006.exe
03:39:44 <kahlil29> Wanted a little help regarding the best way to replace guards with case expressions.... here is the code  http://lpaste.net/358374
03:40:18 <cocreature> rifkin: why are you searching for that? hugs is dead
03:41:00 <cocreature> kahlil29: filterJobByStatus … = case (sortDirection,sortColumn) of …
03:41:52 <cocreature> kahlil29: or even better "… = runFilterJobsQuery jobStatus (limitParam, offsetParam) (case (sortDirection, sortColumn) of …)"
03:42:30 <kahlil29> what if I change the function parameter to a tuple for (sortDirection, sortColumn). Would that help ? 
03:43:10 <cocreature> wouldn’t change a lot, you save a few characters but that’s about it
03:43:39 <cocreature> I would probably also extract a function "SortColum -> SortDirection -> Order Something"
03:43:54 <cocreature> and then you can just call that function in filterJobsByStatus
03:44:24 <kahlil29> so in my matching expressions also I'd have to use tuples like (SortByStatus, Asc) -> queryCommandForThisCase ? 
03:44:40 <cocreature> yes
03:44:42 <cocreature> what’s wrong with that?
03:44:54 <kahlil29> nothing. just confirming. 
03:45:04 <kahlil29> Also the extraction of that function seems like a good idea
03:45:06 <kahlil29> will try
03:45:09 <kahlil29> thanks, again
03:46:24 <pja> rifkin: There appears to be a copy here: http://www.cs.ou.edu/~hougen/classes/Spring-2015/cs2603/materials/
03:46:32 <pja> Why do you want hugs tho?
03:47:55 <rifkin> cocreature: I know it's dead. I need it because a book uses it.
03:49:34 <pja> Looks like the windows binary downloads died on the Hugs site when Google Code went AWOL.
03:49:49 <cocreature> rifkin: I would highly recommend translating the examples from that book to “modern GHC haskell” instead of trying to use hugs
03:49:59 <pja> Which book?
03:51:18 <rifkin> pja: "The Craft of Functional Programming" by Simon Thompson
03:52:04 <pja> 3rd edition of that uses ghci as it's interpreter.
03:52:37 <pja> It's outrageously expensive tho.
03:53:01 <pja> Academic publishing has obviously gone even nuttier since I left academe.
03:54:08 <rifkin> pja: 57 Euro here... yes expensive
04:04:49 <jakub> angerman: i cannot make instances of Lift myself, is there any other way?
04:05:58 <BernhardPosselt> hi, how is the traverse function generated that is used for applicative mapping?
04:06:37 <jakub> angerman: I cannot because I want to support types that may not be instances of Lift, and dont want to impose a need for orphan instances
04:07:35 <BernhardPosselt> i also dont get instance https://hackage.haskell.org/package/base-4.10.0.0/docs/src/Data.Traversable.html#Traversable https://hackage.haskell.org/package/base-4.10.0.0/docs/src/GHC.Base.html#Maybe where
04:07:36 <BernhardPosselt>     https://hackage.haskell.org/package/base-4.10.0.0/docs/src/Data.Traversable.html#traverse _ https://hackage.haskell.org/package/base-4.10.0.0/docs/src/GHC.Base.html#Nothing = https://hackage.haskell.org/package/base-4.10.0.0/docs/src/GHC.Base.html#pure https://hackage.haskell.org/package/base-4.10.0.0/docs/src/GHC.Base.html#Nothing
04:07:36 <BernhardPosselt>     traverse https://hackage.haskell.org/package/base-4.10.0.0/docs/src/Data.Traversable.html#local-6989586621679324784 (https://hackage.haskell.org/package/base-4.10.0.0/docs/src/GHC.Base.html#Just https://hackage.haskell.org/package/base-4.10.0.0/docs/src/Data.Traversable.html#local-6989586621679324785) = https://hackage.haskell.org/package/base-4.10.0.0/docs/src/GHC.Base.html#Just https://hackage.haskell.org/package/base-4.10.
04:07:36 <BernhardPosselt> s/src/Data.Functor.html#%3C%24%3E https://hackage.haskell.org/package/base-4.10.0.0/docs/src/Data.Traversable.html#local-6989586621679324784 https://hackage.haskell.org/package/base-4.10.0.0/docs/src/Data.Traversable.html#local-6989586621679324785
04:07:39 <BernhardPosselt> oops :D
04:07:51 <angerman> jakub: I think you have exhausted my TH knowledge. Sorry that I won't be able to help you any further.
04:08:13 <BernhardPosselt> instance Traversable Maybe where
04:08:14 <BernhardPosselt>     traverse _ Nothing = pure Nothing
04:08:14 <BernhardPosselt>     traverse f (Just x) = Just <$> f x
04:08:26 <BernhardPosselt> isnt this just the functor implementation?
04:08:46 <jakub> angerman: thanks, sorry if i bother you too much
04:09:04 <cocreature> BernhardPosselt: no, take another look at the Functor instance :)
04:09:22 <cocreature> BernhardPosselt: I’m not sure what you mean by “generated”
04:09:58 <BernhardPosselt> traverse works like this right? some_func <$> x1 <*> x2 <*>
04:10:30 <cocreature> no? traverse does not work by using the Functor and Applicative instance. it’s a separate class
04:10:44 <cocreature> and you can’t reduce its behavior to Functor and Applicative
04:10:48 <BernhardPosselt>  i see
04:11:43 <BernhardPosselt> then why does it require Applicatives?
04:11:49 <michalrus> Hey, how do I import `type instance`s from another file?
04:12:25 <cocreature> BernhardPosselt: because to implement "traverse" you are allowed to use the fact that "f" is an instance of Applicative
04:12:53 <BernhardPosselt> even though its not really in use in https://hackage.haskell.org/package/base-4.10.0.0/docs/src/Data.Traversable.html#traverse
04:13:50 <cocreature> I don’t think thet link points to where you would like it to point
04:14:40 <BernhardPosselt> ah right
04:14:44 <cocreature> but you’re probably pointing at the class definition. what matters is the instances. they can use the fact that "f" is Applicative
04:15:13 <cocreature> e.g. in the implementation for Maybe you showed here before, "pure" is used
04:15:21 <cocreature> that only works because you know that "f" is an Applicative
04:15:29 <BernhardPosselt> ah right
04:15:50 <ertes-w> BernhardPosselt: it requires Applicative to support structures with arbitrarily many points…  for example if 'traverse' were restricted to Functor, then [] would no longer be an instance
04:17:00 <cocreature> even Maybe won’t work with Functor
04:17:04 <BernhardPosselt> instance Traversable [] where
04:17:04 <BernhardPosselt>     traverse f = List.foldr cons_f (pure [])
04:17:04 <BernhardPosselt>       where cons_f x ys = liftA2 (:) (f x) ys
04:17:11 <BernhardPosselt> because pure and liftA2?
04:17:16 <ertes-w> yes
04:22:19 <phadej> pure and fmap
04:22:22 <phadej> but yeah
04:25:00 <phadej> Maybe is "Affine" functor, i.e. has at-most one element. So 'class AffineTraversable t where affine :: Pointed f => (a -> f b) -> t a -> f (t b)` would be enough. But pointed is problematic. I (just this week) defined it as `class Affine f where affine :: b -> (a -> b) -> f b -> b -- think 'maybe'`
04:25:20 <cocreature> phadej: I think liftA2 was still directed at []
04:25:34 <phadej> But didn't bothered to think whether those definitions are equivalent somehow
04:26:39 <phadej> *affine :: b -> (a -> b) -> f a -> b
04:27:00 <phadej> (and instances are Identity, Maybe, Proxy and Affine f, Affine g => Compose f g)
04:27:29 <phadej> abstract non-sense way to work polymorphically over Identity or Maybe
04:28:55 <ertes-w> not a big fan of the Pointed class
04:30:18 <ertes-w> because i don't like conditional laws very much: "if there is also an instance of (Applicative f), then …"
04:31:40 <erisco> the wish is probably for Applicative to extend Pointed, and so you would give the laws on Applicative
04:32:07 <hpc> Pointed's relationship with Applicative is a free theorem anyway
04:32:11 <hpc> so that law might as well not exist
04:32:17 <ertes-w> erisco: the correct order is to extend Apply to arrive at Applicative
04:32:32 <ertes-w> much like Monoid is an extension of Semigroup
04:33:01 <ertes-w> 'pure' is a byproduct of Apply…  if it exists, it's unique
04:33:21 <erisco> orthogonal to what I am saying. I am saying the conditional laws are probably not by desire but by accident of how the classes have been defined
04:33:34 <ph88> should i get my binary from ./.stack-work/install/x86_64-linux/lts-8.9/8.0.2/bin/ or ./.stack-work/dist/x86_64-linux/Cabal-1.24.2.0/build/  ??
04:33:48 <ph88> does stack have a command to zip up the binary files or something ?
04:35:16 <saurabhnanda> pretty newbie question... I have  a (Maybe x) and a `foo :: (MonadIO m) => x -> m b` and I want to get a `(MonadIO m) => m (Maybe b)` -- what's the standard combinator to use here?
04:36:59 <cocreature> :t traverse :: MonadIO m => (x -> m b) -> Maybe x -> m (Maybe b)
04:37:01 <lambdabot> MonadIO m => (x -> m b) -> Maybe x -> m (Maybe b)
04:37:02 <cocreature> ^ saurabhnanda 
04:37:28 <Ferdirand> dark magic ! vade retro !
04:38:07 <saurabhnanda> traverse? using it for the first time. same as mapM?
04:38:29 <cocreature> yeah, the only difference is that it has an Applicative constraint
04:38:37 <cocreature> and until not too long ago mapM was specialized to []
04:38:54 <saurabhnanda> it's not working
04:39:08 <cocreature> that’s a pretty shitty description of the error you’re seeing
04:39:10 <saurabhnanda> in fact, I've tried mapM as well
04:39:22 <saurabhnanda> the error message is much longer
04:41:41 <saurabhnanda> let it be... I was missing a typeclas instance.
04:41:46 <saurabhnanda> unrrelated
04:54:17 <lyxia> ph88: get it from install/
04:55:55 <lyxia> ph88: another way is to run stack install with --local-bin-path
05:30:01 <guillaum2> I just discovered that `string` in Megaparsec behaves differently than in parsec, as it does not consume the input on failure. That's wonderful! ;)
05:33:52 <mstruebing> http://lpaste.net/3029671794614206464 why is the first line working but the second one not?
05:35:09 <Ferdirand> because read on Strings does not do what you think
05:35:11 <opqdonut> mstruebing: because the Read String instance expects "\"hallo\""
05:35:21 <opqdonut> > show "hallo"
05:35:24 <lambdabot>  "\"hallo\""
05:37:01 <mstruebing> thx
05:47:22 <osa1> anyone know how to remove "server" header in warp's reponse? I can override it but there seems no way to actually remove it
05:49:40 <osa1> ah, it turns out empty string removes it.
06:08:30 <tabaqui> I want something weird
06:08:49 <tabaqui> Can I rename cabal executable based on f.e. git commit?
06:09:02 <tabaqui> I use stack->cabal->ghc for now
06:09:32 <tabaqui> dunno, maybe stack can eval shell command and pass the result inside cabal
06:12:09 <tabaqui> what I really want is to get executable named "MyProgram_<commit>"
06:36:23 <cocreature> tabaqui: the easiest solution is to just write a script that wraps stack/cabal and renames the executable
06:38:53 <cocreature> you might be able to write some custom Setup.hs hooks if you really want to have cabal/stack do this for you but I don’t see what you gain by doing that
06:42:58 <pierrot> Good morning. I have two modules here: https://glot.io/snippets/etk237gfn7 They are A and B (defined in dio.hs and tria.hs respectively) and I'm asked about what names are exported by A. I'd say S, A.S and the names from the B module: T, B.T, D and B.D. Is that correct?
06:44:03 <tabaqui> cocreature: oh, right, Setup.hs. I've nearly forgotten about it.
06:44:10 <tabaqui> Never wrote it before
06:45:44 <ph88> lyxia, where with it install ? and where will it install with --local-bin-path ?
06:49:58 <pierrot> C and B.C aren't exported by A because the declaration of B makes it to export only the D data constructor of T, right?
06:51:25 <lyxia> ph88: it will install wherever you tell it to with --local-bin-path "wherever/you/tell/it"
06:53:17 <byorgey> pierrot: that sounds right to me
06:53:45 <lyxia> I wouldn't separate unqualified names from qualified ones
06:54:04 <lyxia> Names are (un)qualified by whoever imports them.
06:56:52 <pierrot> byorgey and lyxia : thanks.
06:57:33 <pierrot> The data constructors of S aren't exported because of "S ()"
06:58:05 <pierrot> Is there any difference with "S" (without brackets)?
06:58:20 <mlehmk> like "IO ()" and "IO Int"?
06:58:56 <mlehmk> both are of IO I think, but one carries void, the other an integer
06:59:20 <pierrot> I mean, is "module A (S (), module B) where" equals to "module A (S, module B) where" ?
06:59:32 <mlehmk> I don't think so
06:59:52 <mlehmk> as IO is one thing and IO () another thing
07:00:01 <lyxia> it seems it is
07:00:23 <lyxia> mlehmk: in import lists this it is a different distinction
07:02:25 <ski> s/void/unit/
07:03:02 <pierrot> lyxia: so do you think it's the same?
07:03:11 <lyxia> yes
07:03:30 <pierrot> Cool. Thanks. It was one of my doubts.
07:10:11 <iqubic> Whenever I try to join #emacs I get the following message: banned-from-chan: #emacs·Cannot join channel (+b) - you are banned 
07:10:17 <iqubic> Sorry, wrong place
07:11:34 <pierrot> Then I'm asked about what names are in the scope of the A module and where are defined those names. I have to look at the import statements. As it has "import Prelude ()", there isn't any name from Pelude in the the scope of A. The other import is "import B (T(..))". T(..) means T and all the data constructors of T (and their qualified versions), but all only the ones that are exported by B, right? I 
07:11:40 <pierrot> mean, C and B.C wouldn't be in the scope of A, only D and B.D (apart from T and B.T)
07:13:39 <pierrot> s/but all only/but only/
07:18:26 <pierrot> The last question if there is any ambiguous name. I'd say that D is ambigous (because D is a data constructor of S and is a data constructor of T), but not sure. I've compiled this in ghc and I didn't get any warning.
07:21:42 <byorgey> pierrot: I agree, D is ambiguous.  You only get warnings when you try to *use* an ambiguous name.
07:23:18 <byorgey> pierrot: try adding something to main.hs like   import A; x :: S; x = D    and you should get an error.
07:24:32 <byorgey> pierrot: may I ask where these questions are coming from?  They are rather tedious and nitpicky.
07:25:02 <jchia_1> How can I map a run-time value to corresponding types for type application at run-time without boilerplate? http://lpaste.net/358381
07:26:19 <pierrot> byorgey: lol I think the same. They're quite artificial.
07:27:01 <ski> jchia_1 : try GADT and existential ?
07:28:57 <pierrot> byorgey: can I answer to you in pm?
07:29:07 <byorgey> pierrot: sure
07:29:56 <jchia_1> ski: Can I use GADT to call the corresponding 'version' of a function depending on the run-time value of a sum type? I've never used GADT in anger, not sure where to begin.
07:31:22 <ski> jchia_1 : hm or perhaps just an existential here. `data Choice = forall a. Processable a => MkChoice (Proxy a)' then `case c of MkChoice (Proxy :: Proxy a) -> process @a'
07:32:34 <ski> jchia_1 : assuming that you wanted to only call `process' once, here
07:33:44 <ski> jchia_1 : if you want to do more with `getChoice' than this, a GADT could still be useful
07:34:41 <ski> jchia_1 : i was first thinking of `data Choice :: * -> * where ChoiceA :: Choice A; ChoiceB :: Choice B' then `data SomeChoice = forall a. WrapChoice (Choice a)', `getChoice :: IO SomeChoice' then `case c of WrapChoice (...) -> ...' 
07:37:58 <jchia_1> ski: brilliant, the MkChoice with Proxy works. I'm wondering if I can make use of overloadedlabels for even cleaner code
07:39:39 <ski> jchia_1 : an alternative might be to simply have `data Choice = MkChoice (IO ())' and `case c of MkChoice process -> process'
07:39:45 <jchia_1> I thought I wouldn't have to use proxy again if I use TypeApplications
07:39:57 <ski> but perhaps in your real code you can't / don't want to do that, for some reason
07:41:39 <ski> jchia_1 : hmm. i suspect that `data Choice = forall a. Processable a => MkChoice' with `case c of MkChoice @a -> process @a' won't work, but i suppose you can try
07:41:52 <ski> (apropos replacing `Proxy')
07:42:09 <ongy> does Pink Waffle know meds don't see each other?
07:42:12 <ongy> whoops, wrong chat :D
07:42:56 * ski suddenly feels an urge to make waffles with raspberry jam
07:43:05 <bartavelle> and meds ?
07:43:20 <ski> medium-sized, perhaps
07:43:45 <jchia_1> ski: Nope, it doesn't work
07:43:58 <ski> too bad
07:44:18 <ongy> actually Medium (as in talks to the dead). watching a twitch stream and that was supposed to go into that chat
07:45:48 <jchia_1> ski: Thanks for your help. I learned to appreciate Proxy.
07:45:58 <ski> np
07:48:55 <jchia_1> ski: Here I'm essentially getting from a run-time value to a type that I can use in type application. Isn't this somewhat opposite to what OverloadedLabels does, where given a type-level string, you get back a value?
07:49:30 <ski> hm, i don't think i'm familiar with `OverloadedLabels'
07:49:35 <jchia_1> OK
08:28:53 <kuribas> is there a function a -> Maybe a that returns Nothing when a is partial and throws an exception?
08:29:54 <glguy> kuribas: No, that's not possible in general, but this exists for some cases: http://hackage.haskell.org/package/spoon-0.3.1/docs/Control-Spoon.html
08:31:52 <kuribas> glguy: ah, great :)
08:31:54 <cocreature> and even that requires unsafePerformIO because you can’t catch outside of IO
08:31:59 <cocreature> it’s just hidden :)
08:32:29 <kuribas> > teaspoon (head [])
08:32:31 <lambdabot>  error:
08:32:31 <lambdabot>      Variable not in scope: teaspoon :: t0 -> t
08:32:42 <ertes-w> (undefined = fix id), therefore (spoon undefined = spoon (fix id))
08:32:51 <ertes-w> spoon violates haskell semantics
08:33:08 <dolio> spoon will probably work on fix id.
08:33:15 <dolio> You need a more devious example.
08:33:42 <ertes-w> you might even find that it works when compiled, but not when interpreted
08:33:54 <ertes-w> just don't use it…  catching bottoms is an IO effect
08:35:43 <kuribas> teaspoon (fix id) hangs
08:36:06 <cocreature> kuribas: compiled or interpreted?
08:36:10 <kuribas> interpreted
08:36:23 <AWizzArd> https://blog.merovius.de/2017/09/12/diminishing-returns-of-static-typing.html
08:36:38 <AWizzArd> A nice 5-minute read.
08:37:13 <kuribas> why does it violate haskell semantics?
08:37:24 <kuribas> because it doesn't treat any bottom the same way?
08:37:28 * ski . o O ( "SSL error: error:14077410:SSL routines:SSL23_GET_SERVER_HELLO:sslv3 alert handshake failure" )
08:38:12 <ski> kuribas : the denotational semantics doesn't distinguish between partiality and non-termination
08:39:40 <jle`> you should feel a little bit gross if you have to use spoon
08:39:58 <ski> also, in case you can distinguish different kinds of failure (such as different arguments passed to `error', e.g.), you can then use this to detect evaluation order, which isn't good
08:40:01 <cocreature> jle`: only a little? :)
08:41:10 <ski> (in the `IO' case, we can blame this on general nondeterminacy of I/O, though)
08:42:30 <lyxia> Refining the semantics of Haskell to see exceptions doesn't seem impossible.
08:43:01 <int-e> AWizzArd: I guess the author isn't a Haskell programmer.
08:43:34 <AWizzArd> int-e: likely not, but I liked this one graph that showed the Haskeller’s point of view on this :)
08:43:39 <int-e> AWizzArd: Because trying to use types for 100% assurance in Haskell would be insane for anything but the most boring programs.
08:44:02 <cocreature> what does 100% assurance even mean?
08:44:11 <AWizzArd> true
08:44:14 <cocreature> who defines what is correct
08:44:27 <int-e> Well, I took it to mean that you've written a correct program according to some specification.
08:44:44 <cocreature> my point is in most cases such a specification simply does not exist
08:44:57 <cocreature> or at least not one that is sufficiently precise to say that a program is correct
08:45:05 <int-e> (Also they do mention Idris and Agda, so there should be some space left to go.)
08:46:05 <ertes-w> AWizzArd: premise of the article: "type systems are only for checking correctness"
08:46:29 <ertes-w> AWizzArd: that's why C#, go and java programmers shouldn't write about the merits of type systems
08:46:33 <AWizzArd> In the corresponding reddit discussion thread they also talk about advantages in IDEs.
08:46:34 <ertes-w> because they don't even know what's possible
08:47:16 <ertes-w> an article like that is bound to be nothing but useless flamewar material
08:50:33 <Geekingfrog> ertes-w: besides correctness, I can think of "making illegal state non-representable" but not much. What kind of thing do you have in mind (that could be useful in such discussion with someone «who doesn't even know what's possible»)?
08:50:46 <jle`> it helps guide your development
08:50:50 <jle`> makes it faster to write programs
08:51:12 <jle`> it takes me ten times as long to write anything non-trivial with python because i don't have types and typed holes to help me
08:51:22 <jle`> and i have to constantly refer to documentation to know what works with what
08:52:10 <AWizzArd> It can help to have better runtime efficiency. Faster execution, less memory consumption.
08:52:14 <exarkun> pft just memorize every api and write unit & functional & property tests for everything
08:52:33 <jle`> typed holes help so much :'(
08:52:34 <dolio> Geekingfrog: If your types get fancy enough, you can have the language try to infer the implementation from the types, and only require your input when it can't figure something out.
08:52:42 <AWizzArd> A specialized IntMap can be faster and more memory efficient than a general hash map.
08:52:48 <int-e> AWizzArd: Oh and the green line should rise initially; the point being that certain simple bugs become compile errors rather than runtime errors (possibly delayed because the program trashed some values).
08:53:30 <AWizzArd> What I don’t understand is why static typing should cost so much productivity. Why does it slow you down? Is your code suddenly less reusable?
08:53:38 <jle`> yeah the green line always going down has been the opposite of my experience
08:53:44 <dolio> Haskell already does some amount of type-directed program inference in a way.
08:53:47 <dolio> With type classes.
08:54:15 <dolio> The meaning of such code is undefined without the types.
08:54:52 <haskkellwe> if length x == 1+functionThatGivesDoubles then ...etc. Length is an int, functionThatGivesDoubles gives a double. How can I make this work?
08:54:53 <jle`> AWizzArd: the common idea is that you have to spend time thinking about types, so this takes time
08:55:02 <jle`> haskkellwe: what do you really want to do?
08:55:30 <jle`> haskkellwe: compare if the length is less than or greater than 1 + thing?
08:55:35 <haskkellwe> check if  the length of the string x matches the number 1+functionthatgivesdouble prints
08:55:35 <int-e> . o O ( You *will* think about types. You *may* make this implicit in your program. )
08:55:41 <jle`> also keep in mind that doubles are not functions
08:55:48 <lyxia> haskkellwe: length x == 1 + floor (function)
08:55:53 <int-e> Uh, I meant to write "explicit" there.
08:55:59 <jle`> haskkellwe: hm, do you realize that comparing doubles for equality is inherently wonky?
08:56:00 <AWizzArd> jle`: who will sit down for hours and hours, thinking about types? This is a process that shouldn’t be dramatically time-consuming. At least not in one single session. Bad descisions will cause lots of refactoring though.
08:56:25 <ski> int-e : same thing
08:56:28 <jle`> haskkellwe: then you can do abs (fromIntegral (length xs) - 1 + thing) < 0.0001
08:56:43 <jle`> er, - (1 + thing)
08:57:07 <jle`> comparing doubles for equality is something that really doesn't make too much sense
08:57:22 <dolio> Even Java has code that is meaningless without types, because of function overloading.
08:57:31 <ski> AWizzArd : i think int-e means that, at least in some sense, you're already thinking about types, whether you know it or not
08:57:57 <haskkellwe> jle`: it's not really a double. I mean that its type, but it gives whole numbers
08:58:13 <jle`> haskkellwe: yes, because you go through Double, you already lose guaruntees of equality working
08:58:23 <jle`> why don't you just have it return an Int, instead of a Double ?
08:58:31 <jle`> if it gives whole numbers...you should state that in its type
08:58:38 <haskkellwe> It needs to give doubles in all cases except this one
08:59:05 <dminuoso> If I want to skip any further computations in my custom state, would EitherT be the appropiate way to handle this?
08:59:09 <jle`> you can compare length x with `1 + round thing`
08:59:20 <jle`> you can round it
08:59:22 <jle`> round :: Double -> Int
08:59:24 <ski> dminuoso : `ExceptT'
08:59:41 <jle`> but really it depends on what you want, you're already working with something that's inherently unsound
09:00:08 <jle`> `length x == 1 + round thing` should typecheck
09:00:12 <ertes-w> Geekingfrog: type classes for example…  inference is not only there to reduce the need for writing type signatures, but they can also infer parts of your *program*
09:00:29 <dminuoso> ski, mmm, so I would just "throw" the computed value as an exception?
09:00:41 <ski> yes
09:00:59 <ertes-w> Geekingfrog: similarly a lot of the derivation mechanisms of haskell/GHC are type-bound…  in short: types help me write less code, and write it much faster
09:01:22 <ski> dminuoso : with `ExceptT e (StateT s m) a', the state will survive an exception. with `StateT s (ExceptT e m) a' it won't
09:01:33 <haskkellwe> Thanks jle I'll see if I can reconstruct it so I don't need to do this instead
09:01:45 <Geekingfrog> ertes-w: what do you mean by inferring part of the *program*?
09:01:47 <jle`> haskkellwe: but yes keep in mind that this might behave unexpectedly.  this isn't a haskell thing, just an ieee thing in general for all languages
09:02:22 <ertes-w> Geekingfrog: let the compiler write your code
09:03:03 <ski> @where floating-point
09:03:03 <lambdabot> "What Every Programmer Should Know About Floating-Point Arithmetic" at <http://floating-point-gui.de/> and "What Every Computer Scientist Should Know About Floating-Point Arithmetic" by David
09:03:03 <lambdabot> Goldberg in 1991 at <http://docs.sun.com/source/806-3568/ncg_goldberg.html> and <http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.102.244>
09:03:29 <Geekingfrog> through the generic deriving mechanism you mean?
09:03:38 <ertes-w> Geekingfrog: just look at the following simple example and consider how much code GHC writes for me there:
09:04:05 <ertes-w> @let data V3 a = V3 !a !a !a  deriving (Eq, Foldable, Functor, Ord, Show, Traversable)
09:04:07 <lambdabot>  Defined.
09:04:37 <kuribas> "Why then is it, that we don't all code in Idris, Agda or a similarly strict language?"  Because idris isn't ready for production maybe?
09:04:44 <AWizzArd> ski: exactly, this thinking I support. Even if you are in a very dynamically typed system. You will think about types at some level.
09:05:18 <jle`> Geekingfrog: there are also libraries whose APIs are built around being able to write your entire program by just specifiying their types
09:05:21 <jle`> Geekingfrog: ie, servant
09:05:22 * JuanDaugherty learned about gap functions and floating point long ago
09:05:30 <exarkun> kuribas: Nah
09:05:37 <ertes-w> > ((traverse . ix 0 %~ toUpper) (V3 "abc" "def" "ghi")) ^. id
09:05:40 <lambdabot>  V3 "Abc" "Def" "Ghi"
09:05:44 <jle`> Geekingfrog: servant lets you specify the type of your API, and then writes your entire server for you
09:05:46 <ertes-w> > ((traverse . ix 0 %~ toUpper) (V3 "abc" "def" "ghi")) ^. traverse
09:05:46 <exarkun> kuribas: It's because most programmers don't recognize the problems that a language like Idris solves
09:05:48 <lambdabot>  "AbcDefGhi"
09:05:50 <ski> AWizzArd : it's not obvious that this informal thinking will correspond that closely to any particular formal static type system, though
09:05:56 <kuribas> exarkun: also :)
09:06:01 <exarkun> kuribas: PHP solves all the hard problems already!  How to write a string to a web browser, etc.
09:06:04 <ertes-w> Geekingfrog: ^ this code is almost entirely inference-driven
09:06:16 <ertes-w> not the types, but the code itself
09:06:33 <kuribas> exarkun: except to make maintainable and secure codebases.
09:07:01 <ertes-w> Geekingfrog: through type information GHC knows how to write 'traverse', and through the type class mechanism it knows how to deal with (^.)
09:07:01 <AWizzArd> ski: exactly. I feel how I fail time after time when writing dynamically typed code. Knowledge about Haskell doesn’t help. I really *need* a tool like ghc and be concrete about types. With such a helper it can be made to work.
09:07:30 <ertes-w> Geekingfrog: just thinking of writing something like this in python makes my head explode
09:07:38 <exarkun> kuribas: well, of course they're all wrong :)
09:08:03 <ertes-w> Geekingfrog: but then this is equally difficult, if not worse, in go
09:10:01 <mlehmk> the issue with base2 floating points can be changed with using decimal floating point numbers
09:10:23 <mlehmk> then it is a different problem, but works quite well for money
09:10:50 <Geekingfrog> I'm having trouble with a golang fan colleague of mine, who don't see the value of generics at all because he may need them once or twice in a year of coding (mostly servers and infrastracture tools) -_-'
09:11:04 <mlehmk> although in haskell, you can use fractions
09:11:05 <Psybur> So lets say I have a data T a. Is there an easier way to get the value other than (\(T a) -> a) ?
09:11:23 <ertes-w> Geekingfrog: it's the usual "if you don't know it, you won't miss it" problem
09:11:25 <Geekingfrog> Though the example with typeclasses is nice, if someone doesn't see the value of having generic, it's way too abstract at that point :/
09:11:33 <ski> Psybur : record syntax
09:11:34 <Psybur> Sorry, a data T = T a
09:11:47 <Geekingfrog> ertes-w: yeah, but it makes arguing with him difficult.
09:11:54 <kadoban> Geekingfrog: The sign of a fanboy, IMO. "The langauge I like doesn't have them, therefore they have no value"
09:12:01 <mlehmk> I know generics as, I write an algorithm once and add the datatypes later
09:12:04 <Psybur> ski, anything other than record syntax? :D
09:12:20 <ski> Psybur : define an accessor manually ? :)
09:12:35 <kuribas> ertes-w: x = ["abc", "def", "ghi"]; [string.capitalize(s) for s in x]
09:12:35 <Psybur> ski, anything built in that gets the first value of any data type? :D
09:12:48 <ski> nope
09:12:51 <Psybur> :{
09:12:59 <ertes-w> Geekingfrog: the only way for him to appreciate parametric polymorphism is to use it for a few months…  in other words: if they aren't open for changes, don't waste your time arguing
09:13:09 <jle`> Psybur: what would the type of that thing even be?
09:13:27 <ertes-w> kuribas: you're missing the point
09:13:29 <Psybur> jle`, Any :D
09:13:42 <kuribas> ertes-w: I am just guessing what a pythonist would do...
09:13:43 <jle`> if its type is 'Any', it wouldn't be very useful
09:13:49 <Geekingfrog> ertes-w: yeah, it's more these kind of circular discussion at lunch, a bit like politics with friends when slightly drunk :D
09:13:52 <jle`> cause then you can't do anything with it
09:13:59 <jle`> besides simple things like 'id', 'const', etc.
09:14:19 <ongy> Geekingfrog: how do they implement generic data structures? the horrible unsafe C way?
09:14:25 <Psybur> Yeah I guess you're right, we strict here
09:14:40 <Psybur> Well not strict, strong? Static?
09:14:49 <Geekingfrog> ongy: interface{} ftw, or, more realistically, one only needs hashmap and slices anyway, so why bother ?
09:15:08 <ertes-w> Geekingfrog: and keys are always strings, right?
09:15:28 <Geekingfrog> most of the time, though I don't know if that's a restriction of the language.
09:15:31 <ski> @quote please.talk
09:15:31 <lambdabot> Dave_Benjamin says: please talk to your son or daughter about parametric polymorphism
09:16:21 <ongy> I could talk to my flatmates about it. I should ring the doorbell and ask them if they found to the truth
09:16:46 <JuanDaugherty> there's a handbook of floating point arithmetic now
09:17:37 <JuanDaugherty> 567 pp
09:17:52 <ongy> does it cover all modes of the FPU and the simd instructions?
09:18:03 <JuanDaugherty> not flying cars but still!
09:19:12 <bartavelle> Geekingfrog: just take a loot at https://www.reddit.com/r/programmingcirclejerk/ , you will find many examples of people having stockholm syndrom for their languages, be it go, python, rust or haskell. Unfortunately this is religious territory, and you will have a hard time winning your coworkers over ...
09:19:15 <JuanDaugherty> no, there's an implementation/hardware related section but it's 2000
09:19:41 <JuanDaugherty> actually yes there is 
09:19:50 <JuanDaugherty> forgot that stuff came in the 90s
09:21:34 <mizu_no_oto> ongy: AFAIK, they hacked in generics for a few built-in data structures.
09:21:35 <JuanDaugherty> modes and specific instructions would be model specific anyway
09:21:48 <ertes-w> well, i'm not a religious person, and even i think that haskell is the best general purpose language in existence
09:21:55 <Geekingfrog> I'm already resigned anyway. Go is just too simple to learn compared to haskell, and in a company where people are fairly resistant to change, and a fair bit of developer churn, I'm not going to kill myself trying to push haskell.
09:21:58 <mizu_no_oto> So you're fine if you just want a hashmap, and SOL if you need to implement an oct-tree
09:22:24 <bartavelle> ertes-w: yes, like any sane person would
09:22:33 <Geekingfrog> «who needs complicated data structure? I'm not writing fancy algorithm, just simple backend software»
09:22:43 <Geekingfrog> bah
09:23:10 <ongy> bartavelle: I like the graphs in this one https://blog.merovius.de/2017/09/12/diminishing-returns-of-static-typing.html (somewhat on topic)
09:23:49 <bartavelle> ongy: perhaps you'd like mine? https://i.imgur.com/WXZeLLn.png
09:24:26 <mizu_no_oto> Geekingfrog: If you can't introduce Haskell to your company, you could always try introducing yourself to a company that already uses FP
09:24:48 <mizu_no_oto> You know, invert the problem
09:25:28 <bartavelle> Geekingfrog: I believe that sum types + pattern matching is a more compelling feature compared to parametricity for the unbelievers
09:25:41 <bartavelle> you can often take a tangled bit of logic and make it all simple in haskell
09:25:52 <bartavelle> while showing you can make some invalid states irrepresentable
09:26:11 <bartavelle> bonus point if you rewrite that way a piece of code that is constantly buggy
09:26:30 <bartavelle> (of course, that is probably hard, and will not be that clear in the end)
09:26:54 <ertes-w> bartavelle: your graph is wrong…  i'm a haskell programmer, and for me the curves take off *after* HKT
09:27:00 <ongy> bartavelle: HKT?
09:27:05 <bartavelle> higher kinded types
09:27:09 <ertes-w> ongy: higher-kinded polymorphism
09:27:12 <ongy> ahh
09:27:18 <bartavelle> yeah it should be HKP :(
09:27:19 <monochrom> mizu_no_oto: And this also faces the dual problem of having a stockholm syndrome with their employers :D
09:27:20 <bartavelle> damned
09:27:24 <Geekingfrog> yes, I usually go with some examples around ADT + pattern matching, it's quite simple conceptually, yet immensely powerful
09:27:44 <ertes-w> HKT is particularly difficult to sell
09:28:00 <ertes-w> it's one of the most compelling features of haskell, yet one of the most difficult to explain, i found
09:28:17 <ertes-w> not only to explain what it is, but also what it's good for
09:28:53 <ongy> what exactly is it? be generic over differently kinded types?
09:29:00 <mizu_no_oto> ertes-w: if you have a black board, you could try explaining why you can't make a decent Functor interface in C#/Java
09:29:02 <ertes-w> HKT makes all these categorical concepts like monads actually useful as abstractions
09:29:07 <jle`> be generic over non-* types
09:29:11 <bartavelle> ongy you can abstract over type constructors, not just types
09:29:16 <ertes-w> ongy:
09:29:17 <ertes-w> :t length
09:29:19 <lambdabot> Foldable t => t a -> Int
09:29:46 <ertes-w> note that 't' is not a concrete type…  you will have something like (t = Maybe) rather than (t = Maybe Integer)
09:29:56 <ongy> oh, so just that I can abstract over things that are not just * but also * -> * if I want to?
09:30:02 <ertes-w> yes
09:30:05 <jle`> anything not-*
09:30:15 <bartavelle> ertes-w: make them work in Elm for a while, then show them they rewrite 10x mapM for distinct data types? ;)
09:30:26 <ongy> erm "just" :) this is one of the coolest features
09:30:29 <monochrom> The t there being a variable rather than a hardcoded "List" or "Vector"
09:30:29 <monochrom> In Java you fake this with "List<A> is a subtype of Container<A>"
09:30:33 <ongy> jle`: don't the curry rules apply? :)
09:31:42 <monochrom> So it is hard to sell to people who believe in subtyping.
09:31:45 <ertes-w> bartavelle: make them work in F# and tell the MS boneheads what monads *really* are about
09:31:55 <bartavelle> should work also!
09:32:01 <ski> ongy : also stuff like `data Tree branch a = Leaf a | Branch (branch (Tree branch a))' or `type List ref a = ref (Cell ref a); data Cell ref a = Nil | Cons a (List ref a)', where a parameter of your type has a kind other than `*'
09:32:18 <ertes-w> people look at F# and conclude that monads are…  "uhm…  kinda nice, but what's the big deal?"
09:32:22 <ongy> to be honest, I don't quite see the difference between saying we abstract over the typeclass than abstraction over an Interface in java (but I'm also not to well versed in type theory)
09:32:23 <c_wraith> ertes-w: it's not like Microsoft doesn't know.  They've been the major financial backer of GHC for a *long* time. :P
09:32:26 <mizu_no_oto> monochrom: the big issue in Java that's really simple to point out is the 'loss of information problem' with fluent interfaces or immutable classes and subtyping.
09:32:31 <monochrom> But then I wonder about monad transformers. That may break the fiction-by-subtyping.
09:32:42 <ertes-w> c_wraith: those MS .NET boneheads then =)
09:33:11 <geekosaur> isn't .NET constrained by the relatively simple typing CLR supports?
09:33:21 <geekosaur> which is kinda lowest common denominator
09:33:26 <ertes-w> it is
09:33:48 <bartavelle> ongy: I don't think you can use interfaces to implement something like >>=
09:33:52 <bartavelle> no expert though
09:34:24 <mizu_no_oto> Where the loss-of-information problem is that you don't want to return something as a supertype, usually.  e.g. if you have a Monoid interface, you don't want mappend to return a Monoid, you want it to return the Foo that extends Monoid.
09:34:37 <ski> ongy : if you take two `Container<A>' as inputs, you can't statically guarantee that they're actually using the same concrete container type
09:35:05 <ski> (nor can you guarantee that the container type you return is the same as the one you take as input)
09:35:25 <ongy> ahh, that makes sense
09:35:45 <ongy> then we get to the rust sillyness of iterators
09:36:03 <ongy> mhh, probably not for that reason
09:36:14 <mizu_no_oto> You can solve that for non-generic types with an f-bound ("class Foo extends Monoid<Foo>"), but you start needing HKT fairly quickly.
09:36:17 <monochrom> Oh but people hate that static guarantee too. Because they hate the prerequisite static requirement.
09:36:54 <monochrom> We have "(+) :: Num a => a -> a -> a" and people are like "why can't I give it one Int one Double and it automagically figures out what to return?"
09:36:55 <bartavelle> well, i can just IsInstanceOf forever anyway
09:37:07 <ertes-w> implementing (>>=) is not the problem
09:37:21 <bartavelle> ertes-w: what would be its type?
09:37:27 <monochrom> We have "(<>) :: Monoid a => a -> a -> a" and people are like "why can't I give it one List one Set and it automagically figures out what to return?"
09:37:38 <remexre> I'm trying to debug a "deep" (callstack-wise) function, where I know that one of the type parameters to the function is Show, but signatures (all the way up) don't guarantee it. Is there a way to (only for debugging, I don't care how slow/unsafe it is, it's right before an (error "DEBUG")) dynamically show, or to show an arbitrary a such that it prints <not show> or the like if the value isn't really Show?
09:37:47 <ertes-w> bartavelle: the problem is that you need to implement maybeBind, listBind, etc.
09:38:04 <ski> mizu_no_oto : hm, yea. i saw that mentioned recently, and thought that that was a bit similar to the handling of binary methods and clone methods i OCaml
09:38:05 <bartavelle> ertes-w: yes, exactly
09:38:23 <ertes-w> F# just provides a syntax to hack around that limitation, but then you still can't abstract over monads
09:38:46 <bartavelle> you don't have "functors" like ocaml has in F#?
09:39:39 <ski> remexre : have you tried breakpoints and `:print' ?
09:40:03 <remexre> ski: I'm wasn't running it in GHCi, but I'll try that
09:40:28 <ski> bartavelle : unfortunately F# lacks the ML module system
09:41:16 <bartavelle> ski: and is backpack the exact same thing, or is it quite different, if you happen to know?
09:42:04 <ski> i haven't looked in detail at backpack, only heard it claimed to be "similar"
09:49:58 <remexre> So what I'm actually trying to print is the state inside a newtype that wraps StateT (among other things)
09:50:22 <remexre> So GHCi is giving me a binding _result :: MyMonad fn (...)
09:50:40 <remexre> So I'm doing (_result >> get)
09:50:53 <remexre> Just by itself, I get no-instance-for-show (which makes sense)
09:51:04 <remexre> But :print claims that (_result >> get) is a syntax error?
09:51:49 <bartavelle> perhaps let x = _result >> get, then :print x ?
09:52:09 <remexre> Oh, it doesn't support arbitrary expressions?
09:52:10 <remexre> huh
09:52:29 <bartavelle> IDK
09:52:32 <remexre> x = (_t5::Interpreter fn (InterpreterState fn))
09:52:32 <remexre> :/
09:52:33 <monochrom> I would read the GHC users guide carefully.
09:52:55 <glguy> remexre: What did you think it would do?
09:53:10 <bartavelle> remexre: just type "x" now
09:53:28 <bartavelle> oh but
09:53:29 <bartavelle> no
09:53:35 <bartavelle> you need to runState it
09:53:48 <bartavelle> I suppose, don't know what Interpreter is
09:53:50 <remexre> glguy: recognize that at the breakpoint, fn is actually a Show type
09:54:09 <glguy> remexre: What's a MyMonad:?
09:54:23 <remexre> newtype Interpreter fn a = Interpreter { unInterpreter :: ExceptT (Error fn) (StateT (InterpreterState fn) IO) a }
09:54:31 <remexre> MyMonad == Interpreter
09:54:52 <glguy> remexre: There's no 'a', nor a 'InterpreterState fn' inside an Interpreter fn a
09:55:35 <remexre> glguy: ((_ :: Interpreter fn a) >> get) :: Interpreter fn (InterpreterState fn), no?
09:55:36 <bartavelle> also what does :t _result say ?
09:55:58 <remexre> _result :: Interpreter fn (Symbol, Symbol)
09:56:00 <glguy> remexre: That's the type, yes, and no, there's no 'InterpreterState fn' value there
09:56:51 <remexre> _result's type is the return type of the function I shoved the breakpoint in
09:56:56 <glguy> Your Interpreter type is the type of interpreter actions, not states inside the interpreter
09:56:57 <remexre> glguy: huh?
09:57:04 <remexre> glguy: oh, right
09:57:04 <remexre> :/
09:58:33 <remexre> Is there a way to do this, then? I just want a version of traceShow that uses whatever :print does, to avoid needing to change a bunch of signatures every time I need to add a debug print...
09:58:40 <Psybur> Anybody use Intellij? And anybody know how to set breakpoints/debug with it?
09:59:02 <Psybur> Cant seem to do it for haskell projects
09:59:48 <Psybur> Hmm think I found the instructions on the github for the plugin, may not need any hand holding :D
10:01:22 <monochrom> "get >>= \x -> traceShow x (return x)" maybe? Or a correction thereof.
10:01:44 <cocreature> Psybur: note that breakpoints in Haskell are often not very useful since lazy evaluation means that you’ll jump around a lot in your code
10:02:06 <remexre> monochrom: That works iff I remove all the type annotations in half a dozen files (or add a Show constraint to fn)
10:02:11 <bartavelle> monochrom: the problem of remexre is that he has no Show constraint
10:02:51 <bartavelle> and yes, that is something that I encountered often, where I do not want a show constraint, but I add one everywhere for debugging, and then have to remove it
10:03:38 <cocreature> there was a GHC proposal for some kind of implicit constraints that you could enable and disable for that exact usecase
10:03:56 <cocreature> but it didn’t get accepted :/
10:03:57 <ongy> can you unsafeCoerce to the type for debugging if you *know* which type it is?
10:04:17 <bartavelle> ongy: that sounds like a great idea
10:04:18 <remexre> Oh, quite possibl- wait, no, I get a circular module graph
10:04:35 <remexre> The reason I inject the fn is to break the loop
10:04:48 <bartavelle> oh, just write a megamodule already!
10:04:52 <remexre> :/
10:04:54 <bartavelle> hehe
10:05:20 <ongy> does that compile faster? I remember reading about a C(++) technique of that kind
10:05:35 <monochrom> Perhaps you should have kept the circularity.
10:06:16 <sm> Psybur: I use it, let us know if you succeed
10:06:23 <remexre> monochrom: Possibly, but I didn't want to have to deal with the hs-boot files (shouldn't GHC be able to autogenerate them...?)
10:07:02 <monochrom> No, there is a "I don't know what you mean" problem.
10:07:03 <Psybur> sm: Did not succeed. Looks like everythings there but I cant see to make a run/debug configuration, nor can I click a line and set a breakpoint
10:07:24 <cocreature> I vaguely recall that ezyang was working on autogenerating hs-boot files for recursive imports at some point but I don’t know what happened to that
10:07:38 <sm> Psybur: no standard debugger works that way with Haskell, since it's lazy
10:08:17 <cocreature> sm: that’s not true. the ghci debugger lets you set breakpoints just fine
10:08:27 <cocreature> they just behave a bit differently from other languages
10:08:43 <sm> cocreature: Psybur is expecting a standard IDE-style debugger to work
10:08:49 <Psybur> sm: So I cant set breakpoints from within intellij ?
10:09:00 <sm> no. If you get intellij to be aware of symbols and references, you're doing well
10:09:06 <monochrom> Oh, I guess there isn't such a problem, but it is an antithesis to separate compilation.
10:09:19 <JuanDaugherty> lexsah debugger works, or did, haven't really tested the new working build
10:10:12 <Psybur> sm, so how would I go about hunting where this suspected infinite recursion is going on? Make unit tests for everything?
10:10:17 <dolio> That would only be a problem with getting intellij to interact with GHC's debugger, presumably, not to do with the semantics of the language.
10:10:35 <Psybur> I seem to have written a lot of code without really testing any of it haha
10:10:36 <JuanDaugherty> *leksah
10:10:49 <sm> Psybur: haskellers use print statements (Debug.Trace) and intereactive exploration in GHCI much more than debugging with breakpoints
10:10:50 <remexre> Psybur: Honestly, that's what I'd do; look for recursive functions and quickcheck them
10:11:09 <monochrom> Ah but what if Intellj made assumptions about debugging that's incompatible with GHC's model?
10:11:17 <sm> but there are a few non-standard debuggers that can do it, with some effort. GHCI, leksah, HOOD (?)
10:11:44 <dolio> monochrom: That still has nothing to do with Haskell's semantics.
10:12:02 <Psybur> Guess Ill look into Debug.Trace for now
10:12:18 <sm> Psybur: yes, that's often the quickest way to pinpoint a hang
10:12:32 <dolio> GHC's debugger is a pretty ordinary style of debugger, I think.
10:12:57 <dolio> Set breakpoints at source locations, execute stuff until you get there.
10:14:22 <Psybur> IM seeing mention of being able to turn debug trace on/off but I dont see it anywhere? Is that some flag I pass?
10:15:48 <zachk> write a helper function to wrap trace in a conditional and set a variable called debug=True and have the function rely on the debug variable
10:16:36 <monochrom> No, can't be turned off. Control.Exception.assert is the one that can be disabled.
10:16:41 <sm> Psybur: I don't know if there's anything official, but I use Hledger.Utils.Debug to control it with a commandline flag
10:17:28 <sm> but often add/remove trace, reload in ghci, test is good enough
10:17:57 <Psybur> Ok. Thanks for the input guys
10:19:31 <Psybur> Found it. need a pattern that returns [] ;p
10:20:36 <biglambda> Hi, I’m using the file-embed template haskell package to embed some OpenCL source in my haskell project. How can I get the build system to recompile the embedding code when I make changes to the OpenCL source?
10:21:12 <bwe> Hi, is there a web shop written in Haskell? From full-suite (like Magento) to web framework.
10:21:26 <cocreature> biglambda: how are you building your project?
10:21:39 <biglambda> Right now cabal.
10:21:46 <biglambda> Stack not quite working yet.
10:21:53 <cocreature> biglambda: cabal new-build?
10:22:03 <biglambda> cabal build
10:22:16 <cocreature> hm that should work automatically
10:22:32 <biglambda> Ok. Perhaps it is ;)
10:23:02 <biglambda> I assumed the build system ignores my OpenCL source file.
10:23:08 <biglambda> But I’ll test.
10:23:10 <cocreature> templae-haskell provides an "addDependentFile" package for this usecase which is used by "file-embed"
10:23:22 <biglambda> Aha.
10:23:37 <cocreature> that’s broken in new-build which is why I was asking about that :)
10:23:48 <biglambda> Ok good to know.
10:24:12 <biglambda> I’m sticking with build system I have for a bit.
10:24:20 <biglambda> Thanks for your help.
10:39:51 <whitephoenix> I'm super new and just spent a while trying to figure out how to get this simple function to print i every time around, can someone help? http://lpaste.net/358390
10:40:39 <geekosaur> you probably want to read http://www.vex.net/~trebla/haskell/IO.xhtml
10:41:15 <geekosaur> that is a pure function, there are ways to force it to print something but you will only learn some things about how pure functions get evaluated in various contexts
10:41:39 <geekosaur> (i.e. you get no promises whatsoever about how many times it is called)
10:41:56 <glguy> whitephoenix: http://lpaste.net/358390
10:42:59 <whitephoenix> Awesome, thank you. It feels kind of weird asking a question so simple that I could solve in just about every other language
10:51:20 <ezyang> cocreature: The idea is sound, it just still hasn't happened yet :) 
10:51:48 <cocreature> ezyang: is there some writeup of the progress you made and what’s still left to do?
10:51:50 <ezyang> bartavelle: Backpack is not exactly OCaml Functors, but it is definitely in the same spirit 
10:52:07 <ezyang> cocreature: I didn't actually work on it at all. So the status is in the recursive modules ticket. 
10:53:36 <cocreature> ezyang: alright, thanks for the info!
10:56:50 <NemesisD> is anyone aware of a more granular version of the haskell numeric typeclasses that would give me a typeclass that supports addition, zero but no subtract or negate?
10:57:09 <raynold> ahh it's a wonderful day
10:57:38 <ski> perhaps not what you're looking for, but have you considered `Monoid' and `Sum'
10:57:45 <ski> ?
10:58:45 <NemesisD> i'm actually working with a monoid, but i have a newtype over Int that has a few invariants, some of which are >= 5. i wrote some monoidal aggregation code but right now it requires Num and i don't want to implement that for my type
10:59:10 <NemesisD> the reason being num would allow you to violate the invariant by negating or subtracting two valid values
10:59:47 <NemesisD> so if i had a typeclass Add or something that already had instances for other common types, i could safely add an instance for my newtype
10:59:58 <c_wraith> NemesisD, the most expansive effort I know of in that direction is subhask
11:00:05 <ski> quite possibly there's an `AdditiveMonoid' class somewhere
11:00:10 <dmwit> ?hackage numeric-prelude
11:00:10 <lambdabot> http://hackage.haskell.org/package/numeric-prelude
11:00:31 <dmwit> NemesisD: But I don't understand your comments about requiring `Num` to implement `Monoid`.
11:00:51 <dmwit> Surely you can write a `Monoid` instance that uses `Int`'s `Num` instance without writing a `Num` instance for your wrapper.
11:01:52 <NemesisD> dmwit: i'm using Data.Monoid.Sum right now which has Num a => Monoid (Sum a). i'd like to write a more specific Sum that does not carry such a monolithic constraint
11:02:07 <dmwit> Right. What's stopping you from doing that?
11:02:29 <dmwit> Okay. I understand now.
11:02:37 <dmwit> You still want it to take a type parameter.
11:03:01 <NemesisD> nothing technically, but i've seen enough haskellers complain about Num that i expected there was something extant that did this. i can write my own but it is as useful as the instances it implements
11:03:26 <NemesisD> there's a good chance this module doesn't leave the project so i could just write one instance for my newtype and be done
11:03:49 <c_wraith> for all the complaints about Num, no one has come up with something unequivocally better.
11:04:18 <c_wraith> that isn't due to lack of trying. it might he due to lack of sufficient language features.
11:04:25 <c_wraith> *be due
11:05:03 <NemesisD> is there a CT concept for supporting zero and + only? i guess that's basically just a specific monoid right?
11:05:46 <c_wraith> CT doesn't care what you name your monoid operators. :)
11:07:05 <NemesisD> oh, i suppose i could do a class Add and offer a default instance if you've got Num
11:07:25 <zachk> :t mplus
11:07:26 <lambdabot> MonadPlus m => m a -> m a -> m a
11:07:32 <zachk> :t mzero
11:07:33 <lambdabot> MonadPlus m => m a
11:08:14 <NemesisD> i don't think its a monad though. like you couldn't write a monadplus for int
11:08:42 <Psybur> If I want to return a [IO ()], how come [return ()] returns a [()] and not a [IO ()] ?
11:09:08 <zachk> you are returning from a list monad context Psybur 
11:09:35 <tabaqui> I'm not sure, but arguments in examples look inverted
11:09:37 <tabaqui> https://hackage.haskell.org/package/transformers-0.5.4.0/docs/Control-Monad-Trans-Except.html#v:catchE
11:10:30 <geekosaur> Psybur, is this ghci? ExgtendedDefaultRules will pick [] if it can't decide the m, and being inside a list the normal ghci thing of trying IO first won't apply
11:11:10 <Psybur> I am running it in ghci
11:11:19 <geekosaur> then you will need to specify
11:11:45 <dmwit> Psybur: I challenge your assertion. `[return ()]` does not return `[()]` for me.
11:11:58 <dmwit> :t [return ()]
11:12:00 <lambdabot> Monad m => [m ()]
11:12:08 <dmwit> :t [return ()] :: [()] -- nope
11:12:10 <lambdabot> error:
11:12:10 <lambdabot>     • Couldn't match expected type ‘()’ with actual type ‘m0 ()’
11:12:10 <lambdabot>     • In the expression: return ()
11:12:58 <geekosaur> I think it was actually [[()]] since [()] is missing the m component
11:13:07 <geekosaur> i.e. they are talking about the type inside the outer list
11:13:16 <dmwit> I propose that we wait to explain the problem until we know what it is.
11:13:23 <Psybur> Oh whoops, I had an error on a different pattern
11:13:32 <Psybur> Seems its working now :D
11:13:34 <Gurkenglas> They complains that it's [()] and not [IO ()], and that wouldn't work in your model geekosaur
11:14:07 <Psybur> My b :D
11:43:53 <tabaqui> is there a simple way to enumerate all possible exceptions that function can raise?
11:44:39 <Tuplanolla> No, because all of them can raise some, tabaqui.
11:45:43 <tabaqui> not sure, what you mean
11:46:14 <tabaqui> exception doesn't mean that program is totally crashed or something
11:47:09 <Tuplanolla> You could trigger an exception inside an `unsafeInterleaveIO` thunk or a runtime system failure at any point.
11:48:13 <tabaqui> ok, we don't use unsafe functions
11:48:23 <tabaqui> ah damn
11:48:31 <tabaqui> hGetContents uses it
11:49:05 <tabaqui> it's contrintuitive though
11:49:20 <tabaqui> we have a tree of function calls
11:49:31 <tabaqui> and some of nodes can raise exceptions
11:49:42 <tabaqui> and send them up to the root
11:51:00 <tabaqui> there are no indeterminacy in possible exception types
11:51:27 <ski> each node inside an input data structure could potentially raise a failure, the calling code being responsible
11:52:49 <tabaqui> well, input data is just a function argument
11:53:03 <ski> yes
11:53:50 <ski> point being that it'd be hard to keep track of all this even somewhat precisely, in a non-strict language
11:54:38 <ski> if you output a list, you'd have a set of exceptions associated with the first cell, a separate one for the next cell, &c.
11:54:49 <ski> and also for the elements, of course
11:55:35 <tabaqui> all cells have the same type
11:56:00 <ski> a simplification would be to union all the sets for the cells, and (separately) union all the ones for the elements .. but then that's not as precise, and may not be good enough for keeping track of what you want
11:56:01 <tabaqui> so exception types associated with each are the same too
11:56:20 <ski> i don't see how that necessarily follows
11:56:38 <tabaqui> well, I have some magic function
11:56:45 <ski> if i have `data B = Mk A A', i don't see why one'd necessarily want to union the sets for the two `A' components
11:57:05 <tabaqui> getExceptionTypes takes a type and returns a set of types
11:57:12 <tabaqui> *takes a function
11:57:23 <ski> s/a function/any value/
11:57:32 <tabaqui> yeah
11:57:40 <tabaqui> no!
11:57:44 <tabaqui> wait a sec
11:58:43 <tabaqui> ok, any value
11:59:12 <tabaqui> so, our rules:
11:59:38 <tabaqui> if value is already in normal form, then getExceptionTypes v = empty
12:00:07 <tabaqui> if value is a result of applying a haskell function to some value
12:00:25 <tabaqui> then getExceptionTypes v = gET f <> gET w
12:00:29 <tabaqui> where v = f w
12:00:43 <ski> did you see
12:00:44 <ski> @google a semantics for imprecise exceptions
12:00:45 <lambdabot> https://www.microsoft.com/en-us/research/publication/a-semantics-for-imprecise-exceptions/
12:00:46 <ski> btw ?
12:01:02 <tabaqui> nope
12:01:49 <ski> `gET f' is problematic
12:02:49 <ski> the exceptions possibly raised by evaluating a function expression are distinct from the ones possibly raised when applying that function result to an argument expression
12:04:16 <tabaqui> okay, go few layers down
12:04:22 <ski> (this is similar to how the exceptions for an expression of pair type is distinct from the exceptions for the two components of the pair)
12:04:24 <tabaqui> f = p . q and so after
12:04:55 <tabaqui> look at the lowest level
12:05:00 <ski> `\x -> error "foo"' won't raise an exception. applying it to an input will
12:05:34 <ski> not distinguishing between these two sets means that you'll have to pretend that the former can raise an exception
12:06:13 <tabaqui> I don't want possible exceptions and only them
12:06:39 <tabaqui> it's fine if "gET (const 1 (error "foo"))" returns non empty set
12:06:56 <the_2nd> is there a nicer syntax for foo >>= bar >>= baz ? similar to do for >>   
12:07:36 <reactormonk> the_2nd, `do someVar <- foo`
12:08:00 <the_2nd> but this still requires me to bind the result and reuse it
12:08:01 <tdammers> do { a <- foo; b <- bar a; baz b }
12:08:25 <the_2nd> that's more text than the piping via >>=
12:08:50 <the_2nd> * by nicer I mean less text / shorter
12:09:04 <ski> tabaqui : *nod*. just saying that i suspect that to get a simple enough system, you'll have to make it imprecise enough to not be of much use. i might be wrong, though
12:09:35 <tdammers> the_2nd: yes, if this kind of monaic chaining is actually what you want, I'd just write it with the >>=
12:09:45 <tdammers> the_2nd: or maybe =<< if you prefer reversing the chain
12:10:00 <ski> sometimes `<=<' or `>=>' is nice to use
12:10:10 <tdammers> the_2nd: that's about the nicet syntax you can get for this particular example
12:10:13 <tabaqui> I hope that "Socket.send" cannot return a hundred different exceptions
12:10:48 <the_2nd> well, too bad. but not too bad
12:11:31 * ski . o O ( "well, X. but not X" )
12:11:42 * ski . o O ( contradiction .. profit ? )
12:12:33 <tabaqui> if 0 == 1 then I'm the pope
12:13:26 <ski> missing `else' ? :)
12:14:40 <tabaqui> (if (== 0 1) (am I '(the pope)))
12:15:40 <tabaqui> oh, CL has no ==
12:15:46 <tabaqui> 'eq' then
12:16:41 <ski> fwiw, Scheme has `='
12:24:27 <tdammers> ski: ex falso quodlibet
12:25:16 <Jiehong> Hi everyone on this channel
12:27:02 <Jiehong> I'm currently wondering about what could be the best functional architecture choice for my usecase. I've got several alternative in my mind, but I'd like some advice from smart guys ;)
12:27:27 <tabaqui> FRP, no doubt :)
12:27:57 <Jiehong> @tabaqui: haha, maybe
12:27:58 <lambdabot> Unknown command, try @list
12:28:48 <tabaqui> fine, all 1337 haskell hackers write their own DSL
12:29:20 <tabaqui> but I'm not leet yet )
12:29:49 <Jiehong> So, I've got to write an application whose purpose is to do the following chain of actions:
12:29:55 <tdammers> there's 1337 of us? I thought we were going to avoid success at all cost?
12:30:16 <ReinH> There are, in fact, 1606 of us.
12:30:22 * ski . o O ( "|aMEr5P3eCh" )
12:31:05 <Jiehong> 1. poll a distant serveur for a list of job to do. 2. process those actions (those can take like multiple minutes). 3. answer back with the status of the action done
12:31:41 <tabaqui> I have written a few network services for my job
12:31:54 <tdammers> Jiehong: and you are writing the client only, or do you also get to implement the server?
12:32:03 <Jiehong> Those actions need to be done sequentially (obviously), but I'd like to have more than 1 serie going-on at a given time (like a set limit configured to 10).
12:32:07 <tabaqui> I prefer a bunch of threads with shared TVar (Sequence a)
12:32:20 <ReinH> What you're describing sounds like a queue.
12:32:21 <Jiehong> I'm only writting the client here, so I can't change the API used…
12:32:38 <tabaqui> that receive sockets with blocking functions, put input in the queue
12:32:50 <tabaqui> and pure FRP logic that handles the queue
12:33:10 <Jiehong> Well, so far, I thought about using a stream for my sequence of actions, and have multiple of them, so that each can be on its own thread…
12:33:32 <ReinH> A pool of worker threads seems reasonable. Standard queueing stuff.
12:33:49 <ReinH> Put items in queue, workers take items from queue, pool size is controlled by configuration.
12:34:08 <Jiehong> I also thought about have an actor model, but… no, it's not composable…
12:34:11 <tdammers> ReinH: that would all be the server-side part, wouldn't it
12:34:14 <tdammers> ReinH: the queue, I mean
12:34:27 <ReinH> tdammers: The client can implement its own queue based on the list received from the server.
12:34:43 <tdammers> ReinH: oh, like that, yeah makes sense
12:34:46 <Jiehong> with a queue, I actually thought it might be better to have 1 queue per worker ?
12:34:55 <ReinH> why would you have one queue per worker?
12:34:56 <tdammers> so one thread to poll the server for jobs, and an internal job queue
12:35:10 <ReinH> That's effectively the same as not having a queue.
12:35:26 <Jiehong> what if you have enough threads that the queue doesn't fill up fast enough?
12:35:40 <tdammers> Jiehong: then the threads all block on the queue
12:35:43 <ReinH> "the" queue? I thought you wanted one queue per thread?
12:36:06 <ReinH> one queue per thread is basically the actor model
12:36:13 <ReinH> I don't see how the actor model helps
12:36:15 <tabaqui> btw, is performance a value?
12:36:39 <Jiehong> yes, performance is a value
12:37:26 <Jiehong> also, the actor model comes with some pre-made stuff such as handling the death of a thread, to re-start it, if that happens…
12:37:33 <tabaqui> ReinH: actors can balance the load
12:37:42 <ReinH> If they're fancy work-stealing dequeues then you have the GHC RTS model, but that's just an optimization of the standard queueing approach.
12:37:59 <ReinH> What does "actors can balance the load" mean here?
12:38:32 <tabaqui> if server gets too many input data
12:38:33 <Jiehong> actors themselves don't, their usually is a balancer, like a specialised actor
12:38:48 <ReinH> Queues automatically balance the customers among the servers.
12:38:48 <tabaqui> the queue can be overflowed
12:38:50 <ReinH> That's their function.
12:39:23 <tabaqui> and something have to balance the load and start additional logic workers
12:39:54 <ReinH> Well, earlier Jiehong said they wanted a fixed pool size.
12:40:27 <ReinH> I still don't understand how actors "balance the load" in a way that a normal queueing system doesn't.
12:41:39 <ReinH> I already mentioned that a work-stealing system could be used, but you'd have to justify the significant complexity of such a system as being worth it over a simple queue.
12:42:35 <ReinH> The simplest thing that could possibly work is a queue.
12:42:37 <Jiehong> I'll have to come up with some perf stats before proving it's worth going this way for the work stealing queues
12:43:20 <tabaqui> ReinH: it is the simplest, right, but not the flexiblest
12:43:45 <tabaqui> I don't know what to do if queue will be overflowed
12:44:01 <Jiehong> I came here, because have 1 worker that polls, puts jobs in a queue, having dedicated workers handling the jobs, and put in another queue the results to be processed by 1 worker to send results away sounded like I'd have contentions either in input, or in output (single workers)
12:45:04 <Jiehong> queue overflow in entry should be minimal. we just stop polling for some time I guess
12:45:23 <ReinH> Who said the queue will be overflowed?
12:45:36 <ReinH> And if one queue can be overflowed, so can n queues.
12:45:56 <Jiehong> tabaqui did
12:45:56 <ReinH> Work-stealing is not strictly superior to a simple queue. There are trade-offs.
12:46:09 <ReinH> Well, it isn't tabaqui's queue.
12:46:24 <Jiehong> that's also where "streams" with back-pressure would avoid queues overflows entirely
12:47:12 <ReinH> There are no grounds to assume that the queue would overflow.
12:47:43 <ReinH> As it stands, the problem is barely specified at all
12:48:02 <ReinH> And we're jumping to a lot of conclusions that seem to be attempts to justify extra complexity.
12:48:39 <ReinH> Start with a queue and a pool of workers. Then see what can be improved.
12:49:00 <ReinH> If you don't know how your system behaves, you don't know what tradeoffs you should make.
12:49:29 <ReinH> This is what was meant by "premature optimization is the root of all evil". Optimize for known knowns, not for unknown unknowns.
12:49:45 <Jiehong> sounds reasonable
12:51:20 <Jiehong> I suppose simplicity should be the most important in designs :)
12:51:32 <kuribas> the "premature optimization" quote was not about algorithms, but small linear "optimizations".
12:51:36 <ReinH> Learning should be the most important.
12:52:08 <ReinH> kuribas: And I'm applying it in a more general way that I think is in keeping with the spirit of the statement.
12:52:23 <kuribas> ReinH: sure, I don't necessarily disagree
12:52:46 <kuribas> I think the real meaning of the quote has been long gone ...
12:53:03 <[exa]> speaking about generic truths, don't you know about some reading that would be in the same spirit as 'the art of unix programming', but updated to say atleast 2010?
12:53:06 <Jiehong> as a matter of fact, it's a great statement
12:53:13 <ReinH> Putting a simple system in place (a) improves things and (b) gives you the opportunity to learn more about the problem.
12:53:32 <ReinH> Imagining what potential problems you might encounter and then designing systems to solve them is counter-productive.
12:54:03 <ReinH> Don't overlook the importance of (b).
12:54:04 <Jiehong> ReinH: I'm convinced. I'll do just that
12:54:24 <[exa]> also-- "80%-grade software now is better than 100%-grade software never"
12:54:37 <Jiehong> b) is actually the best way to learn I think
12:54:48 <ReinH> There's a reason Kent Beck said "do the simplest thing that could possibly work".
12:55:12 <kuribas> Jiehong: use the simplest way to solve your problem, but not simpler.
12:56:03 <ReinH> You have to (a) actually do something and (b) that thing must have the potential to work and (c) be no more complex than needed to achieve (b).
12:56:04 <Jiehong> but "the simpliest, not simpler" is actually no trivial in some cases. It can sometimes take even longer than the complex one ^^
12:57:00 <Jiehong> and you don't have a magic metric/tool that tells you "there you got, that's the optimal in term of simplicity for the case"
12:57:40 <jle`> simple
12:57:45 <jle`> build a complex model to evaluate the simplicity of your model
12:57:53 <ReinH> No one said that doing the simplest thing was simple.
12:58:03 <ReinH> Maybe you should do something else that is simpler instead.
12:58:22 <ReinH> Like the simplest thing that has the potential to be the simplest thing that could possibly work.
12:58:55 <[exa]> let's have simplicity classes and simplicity reductions
12:59:22 <Jiehong> exa: that'd be great
12:59:56 <jle`> but then people would wonder if Simple != NSimple
13:00:28 <[exa]> Thing is NP-simple if it is both NP-easy and anything in P is harder
13:00:32 <Jiehong> exa: didn't see the 80% grade sofware line. Well, I've got a deadline, so never is not happening anyway ;)
13:00:47 <ReinH> Someone should define Simplest Hard and Simplest Complete.
13:01:33 <ReinH> [exa]: The converse is Jerry Weinberg's quote, "if it isn't worth doing, it isn't worth doing right".
13:01:36 <Jiehong> alright, thanks everyone. I appreciated this chat.
13:01:42 <[exa]> Jiehong: the original meaning of what I paraphrased badly is that writing 80% software doesn't consume much time and you learn how to write the corrected 100% version quicker
13:01:46 <ReinH> Sometimes the most important thing to figure out is what *not* to do.
13:04:06 <[exa]> ReinH: yeah, like me actually wondering about the correct definition of NP-simple now :D
13:09:14 <knupfer> So, pushed a new version of 'type-of-html', it's now even faster, type safer and compiletimes are more acceptable
13:11:02 <jackhill> knupfer++
13:12:36 <Tuplanolla> Next time a beginner asks for a project, suggest bond graph theory.
13:12:58 <jle`> knupfer: nice :)
13:13:15 <jle`> beautiful
13:15:35 <knupfer> Now, attributes are kinded and at compile time checked that they are valid attributes of a given element
13:21:43 <jared-w> Tuplanolla: Even though I know you're joking with the bond graph theory... lol
13:21:59 * jared-w has been doing the job hunt thing for 2 days and is already sick of it
13:22:20 <Tuplanolla> I'm not completely, jared-w. A libre implementation would be nice.
13:23:27 <jared-w> Is bond graph theory really beginner friendly? I might have something else in mind that is hideously difficult...
13:29:45 <Tuplanolla> I don't see why not. It's just energy flow diagrams.
13:30:08 <Tuplanolla> Making them useful is the hard part.
13:45:02 <infandum> Let's say I run "shake" and it completes 2 out of 5 rules. If I stop it while it's in the middle of rule 3 and re-run it, does it continue by starting at rule 3 or does it restart at rule 1?
13:46:59 <aweinstock> Is there a way to make arithmetic functions on church nats have 'nice' types (e.g. types that unify) without RankNTypes? (context: https://raw.githubusercontent.com/aweinstock314/haskell-stuff/master/ChurchNumerals.hs)
13:49:44 <jared-w> infandum: There's a pretty easy way to find out I guess :p
13:53:48 <aweinstock> alternatively (and possibly equivalently), is there a type that I can give to uwrap that'll make it work with {uplus, umult, uexpo}
13:56:01 <infandum> jared-w: That's true, but this might be easier :)
13:59:43 <knupfer> infandum: It'll start at rule 3 if the dependencies of rule 1 and 2 didn't change
14:00:19 <knupfer> shake is very lazy :)
14:03:23 <dmwit> aweinstock: I don't think so. Church numerals are inherently very polymorphic.
14:03:58 <knupfer> Where is {-# LANGUAGE RecordAsLenses #-}
14:04:31 <dmwit> It's spelled `{-# LANGUAGE TemplateHaskell #-}; import Control.Lens.TH`. ;-)
14:10:05 <pikajude> another way to write html :o
14:23:40 <infandum> knupfer: So am I, so it's perfect!
14:31:51 <ReinH> dmwit: Church numerals are inherently untyped :p
14:53:46 <knupfer> pikajude: If you don't write signatures the syntax is not that intrusive.
15:02:48 <amf> wish: hackage would put the types in the index
15:10:14 <iqubic> Don't they do that already amf?
15:14:14 <dmwit> ReinH: untyped is the most polymorphic type of all
15:15:03 <amf> iqubic: 
15:15:11 <amf> iqubic: not on the salve package index
15:20:15 <dfeuer> Ping bgamari
15:29:48 <iqubic> pong
15:42:26 <ReinH> dmwit: fine you win on a technicality :p
15:42:41 <`Guest00000> hooray
15:50:55 <amf> anyone use a develop build mode for stack / cabal / hpack? i should really stop wasting compile time on -O2
16:11:07 <jeltsch> Is there a package that defines a class for bivariant functors.
16:11:55 <jeltsch> I mean something like the following:
16:12:00 <jeltsch> class BivariantFunctor f where
16:12:04 <jeltsch>     bmap :: (b -> a) -> (a -> b) -> f a -> f b
16:12:30 <kadoban> amf: As far as I understand, -O2 itself is usually not worth the time. I don't really have anything to contradict that, so I usually don't do it.
16:17:49 <amf> ha. are there any good ways to speed up compile times in exchange for other things?
16:18:12 <ertes> amf: one option is not to compile at all
16:18:33 <amf> ertes: implies im good at haskell :)
16:18:34 <hpc> i think you can covince ghc to not do code generation, just typechecking and whatnot
16:18:45 <kadoban> -O seems to be noticeably faster than -O2. Other than that I haven't particularly tried
16:18:57 <hpc> which speeds it up if your goal is "did i type the code right" in some tight IDE loop
16:19:19 <ertes> amf: i use GHCi during most of my development, and it helps when it's integrated into your editor
16:20:11 <hpc> ghci's bytecode generation is usually pretty quick
16:20:29 <hpc> depending on what you're doing, it's possible that ghci exhibits different behavior from compiled code
16:20:36 <hpc> usually with particularly complex FFI dependencies
16:20:57 <hpc> and if you're calling ghci through something like stack, you pay a startup price for that as well
16:22:32 <amf> more or less wanted to make my code / compile / verify it looks good cycle to be faster (working with blaze-html and whatnot)
16:23:05 <ertes> amf: is this for web dev?
16:23:32 <amf> just a gui on top of some parsers i have
16:23:53 <jle`> jeltsch: i've seen this package
16:24:05 <jeltsch> jle`: Wher?
16:24:28 <jle`> let me try to remember
16:24:38 <ertes> amf: as a first step i strongly suggest that you remove the "compile" part from your development cycle, unless you absolutely need it (e.g. for very expensive code or for benchmarking)
16:24:56 <ertes> amf: as the second step you may want to have a look at my 'rapid' library
16:25:09 <ertes> https://hackage.haskell.org/package/rapid
16:25:19 <`Guest00000> Data.ByteString.Internal.accursedUnutterablePerformIO
16:25:30 <ertes> amf: it shortens your development cycle even further
16:25:46 <MarcelineVQ> by remove the compile part do you mean something like what hakyll does, where you can just rerun your compiled hakyll binary for most minor changes?
16:26:23 <MarcelineVQ> Hopefully I'm remembering that behavior correctly
16:26:34 <amf> ertes: neat. ill have to look into that for some other components im working on
16:28:27 <MarcelineVQ> jle`, jeltsch: https://hackage.haskell.org/package/invariant-0.4.3/docs/Data-Functor-Invariant.html
16:28:35 <ertes> MarcelineVQ: no, i mean replacing compilation by GHCi
16:29:09 <MarcelineVQ> ertes: thank you
16:29:47 <ertes> amf: there is a tutorial in the 'Rapid' module…  if it leaves any questions open, feel free to consider that a documentation bug and hit me…  i'll do my best to help you =)
16:31:36 <jeltsch> MarcelineVQ: Thank you.
16:35:28 <GamboPango> amf: IIRC -fno-code only typechecks
16:37:48 <amf> ertes: thanks!
16:38:01 <amf> ha i also forgot about stack being able to pass -j in
16:49:15 <dfeuer> Is Andrew Martin on IRC?
17:02:33 <sm_> hey all.. where can we find the list of GHC language extensions these days ? And what does NoCPP do ?
17:03:00 <hpc> NoCPP is on by default
17:03:00 <Cale> sm: ghc --supported-languages will list them
17:03:03 <hpc> it's the opposite of CPP
17:03:07 <Cale> They're also in the User's Guide
17:03:31 <sm> I've just spent a few minutes searching, but failed to find the list with meanings
17:03:32 <Cale> and yeah, for any language feature X, NoX will turn that feature off.
17:04:24 <Cale> https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/flags.html#language-options
17:04:56 <c_wraith> does - XNoX override a language pragma in the source?
17:05:04 <sm> so a module that has NoCPP in it.. is worried that user might turn on CPP from command line or cabal file, and wants to make sure they don't ?
17:05:28 <sm> thanks, Cale
17:05:53 <glguy> sm: The .cabal file might be listing CPP in other-extensions
17:07:05 <sm> oh, it's for when you have turned on CPP for all files and want to exclude this one, I see
17:22:34 <ridecar2> Hi all - does anyone have any recommendations for getting started with haskell on windows? Would I be better using the Windows Subsystem for linux, also, what editors are best to use?
17:23:09 <glguy> No, you won't be better using the WSL, it will run very slowly with GHC
17:24:57 <ridecar2> Thank you @glguy - what's it like on Windows natively?
17:25:07 <glguy> useable
17:25:09 <rotaerk> ridecar2, perhaps try just downloading stack, and using VS Code with Haskero
17:26:08 <ridecar2> @rotaerk - will do!
17:26:08 <lambdabot> Unknown command, try @list
17:26:25 <rotaerk> I'm not a lambdabot command
17:26:47 <ridecar2> Ahhh - I'm used to using @ to tag people
17:27:27 <sm> rotaerk++
17:27:41 <rotaerk> ridecar2, it's unusual in IRC to do that, but generally speaking doesn't hurt
17:27:52 <rotaerk> it's just there's a bot in here that listens for @blah
17:29:07 <Hafydd> It hurts my eyes.
17:31:59 <rotaerk> guess there are always exceptions
17:35:58 <ridecar2> Go on, I'll ask - what does lambdabot do?
17:38:21 <pacak> > 1 + 1
17:38:24 <lambdabot>  2
17:44:42 <geekosaur> evaluates haskell expressions, provides type information for expressions and kind information for types, supports various useful knowledge bases
17:45:21 <ridecar2> That's awesome!
17:52:35 <muesli4> Are there any devs that work with Clojure on a day to day basis but actually want to work with Haskell? Would you recommend taking a Clojure job? I'm honestly not impressed with Clojure. So far the language has done everything to push me away.
17:55:09 <xzhu> muesli4: I too find the hype about Clojure perplexing. What's the advantage of working with lisp-like language in general?
17:55:29 <remexre> macros let you write essentially as much as compiler passes
17:56:08 <remexre> and -> is really nice for funcprog, so you don't have to read it "backwards"
17:56:37 <xzhu> we have `&` lol
17:56:50 <muesli4> remexre: I actually just used some-> and with a little bit more complex examples macros don't compose at all.
17:57:30 <remexre> I'm not saying Clojure > Haskell, just that that's what my Clojure-using friends try to hype it to me with
17:57:37 <remexre> I find anything on the JVM suspect, personally
17:58:04 <rotaerk> I should learn some LISP language... but I seriously doubt I'll prefer it over haskell
17:59:11 <xzhu> I know that Rick once said that LISP's lack of visual noise and powerful macros makes it easy to write DSL
17:59:28 <xzhu> But I personally find LISP programs hard to read
17:59:41 <cjh`> I'm not convinced that Lisp programs lack visual noise
17:59:49 <muesli4> xzhu: There actually seems to be more noise than in any typical Haskell program.
17:59:53 <xzhu> )))))))))))))))))))))))))))))))
18:00:08 <benzrf> the parens! they burn!
18:00:10 <cjh`> I used to like Scheme quite a bit, but that was mostly from the implementation PoV, I'm not sure I'd ever want to work on a large codebase with other humans using Scheme
18:00:29 <cjh`> Scheme and Lisps are cute and powerful, but that power feels too lenient 
18:00:48 <muesli4> Especially the argument about "no dedicated keywords". In the end you have to learn how different macro work, which is a lot worse IMO.
18:01:57 <cjh`> I like writing my own DSLs, I generally dislike using others :)
18:02:11 <jared-w> Well Lisp's power really comes from meta-programming. So you end up having an incredibly expressive language which allows you to very easily invent your own little universe to program in
18:02:35 <jared-w> Of course, it means you have to learn everyone else's universe every time you write something in code that's not 100% yours :p
18:02:44 <cjh`> but I hate working in someone else's universe, and the universes don't always play well together
18:02:47 <cjh`> exactly :)
18:03:39 <xzhu> is there a lift2 :: Functor f => (a -> b -> c) -> (f a -> f b -> f c) ?
18:03:46 <jared-w> So for a small long-term dedicated team that "gets" it, I honestly don't think you can find much more powerful than lisp since the team's ability to write code just keeps climbing and climbing, especially with macros and carefully and consistently expanding the power of the codebase
18:04:24 <cjh`> jared-w: although joining that team from another is then a lot harder than in other languages
18:04:25 <jared-w> But... Languages like Haskell give you a /ton/ of that expressiveness without having to build up your own universe, and easier refactoring, and you can jump from codebase to codebase very easily compared to lisp codebases
18:04:38 <cjh`> I generally prefer to sacrifice expressive power in order to have simpler code, even if it is a bit more verbose (up to some limits)
18:04:43 <cjh`> and I say that as an ex-perl programmer :p
18:04:49 <jared-w> cjh`: oh yeah I can't imagine trying to onboard someone when 20-30% of your codebase is hand-written macros lol
18:05:10 <pacak> xzhu: Can you implement it using only fmap?
18:05:16 <jared-w> like, can you imagine trying to explain one line of code which expands to 5 differnet handwritten macros which reference 20 others which....
18:05:29 * cjh` nods
18:05:53 <pacak> xzhu: I suspect you need Applicative for tat.
18:06:00 <pacak> :t liftA2
18:06:01 <lambdabot> Applicative f => (a -> b -> c) -> f a -> f b -> f c
18:06:18 <jared-w> cjh`: Also, with haskell you have type classes, type families, lenses, monads, etc... You can get pretty damn perly and terse in your Haskell if you want to :p
18:06:39 <xzhu> pacak: I'm aware of liftA2
18:06:44 <cjh`> jared-w: I often find Haskell a bit too terse actually, but part of that is also my lack of seniority with it (I'm still a newb)
18:07:01 <xzhu> pacak: Seems like the Applicative properties is needed
18:07:07 <jared-w> cjh`: I love the terseness of it but it's definitely an aquired taste
18:07:23 <cjh`> jared-w: and at least the Haskell type system has your back and still gives you static guarantees when using that terseness
18:07:32 <cjh`> jared-w: in scheme/perl your pure-looking macro can do anything it wants inside
18:09:05 <pacak> jared-w: There are people who successfully lisp, perl, javascript and even brainfuck in Haskell..
18:13:21 <jared-w> pacak: true :p
18:14:09 <jared-w> One of my favorite quotes about lisp is the "in any program of sufficient complexity is an embedded lisp somewhere" quote
18:44:50 <monochrom> I uprooted that with "in any sufficient advanced lisp implementation is an attempt at a Haskell implementation" :)
18:58:58 <jared-w> lol funny. I bet that users of TH would greatly appreciate the power and ease-of-use  of lisp macros though.
19:02:08 <andromeda-galaxy> Yes indeed!
19:02:21 <andromeda-galaxy> TH is just so... annoying to work with
19:02:25 <Welkin> andromeda-galaxy: what is your interest in the andromeda galaxy?
19:04:09 <andromeda-galaxy> Welkin: honestly I don't even remember how I started using this as my nick for all things online.
19:05:07 <Welkin> it is on course to collide with the milky way
19:05:16 <andromeda-galaxy> indeed
19:06:09 <andromeda-galaxy> one of my other interests has always been space science/tech even though I don't work anywhere near it, and it somehow seemed like a good idea when I started being on ircs etc a few years ago
19:06:11 <geekosaur> an event which will likely be noticed by approximately nobody
19:06:25 <Welkin> geekosaur: it will be noticed for sure
19:06:36 <Welkin> it won't happen for 3-4 billion years though
19:06:49 <Welkin> so who knows if humanity will be alive to experience it
19:06:55 <Welkin> and if the earth will survive
19:06:55 <andromeda-galaxy> if we're around in 3-4 billion years we'll definitely notice a few high energy phenomena during the merger
19:07:08 <jared-w> The human race will be lucky to make it to 2100, it'll be a miracle if we make it to 3-4 billion lol
19:07:24 <Welkin> humans have only existeed for 200k years
19:07:25 <andromeda-galaxy> yeah
19:07:29 <kadoban> If we make it that long, seems unlikely it'll be one race.
19:07:33 <andromeda-galaxy> long now foundation ftw
19:07:54 <Welkin> earthlings and martians
19:08:08 <Welkin> and europans, and enceleduceans
19:08:09 <kadoban> Pretty much, except probably more exotic than just those.
19:08:40 <andromeda-galaxy> we don't have much of a reference point for space technology. I think that it depends a lot on the timescales over which transporation happen & develop
19:08:42 <Welkin> anyone watch/read The Expanse?
19:08:44 <kadoban> Hard to imagine us still being on one planet, and as soon as you go interstellar ... how long until speciation? Probably not that long.
19:08:55 <Welkin> imagine Trump in space
19:08:58 <Welkin> galactic chairman
19:09:53 <Welkin> I just saw something very odd tonight
19:10:06 <andromeda-galaxy> kadoban: hmm, I think it depends on travel technology. If there's more than a few megayears where you have to spend more than a few years of /subjective/ time to travel between galaxies, then sure. Otherwise I'm not convinced
19:10:07 <Welkin> did you know that linkedin started selling video courses?
19:10:21 <jared-w> kadoban: Well speciation would take a long time still, it just wouldn't be very long relative to evolutionary terms
19:10:24 <Welkin> even ones for programming, and there is even a haskell one O.o
19:10:30 <Welkin> I wonder who the hell their target market is
19:10:36 <andromeda-galaxy> wow. good question
19:10:54 <Welkin> the intersection of haskell programmers and linkedin users is exceedingly small
19:10:59 <andromeda-galaxy> linkedin is crazy. they routinely email dept-grads@university.edu asking dept-grads to join linkedin
19:11:01 <jared-w> Like, I'd imagine humans would still be "intercompatible" for a couple thousand years bare minimum. Of course genetic modding will probably become all the rage within the next few centuries
19:11:04 <geekosaur> they've been selling corporate services for a while now
19:11:07 <muesli4> kadoban: At that point genetics will be in the hands of humans. There is no selection for the most part.
19:11:49 <kadoban> andromeda-galaxy: It's basically guarnteed to be messy whatever happens. As soon as some set(s) of planets stop having constant mixing for a long enough period ...
19:12:00 <Welkin> muesli4: there is always natural selection
19:12:18 <Welkin> as a civilization, we still can't even control our own planet
19:12:21 <Welkin> nowhere even close
19:12:24 <kadoban> muesli4: Maybe. But even then, different planets have different environments, obviously. Why wouldn't we artificially make different choices then?
19:12:27 <andromeda-galaxy> kadoban: "long enough" is probably like a few hundred kilo years though
19:12:52 <Welkin> elon musk plans to have people on mars in 8-10 years
19:12:59 <kadoban> andromeda-galaxy: Is it? We haven't even been around that long have we? How long did the splits between protohumans take and etc.?
19:13:14 <kadoban> I mean some of them lasted quite a while, but how long until they were actually different ...
19:13:22 <Welkin> kadoban: anatomically modern humans have been around for 200k years
19:13:54 <Welkin> before that, ape-like creatures have been around for a few million at least
19:14:06 * kadoban goes to look up timelines of human-ish species
19:14:09 <geekosaur> "nobody knows"
19:14:22 <muesli4> What most people forget: For selection to happen the individuals have to be unable to procreate. In the past this was mostly caused by death before mating. Even today with all technology and no pressure there is barely any selection.
19:14:46 <Welkin> muesli4: there is plenty of selection still
19:14:50 <Welkin> there are justo ther factors
19:15:01 <Welkin> people choose not to have children
19:15:01 <geekosaur> every time they think they have something resembling a timelline, they discover something like H. naledi hat throws it all fgor a loop. meanwhile what little evidence we have suggess genus Homo has been interbreeding wildly for its entire existence
19:15:04 <Welkin> or not to have a mate
19:16:04 <geekosaur> and, as to no selection, http://dx.doi.org/10.1371/journal.pbio.2002458 is just one recent study on that
19:16:06 <muesli4> Welkin: Sure. I would, however, doubt that it has a significant impact.
19:16:26 <Welkin> we artificially select for traits that are not advantageous in general
19:16:49 <Welkin> people who would have otherwise died without advanced medical care
19:17:15 <Welkin> selection is always happening, whether it is "natural" or artificial
19:25:13 <nshepperd> human genetic engineering is in its infancy. the situation will certainly be different in X00 years when most people procreate by generating a genome from whole cloth (or something weirder). speciation could be prevented indefinitely by deliberately preserving 'earthling' genomes. whether people would want to do that is a different matter
19:26:33 <andromeda-galaxy> predicting what people will value in even a few hundred years, let alone thousands, is quite difficult. Without knowing that, it's hard to speculate on what humans might or might not do
19:27:07 <kadoban> It's fairly easy to predict that not every community will do exactly the same things though.
19:27:56 <andromeda-galaxy> but it's definitely posible that there weill be far-reaching commonalities. Like perhaps a preference for retaining a "earth human" genome
19:28:10 <Welkin> we've learned that genetic engineering has its own issues
19:28:14 <Welkin> like monocultures
19:28:20 <Welkin> susceptibility to disease
19:28:59 <kadoban> andromeda-galaxy: Possible, but seems unlikely there wouldn't at least be a counter-culture type thing.
19:30:56 <nshepperd> Welkin: that's an issue with monocultures, not genetic engineering in general
19:31:27 <andromeda-galaxy> that was more my attempt at elaborating on nsheppard's point than my own opinion. but yeah, sure.  Depending on how good we get at genetic engineering and predicting its consequences, it's quite possible that humans-as-we-are-now will purposefully create a number of categorically different life forms
19:32:17 <andromeda-galaxy> there's also the potential of purely synthetic biology or some form of uplift giving rise to more categorically different lines of intelligence
19:32:41 <Welkin> there is a field called synthetic biology
19:32:48 <Welkin> you can already write your own dna
19:32:56 <andromeda-galaxy> yeah, I mentioned it a few sentences ago
19:33:24 <Welkin> the only thing that you can do with it now is splice in genes to make E coli glow in the dark or smell like bananas, though
19:34:28 <andromeda-galaxy> Fun fact: Tom Knight (ARPANET Host #6, ITS, Lisp Machines, Connection Machine, etc.) is now a synthetic biologiest
19:34:37 <andromeda-galaxy> http://www.ginkgobioworks.com/
19:35:34 <andromeda-galaxy> I'm totally not a biologist so this is completely unfounded speculation. But I think it would be interesting to do more work on making synthetic biology easier to "program" and "verify"
19:36:31 <andromeda-galaxy> people in here might be interested by http://www.cs.cmu.edu/~sasb2017/
19:48:01 <dsal> I'm using mapConcurrently on some CPU intensive code.  It seems to spin for a while, but only on one core.  Is there something limiting my concurrency?
19:49:05 <andromeda-galaxy> dsal: are you building with -threaded and passing +RTS -N?
19:49:28 <dsal> Nope.  I'm just running with emacs' ghci thingy.
19:49:34 <geekosaur> assuming you did that ^ make sure you aren't doing something silly like creating a bunch of thunks in threads and forcing them back in the main thread
19:49:53 <geekosaur> ghci is -threaded but likely not using +RTS -N
19:50:01 <dsal> Can I set that from within ghci?
19:50:15 <geekosaur> no, runtime options have to be set early
19:50:25 <andromeda-galaxy> I don't think so. but GHCi interprets, does it even use +RTS -N properly?
19:50:29 <geekosaur> you can run ghci with that, you can't set it in the middle of a session
19:50:30 <andromeda-galaxy> I've always just built parallel code manually
19:55:13 <dsal> ghc: unrecognised flag: -ferror-spans +RTS -N -RTS
19:55:18 <dsal> Am I doing it wrong?
19:56:01 <andromeda-galaxy> dsal: hmm, what did you use to start ghc?
19:56:13 <dsal> Emacs' built-in thing.
19:56:25 <andromeda-galaxy> how are you configuring it?
19:56:38 <dsal> Yeah, that works on the commandline.  :(
19:57:00 <dsal> Hide Haskell Process Args Ghci:   INS DEL Argument: -ferror-spans +RTS -N -RTS
19:57:15 <dsal> It says these are the arguments for starting ghci.  It seems to provide them... somehow.
19:57:21 <dsal> Oh wait.
19:57:28 <andromeda-galaxy> oh I think that you put all 4 arguments in one argument value
19:57:31 <dsal> oooh, this is argv
19:57:31 <dsal> yeah
19:57:40 <andromeda-galaxy> use the INS button?
19:58:27 <dsal> Yeah.  This works now.
19:59:39 <dsal> Still not > 100%
19:59:50 <andromeda-galaxy> I'm not sure if this will work in ghci
20:00:10 <andromeda-galaxy> when you load the file, does ghci print out interpreted, or compiled?
20:00:14 <andromeda-galaxy> if the former, that might be the problem
20:00:57 <andromeda-galaxy> try doing 'ghc -threaded <filename.hs>' and then './filename +RTS -N'
20:02:59 <dsal> Yeah, still pegged at 100%.  I guess... that's good?
20:03:13 <dsal> loading says interpreted.
20:03:31 <andromeda-galaxy> wait, it was still not going over 100% even when you compiled and manually ran it?
20:03:53 <dsal> Yeah.  Let me paste
20:04:33 <dsal> I'm doing a euler thing.  Decided to go hard-core brute force:  http://lpaste.net/358393
20:04:37 <jchia> jchia
20:04:47 <dsal> I don't see what that wouldn't use a ton of threads, though
20:05:51 <dsal> I forgot how to >>= heh  (I don't typically have a main on these)
20:06:20 <geekosaur> that's a symptom of the other thing I mentioned... having to force computations in threads
20:06:28 <geekosaur> else they get forced on output in the main thread
20:06:29 <xzhu> Can anyone summarize the trade of between Map.Lazy and Map.Strict? Seems like Lazy is always better
20:06:35 <xzhu> *trade off
20:06:49 <dsal> geekosaur: I tried forcing before the parallel -- did I not force hard enough?
20:06:54 <andromeda-galaxy> dsal: echoing geekosaur: I think that your threads are just building up thunks
20:07:08 <geekosaur> xzhu, not always --- the default is Lazy but with a handful of things brought in from Strict because they'
20:07:22 <geekosaur> re typically better that way, you can look at the haddock for Data.Map for details
20:09:20 <geekosaur> oh, hm, that parallelization could be nasty
20:09:40 <geekosaur> do it wrong and the first thing it does is force p all in one thread
20:09:57 <geekosaur> uh, the p on line 30
20:10:24 <dsal> OK, I ($!)'d a bit harder.
20:10:33 <dsal> Using all the cores now.
20:11:33 <dsal> You think I shouldn't force p there?  The goal was to make a strict list instead of having each thread compute its own.
20:12:02 <`Guest00000> what does "making grammatical phrases as long as possible, proceeding from left to right" mean?
20:12:13 <dsal> ghci is doing it fine, as well.
20:12:21 <andromeda-galaxy> oh cool
20:12:54 <geekosaur> dsal, I don't know offhand which is more expensive, just thinking that having p shared between all the threads but lazy might work better
20:12:55 <xzhu> geekosaur: I don't see why insertWith is better strict than lazy, and shouldn't whatever argument there is be applied to "union" as well?
20:13:05 <geekosaur> each one computes just what it needs if it's not already
20:13:47 <dsal> Seems about the same on a smaller test.
20:14:06 <dsal> Well, sure, but it's either got to synchronize it into a memo area, or make more copies.
20:14:13 <geekosaur> but likely everything above that wants to be forced, with care that it doesn't do it in a way that forces all of p right then because then you get all of that in one thread (or worse, you don't: then all the cores are computing it at the same time, because ghc does *not* check for multiple entry, the overhead of the locking is too high, it just lets the work be duplicated)
20:14:24 <geekosaur> (because in a pure computation that's perfectly safe)
20:15:08 <dsal> Yeah, I'm not thinking about the safety that much, just what might perform better.
20:15:14 <geekosaur> xzhu, I was hoping their choices would be documented somewhere in there. might need to click through to the source
20:15:42 <geekosaur> beyond that you probably want to contact the maintainer(s) of the containers package
20:18:51 <xzhu> geekosaur: Thanks!
20:18:55 <dsal> Well, the good news is that this is obviously parallel.  The bad news is that this code is dumb.
20:19:14 <xzhu> Is there a way to force memoization for recursive functions?
20:19:38 <jle`> what do you mean by force memoization
20:19:41 <jle`> do you mean memoize?
20:19:48 <xzhu> yes
20:19:52 <jle`> 'force memoization' sounds like memoization happens sometimes but not always
20:19:56 <jle`> in reality, it doesn't happen, heh
20:20:02 <jle`> you can explicitly write a memoizing function
20:20:13 <xzhu> using HashMap?
20:20:41 <geekosaur> there are some memoization packages. the fancy way is to write it as a self-referential CAF (that is, a binding wth no parameters) that generates a list and index the list to get values
20:20:44 <jle`> the typical way you'd do it is by taking advantage of laziness
20:20:51 <dsal> What do you mean "it doesn't happen" ?  It seems to happen sometimes.
20:21:00 <jle`> dsal: functions in haskell are never automatically memoized
20:21:05 <jle`> that's just not a thing
20:21:24 <jle`> you have to actually manually memoize your communities, or use a library that does for you
20:21:25 <kadoban> dsal: In practice if you do "f x" and then later again "f x", it'll recompute. The only time it memoizes is if you give it a name and reuse the name.
20:21:52 <dsal> I very often do the same thing twice and the second time is instant.
20:21:52 <jle`> actually, a lot of people do think that haskell memoizes functions for some reason
20:21:59 <jle`> i used to think it does too
20:22:03 <jle`> but i never had any reason to think so
20:22:08 <jle`> it was just something that popped into my head without justification
20:22:13 <jle`> but, it seems to be a shared delusion
20:22:26 <kadoban> dsal: You'd have to show an example.
20:22:32 <jle`> the reason behind it is some mystery to me.  maybe something like a collective unconscious
20:22:44 <`Guest00000> plz help
20:22:47 <dsal> https://www.irccloud.com/pastebin/b5A1STKd/
20:22:58 <jle`> er
20:23:00 <jle`> look_around is not a function
20:23:02 <jle`> it's a value
20:23:07 <jle`> you might be confusing functions and non-functions
20:23:10 <`Guest00000> i spent two days on an issue which resulted from a wrong understanding of this phrase, and then i realized i don't understand it
20:23:10 <andromeda-galaxy> also btw https://hackage.haskell.org/package/MemoTrie
20:23:17 <jle`> functions are never memoized, but monomorphic values are not re-computed.  that's because of laziness
20:23:17 <orzo> so i'm looking at a heap profile, and i see a problem with PINNED.  I take it that's bytestrings, but is there a way to get more finer grained cost centers than the vague "PINNED" ?
20:23:45 <dsal> https://www.irccloud.com/pastebin/cm0OGHL9/
20:23:52 <jle`> xzhu: a common example of using laziness to memoize functions is with that famous fib example
20:24:00 <geekosaur> `Guest00000, without context that phrase is ... well, not meaningless but dubious 
20:24:17 <kadoban> dsal: Right, that's giving something a name. Once a thing has a name that stays in scope and has the possibility of being used, it'll never be recomputed.
20:24:19 <jle`> dsal: that's because pentagons, the value, is being evaluated
20:24:26 <jle`> try it again if pentagons was a function
20:24:40 <jle`> *functions* are not memoized
20:24:41 <geekosaur> because making grammatical phrases is kinda still very open research topic, and does not specify even language (not that that helps)
20:24:52 <jle`> but thunks are indeed not re-evaluated
20:25:11 <dsal> Hmm...  Pentagons is a lazily computed list.  When I ask for more, it computes more, but it doesn't compute what it's computed.  That feels a lot like memoization.
20:25:32 <jle`> it's the mechanism that memoization libraries use to implement memoization of functions
20:25:45 <jle`> but the point is that these aren't *functions*
20:25:48 <jle`> haskell doesn't memoize *functions*
20:25:49 <geekosaur> dsal, yes, that's what you get with self-referential CAFs
20:25:53 <jle`> the key word is function
20:25:57 <geekosaur> if you make sure they are lazy 
20:26:07 <jle`> haskell doesn't memoize *functions*, but everyone seems to think it does
20:26:25 <jle`> but haskell never memoize functions automatically, you have to explicitly work out a way to memoize things by hand
20:26:30 <orzo> jle`: it'd be cool to have a compiler pragma though
20:26:30 <geekosaur> ("CAF" there is not just meaningless jargon, it is key that this is not something that takes a parameter but a bound lazy *value*)
20:26:44 <jle`> orzo: we have something better
20:26:47 <jle`> higher-order functions :)
20:26:56 <jle`> memoizeMyFunc :: (a -> b) -> (a -> b)
20:27:05 <geekosaur> orzo, I don't think you can without hacking on the bytestring library
20:27:08 <jle`> give an (a -> b) and return a memoizing version of itself
20:27:15 <kadoban> geekosaur: CAF is somewhat synonymous with "thunk" there? Or is that not right?
20:27:17 <jle`> why have a compiler pragma when you can just use a higher order function, heh
20:27:33 <dsal> Hmm...  I guess that makes sense.  It feels like it should memoize because, you know, it *could*.
20:27:38 <orzo> why is that better, jle` ?
20:27:47 <`Guest00000> "In both the lexical and the context-free syntax, there are some ambiguities that are to be resolved by making grammatical phrases as long as possible, proceeding from left to right"
20:27:48 <jle`> cause it's first-class
20:27:52 <geekosaur> kadoban, afaik CAF is precisely a binding with no parameters. since it does not depend on a parameter, it is not a function, but there were no good names for what it is so it got the clunky term "constant applicative form"
20:27:53 <`Guest00000> from Haskell spec
20:27:54 <jle`> instead of relying on compiler magic...
20:28:07 <xzhu> jle`: is `foo 8` a thunk?
20:28:07 <kadoban> Ah, right. That makes more sense
20:28:07 <jle`> if you can just implement it within the library, it's better than making it compiler magic
20:28:14 <jle`> for example, check out the && function
20:28:22 <jle`> the (&&) function could be implemented using compiler magic
20:28:29 <jle`> and indeed, in a lot of programming langauges, it *is* compiler magic
20:28:30 <`Guest00000> it's not very precise-worded
20:28:38 <glguy> dsal: It's often not a good idea to do so. Using the argument as a key could be expensive, or it could mean wasting a lot of memory for arguments that aren't likely to be repeated
20:28:38 <jle`> i can't be implemented within the language, it has to be compiler or interpreter magic
20:28:39 <geekosaur> `Guest00000, that is basically saying that when you have a choice between two ways of parsing something, you prefer the longest one
20:28:47 <jle`> but in Haskell, we can define (&&) within the language, and it works :)
20:29:01 <orzo> jle`: isn't an advantage of pure functional that the compiler can do lots of magic?
20:29:20 <jle`> are we still talking about a memoizing pragma?
20:29:21 <`Guest00000> geekosaur: your answer is as clear to me as that phrase
20:29:27 <geekosaur> this is sometimes referred to as shift/reduce conflict (reduce would be shortest, shift is longest) or as longest token match
20:29:28 <`Guest00000> the longest one counting tokens?
20:29:28 <jle`> or about the compiler analyzing your code and memoizing it automatically
20:29:52 <jle`> the compiler could try to do it, but it's really hard to know what cases you would want things memoize
20:30:09 <jle`> like, the logic to figure that out is pretty awful and not reliable
20:30:31 <jle`> GHC makes simple reductions that are *usually* clear-cut better
20:30:42 <jle`> but deciding to memoize a function or not is a tough call, and usually memoization makes things a lot worse
20:30:52 <geekosaur> `Guest00000, yes, counting tokens and expansions. liek if you are given 3 + 4 + 5, you do not stop after the 3, you do not stop after the 4, you stop after the 5 (provided it is not followed by another operator, in which case you keep going still)
20:31:00 <jle`> so being able to explicitly specify it is better from a usability perspective
20:31:02 <xzhu> What's the Haskell equivalent of this: https://docs.python.org/3.3/library/functools.html#functools.lru_cache
20:31:03 <jle`> same deal with parallelization
20:31:04 <kadoban> It'd be tough to make it non-fragile. Memoization that happened really unreliably would be kind of just worse than not having it.
20:31:29 <jle`> yeah, you would run into surprises when memoization suddently turns off or on because of some weird edge case in compiler magic
20:31:36 <`Guest00000> geekosaur: i have an understanding, but a vague and not formal one
20:31:45 <geekosaur> yes, that's more or less right :)
20:31:50 <orzo> using a pragma, the compiler could memoize with deep info about the garbage collector
20:31:58 <geekosaur> the formal one depends on what exactly you are using for a parser
20:32:12 <jle`> xzhu: but yeah, the common memoization-using-laziness exmapale is fibs
20:32:20 <geekosaur> the informal one is guidance on how to write the parser, without specifying what kind of parser you are writing
20:32:25 <jle`> @let fibs = 1 : 1 : zipWith (+) fibs (tail fibs)
20:32:28 <lambdabot>  Defined.
20:32:29 <jle`> @let fib n = fibs !! n
20:32:31 <lambdabot>  Defined.
20:32:35 <jle`> > fib 1000
20:32:38 <lambdabot>  7033036771142281582183525487718354977018126983635873274260490508715453711819...
20:32:38 <jle`> > fib 1001
20:32:41 <lambdabot>  1137969253983602722575237825522241755727459303537305131450866341766910925361...
20:32:46 * ski wants it to start with `0'
20:32:58 <orzo> in practical cases, we don't want an unbounded memoization, we want some kind of lru cache typically
20:33:03 <ski> (`fib 12' ought to be `144')
20:33:17 <jle`> 'fib' is memoizing, because we take advantage of lazy cells in lists
20:33:26 <geekosaur> justm wheb=never your parser has the option of "gobble more because it would be grammatical here" vs. "stop here because this is a possible stopping point", you do the former
20:33:28 <jle`> so the memoization libraries out there all take advantage of this and make it nice to use
20:33:54 <orzo> any in base?
20:34:05 <geekosaur> but they don't want to say that in a way that makes it specific to GLR or LALR(1) or ...
20:34:12 <jle`> none in base that i know of
20:34:13 <geekosaur> (different parsing mechanisms)
20:34:24 <ski> (also, `fib (-n) = - fib n' for even `n', and `fib (-n) = fib n' for odd `n')
20:34:48 <orzo> i definitely have a PINNED leak
20:34:58 <orzo> it's till angling upward
20:35:10 <orzo> surely i can do better than this, geekosaur 
20:35:41 <orzo> maybe i can name  the cost center using the whole call stack
20:35:42 <orzo> ?
20:35:44 <jle`> xzhu: if you give 'foo 8' a name, and then refer to it later, then they'll both be referring to the same thunk yes
20:35:51 <orzo> would that fix my problem?
20:36:06 <geekosaur> orzo, not that I know of but I don't generally deal with bytestring leaks. I'm the wrong one to ask, beyond that there's no automagical mechanism to make them show up distinctly in a heap profile because of the whole pinned business
20:36:26 <jle`> if you just use 'foo 8' in two different places in your code then they will be two different thunks usually, barring some compiler optimization like CSE
20:37:29 <jle`> @where CSE
20:37:30 <lambdabot> http://www.haskell.org/haskellwiki/GHC:FAQ#Does_GHC_do_common_subexpression_elimination.3F
20:37:33 <`Guest00000> i want to parse Haskell with a simple self-written parser, which currently is an applicative parser `data P k a = P ([k] -> [(a, [k])])`
20:39:39 <`Guest00000> i'm trying to know how i can make it efficient
20:40:04 <dsal> Hey, compiling with +O2 and running on the commandline is faster than running in GHCI in emacs.
20:40:24 <Axman6> yes
20:40:34 <jle`> `Guest00000: if you show us code then we can help you make it more efficient
20:40:43 <jle`> but as it stands now, it's a bit too vague to offer any real help
20:41:37 <jle`> um, one general thing, for that type in particular, a backtracking <|> is pretty inefficient
20:41:52 <dsal> What's a filter/fmap kind of thing?   I've got an IO [x] I want to filter.
20:42:00 <jle`> you can use filter
20:42:02 <jle`> :t filter
20:42:04 <lambdabot> (a -> Bool) -> [a] -> [a]
20:42:31 <dsal> I did fmap filter.  I just figured there was an all-in-one
20:42:32 <Axman6> :t fmap (filter even)
20:42:33 <lambdabot> (Integral a, Functor f) => f [a] -> f [a]
20:42:43 <jle`> you probably want fmap (filter f)
20:42:53 <kadoban> dsal: Not really, there'd be too many possibilities to give them all names
20:42:57 <jle`> and no, you won't really find functions that can be written simply like this
20:43:01 <jle`> fairnbairn threshold and stuff
20:43:02 <Axman6> :t fmap (filter even) `asAppliedTo` (pure [1] :: IO [Int])
20:43:04 <lambdabot> IO [Int] -> IO [Int]
20:43:18 <dsal> Yeah, i keep running into stuff like that.
20:43:26 <jle`> at a certain point, functions take longer to remember their names of and look up than it takes to just write out
20:44:04 <dsal> I agree.  I come from go, where we try to limit concepts and just tell people to reinvent small things.
20:44:55 <Axman6> he "OH I've done this before, I better just do it again" school of programming. What's DRY?
20:45:25 <dsal> Well, that's kind of what this is.  :)
20:45:30 <kadoban> Ehh. fmapFilter doesn't really follow DRY
20:45:31 <dsal> fmilter
20:45:44 <xzhu> I was trying to write a 01-Knapsack solution using Haskell
20:45:46 <kadoban> It just increases the number of things you need to know
20:46:06 <Axman6> :t fmap . filter
20:46:07 <lambdabot> Functor f => (a -> Bool) -> f [a] -> f [a]
20:46:07 <xzhu> knapsack :: Int -> [(Int, Int)] -> Int
20:46:07 <xzhu> knapsack mw ((w, v):xs)
20:46:07 <xzhu>   | mw >= w = max (knapsack mw xs) (knapsack (mw - w) xs + v)
20:46:07 <xzhu>   | otherwise = knapsack mw xs
20:46:07 <xzhu> knapsack mw [] = 0
20:46:09 <dsal> This is basically the memoization discussion, but from the human side instead of the computer side.
20:46:11 <geekosaur> Axman6, 'don;t repeat yourself'
20:46:15 <dsal> Better to recompute than to keep looking stuff up.
20:46:38 <xzhu> But it performed terribly when n is large
20:47:13 <jle`> yeah, what you would solve using memoization in other languages, you'd solve with sharing + laziness in haskell
20:47:17 <Axman6> geekosaur: sorry I missed that, what's DRY?
20:47:29 <xzhu> I just tried MemoTrie, it doesn't help that much :(
20:47:48 <geekosaur> *eyeroll* I quoted it to offset it not to comment... if that made no sense, try the initials
20:48:06 <kadoban> xzhu: Ya, your original looks like it'll perform really badly for the same reason that really naive versions of fibonnaci will, if you're familiar with that.
20:48:14 <kadoban> xzhu: What was your memotrie version?
20:48:39 <geekosaur> this is going who's-on-first sideways >.>
20:48:42 <glguy> geekosaur: He was trying to get you to repeat yourself
20:48:45 <kadoban> geekosaur: I think it was a joke ...
20:49:13 <geekosaur> you'll note I evaded it (not cleverly but what did you really expect?)
20:49:13 <xzhu> kadoban: memo knapsack
20:49:25 <xzhu> import Data.MemoTrie
20:49:37 <xzhu> @where MemoTrie
20:49:38 <lambdabot> I know nothing about memotrie.
20:49:52 <xzhu> @hackage MemoTrie
20:49:52 <lambdabot> http://hackage.haskell.org/package/MemoTrie
20:49:53 <jle`> i think memotrie doesn't work with recursive functions like that
20:50:00 <kadoban> xzhu: Hm, so the problem there is ... hm. So the recursive calls aren't actually changed at all. Let me look what memotrie has in it for this.
20:50:49 <kadoban> xzhu: So you actually need memoFix from that, which can be a tad mindbending the first time around.
20:51:01 <kadoban> (or the 5th)
20:52:29 <kadoban> Want to maybe peek at that and see if it somehow makes sense, and if not we can hopefully explain a bit more and/or show you?
20:54:07 <xzhu> kadoban: I'm confused looking at memoFix's signature ... so seems that I need to rewrite `knapsack` into a higher order function?
20:54:48 <kadoban> Yes, you do. You take as a parameter the function to call when you recurse essentially.
20:55:15 <glguy> You can use MemoTrie with recursive functions: example = memo $ \x -> stuff using x + example (x+1)
20:55:36 <glguy> example2 = memo2 $ \x y -> etc
20:57:20 <xzhu> glguy: but I just tried `memo knapsack` but it didn't help that much
20:57:38 <glguy> xzhu: Yeah, that's not what I wrote
20:58:01 <kadoban> I consider the memoFix form far easier to understand, personally.
20:58:49 <xzhu> glguy: what's the difference? "foo x = etc" desugars to "\x -> etc", so isn't "memo foo" equivalent to "memo $ \x -> etc"?
20:59:10 <xzhu> oh wait
20:59:12 <glguy> xzhu: No, it's not the same
21:01:03 <`Guest00000> :t asAppliedTo
21:01:05 <lambdabot> (a -> b) -> a -> a -> b
21:01:43 <Axman6> it's just const with a specific type
21:06:23 <xzhu> I tried:
21:06:24 <xzhu> knapsack :: (Int, [(Int, Int)]) -> Int
21:06:24 <xzhu> knapsack = memo $ \case
21:06:24 <xzhu>   (mw,((w,v):xs)) ->
21:06:24 <xzhu>       if mw >= w
21:06:24 <xzhu>           then max (knapsack (mw, xs)) (v + knapsack ((mw - w), xs))
21:06:26 <xzhu>           else knapsack (mw, xs)
21:06:28 <xzhu>   (mw,[]) -> 0
21:06:52 <xzhu> However even with -O3 it doesn't seem to be O(N^2)
21:07:07 <xzhu> more like O(2^N)
21:13:56 <dsal> I don't think -O is about big-O
21:14:28 <dsal> That'd be super rad, though.  I'd pass -O(1) to everything.
21:14:30 <xzhu> dsal: No lol, sorry I was just emphasizing that it was fully optimized
21:14:33 <rotaerk> dsal, lol
21:15:05 <xzhu> but in case you weren't following the discussion, we were talking about memoization
21:15:24 <dsal> Yeah, I wasn't.  Oh, this is your memo.  Neat.
21:15:32 <rotaerk> particularly nice for NP-hard problems
21:15:34 <dsal> I'm torn.  My code found the answer to this euler thing I'm doing, but it's slow.
21:15:44 <xzhu> no it's
21:15:48 <xzhu> @package MemoTrie
21:15:49 <lambdabot> http://hackage.haskell.org/package/MemoTrie
21:18:16 <kadoban> dsal: For a bunch of euler, finding the answer quickly is more about math than anything. I'm not quite sure offhand how to do that one quickly.
21:18:51 <dsal> Yeah.  I'm not a mathematician.  I do some pretty awful hacks in some of these, but I'm enjoying them.  :)
21:19:35 <zenspider> newb here... I'd like some help getting set up w/ emacs (very familiar). I went through some online doco that got me set up with a LOT of stuff, but flycheck is driving me nuts. I can't find any variables/customization I can do to turn off 100s of warnings
21:19:38 <kadoban> You might enjoy some other sites more if you don't consider yourself a mathematician, like the usual "competitive programming" sites. But as long as you're having fun.
21:19:49 <zenspider> eg ghc -Wall myfile.hs is clean... emacs should be clean too
21:19:53 <kadoban> competitive gets pretty mathy too, but it's slightly different math
21:20:28 <dsal> I'm enjoying the exercise, at least.
21:20:28 <kadoban> zenspider: They might be warnings from hlint or something similar?
21:20:40 <rotaerk> anyone use spacemacs with dante?
21:21:11 <zenspider> hlint seems happy, I selected haskell-ghc for my checker and once I fix my real errors, it goes nuts with warnings.
21:21:16 <kadoban> dsal: If you get bored I really like some stuff from hackerrank for instance (and they support haskell well)
21:21:17 <zenspider> but hlint doesn't report the errors I'm writing
21:21:32 <zenspider> dsal: might want to check exercism.io 
21:21:33 <dsal> I started out writing a couple useful apps for myself.
21:21:34 <kadoban> zenspider: Hm, I'm not sure what that would be then offhand.
21:21:43 <dsal> exercism is a cool name.  heh
21:22:04 <zenspider> kadoban: what, if anything, do you use for on-the-fly checking?
21:22:24 <kadoban> zenspider: syntastic in vim, with hlint (with customized rules)
21:22:30 <zenspider> I'm still at the "makes dumb mistakes" phase :)
21:22:37 <pacak> neovim + ale + hdevtools
21:22:37 <zenspider> kk
21:22:56 <pacak> Also ghcid is nice but can be derpy sometimes
21:22:59 <dsal> zenspider: I'm just using the thing where it opens a repl in a window and keep using :r -- it does a pretty good job.
21:23:04 <kadoban> I liked exercism quite a bit, it has a nice focus on code review. The haskell section was pretty well populated when I did it, so I learned a lot about how to write code that people won't hate in haskell.
21:23:05 <dsal> I've never done flycheck like things, though.
21:23:34 <zenspider> not sure I know :r... run? 
21:23:43 <zenspider> I'm probably using :load indirectly
21:23:50 <zenspider> but it just pukes 
21:23:51 <dsal> zenspider: :r is reload
21:23:58 <zenspider> doens't give details
21:24:24 <dsal> If you just open up a .hs or .lhs or whatever and have a menu, there's Haskell -> Load file.  It'll start an interpreter with your symbols.  Edit, save, then just :r in the window.
21:24:30 <zenspider> ok... that DOES give details... so that's a huge step up
21:24:43 <zenspider> thank you
21:25:13 <zenspider> and, do I understand this correctly? a .hs file's syntax is different from the repl's syntax?
21:25:15 <suzu> anybody use haskell layer in spacemacs?
21:25:22 <suzu> doesnt seem to play well with stack scripts
21:25:25 <suzu> works great for projects though
21:25:34 <zenspider> suzu: sorry. I'm old school emacs 
21:25:49 <suzu> does haskell-mode in emacs work well with stack script?
21:26:02 <zenspider> eg, I needto use let in the repl but that'll error in the script and vice-versa?
21:26:16 <suzu> nnnno
21:26:31 <suzu> as in, you have syntax highlighting and checking when editing a haskell script
21:26:34 <kadoban> zenspider: In GHC >= 8.0 you shouldn't need let in the repl. But some things still don't work quite the same.
21:27:01 <zenspider> kadoban: kk... so getting eval-in-repl extended for haskell wouldn't make sense
21:27:19 <kadoban> So yeah, the syntax is a tad different. Mostly the difference is ghci expects one line to be an entire thing, like a function definition can't be split on separate lines (well, it can, but you have to tell it you're doing that)
21:27:42 <zenspider> kk. thanks.
21:28:54 <ski> ("In GHC >= 8.0 you shouldn't need let in the repl" :/ ..)
21:29:34 <kadoban> ski: No likie? IMO it's just ... better.
21:29:47 <fizbin> Incidentally, if anyone remembers my asking here last week about adapting web frameworks so that applications could cut off expensive processing if the client connects, I worked out a solution: https://stackoverflow.com/a/46159969/107331
21:29:52 <ski> asking is different from telling
21:31:23 <kadoban> ski: If that was a response to me I didn't understand it.
21:32:39 <ski> the main mode of interaction in the interactor is asking. mainly asking to evaluate (and print) expressions, or execute `IO'-actions, but also asking about type, &c.
21:33:49 <ski> in a source file, the main mode is telling, iow making definitions. asking (in the sense of entering an expression) is only done as a part of a definition
21:35:35 <ski> i think it makes sense to make a clear difference between these two, not giving the impression that "i should be able to copy from the interactor to source, or vice versa, willy-nilly, right ?"
21:36:34 <ski> .. perhaps i feel this way due to experience with logic programming, though
21:41:08 <kadoban> That does seem like a decent reason. I'm just not sure though, practically as a new haskeller, I hated ghci. Which is unfortunate, because it's so damn useful. But it does just kind of slightly hide the fun. Not sure *shrug*
21:47:36 <dmj`> Is there a way, given an fgl graph, to produce an SVG representation of it
22:02:06 <dmj`> oh, dot has an svg backend
22:14:20 <xzhu> > import Data.Function.Memoize
22:14:23 <lambdabot>  <hint>:1:1: error: parse error on input ‘import’
22:15:23 <iqubic> @import Data.Function.Memoize
22:15:23 <lambdabot> Unknown command, try @list
22:15:29 <iqubic> @let import Data.Function.Memoize
22:15:29 <lambdabot>  .L.hs:94:1: error:
22:15:29 <lambdabot>      Failed to load interface for ‘Data.Function.Memoize’
22:15:29 <lambdabot>      Use -v to see a list of the files searched for.
22:15:47 <iqubic> Looks like that's not a valid module
22:16:00 <xzhu> it's from
22:16:07 <xzhu> @package memoize
22:16:07 <lambdabot> http://hackage.haskell.org/package/memoize
22:16:22 <iqubic> Ah, probable not in lambdabot search path.
22:16:36 <iqubic> Can you try loading it in your own local REPL instead?
22:17:09 <xzhu> I can load it locally. I assume it's because lamdabot hasn't installed it
22:17:35 <iqubic> Sounds about right.
22:18:06 <iqubic> What do you need help with in regards to the memoize package?
22:18:31 <xzhu> it doesn't seem to work, or maybe I haven't understood it correctly
22:18:36 <xzhu> I was trying
22:18:50 <xzhu> f a = last [1..a]
22:19:01 <xzhu> let a = f 100000000 in let b = f 100000000 in a + b
22:19:11 <xzhu> which should be slow
22:19:20 <xzhu> on my machine it was about 4 sec
22:19:27 <iqubic> Alright.
22:19:30 <xzhu> twice as much as f 100000000
22:19:35 <c_wraith> that's only 100 million
22:19:42 <c_wraith> computers can count that high really quickly.
22:19:50 <xzhu> after import Data.Function.Memoize
22:19:57 <xzhu> I changed my function to
22:20:05 <xzhu> f = memoize $ \a -> last [1..a]
22:20:27 <xzhu> but `let a = f 100000000 in let b = f 100000000 in a + b` took exactly the same amount of time -- 4 secs
22:20:44 <iqubic> Odd.
22:20:59 <opqdonut> laziness
22:21:01 <iqubic> I don't even understand what that function is doing.
22:21:07 <iqubic> What is the f there?
22:21:14 <opqdonut> memoize is memoizing the thunk returned, I bet
22:21:27 <xzhu> yes
22:21:38 <ongy> should still work, since the thunk should onnly be evaluated once
22:21:39 <glguy> xzhu: what type does f have?
22:21:58 <xzhu> same as the naive `f`
22:21:59 <c_wraith> damn it, glguy beat me to it.
22:22:07 <c_wraith> The problem is almost certainly polymorphism
22:22:26 <xzhu> memoize :: (a -> v) -> a -> v
22:22:35 <c_wraith> that's not the type of memoize. :P
22:22:44 <glguy> and it's not the type of f
22:22:56 <glguy> give f an explicit type signature
22:23:00 <iqubic> glguy: it looks lik it should be f :: Num a => a -> a based on the usage.
22:23:07 <glguy> one without any type class constraints
22:23:13 <iqubic> But I might be wrong.
22:23:38 <c_wraith> iqubic: the constraint is Enum, not Num.  and adding memoize changes it further
22:24:00 <c_wraith> iqubic: well, hm.  both Enum *and* Num
22:24:05 <iqubic> Really? You can used (+) with enum?
22:24:16 <iqubic> Where did you get enum from?
22:24:28 <ongy> :t \x -> [0..x]
22:24:29 <lambdabot> (Enum t, Num t) => t -> [t]
22:24:40 <glguy> where did you see + ?
22:24:48 <opqdonut> Enum comes from .., Num comes from 0
22:25:03 <iqubic> let a = f 100000000 in let b = f 100000000 in a + b.
22:25:09 <iqubic> Look at the end of that.
22:25:14 <iqubic> Adding a + b
22:25:15 <c_wraith> iqubic: that's not the definition of f
22:25:25 <iqubic> It isn't?
22:25:28 <opqdonut> iqubic: that just means a and b are type t with a Num constraint
22:25:33 <opqdonut> doesn't affect f
22:25:37 <ongy> this expression uses f twice, but it's not the definition of f
22:25:42 <iqubic> Oh. I'm misunderstanding this now.
22:25:45 <xzhu> Guys, adding explicit type signature solves it!
22:26:14 <iqubic> What is that explicit type signature? I'm curious and want to know.
22:26:16 <c_wraith> xzhu: well, as long as it's not polymorphic
22:26:26 <xzhu> let f :: Int -> Int; f = memoize $ \a -> last [1..a]
22:26:38 <xzhu> first run halves the time
22:26:46 <xzhu> subsequent runs take no time at all
22:26:52 <glguy> it's ok if it's polymorphic
22:27:06 <glguy> it just can't have type class constraints
22:27:21 <c_wraith> glguy: well, ok, but that will have type class constraints if it's polymorphic
22:28:22 <glguy> sure, but presumably this isn't the actual thing that xzhu wants to memoize, so it's good to understand the details
22:28:41 <opqdonut> is that because type class constraints compile to dictionary passing, so it's more like "f dict = memoize $ \a -> last [1..a]"
22:28:50 <opqdonut> so you get a different application of memoize every time you use f?
22:29:02 <c_wraith> opqdonut: yes
22:29:23 <opqdonut> so a specialize pragma would help?
22:29:26 <c_wraith> if you compiled that with optimizations within a single module, it would probably optimize that out
22:29:38 <c_wraith> and a specialize pragma would have a similar effect
22:29:43 <opqdonut> yeah
22:29:59 <xzhu> wait wait wait slow down ... what is dictionary passing?
22:30:13 <opqdonut> xzhu: it's how type classes are implemented in the runtime
22:30:22 <Guest15904> Anyone have a library recommendation for databse stuff/persistence? was looking at "persistent"
22:30:29 <xzhu> `f dict = memoize $ \a -> last [1..a]` doesn't have dict on the right hand side
22:30:49 <opqdonut> "foo :: Show a => a -> a -> String" turns to "foo :: ShowDict a -> a -> a -> String"
22:31:09 <opqdonut> with something like "data ShowDict a = ShowDict {show :: a -> String}"
22:32:37 <opqdonut> because there are no types in the runtime, so you can't look up the class instance by type
22:34:03 <xzhu> I see
22:36:15 <ongy> the optimizer can decide to inline the dictionary at compile time if it specialises your function. Cale made a plugin that should warn if it doesn't, but either I didn't get that to work, or my codebase is to small for that to happen (also it's experimental and iirc not working 100% correctly)
22:37:48 <pacak> I suspect that plugin is not working.
22:37:51 <Cale> ongy: Well, my plugin just listed dictionary arguments
22:37:57 <fizbin> Hey, can anyone point me at the definition of the "Show" instance for Network.Socket.SockAddr ? The docs say it doesn't have one, but ghci says it does, but I can't find one in the source for the Network.Socket module.
22:38:19 <c_wraith> fizbin: doesn't :info tell you where it's defined?
22:39:04 <Cale> It was working insofar as it reliably printed out all the evidence variables (and the names of the definitions containing them).
22:39:16 <fizbin> c_wraith: :info claims that it's defined in Network.Socket
22:39:18 <xzhu> So is it that `f (a :: Int)` will be turned into `f distInt a`?
22:39:26 <xzhu> on run-time
22:39:42 <Cale> But you'd still have to peek at the core yourself to see what they were about.
22:39:43 <xzhu> where distInt has specialized functions for int?
22:39:43 <fizbin> c_wraith: But I can't find it anywhere in the source.
22:40:36 <fizbin> c_wraith: Ah! Nevermind, I found it.
22:41:19 <fizbin> I was looking at the source by clicking through the "Source" links in the docs to the Eq and Ord instances for SockAddr.
22:41:26 <Cale> There was a problem with it though, in that GHC would actually sometimes be able to further specialise things *after* my plugin ran.
22:41:35 <c_wraith> fizbin: If you note, the instance isn't defined in the same module as the type.  I think that's why haddock didn't list it
22:41:43 <c_wraith> orphan instances strike again!
22:41:45 <pacak> Then probably I was not using the most recent version...
22:41:54 <fizbin> But the Eq and Ord instances are defined in Network.Socket.Types , and the Show instance is in Network.Socket
22:42:27 <fizbin> So when I thought I was looking at the Network.Socket source, I was actually looking at the source for Network.Socket.Types
22:42:48 <ongy> xzhu: afaik if the compiler knows the concrete time at compile time it will inline the dict calls and not do the dict passing. But if it's Num and then instanciated later it would get a dictionary
22:43:24 <c_wraith> ongy: if it has the unfoldings available, and probably a few other conditions
22:44:01 <ongy> when does it not if it knows the type? I thought that's why the object/interface files are so version restricted
22:44:34 <c_wraith> ongy: if it's exported by a different module and the .hi file for the location doesn't contain the unfoldings
22:44:48 <c_wraith> for the exporting module, that is
22:45:15 <ongy> the function or the instance?
22:47:21 <c_wraith> um.  any relevant thing with an implementation.  So instances, or if you were calling a function that relied on an instance, if that function's unfolding wasn't available
22:59:32 <fred-fri> when i make requests with this servant api the slash is uri encoded, this doesn't work with the server in question, how can the uri encoding be prevented? type LUSAPI = "v1/users" :> Capture "tuid" Integer :> Get '[JSON] UserJsonString
23:01:07 <norc_> Im trying to understand some terminology here. Considering the transformer newtype MaybeT m a = MaybeT { runMaybeT :: m (Maybe a) }, why exactly is that label called "runMaybeT" ?
23:01:37 <c_wraith> norc_: because its primary use is as an accessor function
23:03:20 <norc_> c_wraith: So the name arises just from the use? I mean just unpacking it does not actually do anything afaict, because the result will remain in WHNF
23:04:24 <c_wraith> norc_: it's a newtype.  Of course unpacking it does nothing.
23:04:35 <c_wraith> norc_: that's guaranteed for newtypes. :)
23:05:20 <norc_> Ah, heh indeed.
23:05:35 <tdammers>  unpacking merely changes the type
23:05:48 <c_wraith> norc_: but look at it from the point of view of a user saying "I've got a MaybeT IO Foo, how do I get it out of MaybeT?"
23:06:19 <norc_> c_wraith: Ohhh. So the unpacking/packing is a method of "switching" to a different typeclass instance to switch between implementations?
23:06:26 <c_wraith> norc_: yes
23:15:37 <Axman6> fred-fri: my initial response would be "don't do that", is there a good reason for doing that? 
23:16:22 <fred-fri> Axman6: doing what? having the /v1/users in the api type definition?
23:17:01 <Axman6> yeah, it's a pretty odd thing to do.
23:17:16 <Axman6> why not "v1" :> "users" :> ...?
23:17:29 <Axman6> having a / URL encoded is pretty weird
23:18:36 <Axman6> fred-fri: is the url you want supposed to be /v1/users or is it /v1%2Fusers?
23:18:48 <Axman6> if so, you want what I wrote above, splitting the /'s using :>
23:19:34 <fred-fri> Axman6: ok i see what you mean :) thanks
23:47:41 <osa1> GND doesn't work for deriving MonadBaseControl, anyone know an easier way than manually writing same boilerplate for this? (e.g. maybe a TH solution exists somewhere)
23:48:05 <cocreature> osa1: with 8.2 it should work afaik
23:49:36 <osa1> cocreature: really? that's great. do you know which new feature solved it?
23:49:57 <cocreature> osa1: deriving of associated type families
23:50:49 <osa1> awesome
23:51:10 <cocreature> but I don’t know of a TH solution if you’re stuck supporting older GHCs
23:52:08 <osa1> it's fine I have one more reason to upgrade to 8.2.1 now :-)
23:58:32 <sullyj3> is this function a terrible idea?
23:58:40 <sullyj3> multiFilter :: [(a -> Bool)] -> [a] -> [[a]]
