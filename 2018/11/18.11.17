00:54:57 * hackage funflow-nix 0.1.0.0 - Utility functions for using funflow with nix  http://hackage.haskell.org/package/funflow-nix-0.1.0.0 (mpickering)
01:01:16 <uboa> here's an in progress article: http://kawakokosowa.me/2018/11/17/haskell-99-problems-1-to-10.html
01:03:44 <typetetris> Aren't ImplicitParams just like Reader?
01:52:35 <pavonia> Is there a generalized version of catMaybes, e.g. for Alternative instances or so?
02:01:01 <jle`> pavonia: generalized in what way?
02:01:06 <lavalike> :t asum
02:01:07 <lambdabot> (Alternative f, Foldable t) => t (f a) -> f a
02:01:07 <jle`> things other than Maybe?
02:01:16 <jle`> typetetris: sort of, but they're implicit.  that's the point :)
02:01:24 <lavalike> that's a possible one at least
02:01:36 <jle`> typetetris: Reader and (->) is explicit. ImplicitParams is implicit
02:02:41 <pavonia> jle`: Yeah, I'm not really sure myself. I have a list of Parser-like values and want to filter the ones that yield a result value
02:02:53 <jle`> lavalike: i wouldn't really call that a generalization, since asum for lists is not catMaybes
02:02:58 <jle`> pavonia: well, what would the type be?
02:03:00 <jle`> for Parser
02:03:07 <lavalike> jle`: yeah it's different
02:03:18 <jle`> pavonia: a lot of times, confusion happens when you try to find a generic solution for something that makes sense for specific types
02:03:50 <pavonia> That's the case here, I guess
02:03:54 <jle`> if you ask a specific question about Parser, then we mgiht be able to give an answer
02:04:01 <jle`> and if there is a generic soluition then we can recognize it there too
02:05:35 <pavonia> I already got a version using list comprehensions, but I thought this could be done more elegantly through a general function. Now I realize that there's an extra input parameter that probably doesn't make sense in a generalized function
02:05:55 <jle`> what does the version with list comprehensions look like?
02:06:23 <uboa> im back!
02:06:36 <jle`> pavonia: fwiw the witherable package has a typeclass for functions like f (Maybe a) -> f a
02:07:15 <phadej> fwiw, list comprehensions **are** elegant
02:07:29 <phadej> my personal opinion
02:07:35 <jle`> yeah, list comprehensions are beauty
02:09:05 <lavalike> a list comprehension pattern match failure corresponds to mzero, right?
02:10:17 <pavonia> jle`: Meh, I'm using several helper functions here and it's probably not worth assembling a comprehensible sample code now :p
02:10:35 <jle`> what is the type of the funcion you would be looking for?
02:10:42 <uboa> list comprehensions are great! i use them a lot in python
02:11:48 <pavonia> jle`: [Result a] -> [a] where Result is form this package http://hackage.haskell.org/package/json-0.9.2/docs/Text-JSON.html#t:Result
02:12:15 <uboa> so far i'm really enjoying reading haskell!
02:12:19 <uboa> er learning
02:12:30 <pavonia> It already has an Alternative instance, I thought this could be used here somehow
02:12:30 <maerwald> pavonia: don't strive for elegance, strive for simplicity.
02:13:20 <jle`> list compies are nice for this
02:13:41 <pavonia> maerwald: elegant and simple solutions often correlate with each other
02:13:48 <jle`> sometimes
02:14:15 <jle`> if you have prisms for Result you can do mapMaybe (preview _Ok)
02:14:28 <maerwald> pavonia: no
02:14:40 <maerwald> Ime
02:16:03 <lavalike> I don't know if there are facilities to get the mapping from a Foo a to a [a] automatically, but if you define such a function foldMap does catMaybes-like things
02:16:06 <lavalike> > foldMap (\case Nothing -> []; (Just x) -> [x]) [Just 1, Nothing, Just 2]
02:16:09 <lambdabot>  [1,2]
02:16:09 <jle`> unsafeCoerce (lefts @a @String)
02:16:19 <jle`> hehe
02:16:44 <jle`> lavalike: that's just mapMaybe
02:17:01 <jle`> if you foldMap to 0-or-1 item lists
02:17:10 <lavalike> jle`: yeah I mean, if there's a way to generate the first function with some deriving mechanism or something, you can do it for non-Maybe types (:
02:17:33 <jle`> lavalike: i mean, instead of returning [] or [x], you can return Nothing or (Just x)
02:17:46 <lavalike> ah interesting didn't know that one
02:17:51 <jle`> :t mapMaybe
02:17:52 <lambdabot> (a -> Maybe b) -> [a] -> [b]
02:18:16 <jle`> mapMaybe (\case Ok x -> Just x; Error _ -> Nothing)
02:19:18 <jle`> if you have a Generic/Data instance you can do it generically with GHC.Generics stuff, but that's probably def overkill, heh
02:19:24 <lavalike> :D
02:19:44 <lavalike> or in the case of a structural equivalent to Maybe, unsafeCoerce it
02:19:45 <jle`> but that package doesn't derive Generic for its type
02:19:57 <jle`> yeah, that's what my `unsafeCoerce (lefts @a @String)` solution was
02:20:02 <jle`> :t lefts
02:20:03 <lambdabot> [Either a b] -> [a]
02:20:48 <lavalike> from list comprehension to reading comprehension (:
03:01:51 --- mode: glguy set +v anon
03:24:18 --- mode: glguy set +v govno
03:36:11 <philippD> Is anyone going to be at the CCC this year? Last year there was a small improvised Haskell table.
03:40:22 <mniip> cartesian closed conference?
03:44:09 <dibblego> @type (^.. folded . _Left)
03:44:10 <shachaf> cartesian closed category computer club communication congress
03:44:10 <lambdabot> Foldable f => f (Either a c) -> [a]
03:44:56 * geekosaur suddenly wonders if that CTF a few years ago managed to become a "gateway drug" of some kind
03:45:05 <geekosaur> grantingt hat wasn't CCC
04:16:45 <shiona_> I assume there is no argument parser that can take commandline strings like '-a a_of_type1 -b else -A a_of_type2' and provide the arguments to the application in something like Args { as = [Type1A "a_of_type1", Type2A "a_of_type2"], b = "else" }
04:18:04 <shiona_> in this case the ordering of the 'as' is also important, so I cannot take two lists 'as_of_type1' and 'as_of_type2' and concatenate them
05:21:47 * hackage inspection-testing 0.4.1 - GHC plugin to do inspection testing  http://hackage.haskell.org/package/inspection-testing-0.4.1 (JoachimBreitner)
05:45:42 <Ariakenom> Is there a reason Float has Ord but not Bounded?
05:50:24 <__monty__> Ariakenom: Probably because of +/- infinity?
05:54:31 <philippD> It seems that Prelude only contains Bounded instances for discrete types.
05:58:02 <Ariakenom> philippD: what does that mean?
05:58:16 <Ariakenom> __monty__: why are those not the bounds I mean
05:58:55 <__monty__> philippD: Float *is* discrete though, no?
06:01:28 <Axman6> also there are values outside -inf to +inf; all the NaNs
06:01:55 <Axman6> it really doesn't make much sense for them to have bounded instances
06:02:29 <Axman6> it also doesn't really make sense to have enum instances, IMO, but they do
06:03:55 <Axman6> > takeWhile (\(a,b) -> a /= b) $ zip`ap`tail $ [1::Float..]
06:03:56 <lambdabot>  error:
06:03:57 <lambdabot>      Not in scope: type constructor or class ‘Float..’
06:03:57 <lambdabot>      No module named ‘Float’ is imported.error:
06:04:02 <Axman6> > takeWhile (\(a,b) -> a /= b) $ zip`ap`tail $ [1::Float ..]
06:04:04 <lambdabot>  [(1.0,2.0),(2.0,3.0),(3.0,4.0),(4.0,5.0),(5.0,6.0),(6.0,7.0),(7.0,8.0),(8.0,...
06:04:18 <Axman6> > dropWhile (\(a,b) -> a /= b) $ zip`ap`tail $ [1::Float ..]
06:04:24 <lambdabot>  mueval-core: Time limit exceeded
06:05:28 <tsaka__> stack haddock does not build documentation for packages specified in extra-deps in stack.yaml
06:05:34 <tsaka__> Any way to make it do it?
06:07:34 <Ariakenom> Axman6: which is why I said Ord but not Bounded yeah
06:13:16 <__monty__> Ariakenom: Hmm, I guess nan can be compared but messes with bounds, so Ord but not Bounded. Because +inf/-inf is both greater than and less than nan.
06:16:52 <philippD> __monty__: would you call floating point numbers discrete?
06:20:28 <Ariakenom> philippD: I would
06:20:48 <Ariakenom> __monty__: They break Set etc with the Ord instance
06:22:06 <__monty__> Ariakenom: Ord or Eq?
06:22:13 <__monty__> philippD: Me too.
06:22:42 --- mode: glguy set +v adilbenmoussa
06:29:09 <c_wraith> __monty__: actually, Ord, because of NaN
06:30:15 <c_wraith> > ((0/0) < 1, (0/0) > 1)
06:30:17 <lambdabot>  (False,False)
06:30:26 <c_wraith> Set depends on Ord instances being antisymmetric
06:31:41 <__monty__> I figured it'd only use <, or >.
06:32:09 <c_wraith> > (0/0) == (0/0)
06:32:11 <lambdabot>  False
06:32:18 <c_wraith> also reflexive
06:32:54 <__monty__> Yeah but that's the Eq instance, is it not?
06:33:17 <c_wraith> Sure, but the dependency on antisymmetry still exists.
06:33:26 <c_wraith> Even if all the comparisons are done with <
06:33:51 <c_wraith> every rebalancing operation depends on it
06:34:33 <c_wraith> rotations are only valid if < is antisymmetric
06:36:14 * hackage inspection-testing 0.4.1.1 - GHC plugin to do inspection testing  http://hackage.haskell.org/package/inspection-testing-0.4.1.1 (JoachimBreitner)
06:37:39 <c_wraith> ie, the simplest BST rotation is something like Node A (Node B Nil Nil) Nil ---> Node B Nil (Node A Nil Nil)
06:38:09 <c_wraith> But that's only valid if B < A ==> A > B
06:42:59 <c_wraith> Or, to make it exactly about antisymmetry, if (B < A) ==> not (A < B)
06:46:24 <__monty__> Ok, makes sense, interesting.
06:47:43 <c_wraith> But you're right, the Eq instance also breaks Set.  You can put as many NaNs in as you want!
06:50:21 <ggole> Sets of floating points seems pretty dubious in general.
06:50:34 <__monty__> c_wraith: All those poor old ladies.
06:51:03 <c_wraith> __monty__: oh, I was leaning a different direction and thinking about how stuffed I am after a visit to an Indian buffet.
06:51:28 <c_wraith> ggole: also true, but that's for reasons totally unrelated to breaking assumptions the BST implementation makes. :)
06:51:33 <ggole> Maybe if the discrimination were done at the bit level
06:52:23 <ggole> eg, give me the set of constants used in this JS program.
06:53:03 <c_wraith> It's perfectly well-defined.  The problem is people expecting that if 0.3 is in the set, (0.1 + 0.2) is also in the set
06:53:06 <ggole> Perhaps that's not even a set of floats, though.
06:53:50 <ggole> You also get issues like negative zeroes
06:54:03 <c_wraith> < -0 < 0
06:54:06 <c_wraith> err
06:54:09 <c_wraith> > -0 < 0
06:54:11 <lambdabot>  False
06:54:16 <ggole> I put x in a set, I take x out, now it behaves differently
06:54:31 <ggole> -0 doesn't break the set invariants
06:54:34 <c_wraith> So the only thing of interest there is that whichever of 0 and -0 was inserted first wins
06:54:43 <ggole> Right.
06:55:49 <c_wraith> But I agree - any case where a Set of floating point values actually works could also be represented without floating point values.  And everyone would be happier.
07:04:35 <__monty__> I guess people just expect to be able to compare floats so it was a pragmatic choice.
07:04:46 * hackage graphql-w-persistent 0.3.1.0 - Haskell GraphQL query parser-interpreter-data processor.  http://hackage.haskell.org/package/graphql-w-persistent-0.3.1.0 (jasonsychau)
07:07:24 <ggole> Well, comparing floats is mostly fine
07:09:47 <tsaka__> how to force stack rebuild of the local, main package (executable)? force-dirty does nothing
07:11:25 <MarcelineVQ> can try  stack clean && stack build
07:11:50 <tsaka__> that works, thank you
07:15:16 * hackage homplexity 0.4.4.0 - Haskell code quality tool  http://hackage.haskell.org/package/homplexity-0.4.4.0 (MichalGajda)
07:36:16 * hackage homplexity 0.4.4.1 - Haskell code quality tool  http://hackage.haskell.org/package/homplexity-0.4.4.1 (MichalGajda)
07:41:13 --- mode: glguy set +v govno
07:49:15 <amx> why is binary 0.10.0.0 deprecated?
07:49:29 <amx> I sort of get why 0.9.0.0 was a mistake, but not 0.10.0.0
07:51:29 <Welkin> who says it's deprecated?
07:51:38 <Welkin> is that what the red version number means on hackage?
07:52:05 <amx> that's the next thing, you have to inspect element, the class name is "deprecated"
07:52:18 <Welkin> well, it's obviously not deprecated
07:53:55 <[exa]> git master says '0.8.6.0'
07:54:07 <[exa]> isn't 0.10.0.0 more like beta testing?
07:55:11 <MarcelineVQ> it does have deprecation coloring
07:55:30 <MarcelineVQ> not that that's especially informative
07:56:16 <Welkin> there are all kinds of weird colorations on hackage
07:56:35 <Welkin> random old version of some packages are colored yellow for "unpreferred" or randomly red
07:56:40 <Welkin> and some are green
07:56:45 <Welkin> whatever the green means
07:56:52 <amx> what is the CSS classname? :)
07:56:55 <Welkin> I can't seem to find the green ones right now, but I know they exist
07:57:54 <amx> oh, 0.10.0.0 was another re-release of 0.8.6.0
07:57:59 <Welkin> I only look at the readme to see if it is deprecated
08:00:49 <amx> do you have an example at hand for yellow?
08:01:21 <amx> ah found one
08:01:23 <amx> http://hackage.haskell.org/package/hspec
08:08:57 <ab9rf> i feel randomly green sometimes
08:14:11 <Welkin> kermit?
08:20:46 * hackage nanovg-simple 0.4.0.0 - Simple interface to rendering with NanoVG  http://hackage.haskell.org/package/nanovg-simple-0.4.0.0 (CthulhuDen)
08:28:46 * hackage graphql-w-persistent 0.3.1.1 - Haskell GraphQL query parser-interpreter-data processor.  http://hackage.haskell.org/package/graphql-w-persistent-0.3.1.1 (jasonsychau)
08:40:24 <Welkin> > 0/0 == 0/0
08:40:27 <lambdabot>  False
08:40:42 <Welkin> 0 `div` 0 == 0 `div` 0
08:40:46 <Welkin> > 0 `div` 0 == 0 `div` 0
08:40:47 <lambdabot>  *Exception: divide by zero
08:41:06 <Welkin> floating-point division by zero should be an exception too
08:41:08 <Welkin> what
08:41:18 <Welkin> IEEE is messed up
08:43:42 <__monty__> Can't very well claim to implement floats and not have 0/0 return NaN.
08:48:16 <Solonarv> sure you can, you just can't claim to implement *IEEE* floats
08:50:59 <__monty__> Not really though. Whenever you say float people expect IEEE floats.
08:52:09 <Welkin> what's the point of NaN?
08:52:22 <Welkin> to me it feels like null, a huge mistake
08:54:27 <__monty__> It's pointless arguing about the point of NaN is my point.
08:55:23 <__monty__> Note that I'm not saying 0/0 should always be NaN, afaik Rational doesn't have a NaN concept?
08:56:23 <[exa]> Welkin: that roughly translates to: what's the point of having numbers with non-total operations?
08:56:39 <Solonarv> while it is indeed sensible to have NaN, it is much less sensible to have NaN ≠ NaN
08:56:51 <Solonarv> (IMO)
08:57:07 <[exa]> it's not very sensible to allow exact comparison on floats
08:58:03 * [exa] votes for `unsafeEqCompareFloats`
09:00:54 <govno> what is a Rank2Types ?
09:02:45 <__monty__> Solonarv: If NaN == NaN there wouldn't be a straightforward way to figure out you're holding a NaN and you can stop the nonsense computations.
09:03:10 <__monty__> x == x is a property of *numbers* NaN isn't a number so it doesn't have to share their properties : )
09:03:15 <Solonarv> we have 'isNan' for a reason
09:03:49 <maksim__> is there any way to make partially applied type synonyms work?
09:03:50 <philippD> There are is more than one NaN in an IEEE 754 number.
09:03:55 <__monty__> But that's not guaranteed to be available, apparently it wasn't originally.
09:04:23 <Solonarv> I'm not saying haskell should deviate from IEEE 754, I'm saying (parts of) IEEE 754 were a mistake
09:05:41 <__monty__> And I'm very skeptical of that. Unless you're a hardware expert I don't think you can change my mind.
09:06:45 <[Leary]> What does "x == x" doesn't have anything to do with numbers? That's just reflexivity, a property of every equivalence relation on anything whatsoever.
09:07:53 <__monty__> [Leary]: But NaN isn't in the domain of the equivalence relation.
09:09:11 <Solonarv> having == be an equivalence relation is incredibly useful!
09:09:48 <__monty__> Solonarv: But an equivalence relation that works on numbers doesn't have to work on not-a-numbers.
09:10:24 <Solonarv> sure, if there were a "float-that-is-not-NaN" type I'd agree with you
09:10:56 <[Leary]> Again, I don't think "numbers" have anything to do with this. NaN :: Float, so if (==) is to be an equivalence relation on Float reflexivity demands NaN == NaN.
09:11:14 <Solonarv> but as it is == on IEEE floats is either an equivalence relation or not, and the standard demands that it isn't
09:11:56 <Welkin> in languags where every number is a float, that is a bigger problem
09:12:08 <Welkin> (i.e. javascript)
09:12:16 <Solonarv> nowhere is it said "== is an operation defined on numbers only"... after all it works just fine on e.g. [Ordering]
09:13:18 <__monty__> Solonarv: Is it said anywhere "== has to be an equivalence relation?"
09:14:15 <Solonarv> yes: the documentation for Eq (which also comes with a disclaimer along the lines of "floats break this law, but the instance is still useful-ish")
09:14:35 <ski_> govno : it's when a function takes a polymorphic argument
09:15:09 <maksim__> what is the name for the kind of polymorphism that haskell uses?
09:15:15 <maksim__> is it not parametric polymorphism?
09:15:29 <monochrom> Yes.
09:16:18 <ski_> with class constraints, it's sometimes called bounded parametric polymorphism
09:16:28 <maksim__> cool
09:16:35 <Solonarv> the exact phrasing is "The haskell report defines no laws for Eq. However, == is customarily expected to be an equivalence relation.", and there's a further disclaimer on the Float and Double instances
09:16:38 <Solonarv> https://hackage.haskell.org/package/base-4.12.0.0/docs/Data-Eq.html#t:Eq
09:16:44 <maksim__> i've seen people use the phrase ad-hoc polymorphism 
09:16:48 <ski_> (though perhaps that term more properly refers to combining parametric polymorphism with subtyping bounds)
09:16:49 <__monty__> Solonarv: "Customarily" doesn't sound like all that strong a rule.
09:16:49 <maksim__> and wasn't sure what the relation was
09:16:54 <kadoban> I thought it's more ad hoc polymorphism
09:17:10 <ski_> maksim__ : yes, that's also used
09:17:11 <govno> ski_: whereis it define in GHC?
09:17:26 <Solonarv> kadoban: that's specifically the term for haskell's type class system
09:17:46 <Solonarv> __monty__: indeed. Why do you think the docs are so wishy-washy??
09:18:08 <ski_> govno : there should be a subsection at <https://www.haskell.org/ghc/docs/html/latest/users_guide>, i think
09:18:37 <__monty__> Solonarv: Because sometimes you may want to compare values of a type that doesn't allow for an equivalence relation?
09:18:43 <govno> ski_: I think find htis realization in GHC
09:18:51 <kadoban> The the parametric part just comes in in terms of unconstrained types?
09:18:57 <kadoban> Then*
09:18:58 <Solonarv> __monty__: sure, but float *does* allow for an equivalence relation
09:18:59 <monochrom> Maybe "type class polymorphism" and be done with it.  "Ad hoc" had something slightly different in mind before Haskell.
09:19:15 <[Leary]> Imo Float should just give up its instances to IeeeFloat and SpookyFloat newtypes. The former can be conformant while the latter can be lawful.
09:19:24 <__monty__> Solonarv: Not as I see it, because IEEE float means whatever adheres to the standard.
09:20:18 <Solonarv> Sure. The '==' operation as defined in the IEEE standard is not an equivalence relation. But there's no law stating that the '==' operation in Eq Float must be identical to the IEEE '=='
09:20:25 <ski_> govno : an example would be `blah :: (forall a. [a] -> [a]) -> (String,[Bool]); blah f = (f "False",f [False,True])'. note that `blah' is *not* polymorphic (but its argument `f', is *required* to be polymorphic)
09:20:31 <__monty__> [Leary]: Are SpookyFloats, really that useful? And, why not newtype it?
09:20:32 <monochrom> (==) is a bit tricky for IEEE754 because -0 and +0.  (==) will not distinguish them, but \x -> 1/x will.
09:20:57 <__monty__> Solonarv: Law of Demeter : >
09:21:07 <monochrom> Here the broken expectation is Leibniz: if x==y then for all f, f x == f y.
09:21:39 <ski_> > map atan [-1/0,1/0]
09:21:41 <lambdabot>  [-1.5707963267948966,1.5707963267948966]
09:21:57 <monochrom> OTOH sure, (==) for IEEE754 is still an equivalence relation.
09:22:12 <Solonarv> I'm not actually sure what you're arguing, __monty__ - are you saying: there is nothing unfortunate about the IEEE754 definition of floating point equality, and it never causes any problems?
09:22:38 <int-e> monochrom: what about reflexivity?
09:22:40 <Solonarv> or are you saying "The Eq Float instance should conform to IEEE754's notion of floating point equality"
09:23:01 <Solonarv> > 0/0 == 0/0 -- monochrom
09:23:02 <__monty__> Solonarv: The latter.
09:23:03 <lambdabot>  False
09:23:12 <monochrom> Leibniz is the most important and most neglected aspect of equality.  Millions of people-hours were wasted on confusions and debates on referential transparency because people forgot to think about Leibiz explicitly.
09:23:30 <monochrom> Oh, NaN eh?
09:24:36 <Solonarv> __monty__: what about not having an Eq instance for Floats/Doubles (and instead having some 'ieeeEquals' operator) ?
09:24:39 <int-e> But FWIW, I think it's a good practical decision to stick to IEEE 754 semantics *and* use the normal Ord and Eq instances for floats.
09:24:54 <Solonarv> sure, for practical reasons it makes sense
09:25:01 <monochrom> Yes, IEEE754 is pretty practical.
09:25:54 <monochrom> And +0 vs -0 is super-important because 1/x and tan and atan etc.
09:26:19 <Solonarv> although I'm not sure how practical NaN ≠ NaN in particular is - I seem to vaguely recall it being that way to avoid needing to add a new primitive operation, or something?
09:26:40 <dogonthehorizon> Hi all, I'm working through Thinking with Types and am trying to write SmallCheck instances for the heterogenous list example. I've tried to follow the same pattern for implementing my own Serial instance like we did for Ord/Show but the type errors I'm getting from GHC make me think that my constraints are not correct. Can someone take a look at this Gist and point me in the right direction? Do I need to
09:26:41 <dogonthehorizon> define separate instances for Serial like I did before implementing the closed All family? https://gist.github.com/dogonthehorizon/1f349233696c14cd518c5d5cc65e4048
09:26:44 <__monty__> Solonarv: Yes, isNaN().
09:27:02 <monochrom> Ideally you don't even see NaN; you should receive an exception.
09:27:35 <int-e> Solonarv: It's just that *any* comparison involving an NaN is false. Typically these two NaN's would be results of different expressions anyway.
09:27:36 * ski_ . o O ( signalling NaNs )
09:27:52 <monochrom> But this is something from the 80s and 90s, and with communities who have only heard of Fortran and C.  "wtf is exception?"
09:27:57 <__monty__> int-e: Non /= : >
09:28:12 * ski_ . o O ( three-valued SQL logic )
09:28:53 <int-e> . o O ( any general statement about floating point numbers is false )
09:29:18 <govno> ski_: thanks
09:29:29 <dmwit> dogonthehorizon: Fun error. =)
09:29:38 <__monty__> int-e: Nice russel's : )
09:29:51 <dmwit> dogonthehorizon: The basic point that error is making is this: just from the type alone, we can know whether we should be producing an `HNil` or a `(:#)`.
09:30:04 <dmwit> dogonthehorizon: So it makes no sense to choose between them.
09:31:03 <dmwit> dogonthehorizon: On solution would be to explicitly write two instances for `Serial m (HList [])` and `(Serial m t, Serial m (HList ts)) => Serial m (HList (t:ts))`.
09:31:15 <dmwit> s/On/One/
09:32:13 <dmwit> dogonthehorizon: An interesting parallel: in each of your previous instances, where you were consuming rather than producing HLists, why didn't this come up?
09:32:19 <Solonarv> that's usually how these things are done, yes
09:32:35 <dmwit> dogonthehorizon: Answer: because, once again, just from the type alone we know that in each case one of the two branches of your case on the HList will never match.
09:32:41 <monochrom> You also look at Unix signals and C setjmp and longjmp, and you understand that back then, even Ken Thompson and Dennis Ritchie could not think up clean ways to raise and catch errors.
09:32:55 <dmwit> dogonthehorizon: So in each previous case, you always have dead code. But GHC is okay with dead code.
09:34:17 <dogonthehorizon> dmwit: Thanks for the thorough explanation! So the `All` isn't a panacea, there will still be cases where we have a bit more boilerplate because GHC can't decide between instances?
09:34:30 <dogonthehorizon> Or types, rather
09:38:38 <dmwit> "can't decide"?
09:39:54 <dmwit> (I think your intuition sounds sensible on the surface, and meshes well with my gut reaction, but when I try to unpack the intuition to use it to make a prediction I find I can't.)
09:41:28 * hackage chiphunk 0.1.0.0 - Haskell bindings for Chipmunk2D physics engine  http://hackage.haskell.org/package/chiphunk-0.1.0.0 (CthulhuDen)
09:44:53 <dmwit> Interesting. I wonder why they created chiphunk instead of using (or maybe updating) Hipmunk.
09:48:01 <Welkin> does IntMap require Int specifically? Or can you use Int64 for the keys?
09:48:50 <Solonarv> Int specifically; it's not polymorphic
09:49:23 <Solonarv> but in practice, on a 64bit system Int will be the same thing as Int64 anyway (IIRC)
09:50:28 <dmwit> I expect if you want an Int64Map you could get one pretty quick by copy-and-paste and a little bit of fixup.
09:51:03 <Solonarv> yeah I was about to say that
09:51:14 <Welkin> what is the difference I don't want to bother with the trouble
09:51:17 <dmwit> If you automate it suitably it might even be worth making a package with variants for {Int,Word}{8,16,32,64}.
09:51:24 <Welkin> I have Int64 and need to use a map
09:51:32 <Welkin> I usually use Data.Map for everything
09:51:39 <Solonarv> Int64 is guaranteed to be 64bit, Int is only guaranteed to cover a slightly smaller range
09:51:40 <Welkin> not sure what the main benefit of using IntMap is
09:51:44 <dmwit> speed
09:51:51 <Solonarv> it's faster
09:52:01 <Solonarv> it might also be more memory efficient, I'm not sure
09:53:21 <geekosaur> there's an EnumMap package already (it just wraps everything in toEnum/fromEnum iirc)
09:54:12 <Welkin> I am using an STMMap (HAMT) to store byestring keys pointing to values that are each Map Int64 UTCTime
09:54:47 <Welkin> I need to update and invalidate old timestamps
09:55:04 <Welkin> mapping or folding over the Map could work of course
09:55:18 <Solonarv> I think there was some noise about a backpack-based variant of containers
09:55:26 <Welkin> I'm wondering if I could expire old values more easily using filter on the values
09:55:30 <Welkin> but that would require a bimap
09:56:20 <Welkin> or if that would give any benefit at all other than maybe a nicer interface
09:57:22 <Welkin> `Map.filter (\_ time -> diffUTCTime now time <= 0 )`
09:57:26 <Welkin> that seems good enough
09:58:09 <Welkin> other way around
09:58:11 <Solonarv> yeah that seems fine, and wouldn't require any change if you switched to IntMap (or a variant of it)
09:58:32 <Welkin> oh, I need collect the old values
09:58:36 <Welkin> so a partition is better then
09:59:03 <Welkin> I don't see a partition, so a fold it is
10:00:15 <Welkin> ah, my Int64 keys are also wrapped in a newtype
10:00:28 <Solonarv> IntMap has a 'partition' function
10:01:47 <Welkin> I see, lazy Map has it too
10:02:32 <Solonarv> yeah, IntMap has pretty much the same API as Map
10:03:18 <monochrom> HashMap is probably very similar to IntMap internally.
10:03:21 <Welkin> I can directly compare UTCTime even
10:03:23 <Welkin> cool
10:03:37 <Welkin> > getCurrentTime
10:03:39 <lambdabot>  error: Variable not in scope: getCurrentTime
10:03:59 <Welkin> > Data.Time.currentTime
10:04:02 <lambdabot>  error:
10:04:02 <lambdabot>      Not in scope: ‘Data.Time.currentTime’
10:04:02 <lambdabot>      No module named ‘Data.Time’ is imported.
10:04:03 <Solonarv> monochrom: I don't know exactly what that term means, but IntMap uses big-endian patricia trees while HashMap uses HAMTs
10:04:09 <Solonarv> Welkin: lambdabot doesn't have IO
10:04:12 <Solonarv> try yahb instead
10:04:19 <Welkin> how do I use yahb
10:04:27 <Welkin> I'll just use ghci
10:04:27 <Solonarv> % getCurrentTime
10:04:28 <yahb> Solonarv: ; <interactive>:374:1: error:; * Variable not in scope: getCurrentTime; * Perhaps you meant `getCurrentCCS#' (imported from GHC.Exts)
10:04:34 <Solonarv> yahb *is* a ghci
10:04:52 <Solonarv> just type '% ' followed by whatever you'd type into ghci
10:05:27 <Solonarv> % :! ls
10:05:28 <yahb> Solonarv: ClosureType.hs; LC.hs; ghci
10:07:49 <Welkin> % import Data.Time
10:07:49 <yahb> Welkin: 
10:08:05 <Welkin> % getCurrentTime >>= \now -> pure (addUTCTime nominalDay now) >>= \tomorrow -> pure (tomorrow >= now)
10:08:05 <yahb> Welkin: True
10:10:57 <Solonarv> what's with all the '>>= \... -> pure ...'
10:13:19 <Welkin> just the first thing I though to type
10:13:23 <Welkin> I know I can simplify it
10:13:33 <Solonarv> % do { now <- getCurrentTime; let tomorrow = nominalDay `addUTCTime` now; pure (tomorrow >= now) }
10:13:33 <yahb> Solonarv: ; <interactive>:378:96: error: parse error on input `}'
10:13:55 <Solonarv> % do { now <- getCurrentTime; let { tomorrow = nominalDay `addUTCTime` now }; pure (tomorrow >= now) }
10:13:55 <yahb> Solonarv: True
10:14:02 <Welkin> % getCurrentTime >>= \now -> pure ((addUTCTime nominalDay now) > now)
10:14:02 <yahb> Welkin: True
10:14:25 <Welkin> I prefer to avoid do-notation for small code segments
10:14:45 <geekosaur> you've got a classic use case for fmap there
10:15:03 <Solonarv> actually I'd prefer <&> here, tbh
10:15:30 <Welkin> % getCurrentTime <&> \now -> (addUTCTime nominalDay now) > now
10:15:31 <yahb> Welkin: True
10:15:50 <Welkin> I've never used <&>, but I use |> (the same operation in elm) all the time
10:16:07 <Welkin> oh, actually no
10:16:14 <Welkin> |> is `flip ($)`
10:16:37 <Welkin> damn dollar signs in haskell always break my programs/cause type errors
10:16:44 <Welkin> I'm avoiding them more lately than ever before
10:17:14 <monochrom> But you can use fmap as "fmap" rather than "<$>".
10:17:31 <Welkin> yeah
10:17:35 <Welkin> I do use <$> a lot though
10:17:39 <Welkin> just not ($)
10:17:51 <c_wraith> that makes no sense.  They introduce problems in exactly the same situations.
10:18:07 <Welkin> what?
10:18:28 <Welkin> $ always breaks my monadic pipelines
10:19:02 <Solonarv> that's not $ introducing errors, that's just it being the wrong function for what you want to do
10:19:21 <c_wraith> It would be the right function if you used it with <=<
10:19:52 <Welkin> remembering the precedence rules for everything is too much work
10:20:18 <Welkin> :t (<=<)
10:20:18 <c_wraith> For everything, sure.  But $ exists solely because it's precedence 0
10:20:19 <lambdabot> Monad m => (b -> m c) -> (a -> m b) -> a -> m c
10:20:29 <Welkin> I've never used the fish before
10:23:01 <MarcelineVQ> % for (Just [1,2,3]) \x -> id x
10:23:01 <yahb> MarcelineVQ: [Just 1,Just 2,Just 3]
10:23:19 <MarcelineVQ> now with less $
10:24:17 <Solonarv> eh, I just use it infix: blah `for` \x -> ...
10:25:59 <monochrom> And me:
10:26:02 <MarcelineVQ> I can't do the more useful do example because it's multiline :O
10:26:15 <monochrom> > (\x -> x) `traverse` Just [1,2,3]
10:26:17 <lambdabot>  [Just 1,Just 2,Just 3]
10:27:55 <Solonarv> eh? sure you can; just use semicolons + braces instead of layout
10:28:44 <monochrom> Real reason: too lazy.
10:29:31 <Solonarv> heh
10:32:34 <MarcelineVQ> % when (2 > 0) do { print 1 }
10:32:35 <yahb> MarcelineVQ: 1
10:33:15 <MarcelineVQ> but it's more interesting with the indentation since that's how you see do so often
10:35:11 <Solonarv> whew, BlockArguments only got added in 8.6.1
11:35:24 --- mode: glguy set +v numm1
11:37:56 <numm1> repeated :: (Eq a) => [a] -> Int repeated [] = 0 repeated [x] = 0 repeated [x,y] = if x==y then 1 else 0 repeated (x:y:xs) = if x == y then 1 + repeated(y:xs) else repeated(y:xs)   main :: IO() main = do   print $ repeated []   print $ repeated [1,2,2,3,4,4,4]   print $ repeated "aaa"
11:38:05 <numm1> repeated :: (Eq a) => [a] -> Int repeated [] = 0 repeated [x] = 0 repeated [x,y] = if x==y then 1 else 0 repeated (x:y:xs) = if x == y then 1 + repeated(y:xs) else repeated(y:xs)   main :: IO() main = do   print $ repeated []   print $ repeated [1,2,2,3,4,4,4]   print $ repeated "aaa"
11:38:14 <numm1> anyone that can help
11:38:42 <numm1> im geting ambigiuos type variable error
11:38:56 <numm1> when calling the method with a empty list
11:39:36 <[exa]> try specifying the type of the list, eg. using ( [] :: [Int] )  instead of []
11:40:45 <[exa]> btw please paste your code on gists or so, this cannot be read very well
11:42:03 <[exa]> anyway -- the main problem is that Haskell needs to know which Eq instance to choose for running 'repeated []', but there's no information to get it
11:44:14 <numm1> how do i specify to use Int type for []
11:44:39 <numm1> in the method signature
11:46:55 <[exa]> numm1: as I wrote above: []::[Int]
11:47:00 <[exa]> (inline in the code)
11:47:08 <lyxia> repeated :: [Int] -> Int
11:50:20 <Welkin> I can't quite figure out how to mapM over this https://hackage.haskell.org/package/stm-containers-1.1.0.2/docs/StmContainers-Map.html
11:50:40 <Welkin> it looks like unfoldlM is maybe what I want
11:50:56 <Welkin> which uses some package called deferred-folds
11:53:03 <glguy> I wouldn't expect to be able to mapM over this
11:53:18 <glguy> similar to how you can't mapM over an IOArray
11:53:46 <maksim_> is there another language where you can use infix operators as prefix?
11:53:46 <Welkin> or some way to get all the keys
11:53:54 <numm1> my current method signature is "repeated :: (Eq a) => [a] -> Int" as it should work for list of Chars and INts
11:54:00 <Welkin> I need to update all of the values at once
11:54:10 <numm1> but it doesnt work for empty list []
11:54:36 <lyxia> maksim_: ocaml
11:54:42 <kark> maksim_: SML
11:54:45 <maksim_> lyxia, any of the mainstream languages?
11:55:02 <Welkin> if it has a book by oreilly, it's mainstream
11:55:07 <maksim_> lol
11:55:59 <Welkin> https://hackage.haskell.org/package/deferred-folds-0.9/docs/DeferredFolds-UnfoldlM.html#t:UnfoldlM
11:56:06 <Welkin> what is a Gonzalez fold?
11:56:16 <Welkin> something Gabriel came up with?
11:56:28 <Solonarv> yes, IIRC it's the Foldl package
11:56:45 <kark> maksim_: you could also just wrap the operator with a function
11:56:59 <maksim_> kark, i'm just writing down a dictionary of haskell syntax
11:57:10 <kark> oic
11:58:13 <Welkin> okay, so using STMContainers.Map is making things quite a bit more complicated
12:00:37 <Welkin> on an interval, I want to map over all of the entries and update them
12:00:39 <Welkin> https://hackage.haskell.org/package/deferred-folds-0.9/docs/DeferredFolds-UnfoldlM.html#v:mapFoldMInput
12:00:42 <Welkin> is this what I want?
12:00:53 <__monty__> maksim_: Lisp's usually use prefix for things you'd expect to be infix.
12:00:59 <Welkin> "Lift a fold input mapping function into a mapping of unfolds "
12:01:00 <Welkin> what
12:01:36 <Welkin> how would I use this? mapFoldMInput :: Monad m => (forall x. FoldM m b x -> FoldM m a x) -> UnfoldlM m a -> UnfoldlM m b 
12:04:37 <Welkin> I've never seen this foldl library before
12:08:18 <Welkin> another option is to update each entry when some other action is being performed on it
12:08:29 <Welkin> then I don't need a dedicated thread for it
12:08:41 <Welkin> but it won't update on a regular interval
12:10:44 --- mode: glguy set +v lemastero
12:14:06 <lemastero> Seems to be no usage of Density Comonad. Anyone can proove me wrong?
12:19:16 --- mode: glguy set +v lemastero_
12:22:21 --- mode: glguy set +v lemastero
12:23:16 <Solonarv> I don't remember hearing about anyone using it, but that doesn't necessarily mean anything
12:24:04 <ab9rf> i thought i saw something the other day
12:24:09 <Solonarv> you could set up a script to pull package sources from hackage and search them for uses of "Control.Comonad.Density'
12:24:28 <ab9rf> but that might have been the codensity monad
12:24:57 <Solonarv> Codensity is very useful - it's a better-behaved ContT in many respects
12:25:11 <ab9rf> yeah, that is likely what it was
12:25:11 <c_wraith> less powerful, but usually in exactly the way you want less power.
12:27:34 <lemastero> Thanks for suggestion to pull sources from Hackage :) Might be the right way to go.
12:36:39 --- mode: glguy set +v kalhauge
12:37:09 <kalhauge> I don't know if this is the right place to ask, but here we go:
12:37:56 <kalhauge> I have a problem that requires predicates that can do abitrary monadic computations.
12:38:14 <nitrix> Define "arbitrary"/
12:38:27 <koz_> kalhauge: By 'arbitrary monadic computations' do you mean 'computations in an arbitrary monad'?
12:38:36 <kalhauge> newtype PredicateM m a = PredicateM { runPredicateM :: a -> m Bool } 
12:38:49 <kalhauge> Sorry (slow typer)..
12:39:30 <kalhauge> It is pretty obivous that PredicateM m is a contravariant functor in a , as long a m is an applicative. 
12:39:51 <kalhauge> .. sorry always 
12:40:04 <kalhauge> instance Contravariant (PredicateM m) where
12:40:05 <kalhauge>   contramap f g = PredicateM $ runPredicateT g . f
12:40:41 <kalhauge> I can also see that PredicateM is a functor over the category of monads:
12:41:05 <kalhauge> instance MonadFunctor PredicateM where
12:41:06 <kalhauge>   mmap fn pred =
12:41:06 <kalhauge>     PredicateM $ \a -> fn $ runPredicateM pred a
12:41:52 <kalhauge> But I'm looking for the name of something that is effectfully contravariant in a:
12:42:28 <kalhauge> contrmapM :: (a -> m b) -> PredicateM m b -> PredicateM m a
12:43:37 <kalhauge> contrmapM fm pred = PredicateM $ fm >>= runPredicateM pred
12:44:29 <kalhauge> Do anybody know which typeclass or name such a MonadFunctor should be called?
12:44:47 <kalhauge> .. or rather is called.
12:46:19 <edwardk> lemastero: unlike codensity afaik it never helps asymptotics
12:49:03 <kalhauge> koz_: yes I thinks so :)
12:49:54 <koz_> kalhauge: This might actually be a good question to edwardk.
12:50:08 <c_wraith> Honestly, I'm surprised he hasn't answered yet.
12:50:10 * edwardk looks up
12:50:25 <edwardk> c_wraith: walking at the store
12:50:43 <c_wraith> what do you mean, "finding items to buy " and "not running into people" are priorities? :P
12:51:00 <edwardk> replacing my laptop
12:51:11 <koz_> edwardk: Ah, taking advantage of the sales?
12:51:23 <edwardk> was going to stream then my mac decided it wasnt going to accept power any more
12:51:40 <koz_> edwardk: I know the feeling. My laptop decided to have its hard drive die today.
12:51:46 <edwardk> taking advantage of sales would involve waiting til post thanksgiving
12:51:59 <koz_> edwardk: I guess we get sales earlier or something.
12:52:11 <lemastero> https://github.com/ekmett/ekmett.github.com/blob/8d3abab5b66db631e148e1d046d18909bece5893/haskell/Origami.hs#L226 edwardk you have used it!
12:54:55 <Welkin> edwardk: how old was your mac?
12:55:42 <edwardk> lenatsero: wow that is old
12:55:55 <lyxia> kalhauge: maybe at this point you could use a more general class of functors than Functor (which only defines endofunctors on Hask), then your functor is one between Kleisli^op and Hask.
12:56:10 <edwardk> welkin: they replaced it for a similar issue ~6 months ago
12:56:46 <Welkin> edwardk: one of the new models?
12:57:17 <edwardk> macbook whatever model was current a year ago
12:57:33 <Welkin> I'm still using my 4.5 year old one
12:58:10 <kalhauge> lyxia: Thanks! I'll look at that!
12:58:27 <Welkin> is there any way to map over this? Or do I need to find another way to do it? https://hackage.haskell.org/package/stm-containers-1.1.0.2/docs/StmContainers-Map.html
13:00:13 <edwardk> welkin: using the listt conversion
13:00:32 <Welkin> not using unfoldlM?
13:00:55 <Welkin> I need to update the values, not just map over it
13:02:56 <Welkin> I see, maybe listT with traverse_ to perform the updates
13:08:46 <kalhauge> lyxia: I have looked at the Control.Arrow module, and it does not complete fit, at least I can't really see it. 
13:10:32 <kalhauge> Even though: Kleisli m a Bool ~ PredicateM m a, its not the composition of Kleisli arrows I'm after. Maybe I'm missing something.
13:10:37 <lyxia> kalhauge: there isn't a generalized class of functor in base
13:12:00 <lyxia> You'll only find the Category class and the Kleisli type, and that's about it.
13:13:02 <kalhauge> lyxia: Ah, okay.. I guess I have to think more about this.. Do you know of an example I can read up on? 
13:14:23 <kalhauge> lyxia: Do you know of a module that defines more complicated Functor types that I can learn from? 
13:14:58 <lyxia> https://hackage.haskell.org/package/categories-1.0.7/docs/Control-Categorical-Functor.html
13:15:10 <lyxia> I may be making this more complicated than it needs to be though
13:15:34 <lyxia> The main advantage of generalizing like that is you don't need to come up with a name for your class
13:17:41 <kalhauge> Thank you very much lyxia++, I will see if I can get it to work. 
13:38:50 --- mode: glguy set +v amarrella
13:41:09 <amarrella>  /msg NickServ VERIFY REGISTER amarrella lhylrvfhhpgn
13:41:22 <geekosaur> "oops"
13:41:26 <amarrella> lol
13:41:29 <amarrella> sorry
13:41:38 <MarcelineVQ> best to automate that in your client :>
13:43:54 --- mode: glguy set -v amarrella
13:46:01 <monochrom> Good password :)
13:46:28 <hpc> mine is just 12345
13:46:31 <hpc> i should change it to that
13:46:34 <amarrella> that's not the password at least :P 
13:46:44 <MarcelineVQ> 12345? what a conincidence, that's the same combination as my luggage
13:47:08 <kerrhau> mine is the same as my credit card pin: 0000
13:48:34 <Solonarv> pah, I know those aren't your real passwords - if you type your real password it gets turned into a bunch of asterisks automatically
13:48:42 <Solonarv> here's my password: **************
13:48:43 <kerrhau> ****
13:48:55 <kerrhau> weird
13:48:59 <MarcelineVQ> everything old is new again :>
13:49:00 <kerrhau> ****1
13:49:06 <kerrhau> looks like it works
13:49:18 <monochrom> **** is too short. :)
13:50:13 <kerrhau> o_O
13:53:01 <zachk> password
13:53:05 <zachk> hey that doesnt work!
13:55:33 <kark> hunter2
14:08:36 <hpc> :t (*************)
14:08:37 <lambdabot> Password
14:08:53 <noidedsuper> :t liftA2 <*>
14:08:54 <lambdabot> error:
14:08:54 <lambdabot>     parse error (possibly incorrect indentation or mismatched brackets)
14:09:00 <noidedsuper> :t liftA2 (<*>)
14:09:01 <lambdabot> (Applicative f2, Applicative f1) => f1 (f2 (a -> b)) -> f1 (f2 a) -> f1 (f2 b)
14:09:30 <noidedsuper> I forget, is the composition of two applictives always an applicative? 
14:10:01 <MarcelineVQ> you forget, or it's a homework question?
14:10:22 <noidedsuper> I forget. I'm not in school.
14:10:44 <Solonarv> noidedsuper: yes, it is. See the Applicative instance for Compose.
14:12:04 <Solonarv> informally: pure = Compose . pure . pure; (Compose fgk) <*> (Compose fgx) = Compose (liftA2 (<*>) fgk fgx)
14:12:53 <noidedsuper> Neat!
14:14:11 <zachk> does anyone know what units http://hackage.haskell.org/package/time-1.8.0.2/docs/Data-Time-Clock.html#t:NominalDiffTime is defined in? is it microseconds?
14:14:33 <hpc> "It has a precision of 10^-12 s"
14:14:52 <hpc> it uses http://hackage.haskell.org/package/base-4.12.0.0/docs/Data-Fixed.html#t:Pico
14:15:10 <hpc> there's a "source" button on the right you can use to see how it's defined
14:17:47 <monochrom> The unit is seconds.  But you can do fractions down to 10^-12.
14:18:36 <monochrom> And 10^-12 will mean 10^-12 seconds, therefore.
14:19:05 <dmwit> The question is kind of strange, since the link included in the question appears to answer the question. "Conversion functions will treat it as seconds. It has a precision of 10^-12 s." is a direct quote. So what is the actual question?
14:20:23 <monochrom> Perhaps due to confusion.  And confusion, due to wrong assumptions.
14:22:15 <monochrom> Wrong assumptions can come from other, inferior programming languages and/or libraries.  Typical C time libraries use integer types and therefore there is always tension between "1 means 1 second but now I can't do finer-grain times" and "1 means 10^-6 seconds oh but now I have to remember it's -6 not -9 because I saw another library that uses -9".
14:22:34 <monochrom> Haskell has a decent Num class therefore it can do the best of both worlds.
14:23:08 <noidedsuper> The Num class could definitely be improved though
14:23:16 <monochrom> and to hell with "everything is an integer".
14:23:21 <monochrom> Yeah.
14:23:38 <hpc> everything is a natural instead
14:23:44 <monochrom> But Num is a low-hanging fruit compared to inferior languages.
14:23:53 <noidedsuper> Honestly I just want a `Group` type in base 
14:23:59 <noidedsuper> *typeclass
14:26:35 <zachk> would pico be 10^-9 ? 
14:26:50 <monochrom> We're literally half way there. Because now semigroup is in base.  Pun intended.
14:27:06 <ion> Everything is an IEEE Double
14:27:17 <hpc> zachk: did you follow my link to the Pico docs?
14:27:23 <monochrom> -9 is nano, -12 is pico.
14:27:26 --- mode: glguy set +v jrp
14:27:38 <monochrom> But this is something you could ask Google.
14:27:40 <zachk> looking at them now
14:28:07 <monochrom> Although, maybe Google gave you two text editors instead.
14:28:43 <monochrom> Anyone up for creating a new editor called kilo? >:)
14:29:02 <jrp> The news summary page on Haskell.org seems to have been broken for the best past of a week, if not more.  Does no one care?
14:29:51 <noidedsuper> Wow I only now realized it's called a "semigroup" because it's literally a group with half the laws 
14:30:43 <monochrom> :)
14:31:22 <mniip> I'd say a third of the laws?
14:32:10 <noidedsuper> Groups have closure, associativity, an identity, and inverses
14:32:17 <noidedsuper> Semigroups have closure and associativity
14:32:53 <monochrom> closure is kind of meh. You always have closure.
14:33:16 <dminuoso> monochrom: What do you mean by "always"
14:33:30 <dminuoso> monochrom: Not every set is closed under every operation.
14:33:44 <dstolfa> ^
14:33:47 <noidedsuper> Integers aren't closed under multiplicative inverses for example 
14:33:54 <monochrom> Every category has an implicit closure axiom.
14:34:02 <dstolfa> not everything is a category
14:34:19 <noidedsuper> I think every group is, right? 
14:34:28 <dstolfa> sure, but that has closure in its axioms
14:34:38 <dstolfa> in fact every group is a groupoid with a single object whose hom-set is an automorphism set
14:34:40 <dstolfa> to be precise
14:34:47 <dstolfa> but that doesn't mean you always have closure
14:34:49 <hpc> i think what monochrom is getting at is that everything he considers interesting is in the context of CT :P
14:34:52 <monochrom> This just means your "everything" is wider than my "always".
14:34:53 <Solonarv> closure is implicit in haskell
14:35:01 <monochrom> Not just me.
14:35:27 <dstolfa> hpc: right, but the point is that category theory is a lot about writing down the right thing to form a category, which can be very difficult sometimes
14:35:27 <dminuoso> Solonarv: Kind of depends on how you define your scope.
14:35:29 <monochrom> You look at vector space axioms and it has closure.  You look at topology axioms and it has closure.  Etc etc.
14:35:45 <Solonarv> eh, sure
14:35:47 <dminuoso> Solonarv: I can think of operations on types where you do not have closure.
14:36:06 <Solonarv> writing "(<>) :: a -> a -> a" already implies closure, is what I meant
14:37:57 <dstolfa> keeping it very simple, subtraction with naturals is not closed
14:38:00 <dstolfa> so eh
14:38:03 <dstolfa> i disagree with the above
14:38:14 <monochrom> You're just nitpicing my "always".
14:38:27 <dstolfa> monochrom: i'm replying to Solonarv here, sorry for being implicit
14:39:17 <Solonarv> yeah, which means it shouldn't have that type if we wanted to be entirely correct
14:39:56 <Solonarv> (or it should just turn negative results into zero)
14:41:08 <noidedsuper> I think the problem here is mostly coming from two different viewpoints. If you're talking about programming, then yeah, you always have closure anyways. If you're talking about pure abstract algebra, then "closure" is generally the first thing you prove.
14:42:01 <monochrom> I'm also of the opinion that type signatures are superior to closure statements.
14:42:54 <hpc> ^ x100
14:43:04 <Welkin> closure statements?
14:43:16 <noidedsuper> Saying "This operation is closed over these elements" 
14:43:30 <noidedsuper> I suppose at least
14:43:37 <Welkin> oh in math terms
14:43:38 <dstolfa> monochrom: well, they're just stronger rather than superior really
14:43:52 <dminuoso> "noidedsuper | I think the problem here is mostly coming from two different viewpoints. If you're talking about programming, then yeah, you always have closure anyways."
14:43:57 <dminuoso> noidedsuper: That's fundamentally wrong.
14:44:04 <monochrom> Anyway my "always" has a scope in mind and while it's reasonable to ask me to clarify what scope I have in mind, it becomes insulting when you starting "educating" me that there is a world outside that scope as if I didn't know and needed education.
14:44:23 <dminuoso> noidedsuper: It's misguided by the notion that a set could only be a type, and that the only operations you are looking at have the shape `a -> a` or `a -> a -> a`
14:44:26 <monochrom> Most people's "always" and "every" have scopes.
14:44:50 <hpc> monochrom: i think you mean everyone's :P
14:45:12 <monochrom> OK yeah wth I don't need to play safe anymore.
14:45:15 <dminuoso> noidedsuper: The fundamental operation of "subtraction of naturals" is not closed, even if I encode naturals with Integer.
14:45:41 <monochrom> EVERYONE's "always" and "everyone" have scopes.
14:46:01 <noidedsuper> dminuoso, yeah that's true. My bad.
14:46:29 <noidedsuper> I guess what I was trying to say is "There's no need to formally encode closure in the Semigroup class because the type signature gets you as close as Haskell can get for free"
14:46:56 <dminuoso> Money for nothing and theorems for free!
14:47:01 <noidedsuper> :t (<>)
14:47:02 <lambdabot> Monoid m => m -> m -> m
14:47:20 <noidedsuper> ^ free "closure" 
14:47:21 <monochrom> Also my "type signatures" could be dependent types and predicate subtypes so for example (/) :: real -> { x :: real | x/=0 } -> real
14:47:55 <noidedsuper> I said in Haskell! Maybe someday we'll be able to write that but DependentHaskell isn't done yet :D
14:48:06 <dminuoso> noidedsuper: I'd say it *is* formally encoded by means of the type system.
14:48:16 <monochrom> noidedsuper: Yes this is also why I prefer type sigs to closure statements.
14:48:52 <monochrom> And type sigs can do more, really shines when you do "multi-sorted algebras".  (Their "sort" just means our "type".)
14:49:38 <noidedsuper> I guess you could consider it formally encoded. So then my statement is "there's a reason that the haddock for the Semigroup class doesn't actually mention closure explicitly as a law the instance should satisfy, as the type signature fundamentally expresses that already"
14:50:14 <noidedsuper> monochrom, type signatures are trivially more powerful than statements of closure (as you can encode any statement of closure with a type signature rather trivially), but I don't think closure statements are useful
14:50:29 <dminuoso> noidedsuper: Well it requires you to be able to define your set as a type.
14:50:32 <noidedsuper> The concept of closure is certainly easier to teach to students just starting to do abstract math, for example
14:50:54 <hpc> i can see types being easier, honestly
14:51:04 <hpc> because they're already taught way earlier
14:51:05 <dstolfa> noidedsuper: again, unless you want to discard the entirety of HoTT as broken and useless, you need closure to even construct the first building block of it, a type itself
14:51:15 <hpc> 1.5 is a real number and a rational number but not an integer
14:51:34 <dminuoso> dstolfa: Im curious. How is that?
14:51:41 <noidedsuper> I don't even know what HoTT is ;-;
14:51:47 <Solonarv> homotopy type theory
14:51:57 <Solonarv> (I don't understand it, I just know the acronym)
14:52:07 <hpc> it's the HoTT new thing in abstract math
14:52:17 <mniip> 11/18/2018 [01:37:37] 18<dstolfa18> keeping it very simple, subtraction with naturals is not closed
14:52:21 <mniip> wtf is "subtraction"
14:52:28 <mniip> I only know monus :P
14:52:33 <hpc> mniip: co-addition
14:52:52 <mniip> hpc, well monus is a retraction of addition :P
14:53:34 <dstolfa> dminuoso: because in order to construct HoTT as an internal language of an (infinity, 1)-topos, you start by interpreting a type as an infinity groupoid, and for that you need closure
14:53:42 <hpc> in prolog addition and subtraction would be the same function :D
14:53:58 <mniip> excuse me did you just say function
14:54:11 <monochrom> Isabell/HOL uses types and type classes.  This is like Haskell classes plus you get to write down machine-readable laws.  Your instance code also has to include machine-readable proofs.
14:54:19 <hpc> oh yes, the same functor
14:54:29 <hpc> (scroll back a few days to get /that/ joke)
14:54:38 <dmwit> hpc: Types are hard. I recently learned (and have probably already bored people in here with this... sorry) that nobody even knows if e+pi is rational.
14:54:39 <mniip> I have it vaguely in my memory
14:54:40 <monochrom> A monoid class in Isabell/HOL is like Monoid in Haskell plus laws and proofs.
14:54:51 <mniip> that's how I knew what you're talking about
14:55:07 <dstolfa> dmwit: types are very hard indeed
14:55:14 <dstolfa> dmwit: i couldn't agree more with that statement :-)
14:55:21 <hpc> dmwit: that just means values are hard
14:55:30 <ManuRahim> I need rss2irc
14:55:44 <ManuRahim> I tried rss2irc http://postbin.in/syndication.php?fid=&type=rss2.0&limit=15 irc.spotchat.org/#postbin.in/mybot
14:55:55 <ManuRahim> not working
14:55:55 <dmwit> (I guess we're probably pretty confident that e+pi is not an integer, though, so we got that going for us, which is nice.)
14:56:12 <mniip> dmwit, hmm, but is it a real number
14:56:28 <mniip> I've seen various kind of magic done with e to make fake numbers
14:56:53 <monochrom> Type is not hard.  Dependent type is.
14:57:00 <dmwit> I'm told that real numbers are closed under addition.
14:57:04 <mniip> monochrom, that or quotient types
14:57:04 <ManuRahim> I think rss2irc belongs to haskell
14:57:15 <monochrom> Yeah.
14:57:38 <monochrom> Decidable type systems are not hard.  Undecidable type systems are.  There, I think I nailed it.
14:57:51 <mniip> decidable type systems are also extremely dull
14:58:03 <mniip> I mean, cmon, arithmetic?
14:58:05 <monochrom> But meh, of course undecidable problems are hard.  Computers can't do them, so nevermind humanity.
14:59:02 <mniip> dmwit, also I heard addition of reals is undecidable so we can never know what e+pi is
14:59:38 <dstolfa> mniip: STLC is pretty trivial and System F is easy -- but things can be a bit tricky if you do CPS and want to add quasi-negation, things get a bit insanely recursive when we add pi and sigma types though
14:59:53 * mniip . o O ( are there e types )
15:00:24 <rain1> e is a type
15:00:48 <mniip> dstolfa, STLC is turing incomplete
15:00:50 <hpc> mniip: we just haven't made a sufficiently interesting decidable type system
15:00:51 <mniip> system f is undecidable
15:01:07 <rain1> what do you mean by "system f is undecidable"?
15:01:10 <mniip> hpc, again, arithmetic?
15:01:38 <noidedsuper> rain1, https://www.sciencedirect.com/science/article/pii/S0168007298000475
15:01:45 <mniip> hm, I guess I was looking at the comment that inference in system F is undecidable
15:01:53 <dstolfa> mniip: right, but what's your point there?
15:02:19 <dstolfa> mniip: they're still quite easy to deal with, whereas adding dependent types makes things horribly recursive
15:02:21 <mniip> I'm pretty sure inhabitantness checking in system f is undecidable as well
15:02:27 <mniip> because it can encode arithmetic
15:02:39 <dstolfa> mniip: i think you want to use the word "incomplete" here
15:02:45 <dstolfa> undecidable is a bit ambigous here, at least to me
15:03:26 <mniip> the language of inhabited types is not computable
15:03:31 <mniip> the language of inhabited types is not decidable*
15:03:36 <rain1> oh 
15:03:43 <rain1> there are 2 different ways to do System F
15:03:45 <noidedsuper> Both typeability and typechecking are undecideable in system f. System F is also incomplete because it disallows unrestricted recursion.
15:04:09 <rain1> church style, where lambdas are annotated with types - type checking is decidable for this one
15:04:12 <monochrom> If you like undecidable systems, there is still a line between enumerable (semi-decidable) and not-even-enumerable.
15:04:30 <monochrom> And furthermore the whole arithmetic hierarchy.
15:04:31 <rain1> but typechecking curry style, where you don't have those extra annotations is undecidable
15:04:43 <spion> Does Haskell have some mechanism to discover why a typeclass instance cannot be derived for a certain type?
15:04:51 <monochrom> No.
15:05:04 <mniip> yes, you join #haskell and post your code
15:05:07 <monochrom> Oh, apart from downright "not even the right kind".
15:05:27 <noidedsuper> mniip, when are they gonna add that to the compiler? :P
15:05:31 <mniip> usually it's extremely obvious
15:05:43 <mniip> (to an experienced user)
15:05:45 <noidedsuper> extremely obvious to people who are fairly experienced 
15:05:47 <spion> due to instances that are implemented conditionally i.e. `instance X T1 => Y T1`... its sometimes difficult to figure out why the final type doesn't implement the typeclass
15:06:15 <glguy> It'll probably be more useful to talk about the specific issue you're having
15:06:26 <mniip> spion, you can poke StandaloneDeriving at it in that case
15:06:52 <monochrom> Yeah, don't generalize your question.
15:07:45 * mniip . o O ( Does Haskell have some mechanism to solve problems )
15:08:08 <spion> monochrom, my specific issue is with PureScript / validation unfortunately. I should probably ask in #purescript - was just curious if Haskell had some mechanism to search through those conditional implementations :)
15:08:31 <monochrom> mniip: Not sure you already heard this from me: In math channels student do ask "how do I solve equations?" and they only meant "my homework is 2x+5=7"
15:08:57 <mniip> I frequent one of the math channels
15:09:01 <mniip> haven't seen that one yet
15:09:15 <monochrom> The cases I saw were a long time ago.
15:09:18 <ManuRahim> how to post rss to irc channel?
15:09:25 <hpc> they need to learn how to ask questions
15:09:26 <spion> monochrom, i.e. my validation-error wasn't deriving Alternative because my error type was missing Semiring: https://github.com/purescript/purescript-validation/blob/v4.0.0/src/Data/Validation/Semiring.purs#L90
15:09:34 <hpc> "math can't solve 2x+5=7"
15:09:40 <monochrom> Back then reddit and *-overflow and stuff didn't exist.
15:10:00 <mniip> monochrom, and you pointed them at algebraic geometry right?
15:10:00 <MarcelineVQ> hpc: "the answer to 2x+5=7 is 4"
15:10:16 <monochrom> No! I pointed them at undecidability!
15:10:34 <mniip> damn right
15:10:45 <mniip> we can't solve 2x+5=7 because it's an undecidable problem
15:11:07 <hpc> 2x+5=7 is closed under confluence
15:11:38 <mniip> 2x+5=7 is closed under toLower
15:12:21 <spion> was hoping for errors like No type class instance was found for Control.Alt.Alt (V (Array String)) - The only instance for Control.Alt.Alt (V (Array String)) is conditional, and no type class instance was found for Semiring (Array String)
15:12:35 <Welkin> > map toLower "2x+5=7"
15:12:37 <lambdabot>  "2x+5=7"
15:12:39 <Welkin> :D
15:12:50 <Welkin> > map ord "2x+5=7"
15:12:52 <lambdabot>  [50,120,43,53,61,55]
15:12:53 <monochrom> I think you ought to say "invariant under toLower" instead.
15:12:54 <glguy> spion: As long as you haven't slipped up and used FlexibleInstances in Haskell, that's not an issue
15:12:55 <[exa]> hm math, let's curry-howard it to dependent types and wait for haskell to get support
15:13:28 <monochrom> Hey! Does Purescript have Control.Alt.Del? >:)
15:14:29 <noidedsuper> I'm trying to think what you could put in a module with that joke name
15:14:41 <glguy> changePassword
15:14:45 <noidedsuper> I guess a typeclass for alternatives with some notion of either "deltas" or "deleting" 
15:14:52 <mniip> I could understand enabling 50 haskell extensions but FlexibleInstances? that's too much of a slip up
15:14:53 <noidedsuper> Neither of which make much sense 
15:14:53 <Welkin> here is the only equation solver you will ever need `head . sum . map ord`
15:14:54 <mniip> :P
15:15:12 <glguy> noidedsuper: Also: taskManager
15:15:19 <Welkin> > head . sum . map ord $ "2x+5=7"
15:15:21 <lambdabot>  error:
15:15:21 <lambdabot>      • Couldn't match type ‘Int’ with ‘[c]’
15:15:21 <lambdabot>        Expected type: [Char] -> [c]
15:15:22 <mniip> Welkin, type error
15:15:50 <noidedsuper> That's expected behavior, you're trying to make a programming language solve algebra. Of course you get a type mismatch.
15:15:51 * glguy was wondering what that lambdabot message meant
15:16:08 <noidedsuper> (I swear to god if somebody links a TypeFamilies equation solver)
15:16:18 <Welkin> > sum . map ord $ "2x+5=7"
15:16:19 <Welkin> oops
15:16:20 <lambdabot>  382
15:17:02 <glguy> > length (filter ('x'==) "2x+5=7")
15:17:04 <lambdabot>  1
15:17:06 <Welkin> I wanted it to throw an exception on an empty list though
15:17:09 <Welkin> would be more fun
15:17:32 <monochrom> sum . map ord . tail
15:23:27 --- mode: glguy set +v fen
15:23:35 <fen> hi, how is this vector slower than list? https://bpaste.net/show/c7debc282ff9
15:27:53 <fen> hmm, might be the maybes from safe access.. 1 sec
15:27:55 <Welkin> fen: every time you update a vector the entire data structure gets copied to a new memory location
15:28:17 <Welkin> to update a vector in-place (without copying) you need to use ST
15:28:25 <fen> so it should use mutable state?
15:28:33 <Welkin> it's safe mutation
15:29:05 <fen> this? http://hackage.haskell.org/package/vector-0.12.0.1/docs/Data-Vector-Mutable.html
15:29:06 <Welkin> however this is pointless depending on how you use it
15:29:18 <fen> well it repeatedly applies the blur
15:29:24 <fen> so it might be worth using
15:29:26 <Welkin> because you need to copy the vector first (unfreeze it) then copy it again (freeze it)
15:29:39 <Welkin> if you are doing one operation on the whole vector all at once, then it will benefit you
15:29:48 <lyxia> well it's using imap
15:29:50 <fen> it uses imap
15:29:52 <Welkin> unfreeze, do the huge operation, freeze
15:29:56 <Welkin> that means 2 copies
15:30:13 <fen> but it uses imap very many times with the same function
15:30:32 <fen> why would it need to be frozen each time just to be unfrozen again?
15:30:56 <Solonarv> > toGoedel . map (fromIntegral . ord) $ "2x+5=7"
15:30:58 <lambdabot>  8786073424148855868791358419376215209314683017818287213664988663667799915234...
15:31:04 <Welkin> :t toGoedel
15:31:04 <Solonarv> there, solved!
15:31:05 <lambdabot> (Integral b, Integral a) => [b] -> a
15:31:08 <fen> oh, you mean do everything...
15:31:13 <fen> ok, lets try that
15:31:25 <fen> might just see if its faster without Maybe first
15:31:30 <Solonarv> toGoedel xs = product $ zipWith (^) primes xs
15:32:05 <lyxia> the problem is you're producing thunks, and the thunks in the vector are slower than those in the lists
15:33:05 <Welkin> for these kinds of operations you may as well convert to a list, transform it, then convert to a vector
15:33:14 <Welkin> then you can pattern match instead of using indices
15:33:59 <fen> what kind of problem?
15:34:35 <fen> are; ""these" kinds of operations"
15:34:42 <fen> ?
15:35:22 <Welkin> oh I see
15:35:26 <fen> lyxia: what about these thunks? is that what the ST mutability is supposed to solve?
15:35:31 <Welkin> imap already does it all at once (in-place)
15:35:40 <Welkin> so using imap means you are already only doing 2 copies
15:35:47 <zachk> would regular Control.Concurrent.Chan work fine with Process from cloud-haskell/distributed-process?
15:36:06 <zachk> i need it because a library i am using provides a chan to output events
15:36:07 <lyxia> there's no way converting to lists to apply this blur function is going to be faster than allocating a vector and putting elements in it like map does, if the program's optimized properly
15:36:32 <fen> what optimisations might be done?
15:36:40 <fen> ST?
15:36:59 <lyxia> ((a + x) / 4) + (b / 2) : blur' b x xs   <- the first element is going to be a thunk
15:37:02 <Welkin> zachk: I'm sure you can convert from Chan to Process if they both use MVar (or TVar)
15:37:13 <fen> not really understanding how freeze is supposed to help
15:37:26 <lyxia> f (xs !? (i-1)) x (xs !? (i+1))   <- that's another thunk, and it's much more costly to evaluate
15:37:51 <zachk> I need to read from the Chan in the process Monad, it compiles, but doesnt seem to be working
15:38:04 <fen> its almost twice as fast as checking the index though
15:38:11 <fen> like, to test if its the start or the end
15:38:42 <fen> maybe ! vs !? isnt really going to help much...
15:39:09 <lyxia> fen: what are you talking about? this second expression does bounds checking and whatever the first one I quoted does
15:39:23 <lyxia> therefore it will be slower to evaluate
15:39:31 <spion> in case anyone cares - turns out, I imported the wrong validation module - the semigroup one. when importing the semiring one I get a not-a-Semiring error :)
15:39:41 <fen> was trying another implementation more like;
15:39:42 <fen>   g 0 x = x + (xs ! 1) / 2
15:39:51 <fen>   g i x = (((xs ! (i-1)) + (xs ! (i+1))) / 4) + (x / 2)
15:40:04 <Welkin> zachk: looking at the source for Process, it's really complicated
15:40:05 <Welkin> haha
15:40:14 <Welkin> of course it uses MVars at some point
15:40:28 <zachk> ived used Process just fine TVars
15:40:29 <Welkin> can't you pipe from the Chan into the Process's queue?
15:40:45 <Welkin> using readChan
15:41:21 <fen> lyxia: is there any way to get the vector version as fast as the list version, or any way to make the list version faster?
15:41:37 <zachk> I am using this http://hackage.haskell.org/package/fsnotify-0.3.0.1/docs/System-FSNotify.html and am trying to send a cloud haskell message to another program to notify it when a file has been changed so it can shutdown
15:42:07 <lyxia> fen: you can make the list version faster by forcing the thunk before returning the cons:   let !h = ((a + x) / 4) + (b / 2) in h : blur' b x xs
15:42:28 <lyxia> for vector I'm still looking for a good solution...
15:42:29 <Welkin> zachk: did you try what I mentioned above?
15:42:52 <fen> should it try to store the previous values to save accessing them again?
15:44:22 <fen> and what about the initial cons in the list version? should that be strict aswell? (guessing that stops it being a thunk, and forcing its evaluation is somehow faster)
15:44:32 <zachk> no, i am moving more of the code into a regular IO monad 
15:44:37 <Welkin> zachk: `send threadId =<< readChan chan`
15:44:46 <fen> or is it not as important because its just at the start and only happens once 
15:44:49 <lyxia> fen: yes that should be forced too
15:44:58 <lyxia> fen: and yes it's not as important
15:44:59 <Welkin> with a liftIO
15:45:01 <zachk> that needs to still be in the Process monad cause send has type process
15:45:12 <Welkin> zachk: `send threadId =<< liftIO (readChan chan)`
15:45:49 <lyxia> fen: for vector you need something like a strict imap but there isn't one, so you have to implement it by hand using mutable vectors
15:45:54 <lyxia> I'm trying to do that
15:46:05 <fen> not sure how to share the values with imap... 
15:46:22 <fen> like, to save retriving them using ! 3 times
15:47:07 <fen> maybe thats whats slowing it down
15:47:31 <fen> would the strictness solve that?
15:48:04 <fen> maybe a similar recursive thing to the list version would work, will try
15:48:05 <lyxia> no it would not
15:48:35 <fen> (like, accumulating an int index and passing the previously obtained values)
15:48:47 <lyxia> try using an unboxed vector instead
15:48:53 <fen> oh, ok
15:49:40 <lyxia> even if you don't do anything about those duplicate lookups, the speed up might still be significant
15:51:40 <fen> is the idea that unboxing would keep the recently accessed values in haskells magic recent memory?
15:51:50 <fen> or is that just sillyness/
15:52:29 <hpc> a "box" in ghc is its implementation of a thunk and such
15:52:33 <Welkin> fen: what is your obsession with the performance of theorectical problems?
15:52:41 <hpc> whatever makes an Int more special than just however many bits in ram
15:52:47 <Welkin> theoretical*
15:53:00 <hpc> so unboxing takes that away, so an unboxed vector of ints is going to be more like C's int[]
15:53:01 <Welkin> are you building something?
15:53:10 <fen> oh wow thats way faster!
15:53:11 <hpc> to describe it loosely
15:53:14 <fen> almost 500x !
15:53:16 <fen> https://bpaste.net/show/20c2e439116c
15:53:20 <lyxia> fen: no it's that unboxed vectors cannot hold thunks so you can hope that GHC doesn't generate thunks
15:53:34 <hpc> #ghc is a good place to ask about the hairy details of boxing
15:53:48 <fen> 50x ...
15:53:52 <fen> erp
15:53:58 <fen> still!
15:54:01 <fen> pretty good
15:54:08 <fen> now the problem is that the list version is slower
15:54:45 <noidedsuper> :t Just . id 
15:54:46 <lambdabot> a -> Maybe a
15:54:47 <lyxia> I'm not sure how that's a "problem"
15:54:58 <fen> never mind that
15:54:59 <fen> it is
15:55:09 <noidedsuper> I just checked the type of composition with identity 
15:55:20 <noidedsuper> maybe I should never program Haskell again tbh 
15:55:20 <Welkin> a list is not often used as a data structure in haskell (or functional programming for that matter)
15:55:25 <Welkin> it is more often a control structure
15:55:30 <fen> (basically, just want to be able to use nice recursion schemes instead of nasty indexes)
15:55:56 <Welkin> > const id -- noidedsuper 
15:55:59 <lambdabot>  error:
15:55:59 <lambdabot>      • No instance for (Typeable b0)
15:55:59 <lambdabot>          arising from a use of ‘show_M8433427683887261711115’
15:56:00 <Welkin> :t const id -- noidedsuper 
15:56:01 <lambdabot> b -> a -> a
15:56:04 <ski> noidedsuper ?
15:56:17 <Welkin> id can be useful
15:56:23 <Welkin> if you like to code golf
15:56:29 <fen> Welkin: how is that useful? a list as a "control structure" ?
15:56:37 <noidedsuper> f . id has the same type as f for all f 
15:56:41 <Welkin> fen: think of it as an iterator
15:56:43 <Solonarv> I feel like we wend over this yesterday
15:56:51 <noidedsuper> I forgot about that
15:56:58 <fen> yeah, but that was something that never got solved
15:57:00 <noidedsuper> The "stop programming Haskell forever" was a joke
15:57:10 <fen> anyway, to avoid repeating that
15:57:23 <fen> how can this list be made to be as fast as an unboxed vector?
15:57:37 <Welkin> your question itself makes no sense
15:57:45 <fen> the paste is above
15:57:57 <fen> to accompany the question 
15:58:01 <Welkin> how can a slow data structure be made magically faster than one without pointers?
15:58:02 <ski> noidedsuper : `f . id = f' is like `n * 1 = n', `n + 0 = n' or `xs ++ [] = xs'
15:58:30 <fen> i feel like your asking me that question
15:58:46 <fen> but, thats the question right?
15:58:51 <fen> why is haskell being slow!?
15:58:56 <fen> must be my fault!
15:58:59 <Welkin> haskell is not slow
15:59:03 <Welkin> compared to what?
15:59:23 <fen> there is a pretty concise paste showing exactly the comparison in question
15:59:27 <Welkin> is this a theoretical question or do you have a real world use case?
15:59:31 <ski> noidedsuper : in `(.) :: (b -> c) -> (a -> b) -> (a -> c)', if you feed the right argument as `id', that means `a' and `b' will be the same type. so the left argument type, `b -> c' will then be the same as the result type, `a -> c'
15:59:41 <fen> thats irrelevant 
16:00:26 <Welkin> lists will always be slower than an unboxed array for certain operations because of the fundamental differences between them
16:00:32 <Welkin> you can't change that
16:01:07 <fen> can you describe what you mean? by saying specifically which situations your referring to?
16:01:21 <fen> *for certain operations*
16:02:15 <noidedsuper> fen, Isn't the real question here "how can I make the given operation in code faster"
16:02:18 <noidedsuper> For the stuff in your paste 
16:02:23 <buhman> any vector operation?
16:02:33 <tauoverpi[m]> <freenode_fen "can you describe what you mean? "> Any operation where you need to access the next element as lists are O(n)
16:02:42 <fen> noidedsuper: yes, thats exactly the question
16:03:13 <Welkin> lists are the wrong data structure for this use case
16:03:15 <Welkin> stick with a vector
16:03:37 <Solonarv> generally speaking, lists will only beat arrays when you're only touching the front of the list *and* you don't need to have the entire list in memory at once (i.e. can use it in a streaming fashion)
16:03:44 <fen> the example does not apparently fall into Welkins unspecified situations where list would be faster...
16:03:55 <fen> it was an attempt at a counterexample
16:04:04 <fen> to "vectors are best"
16:04:11 <Welkin> no one ever said that
16:04:20 <fen> and thats the real motivation for trying to get the list version fast
16:04:32 <Welkin> every data structure is different, and has strengths and weaknesses
16:04:38 <fen> Welkin: whats your obsession with things no one said!?
16:04:44 <Welkin> have you learned about data structures before?
16:04:53 <fen> why?
16:05:10 <Welkin> if you haven't, learning some basics will answer all of your questions
16:05:18 <fen> oh right
16:05:35 <noidedsuper> I think the confusion here is that your code is running slower with a Vector than with a list and you're confused as to why that's happening
16:05:51 <noidedsuper> and the conversation somehow got into the "list vs vector performance" thing
16:05:54 <Welkin> noidedsuper: that was already solved using unboxed vectors
16:06:13 <Welkin> noidedsuper: now they want to know how to make the list version as fast as the unboxed vector version
16:06:21 <noidedsuper> fen, after using unboxed vectors, was your code made faster? 
16:06:36 <noidedsuper> If so, what is the reason you'd want a list instead? It seems like your problem should be solved.
16:07:15 <fen> noidedsuper: no, it started as list vs vector performance, this benchmark was initially exciting because of a slow vector implementation being slower than list. but with lyxias strictness optimisations, its now faster (as apparently it always will be) and there is dismay at the futility of trying to use list#
16:07:46 <fen> noidedsuper: https://bpaste.net/show/20c2e439116c
16:07:52 <fen> yes, much faster
16:08:15 <noidedsuper> Lists in Haskell are built to have nice properties when used as a *functional* list. IE, they are good for the use case where you consider one element at a time, and gradually transform it.
16:08:27 <noidedsuper> lists are quite fast for a sequence of maps and filters with a fold at the end
16:08:36 <fen> and the reason is that recursive functions for list deconstruction have been developed
16:08:42 <fen> but are not useful in practive
16:08:58 <Rembane> It depends on the algorithm... 
16:09:04 <noidedsuper> They are useful in practice, depending on what you want to do with them.
16:09:15 <fen> since keeping track of a representation used for directing memory access will apparently always be faster than shifting nested zippers around 
16:09:52 <fen> this "central difference scheme" example is typical for the use of zippers
16:10:14 <fen> though its expanded into a more direct form for this example for brevity
16:10:28 <noidedsuper> Well, yes, using direct memory access is always going to be faster. But it can be a lot harder to compose.
16:10:39 <fen> this is the essence of the problem
16:11:02 <fen> the more composable readable and elegant code is being slow
16:11:09 <fen> and thats the real annoying thing
16:11:23 <noidedsuper> That's sort of a fundamental truth of programming in most cases, isn't it? 
16:11:28 <fen> its quite concerning for the whole business 
16:11:34 <fen> no!
16:11:41 <Solonarv> that's why you write abstractions on top of the messy close-to-the-metal code
16:11:42 <fen> there is no reason lists should be being slow like this
16:11:53 <fen> apparently it scatters itself among memory
16:11:56 <fen> it shouldnt do that
16:11:59 <Solonarv> there is plenty of reason for *linked* lists to be slow
16:12:03 <fen> maybe there is something else at the heart of this
16:12:05 <noidedsuper> You can't be lazy without doing that
16:12:12 <fen> oh
16:12:16 <noidedsuper> So you'd lose the laziness abstraction, which is *extremely nice* to have
16:12:37 <noidedsuper> If you want to be lazy *and* pure you're kinda stuck with slow
16:12:43 <fen> Solonarv: why mention linked lists?
16:12:58 <noidedsuper> Because linked lists are basically the only way to implement a lazy, pure list
16:13:01 <noidedsuper> So that's what Haskell uses 
16:13:07 <Solonarv> because Haskell's  [] is a linked list
16:13:09 <fen> oh, sum types?
16:13:13 <noidedsuper> despite them being quite slow due to all the memory scattering they imply
16:13:14 <MarcelineVQ> pointer chasing is slower than pointer incrementing, iiuc
16:13:19 <Solonarv> I was just being more specific than "list"
16:13:31 <kark> linked lists don't imply willy-nilly allocation
16:13:34 <fen> oh, recursive typess
16:14:00 <noidedsuper> Linked lists don't imply that but in practice they're nearly-always implemented as "malloc a new element"
16:14:15 <Solonarv> yes, recursive types can't be implemented without some sort of pointers
16:14:20 <noidedsuper> Because if your use case is "I need to access data in this sequence really, really quickly, and mutate it for even better performance" 
16:14:34 <noidedsuper> why the hell would you use a linked list with some kind of nice "allocate everything together" algorithm rather than just using a vector 
16:14:34 <Solonarv> otherwise how would you know how much memory a cons-cell takes?
16:14:44 <fen> Solonarv: it was confusing because a binary tree with cycles can be used to implement a double linked list, which is something totally different to how you mean "linked"
16:14:47 <kark> you can always use a smarter allocator than malloc
16:15:13 <noidedsuper> You could. I bet there's some C++ close out there that uses std::list<> with some kind of pool allocator thingy in order to achieve some specific task 
16:15:28 <noidedsuper> But in practice code that needs to be fast should also have sequential memory access
16:15:40 <noidedsuper> And maybe even use SIMD
16:15:43 <fen> is any of this going to end up with a faster way to do recursive datatypes/
16:15:45 <fen> ?
16:16:32 <fen> the use case can be restricted to total update of all values at each update
16:16:36 <fen> like fmap
16:16:39 <kark> SIMD of course needs packed data, but if your entire list fits in the cache then how bad can the performance be
16:17:33 <noidedsuper> If your use case is "total update of all values at each update," then using functional data structures is not going to be very fast on modern systems. You're much better served with using a vector, as you can see with your benchmarks.
16:17:34 <fen> why cant the list just place all the memory nicely?
16:18:12 <noidedsuper> I answered that already.
16:18:19 <fen> noidedsuper: the total update can have the option of using pathologically intense local stencils
16:18:41 <fen> and that this locality in the datatype should correspond to fast memory access
16:18:44 <fen> or something...
16:18:48 <pikajude> what's the best way to share a value across multiple threads, but also be able to be notified whenever it changes
16:18:52 <pikajude> like an MVar/Chan combination
16:19:20 <monochrom> I wonder if I should answer "observer pattern" :)
16:19:29 <noidedsuper> What do you mean by "share?" 
16:19:40 <monochrom> Maybe s/share/accessible/
16:19:50 <pikajude> right
16:19:53 <pikajude> what's the best way to accessible a value
16:20:02 <monochrom> and it's a mutable variable
16:20:08 <noidedsuper> You could fairly trivially write a datatype that's basically a (TChan, TVar) tuple sort of thingy
16:20:25 <Rembane> Copy it! 
16:20:28 <fen> maybe its time to write the benchmark with zipper
16:20:29 <pikajude> it's a config object, every thread needs to be a reader, and some also need to be a writer
16:20:41 <fen> and do some wider stencils
16:20:50 <fen> cant figure out why that shouldnt work...
16:21:16 <Welkin> pikajude: TMVar
16:21:18 <monochrom> When the config changes, is it OK to kill them threads so you get a fresh start? >:)
16:21:32 <pikajude> sure
16:21:37 <pikajude> it would be really annoying for the user, though
16:22:42 <pikajude> TMVar is just STM MVar isn't it
16:22:59 <monochrom> Yeah.
16:23:59 <pikajude> i guess i could just have a (TMVar, TChan) and atomically update both
16:24:22 <monochrom> So perhaps Chan and dupChan help you.  Or TChan and dupTChan if you want to go T.
16:24:59 <monochrom> dup(T)Chan has the same power as observer pattern's "register me as a subscriber".
16:25:38 <monochrom> except in message queue terms and no need for a global registry.
16:26:05 <fen> just incase, the reason the stencils might be misleadingly thought to be faster, is because of the lazy access over the zipper. as in, the forwards and previous ellements can have (take n) where a stencil is of width 2*n+1
16:26:32 <monochrom> If the message itself already contains the new data, you don't even need a separate TMVar or MVar or TVar.
16:26:37 <fen> is there maybe this lazyness can be better than having to reaccess the memory using indexes for unboxed vactors?
16:26:55 <monochrom> Also why TMVar anyway?  TVar suits better.
16:27:04 <pikajude> wasn't my suggestion
16:27:11 <fen> like, is making the stencil very much wider and using take, which is lazy, going to help at all?
16:27:57 <monochrom> MVar is a length-1 message queue.  If you don't intend a message queue (e.g., if you intend a shared mutable cell), MVar is actually error-prone.
16:28:51 <zachk> mmm fsnotify just seems broken when even called before my distributed-process code and not using channels :( it worked earlier today without all my imports for distributed-process :(
16:29:28 <Welkin> I thought the semantics of MVar were wanted because pikajude said it's a configuration object, so it will block when a thread is writing to it
16:30:07 <pikajude> it's a bot, so, web interface for updating config + text interface for updating config
16:30:48 <pikajude> strictly speaking i don't need a way to notify listeners that the config has changed
16:30:53 <pikajude> that would just be a nice-to-have for the UI
16:32:51 <fen> eh!? how is take O(1) for vectors!? 
16:32:52 <Welkin> zachk: what about https://linux.die.net/man/1/inotifywait
16:33:47 <lyxia> fen: because you can index into the same chunk of memory
16:34:12 <fen> whats the point in zippers!?!/!?!?
16:35:05 <pikajude> lol
16:35:50 <lyxia> you have the most puzzling of questions
16:35:52 <fen> oh yeah, so funny
16:36:01 <pikajude> is the IO action passed to modifyMVar_ run multiple times?
16:37:21 <Welkin> pikajude: why would it?
16:37:29 <Welkin> https://hackage.haskell.org/package/base-4.12.0.0/docs/src/Control.Concurrent.MVar.html#modifyMVar_
16:37:55 <pikajude> ohhh
16:38:00 <pikajude> so the *retry* part is takeMVar
16:38:29 <pikajude> although if the IO action itself throws an exception it could still leave things in an unfinished state
16:38:39 <fen> totally dismayed 
16:38:51 <Welkin> no, the IO action is wrapped in onException
16:39:01 <Welkin> what retry are you talking about?
16:39:13 <Welkin> do you mean modifyTMVar_?
16:39:21 <pikajude> no
16:39:32 <Welkin> there is no retry in modifyMVar_ that I can see
16:39:40 <pikajude> modifyMVar_ blocks until the MVar is readable, doesn't it
16:39:40 <Welkin> just a mask for async exceptions
16:39:43 <Welkin> yes
16:39:53 <pikajude> so the "retry" here is "keep trying to read the MVar until you can read it"
16:40:02 <pikajude> yeah i had this completely wrong
16:40:21 <Welkin> well it's smarter than that
16:40:40 <pikajude> sure
16:40:41 <Welkin> it gets notified when it unblocks by the runtime
16:40:42 <pikajude> but in layman's terms
16:40:56 <pikajude> i mean in the end there's just a poll
16:40:58 <Welkin> at least that is my understanding of how it works in STM 
16:41:02 <pikajude> yeah
16:41:16 <Welkin> not totally certain about normal MVar, but I assume it's the same
16:41:29 <pikajude> i think mvar is TMVar with all the operations wrapped in atomically
16:41:48 <Welkin> it's not
16:41:49 <pikajude> oh actually it's not
16:41:51 <pikajude> it's MVar#
16:41:54 <pikajude> clever...
16:41:56 <Welkin> did you read the parconc book?
16:42:03 <Welkin> it explains all of this is great detail
16:42:13 <Welkin> it's also the best haskell book I've read
16:42:15 <fen> whats the point in continuing if lazyness is slower!
16:42:19 <Welkin> (still reading)
16:42:56 * hackage diagrams-pgf 1.4.1 - PGF backend for diagrams drawing EDSL.  http://hackage.haskell.org/package/diagrams-pgf-1.4.1 (BrentYorgey)
16:43:00 <pikajude> i haven't read a book in about 3 years
16:43:07 <Welkin> https://web.archive.org/web/20161123130313/http://chimera.labs.oreilly.com:80/books/1230000000929/ch03.html
16:43:19 <fen> developing algorithms based on lazy recursion on recursive datatypes 
16:43:22 <Welkin> well, oreilly messed up so it's not online anymore, but it's in the web archive
16:43:33 <Welkin> and you can download a mirror using `wget -mk <url>`
16:43:51 <Welkin> I have the pdf from the humble bundle
16:44:11 <ski> @where PCPH
16:44:11 <lambdabot> "Parallel and Concurrent Programming in Haskell" by Simon Marlow in 2013 at <http://community.haskell.org/~simonmar/pcph/>,<http://chimera.labs.oreilly.com/books/1230000000929/>,<https://web.archive.
16:44:12 <lambdabot> org/web/20180117194842/http://chimera.labs.oreilly.com/books/1230000000929>
16:44:13 <fen> its the whole basis of this language!
16:44:16 <Welkin> oh
16:44:19 <Welkin> here it is https://www.oreilly.com/library/view/parallel-and-concurrent/9781449335939/ch01.html
16:44:35 <pikajude> neat!
16:44:41 <pikajude> i will never read this
16:44:51 <pikajude> but it'll be useful to have in my browser history
16:45:03 <ski> not computing something can be more efficient that computing it
16:45:13 <Welkin> just sit down one weekend and read the chapters on concurrency
16:45:19 <pikajude> this is the weekend
16:45:22 <Welkin> that's what I did
16:45:24 <fen> ski: just dont access that index of a vector
16:46:02 <fen> lazy algorithms for recursive datatypes are slow
16:46:22 <pikajude> i actually used Strict for the first time just now
16:46:24 <pikajude> it was cool!
16:46:48 <noidedsuper> If your only need is high performance than you should be writing C++ or maybe FORTRAN. Haskell has other goals, although it's still much faster than a lot of other languages (even traditionally "fast" ones like Java in some cases)
16:47:24 <fen> its not that. its the style.
16:47:25 <Solonarv> Rust is probably a good choice too
16:48:10 <fen> even just within haskell
16:48:39 <noidedsuper> Rust is a good choice but if "as fast as possible" is your only goal you're gonna be using a lot of unsafe blocks, because you're going to be doing a bunch of really gross stuff to save microseconds
16:49:29 <fen> no scientific computations are going to use slow code
16:50:31 <fen> haskell is still good for other reasons, readability etc, but its this choice between lazy recursion and vectors thats bumming me out
16:50:55 <kark> ain't no free lunch
16:51:19 <fen> ill tell that to the clients
16:51:31 <noidedsuper> typically you use lists until you realize they're too slow, then switch to vectors for those areas. Which you'll rarely have to do in practice.
16:52:12 <fen> all the lazy pointer zipper stuff is essentially worthless
16:53:01 <kark> that's only true if speed is the only measure of worth
16:53:07 <monochrom> Or switch to Set.
16:53:25 <monochrom> Or priority queue. Or...
16:53:29 <fen> unfortunately the market dictates that this is the most significant value 
16:54:10 <monochrom> I say this because I saw world-class competition programmers using a C array for priority queue and every takeMin is an exhaustive search.
16:54:39 <fen> Set is based on "Binary search trees of bounded balance", the pointers were supposed to be better than this for local stuff, like stencils
16:54:54 <kark> monochrom: that's not necessarily a terrible idea depending on how much stuff you're dealing with
16:54:58 <fen> "deriving performance boosts from structured access patterns"
16:55:27 <kark> blowing through sequential data is pretty much what a CPU is best at
16:55:35 <kark> and vectorization makes it even faster
16:56:28 <kark> of course you wouldn't use a linear search for massive amounts of data, but if you're not, then...
16:56:50 <fen> all thats fast random access concerns
16:56:54 <fen> these are global updates
16:57:10 <fen> the closeness of surrounding data was supposed to help
16:57:45 <fen> save using fastish random access when a pointer/zipper could retain the position and lazily access its surroundings
16:59:22 <fen> but thats ruined if haskell cant even get the next item in a list as quickly as it could look it up from a memory address
16:59:37 <kark> that's up to the compiler
16:59:43 <fen> which is failing
16:59:55 <kark> doesn't mean it can't get better :)
16:59:58 <fen> hence the wailings of "why are lists so slow!?"
17:00:29 <fen> apparently, recursive types, as "linked lists" are just crap
17:00:45 <fen> and there is apparently no way round this
17:03:27 <kark> again, it depends on what you're doing. i don't think anybody in their right mind would use linked lists to do number crunching. but how can you write something like a compiler without recursive data types?
17:04:25 <fen> ok, well there is one main consideration here, using nested zippers for ComonadStore aka Pointer, can only work for hypercubic grids
17:04:39 <fen> Cartesian
17:04:48 <fen> and so the index is [Int]
17:05:11 <fen> so its faster to use Vectors
17:05:30 <fen> and just do algebra on the index to get the surrounding indexes
17:05:46 <fen> as that sum is faster than repositioning the pointer
17:06:05 <fen> :t seek
17:06:06 <lambdabot> error: Variable not in scope: seek
17:06:11 <fen> % :t seek
17:06:11 <yahb> fen: ComonadStore s w => s -> w a -> w a
17:06:23 <fen> here s is the [Int]
17:06:35 <fen> where the length of the list is the dimensionality of the grid
17:08:38 <fen> so, given the regularity of this data and the structured access patterns, why cant the compiler just store this nicely in memory and make it as fast as Vectors!?
17:09:12 <fen> can giving type annotations for the extent of the grid help?
17:09:43 <fen> help the compiler not be slow at this seek opperation which is basically just pattern matching on cons
17:09:53 <Solonarv> it could, if the compiler looked at them
17:10:11 <fen> how to get it to?
17:11:05 <Solonarv> write a (probably rather large) patch implementing that functionality in the compiler
17:11:08 <fen> it can be possible to use (:) fastly?
17:11:41 <fen> when type annotation can make it correspond to good memory access?
17:12:25 <Solonarv> there's still the issue that the compiler doesn't even know how many dimensions your space has, because you're indexing with [Int]
17:12:32 <fen> Solonarv: not going to be able to rewrite the compiler! can hardely understand the issue to begin with!
17:13:01 <fen> a lack of understanding which has led to years of writing slow haskell code which is no use to anybody!
17:13:46 <monochrom> When confronted with an [Int] of unknown length, some people say, "I know, I will use type-level list length".  Now they have two problems.
17:14:16 <Solonarv> if slow code were automatically of no use to anybody, languages like e.g. python or java wouldn't exist
17:14:21 <fen> its a pretty global variable, the dimensionality of the problem. like 3d graphics for instance
17:14:33 <fen> a newtype can wrap grids of common dimensionality
17:14:57 <Solonarv> Sure. So, use types that reflect that like e.g. V3 (from 'linear')
17:15:16 <Solonarv> instead of hoping for the compiler to magically guess what you mean
17:15:30 <fen> no, its a type annotated depth 3 free difference zipper
17:15:56 <MarcelineVQ> monochrom: ;_;
17:16:27 <fen> whose comonad instance is derived from its traversable based on a FIFO graph superclass
17:16:50 <fen> and corresponds to a one hole context which can allow its surrounding neighbours to be accessed lazily
17:17:08 <Solonarv> if you have a 'Free whatever', that thing's nesting depth is not known at compile time - that's the whole point of Free !
17:17:13 <fen> (and is also slow compared to the normal way of doing it, dont buy by software...)
17:17:45 <fen> its a type level nat Free
17:17:48 <monochrom> Next April 1st I should take it to its logical conclusion and create "an integer type that has its value statically known at the type level".
17:18:25 <fen> 'n ?
17:18:33 <monochrom> For example "x :: StaticallyKnownInteger 5; x = 5"
17:19:14 <Solonarv> that already exists, in 'singletons'
17:19:49 <monochrom> But is it an instance of Num?
17:20:09 <fen> https://bpaste.net/show/42e14720a6d2
17:21:00 <monochrom> And perhaps I should conclude in another direction and go "a list type that has its full content statically known:  xs :: StaticallyKnownList [1,2,3]; xs = [1,2,3]"
17:21:13 <monochrom> Like, why stop at length. >:)
17:22:13 <fen> are you saying that recursive datatypes can be useful as they can be reflected?
17:22:20 <monochrom> With type-level programming you can factorize a number into primes in O(1) run time.  (The factoring algorithm ran at compile time.)
17:22:23 <fen> or, levified?
17:22:40 <Solonarv> no, we're just joking around at this point
17:22:40 <monochrom> There is a C++ program that showed that.
17:23:02 <fen> ok, so, the algorithms are still good for type level programming?
17:23:12 <fen> well, not until laziness is at type level...
17:23:19 <Solonarv> At the type level you don't get much better than a linked list, anyway
17:23:25 <ski> monochrom : heh, reminds me of "Type Specialisation for the Lambda-Calculus; or, A New Paradigm for Partial Evaluation based on Type Inference" by John Hughes in 1996 at <http://www.cse.chalmers.se/~rjmh/Papers/typed-pe.html>
17:23:28 <Solonarv> I suppose you could implement a type-level Seq, maybe
17:24:02 <fen> "might compile faster soon"
17:24:38 <monochrom> I thought of type-level binary trees. Actually I believe that you will need them when you try to prove binary search tree algorithms correct using types.
17:25:09 <monochrom> (Type-level merely tree size will not suffice, for example.)
17:25:43 <monochrom> (You will want to know subtree sizes too, subsubtree sizes, ... recursively.  Now you have a tree.)
17:42:15 <monochrom> ski: That page is very orange.
17:42:36 <monochrom> <body bgcolor=#FF6600>  Oh God
17:50:20 <ski> (hm, 'sbeen a long time since i saw color on that site)
17:55:09 <jle`> well now that the idea is in my head, i have to make a non-empty containers library now
17:56:03 <jle`> oh hey someone already made it, but its api is barebones
17:57:09 <jle`> i hope they will accept a huge PR
17:57:31 <monochrom> haha
17:57:58 <monochrom> "commit 78be023... : Actual implementation"
17:57:59 <stylewarning> What is an example of a relatively standard mutually recursive ADT?
17:58:30 <jle`> stylewarning: do you mean standard as in, commonly used?
17:58:34 <stylewarning> jle`: yes
17:58:44 <stylewarning> (not Haskell 98 standard, or w/e, necessarily)
17:58:56 <monochrom> ListT
17:59:22 <jle`> i suppose Tree from containers 'kinda' counts, except the other type is not an ADT but a type synonym
17:59:36 <jle`> but it could have been an ADT.
18:00:47 <monochrom> AST for a programming language in which a declaration can contain expressions, and an expression can contain declarations.
18:00:48 <stylewarning> monochrom: That's a good example; I was hoping for something that wasn't making use of higher-kinded types (is that the right term?)
18:01:01 <ski> higher-order types
18:01:06 <stylewarning> oops, right
18:01:25 <monochrom> Or commands can contain expressions, and expressions can contain commands.
18:01:41 * ski smiles at "commands"
18:02:53 <nitrix> MarcelineVQ: ping
18:03:25 <MarcelineVQ> active sonar!?
18:03:37 <nitrix> MarcelineVQ: Would you happen to play Amumu?
18:03:42 <stylewarning> I wonder if something like a sequence of chess moves is a good example, where you have separate White/Black types that are distinguished at the type-level as opposed to value-level.
18:03:51 <MarcelineVQ> OwOmu? never heard of it
18:03:57 <monochrom> Yes, you can do that too.
18:04:09 <MarcelineVQ> Oh it's lol champ?
18:04:18 <nitrix> MarcelineVQ: Okay, might just be a coincidence. Someone with the same username as you playing a character in some game, nevermind :)
18:04:31 <nitrix> What are the odds :o !
18:04:39 <nitrix> I made a Haskell joke and they were really puzzled :P
18:04:41 <MarcelineVQ> I'm sure it's entirely coincidental, I'd never be caught playing a game for little children!
18:05:19 <MarcelineVQ> <_< >_>    It is coincidence in this case though.
18:07:20 <MarcelineVQ> Importantly, I'd never play a mummy, somenoe playing a mummy is trying to trick you.
18:26:30 <lockestep> is there a Haskell roadmap for 2019?
18:30:32 <Shockk> is there a way to apply a list of monadic actions to an initial value, in sequence, getting the result of the final action?
18:30:45 --- mode: glguy set +v RubenM
18:30:50 <Shockk> i.e. pure a >>= f >>= g >>= h >>= i
18:31:22 <lockestep> should be pretty easy with fold I feel like
18:31:27 <Shockk> something like:  blah [f, g, h, i] a
18:31:54 <Solonarv> assuming the types match, you could do something like 'foldr (>>=) (pure a) [f, g, h, i]'
18:32:10 <Solonarv> :t foldr
18:32:11 <lambdabot> Foldable t => (a -> b -> b) -> b -> t a -> b
18:32:14 <lockestep> would foldr do the actions in reverse order?
18:33:06 <Shockk> hmm
18:33:16 <Solonarv> ah, might need some plumbing I suppose
18:33:33 <Shockk> would foldM be useful here or not?
18:33:40 <Solonarv> I don't think so, actually
18:33:45 <lockestep> :t foldM
18:33:47 <lambdabot> (Monad m, Foldable t) => (b -> a -> m b) -> b -> t a -> m b
18:34:14 <Solonarv> if foldr messes up the order, foldl' (=<<) should be right I think
18:35:31 <Shockk> hmm okay I'll try that, thanks
18:35:46 <lockestep> I'm tired but why not foldl' (>>=)?
18:35:57 <lockestep> am I missing something obvious?
18:36:02 <Solonarv> types don't match
18:36:03 <RubenM> Hi guys, I have a question
18:36:10 <RubenM> https://gist.github.com/JulienMar/6b8e45dcd0aff95e265ca1f7079df686
18:36:18 <RubenM> I have something like this
18:36:24 <Shockk> another thought just now; is it possible to do like.. fmap a [pure f, pure g] or something?
18:36:40 <RubenM> But functionOnList is never called, and I have no idea why not
18:37:01 <lockestep> shockk yes, depending on the type of a
18:37:16 <RubenM> Could anyone help me out?
18:38:38 --- mode: glguy set +v Phil25
18:39:04 <lockestep> how do you know it's never called RubenM?
18:39:04 <Phil25> How do you know it's not being called?
18:39:31 <geekosaur> RubenM, I see no use of its result. unless you have typoed "whole" where you intended "wholeList"?
18:40:41 <Solonarv> Shockk: you want 'foldr (>=>) pure [f, g, h, i] initialValue'
18:40:44 <MarcelineVQ> :t foldr (>=>)
18:40:45 <lambdabot> (Monad m, Foldable t) => (a -> m c) -> t (a -> m a) -> a -> m c
18:40:49 <MarcelineVQ> doh
18:41:25 <Solonarv> I don't think there's an "endokleisli" monoid pre-defined anywhere, so no foldMap here
18:42:33 <Shockk> Solonarv: oh, that'd do what I want?
18:42:37 <Solonarv> yup
18:42:48 <Solonarv> orders them correctly, too (i.e. from left to right in the list)
18:43:00 <RubenM> geekosaur indeed, I intended wholeList
18:43:00 <Shockk> ahh great, thanks 
18:43:01 <Solonarv> I even tried it in ghci!
18:50:38 <Shockk> hmm I'm having trouble with that code
18:50:44 <lockestep> I don't really get the same feeling of forward progress in Haskell that I do in rust
18:51:02 <Shockk> should foldr (>=>) pure [parse, analysis] contents   be the same as   parse contents >>= analysis  ?
18:51:54 <lockestep> because with rust it feels like there are big projects with effort from the community and I can see them getting done once a year
18:52:12 <lockestep> in Haskell this might be happening but if it is it's less well marketed
18:54:02 <MarcelineVQ> Shockk: parse contents >>= analysis >>= pure
18:55:38 <Shockk> MarcelineVQ: is that not equivalent to parse contents >>= analysis ?
18:56:39 <MarcelineVQ> afaik, but if you ever have something other than pure there it's good to have the whole unrolling in mind
18:56:49 <Shockk> ah right true
18:56:58 <Shockk> hmm but I'm having trouble with my code, in that case
18:57:09 <Shockk> previously I had the following line, which worked:
18:57:37 <Shockk> case parse contents >>= Semantics.analysis of
18:57:58 <Shockk> now I have the following which isn't working unfortunately:
18:58:08 <Shockk> case foldr (>=>) pure [parse, Semantics.analysis] contents of
18:58:56 <MarcelineVQ> isn't working isn't descriptive, textbin your errors at least and ideally the code the errors mention :>
19:00:24 <Shockk> hmm hang on
19:02:08 <Shockk> MarcelineVQ: here: https://gist.github.com/shockkolate/e25baea70c92f3cca8b403f9336d5ecb
19:02:32 <Shockk> I just provided the type sigs for parse and analysis, if that's okay
19:06:36 <MarcelineVQ> oh, ehe
19:06:39 <Shockk> oh wait, I just realized I'm being stuypid, and I can't put them in a list when they're different types
19:06:42 <Shockk> right?
19:06:43 <MarcelineVQ> [parse, Semantics.analysis] 
19:06:44 <MarcelineVQ> yeah
19:06:54 <Shockk> hmm lol
19:07:26 <Shockk> is there a way I can do what I'm wanting to do here? i.e. have my actions in a list or something similar?
19:08:15 <Shockk> maybe what I'm trying to do is an antipattern here
19:08:35 <Shockk> though I'm wondering if it can be done with an existentially quantified list or something
19:08:50 <MarcelineVQ> it might be something you want to come back to later, unless the goal of the exercise is to solve this one thing
19:09:11 <Shockk> hmm, I mean I don't need it right now, so that's true, I can leave it for now
19:09:45 <Shockk> just thinking to the future where I might want to be able to select which analysis passes to run, say from the cli args or something
19:13:54 <Shockk> actually I can do this more easily by just using `parse` in place of `pure`, as it's the only pass that will have a different type 
19:14:36 <Shockk> oops no, I would just use an initial value of (parse contents) actually
19:26:58 <lockestep> ghc compiles based on modules right?
19:27:06 <lockestep> if I change a function it recompiles the whole module?
19:34:36 <pgiarrusso> lockestep: yes
19:35:38 <pgiarrusso> (almost all compilers I know recompile a whole file if anything in it changes)
19:36:30 <pgiarrusso> if you change the type of an exported function, GHC also recompiles files depending on it
19:42:02 <kark> if only there were more incremental compilers
20:01:25 <lockestep> 10,99pgiarrusso99,99, rust works based on functions I believe 
20:10:31 <geekosaur> only if it's compiling every function to a temorary file or it's restricting you to oen function per source file
20:11:27 <geekosaur> also things like caching a checksum  per function and checking tose as it reads the source file
20:12:47 <lockestep> probably the second one
20:12:57 <geekosaur> requires both
20:13:49 <geekosaur> I know it's "in" to believe computers are made of magic, but it takes actual work and in this case extra files to do that
20:15:19 <ski> (or you could compile without necessarily reading files)
20:16:31 * ski . o O ( incremental/adaptive computation, attribute grammars )
20:17:53 <geekosaur> o, looks like it;s doing a fair amount of caching (somewhat badly at present, according to the ticket tracking it)
20:18:03 <geekosaur> so it can do that but your'e using 2-3x the disk space or more
20:18:24 <geekosaur> I'm sure the small-VM folks are pleased
20:20:17 <geekosaur> looks like they have quite a ways to go to actually achieve incremental compilation?
20:22:46 <crobbins> does stack automatically compile c sources? i have cc-options and c-sources specified in the package.yaml (which i can see are then present in the cabal file) but the c compiler doesn't kick in.
20:32:31 <crobbins> ok, i think the problem may have to do with being on a case-sensitive fs (osx) and having the same name for a c source and an hs source
20:34:15 * ski has a problem once with extracting a file named `Aux.hs' from an archive, on windows
20:34:46 <ski> (turned out that windows didn't like files named "Aux")
20:34:52 <crobbins> yes! been there before
20:34:53 <crobbins> sad times
20:35:00 <geekosaur> yep
20:35:25 <crobbins> so i was hoping to just hook into the c compiler stage and compile objective-c, but it seems the arguments from cc-options aren't getting passed correctly
20:35:37 <crobbins> probably another dumb thing
20:43:23 <lockestep> 8,99geekosaur99,99 lmao may be surprising but I'm not a complete idiot
20:43:35 <lockestep> and I see no reason it couldn't be done without extra files
20:43:48 <geekosaur> then you don't know how object files work.
20:44:05 <geekosaur> and how you don't get to arbitrarily redefine them bcause the file format is set by the kernel
20:44:33 <geekosaur> (o, there are ways, but now instead of extra files you get fat files)
20:44:40 <lockestep> yeah
20:45:10 <geekosaur> and the ticket already notes that people comlain abut disk space even witout incremental compile suppot
20:46:36 <lockestep> could you link the ticket you're talking about?
20:46:59 <geekosaur> https://github.com/rust-lang/rust/issues/47660
20:47:48 <lockestep> I don't really use rust so idk
20:51:02 <lockestep> I wonder what kind of speed improvements they got
20:56:29 <lockestep> I also wonder how much it would help Haskell
21:00:57 * ski idly wonders why lockestep speaks in color
21:01:16 <ski> (color making it impossible to read the actal message, i might add)
21:01:16 <lockestep> ski what do you mean?
21:01:26 <lockestep> it's probably my client
21:01:59 <lockestep> I'm using an app called revolution irc, I haven't changed any defaults to change my text color or anything
21:02:38 <ski> when you addressed pgiarrusso and geekosaur, those messages had green background as well as foreground (same color here) (apart from the nickname itself, which was a different foreground color. different colors for the two nicks, curiously enough)
21:17:39 <Shockk> quick question; right now I've got something like the following applicative-style code:   Op op <$> primaryAnalysis lhs <*> compoundAnalysis rhs
21:18:03 <Shockk> is there a way for me to do an additional thing with the result of `primaryAnalysis lhs`, but discard the result and still pass it through the <$> ?
21:18:41 <Shockk> for example I was initially thinking like:   Op op <$> typeCheck <* primaryAnalysis lhs .. etc
21:18:56 <ski> "do an additional thing with the result" sounds monadic
21:18:59 <ski> are you in a monad ?
21:19:06 <Shockk> I am yep
21:20:05 <ski> if `additionalThing :: T -> M T', for your monad `T', and returns the argument `T', you can use `additionalThing =<< primaryAnalysis lhs'
21:20:14 <freeman42x]NixOS> anyone run into this issue? https://github.com/haskell/haskell-ide-engine/issues/947
21:20:18 <ski> (i suspect you need brackets around that, haven't checked)
21:21:35 <ski> @type liftA2 (<*) pure
21:21:36 <lambdabot> Applicative f => (a -> f b) -> a -> f a
21:21:37 <Shockk> ski: hmm but, what if `additionalThing :: a -> m b` for exmaple, but if I want to discard that b value and return the same a value?
21:21:44 <Shockk> oh
21:21:53 <ski> not sure whether there's a nicer thing for that
21:22:45 <Shockk> that seems okay; though at that point I might just write it out more verbosely so it's easier to undersatnd
21:22:50 <ski> @type ((<$) <*>)  -- slightly simpler
21:22:52 <lambdabot> Functor f => (a -> f b) -> a -> f a
21:23:32 <ski> (`liftA2' vs. `(<*>)' here is on `(a ->)', of course)
21:26:36 <fresheyeball> hey out there
21:26:48 <fresheyeball> I have a weird one and just need a rubber duck
21:27:02 <fresheyeball> it's simple I run hpack and it doesn't "other-modules" all my files
21:27:12 <fresheyeball> for some reason certain modules are not showing up in the .cabal file
21:27:55 <fresheyeball> but then I get These modules are needed for compilation but not listed in your .cabal file's other-modules:
21:27:58 <fresheyeball> when I compile
21:39:23 <lockestep> 8,99ski99,99 does this text have a weird color
21:41:15 <lockestep> if so, I think it's a bug with my irc client + Android p
21:41:34 <ski> yes it did
21:42:15 <pavonia> Uh, green text on a green background
21:45:43 <luminous> for me it's a yellow text on a transparent background
21:46:04 <luminous> i'm on IRCCloud
22:47:01 <lockestep> luminous, ski, it happens when I copy and paste a name on my phone
22:47:10 <lockestep> thanks, I won't do that anymore
23:05:05 <dminuoso> noidedsuper: Homo
23:05:15 <dminuoso> Okay. So that went wront.
23:05:30 <dminuoso> Alt-tab-enter mishap. :<
