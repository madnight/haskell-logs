00:33:40 --- mode: glguy set +v Gz__
00:33:51 <Gz__> hello
00:34:27 <Ariakenom> Gz__: Hello world!
00:34:59 <Gz__> i am new here and want to learn haskell 
00:35:09 <Gz__> i am beginner
00:36:26 <ventonegro> Gz__: https://www.seas.upenn.edu/~cis194/spring13/lectures.html
00:38:01 <Gz__> <ventogen> thank you vvery much
00:38:19 <ventonegro> Gz__: No problem
00:39:22 <Gz__> <ventonegro> are you professional haskell programmer ?
00:44:41 <ventonegro> Unfortunately, no
01:29:25 --- mode: glguy set +v Miguel
01:29:43 <Miguel> Hi can someone help me with something?
01:30:00 <Guest66045> Hi can someone help me with something?
01:30:41 <[exa]> don't ask to ask, just ask
01:31:13 <Guest66045> How i convert a Matrix to a string?
01:31:53 <[exa]> using show?
01:32:17 <Guest66045> yes
01:35:46 <Guest66045> I have to convert a State that brings a Matriz, a list of players and a list of shots to a String ------ (State matrix listPlayers listShots) zip the state to a String
01:40:47 <ski> Guest66045 : go ahead ?
01:42:09 <Guest66045> I have to convert a State that brings a Matriz, a list of players and a list of shots to a String ------ (State matrix listPlayers listShots) zip the state to a String
01:42:46 <Guest66045> how i do that using show
01:44:54 <ski> Guest66045 : use `show' to convert those three parts, then combine them however you wish ?
01:48:19 <Guest66045> like this: show m ++" "++ show js ++" "++ show ds ?
01:50:12 <ski> Guest66045 : perhaps. depends on what you want to achieve
01:51:46 <Guest66045> after that i want to do the opposite use the string to have back my State using read 
01:55:07 <ski> Guest66045 : how about deriving `Show' on your state data type, then ?
01:55:14 <ski> (as well as `Read')
01:56:23 --- mode: glguy set +v rckd
01:57:57 <Guest66045> i already have deriving, show and read in my data types
01:59:28 <ski> then you don't have to define any new function to "convert a State ... to a String", just use `show' ?
02:03:14 <Guest66045> and reading the state from a string? Like : [[..]] [..] [..] it will work?
02:03:53 <Guest66045> ski++
02:03:59 <olligobber> that's what `read' is for
02:04:12 <ski> or `reads' or `readMaybe'
02:04:28 <olligobber> oh readMaybe is a thing? I should use that more often
02:05:12 <Guest66045> i will try it, thank you
02:21:13 <olligobber> ooh, I like this https://hackage.haskell.org/package/base-4.12.0.0/docs/Data-Void.html
02:26:53 <olligobber> @source absurd
02:26:53 <lambdabot> Unknown command, try @list
02:29:30 --- mode: glguy set +v woodson
02:29:30 <woodson> :t liftA2 id
02:29:32 <lambdabot> Applicative f => f (b -> c) -> f b -> f c
02:29:50 <woodson> can anyone explain why when the id function is passed as the first argumen of liftA2 the type signature changes to
02:29:55 <woodson> :t liftA2 id
02:29:56 <lambdabot> Applicative f => f (b -> c) -> f b -> f c
02:30:04 <woodson> :t liftA2
02:30:05 <lambdabot> Applicative f => (a -> b -> c) -> f a -> f b -> f c
02:30:47 <phadej> :t id
02:30:49 <lambdabot> a -> a
02:30:56 <woodson> I see that f a changed to the (b -> c)
02:31:23 <phadej> :t id :: (b -> c) -> (b -> c)
02:31:24 <lambdabot> (b -> c) -> b -> c
02:31:44 <phadej> a ~ b -> c; work out the details with pen and paper
02:32:38 <amx> huh, so a constraint < 1.0 will happily be fulfilled by 1 in cabal, that's a bit surprising
02:33:05 <phadej> yes, 1.0 and 1 are different versions
02:35:07 <[exa]> wow
02:35:33 <ski> olligobber : `absurd v = case v of {}'
02:35:48 <ski> (imho `absurd' should be called `void')
02:35:58 <woodson> :t (<*>) (fmap f x)
02:35:59 <lambdabot> error:
02:35:59 <lambdabot>     â€¢ Couldn't match expected type â€˜f ()â€™ with actual type â€˜Exprâ€™
02:35:59 <lambdabot>     â€¢ In the second argument of â€˜fmapâ€™, namely â€˜xâ€™
02:36:23 <ski> @type (<*>) (fmap ?f ?x)
02:36:24 <lambdabot> (?x::f a1, ?f::a1 -> a2 -> b, Applicative f) => f a2 -> f b
02:36:34 <ski> @type \f x -> (<*>) (fmap f x)
02:36:35 <woodson> this is the implementation (<*>) (fmap f x)
02:36:36 <lambdabot> Applicative f => (a1 -> a2 -> b) -> f a1 -> f a2 -> f b
02:36:55 <woodson> Now, I see it
02:37:13 <ski> aka `\f x y -> f <$> x <*> y'
02:37:22 <woodson> fmap is expect (b -> c), therefore, x must equal to f (b -> c)
02:37:25 <ski> @src liftA2
02:37:25 <lambdabot> liftA2 f a b = f <$> a <*> b
02:37:57 <ski> (which is also equal to `\f x y -> pure f <*> x <*> y')
02:38:35 <woodson> yea, I was more concerned about the type definition of 
02:38:40 <woodson> :t liftA2
02:38:41 <lambdabot> Applicative f => (a -> b -> c) -> f a -> f b -> f c
02:39:13 <woodson> because the first argument changed and it threw me off a bit
02:39:20 <ski> (type signature, not definition)
02:39:20 <woodson> :t liftA2 id
02:39:21 <phadej> about: 1.0 vs 1, that's what you get when you represent Version (public interface!) as [Int]
02:39:21 <lambdabot> Applicative f => f (b -> c) -> f b -> f c
02:39:29 <phadej> then [1,0,0,0] /= [1]
02:39:36 <woodson> yes definition**
02:39:49 <ski> no, signature :)
02:40:09 <woodson> lol, sure. :)
02:40:20 <woodson> anyway, thanks for replying!
02:40:28 <ski> np
03:08:16 * hackage wreq 0.5.3.0 - An easy-to-use HTTP client library.  http://hackage.haskell.org/package/wreq-0.5.3.0 (ondrap)
04:49:57 <shiona_> I reimplemented my parallel filtering in pipes and it works. But now I would like to (every now and then) peek the current value
04:50:40 <shiona_> but it seems pipes can either work in a FIFO style or in what it call Latest, which is what peeking seems to be
04:56:34 <trcc> Not to spam but this seems pretty cheap: https://www.udemy.com/courses/search/?q=haskell&src=sac&kw=haskell anyone familiar with any of them?
04:58:52 <lavalike> shiona_: I'm not extremely familiar with pipes, but I would guess you put a pipe in between and have it yield all the values it consumes, whilst also doing something every nth one (or some such)?
05:01:18 <shiona_> lavalike: I could have the Pipe take in an extra Output that is created with the Latest into which it could feed all values it goes through. Another thread could then take the respective Input and take and print values every now and then
05:01:34 <shiona_> but that seems very inelegant and possibly slow
05:03:09 <shiona_> When I was asking the last time around I was given a bunch of libraries to take a look at, I just took the first one that was Pipes. I thought maybe someone could tell me if any of the libraries would be a better fit to my work
05:05:54 <lavalike> what is your work?
05:09:07 <adius> If somebody is looking for a cool open source project to build in Haskell, I'd really love a Haskell implementation of https://github.com/simonw/datasette ðŸ˜‹
05:09:27 <shiona_> I have a String Producer, a bunch of threads that filter these strings using IO (!) and then feed back to a single thread that probably just prints these out. And the latest idea I got was that I want to periodically check how far the filters have gotten in the produced strings
05:10:02 <shiona_> I probably could also do with a TVar/MVar and just do (+1) for each processed string
05:16:16 * hackage bulletproofs 0.4.0 -   http://hackage.haskell.org/package/bulletproofs-0.4.0 (sdiehl)
05:18:17 * hackage mealstrom 0.0.1.0 - Manipulate FSMs and store them in PostgreSQL.  http://hackage.haskell.org/package/mealstrom-0.0.1.0 (amx)
05:47:52 --- mode: glguy set +v fen
05:48:18 <fen> any ideas how to extend this to 2d? https://bpaste.net/show/66e90aa344f3
05:48:32 <fen> its like a double linked list version of a zipper
05:48:42 <fen> (cyclic references)
05:49:38 <fen> but, when nesting them, it seems like it might not be possible to do this kind of cyclic sharing of references over different paths to the same gridsquare
06:13:41 --- mode: glguy set +v habile
06:14:11 <habile> hello
06:17:08 --- mode: glguy set +v helen_
06:19:38 <Solonarv> Constant (or Closed?) Applicative Form
06:20:01 <hyperisco> constant
06:20:09 <hyperisco> now if only I could remember what CAF is about
06:21:36 <ski> adius : oh, i was thinking that had something to do with <https://en.wikipedia.org/wiki/Datasette> ..
06:23:04 <hyperisco> is CAF just what other languages call CTFE?
06:23:35 <helen_> function (function (function input tuple )) <- hey is there anyway to make this more efficient
06:24:16 <hyperisco> helen_, what are those terms?
06:24:32 <adius> ski: I guess it kind of has ðŸ˜›. It provides an interface to a SQLIte database while the original provided an interface to a casette 
06:24:48 * ski smiles
06:25:08 <helen_> it just means like it goes  - > call a function (function(function(an input tuple))
06:25:28 <ski> helen_ : the same function ?
06:25:31 <hyperisco> I don't know what you mean. Can you show some code?
06:26:23 <ski> > foldr (.) id (replicate 3 f) x  -- avoiding repetition of `f'
06:26:25 <lambdabot>  f (f (f x))
06:27:05 <ski> (but if you really mean make it more efficient, rather than more nice to write and read, then you probably have to tell more about what your function does)
06:31:53 <MarcelineVQ> hyperisco: CAFs aren't executed at compile time, though the intention is that they're executed once and shared. they're something of a static thunk
06:32:38 <hyperisco> and here I thought GHC avoided such things
06:33:06 * ski . o O ( "run-time compilation" )
06:33:23 <hyperisco> at least, that is an argument I read against common subexpression elimination
06:33:53 <MarcelineVQ> static in the sense of known memory location, https://ghc.haskell.org/trac/ghc/wiki/Commentary/Rts/Storage/GC/CAFs
06:34:59 <hyperisco> that aside from the compilation expense, predicting runtime behaviour becomes more difficult, least because you're not so sure if the optimisation fires or not
06:35:33 <hyperisco> so these CAFs are just going to linger in memory forever
06:36:22 <hyperisco> that sucks if your intent was to, say, traverse a large list, where you'd otherwise just have one cons in memory at a time
06:38:57 <MarcelineVQ> Dunno about that, the page suggests cafs can be collected, though I don't understand these pieces well
06:39:42 <hyperisco> if you collect it then you have to recompute it later, so what's the point
06:40:20 <ski> "So GHC goes to strenuous efforts to track when a CAF \"can no longer be referred to\"."
06:40:49 <ski> (so i assume there is no "recompute it later")
06:40:53 <hyperisco> at the same time it is a little screwy to expect that  () -> [Int]  is going to prevent the preallocation of a list of Ints
06:41:25 <hyperisco> ski, okay but then it will have to be so conservative as to still be a problem
06:41:29 <MarcelineVQ> https://ghc.haskell.org/trac/ghc/browser/ghc/compiler/cmm/CmmBuildInfoTables.hs has more info as well, though I can't read that text color so I can't say if it's helpful
06:41:56 <ski> well, less conservative than "keep all top-level thunks"
06:42:30 <ski> (text color ?)
06:42:37 <MarcelineVQ> It definitely looks useful from what I can read
06:42:43 <MarcelineVQ> ski: light grey on white
06:42:49 <hyperisco> like, throw a function which uses the CAF into some list, then move the list aboutâ€¦ who knows if that function comes out and is evaluated again
06:43:02 <hyperisco> so their static analysis is going to quickly devolve to "never get rid of it"
06:43:11 <MarcelineVQ> One of my major programming peeves if comment text being background noise.
06:43:13 <maksim_> why is Reader a thing if there's already an instantiation for ((->) r)
06:43:13 <ski> mhm. i wouldn't know. i viewed it in W3m (terminal-based browser)
06:43:23 <MarcelineVQ> Comment text is at least as important as code.
06:43:49 <ski> maksim_ : clearer error messages, explicit isomorphism ?
06:44:01 <maksim_> ski, what do you mean by explicit isomorphism?
06:45:01 <maksim_> imho runReader obfuscates greatly what's going on
06:45:25 <ski> in `newtype Reader r a = Reader {runReader :: r -> a}' the data constructor `Reader :: (r -> a) -> Reader r a' and the field selector `runReader :: Reader r a -> r a' are inverses, and form an isomorphism between the (new/"abstract") type `Reader r a' and the (representation) type `r -> a'
06:46:25 <maksim_> okay but that seems like an isomorphism just for the sake of having an isomorphism?
06:46:47 <ski> if you see a type error involving `(->)', it could be related to any function issue, such as forgetting argument, &c. not necessarily with using the environment functor/idiom/monad
06:46:52 <lyxia> maksim_: it's really mostly defined in transformers for consistency
06:46:53 <maksim_> that's true
06:46:55 <MarcelineVQ> From another angle: Reader is a specialization of ReaderT and Identity, as State is a specialization of StateT, and other transformers, so it's good to have in keeping with expectations as well
06:47:09 <hyperisco> maksim_, why have any abstraction when its implementation will do?
06:47:46 <maksim_> hyperisco, abstraction here is leaky because you have to use runReader to unpack
06:47:52 <maksim_> or maybe not
06:47:59 <maksim_> i guess runReader could do whatever
06:47:59 <hyperisco> no it isn't leaky
06:48:01 <hyperisco> Reader is a monad with a read effect
06:48:16 <hyperisco> I think we call it "ask"
06:48:31 <Ariakenom> :t ask
06:48:32 <lambdabot> MonadReader r m => m r
06:49:02 <maksim_> yea i guess that's fair
06:49:35 <hyperisco> that Reader is implemented by a function is beside the pointâ€¦ it is a thin veil, yes, but we shouldn't want everything to be so difficult should we
06:50:02 <maksim_> well to be honest i wonder if there is another implementation?
06:50:13 <maksim_> are there other instantiations of MonadReader?
06:50:23 <hyperisco> mhm
06:50:43 <maksim_> such as?
06:51:44 --- mode: glguy set +v bec
06:53:38 <bec> Hello o/
06:54:03 <hyperisco> \o
06:54:28 <MarcelineVQ> Foo s m | m -> s    is m uniquely determining s or s uniquely determining m? guessing the former since -> is like implication?
06:54:50 <hyperisco> MarcelineVQ, m determines s, it is a functional relation
06:56:06 <MarcelineVQ> thank you, how do you readFoo a b c | a b -> c ?  simply that a particular a and b determine c?
06:56:14 <hyperisco> that's right
06:56:18 <MarcelineVQ> awesome
06:56:58 <maksim_> i'm curious about this
06:57:05 <maksim_> where is this functional dependency specified?
06:57:10 <maksim_> or does the compiler just figure it out?
06:57:43 <maksim_> i ask because last night i was trying to understand type families
06:57:55 <MarcelineVQ> https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/glasgow_exts.html#functional-dependencies https://pdfs.semanticscholar.org/510a/8d5e0c28a5ec21a5e8b7aeaca526b7c96e87.pdf
06:57:57 <ski> MarcelineVQ : logically it means `forall m. unique s. Foo s m' (where by `unique' i mean there is at *most* one thing having the property). that's equivalent to `forall s0 s1 m. (Foo s0 m,Foo s1 m) => s0 = s1'
06:57:58 <maksim_> (which is apparently implemented using functional dependencies?)
06:58:16 <hyperisco> eh I don't think there's anything special between them
06:58:27 <hyperisco> type families are functions, sure, but that might be it
06:58:41 <bec> I'm looking into Generics + type applications, and I was wondering if there was an alternative to `from` to get a Rep for my type. I guess Rep is not what I want, since `from` also gives me the values from the thing it deconstructs. Does anyone have a pointer that would help me?
06:59:06 <MarcelineVQ> fundeps and associated data types are related in what they achieve, I've heard
06:59:13 <ski> if you know anything about relational data base theory, then functional dependencies on parameters of type classes are pretty similar to functional dependencies on attributes of relations there
06:59:24 <maksim_> ski, interesting 
06:59:32 <ski> (a type class is a relation on types)
07:00:07 <hyperisco> you use fundeps to help with instance resolution, in practice
07:00:14 <maksim_> it's funny i've heard people complain that learning about a haskell feature often requires reading a PLT paper
07:00:20 <ski> practically, an FD like above on `Foo' means (a) that you're forbidden from making two instances with the same `m', but different `s's
07:00:24 <maksim_> i wonder if i'll become a pl theorist in the process therefore
07:00:56 <ski> and (b) that if it infers constraints `Foo s0 m' and `Foo s1 m', then it knows that `s0' and `s1' must be the same, so it can unify those two, replacing the two constraints by a single constraint
07:01:24 <ski> this can reduce type ambiguity in expressions and definitions, so that you need to have less (or no) type ascriptions
07:02:21 <MarcelineVQ> ski: thanks for your response, phrasing Foo that way is helpful
07:02:47 <ski> yw
07:03:23 <MarcelineVQ> koz_: get far in the types book yet?
07:04:11 <maksim_> are there table driven parser libraries in haskell?
07:04:16 <maksim_> or only recursive descent?
07:04:21 <ski> @where Happy
07:04:21 <lambdabot> http://www.haskell.org/happy/
07:04:22 <hyperisco> maksim_, Happy
07:04:29 <maksim_> oh interesting
07:04:45 * ski . o O ( "Happy, Happy. Joy, Joy." )
07:04:45 <hyperisco> and I guess Alex or w/e it is called counts, as a lexer
07:04:55 <maksim_> maintained? latest news is from 2010?
07:05:01 <ski> @where BNFC
07:05:01 <lambdabot> BNF Converter, http://bnfc.digitalgrammars.com/
07:05:10 <ski> might also be interesting to check out, in relation to this
07:06:09 <hyperisco> reminds me that sometime I want to make a sensible lexer generator, because they all painfully suck
07:06:40 <MarcelineVQ> maksim_: it is, the website isn't neccesarily. happy and alex are required ghc tooling. if they didn't work ghc wouldn't compile
07:07:14 <ski> @hackage happy
07:07:15 <lambdabot> http://hackage.haskell.org/package/happy
07:07:25 <hyperisco> regular language is a beautiful space massacred by lexer generators
07:07:28 <ski> revised in spetember this year
07:07:57 <maksim_> cool thanks
07:08:15 * ski . o O ( regular language intersection, implication/division/differentiation )
07:08:23 <hyperisco> exactlyâ€¦
07:09:16 * hackage google-isbn 1.0.3 -   http://hackage.haskell.org/package/google-isbn-1.0.3 (apeyroux)
07:10:55 <hyperisco> for example, in Alex you give rules by precedence so you can say what the keywords are before you let everything else be an identifier
07:11:35 <hyperisco> well, how about identifiers are the language [a-z][a-z0-9] subtract let|in|where|etc
07:12:14 <f-a> does anyone know if haskell survey data results were published somewhere? 
07:13:00 <MarcelineVQ> given the responses they're probably not done going through them ehe
07:13:22 <f-a> ta
07:13:27 <f-a> *thanks MarcelineVQ 
07:13:42 <ski> @where HCAR
07:13:42 <lambdabot> https://wiki.haskell.org/Haskell_Communities_and_Activities_Report
07:13:55 <f-a> to be honest I am mostly interested about the raw data than presentation, but I am fine waiting
07:26:46 * hackage jaeger-flamegraph 1.1.0 - Generate flamegraphs from Jaeger .json dumps.  http://hackage.haskell.org/package/jaeger-flamegraph-1.1.0 (fommil)
07:32:57 <fen> up and down would be different to left and right, being fmapped into the rows instead of acting on the column of row pointers 
07:33:13 <fen> so going up and then left is not the same as going left and then up
07:33:44 <ski> commuting paths ?
07:33:51 <fen> left then up is the only way really, and up then left just references this, which means up then down then left then up
07:34:06 <fen> so that one navigational path is slower than the others
07:34:13 <fen> ski: just for cartesian grids
07:34:34 <fen> ski: https://bpaste.net/show/66e90aa344f3
07:34:38 <fen> thats the 1d version
07:34:48 <fen> the problem is from the Comonad for the 2d version
07:34:55 <fen> if its going to be a ComonadStore
07:35:18 <noumenon> if you can actually read that paste easily, that's some Matrix level stuff right there
07:35:20 <fen> then navigation should be as fast in any order
07:35:57 <fen> noumenon: what?
07:36:51 <noumenon> "You get used to it, though. Your brain does the translating. I don't even see the code. All I see is blonde, brunette, redhead."
07:36:55 <MarcelineVQ> This particular paste isn't really unreadable. did you notice that Zipper has the shape of a binary tree fen?
07:37:22 <fen> yeah it could be a Cofree
07:37:35 <fen> really it should be converted from NonEmpty...
07:37:48 <ski> hm, `toZipper' looks like it's TyingTheKnot. is it a doubly-linked list ?
07:38:08 <fen> its more the shape of a "one hole context" than the ([a],[a]) version
07:38:16 <fen> ski: yes
07:38:29 <ski> then i'm not sure it qualifies as a zipper ?
07:38:32 <fen> and could easily be made to have cyclic boundary conditions as a result
07:38:52 <fen> ski: why not? 
07:39:06 <ski> how do you do local update ?
07:39:33 <fen> that wasnt needed for the example
07:39:41 <ski> `(a -> a) -> Zipper a -> Zipper a', i.e., only changing the element in focus
07:40:00 <fen> there are also, concatenation and Monad instances, Traversable e.t.c. 
07:40:31 <fen> its just a least sufficient code to show it has a valid comonad instance
07:40:50 <ski> you'd prefer not having to reconstruct the whole structure, just to be able to adapt the locally accessible data
07:41:03 <fen> the point is its lazy
07:41:12 <fen> the duplicate can be used
07:41:26 <fen> but this isnt easily the case for 2d
07:41:35 --- mode: glguy set +v kritzcreek_
07:41:44 <ski> if you pass in a doubly-linked list, with sharing, do you ensure the corresponding sharing of the output of `update' ?
07:41:53 <fen> lazy
07:42:11 <fen> it might be expensive to update the whole structure, but if its not called it wont be evaluate
07:42:21 <fen> thats the point
07:42:30 <ski> if you unfold the cyclic graph to an infinite tree, than that's not that good
07:42:37 <fen> dont do that then
07:42:47 <ski> don't do local updates ?
07:42:58 <fen> dont do take 100000 on them
07:43:43 <fen> dont go forwards and backwards over and over
07:44:06 <fen> it becomes important to be able to collect the navigations and eleminate the ones that are inverse
07:44:15 <fen> like left . right = id
07:44:22 <fen> thats probably just MonoidAct
07:44:32 <fen> or, with an inverse 
07:44:34 <ski> in my mind, one of the driving points of introducing zippers was to have cheap (O(1), typically) local update and navigation
07:44:38 <fen> GroupAct
07:44:58 <fen> ski, does it not still have this? 
07:45:28 <fen> from the fact that by lazyness, the distant updates will never fire
07:45:38 <ski> i don't know, i've not seen an implementation of `updateHere :: (a -> a) -> Zipper a -> Zipper a'
07:45:49 <fen> sure, the change would propegate
07:46:21 <fen> maybe thats quite similar to this up then left /= left then up for the 2d extension
07:46:28 <fen> its about trying to share the results
07:46:43 <fen> like where the let bound `z' is used in the paste
07:46:53 <fen> but, for whole rows...
07:46:56 <fen> its confusing
07:47:03 <fen> not sure how to do it
07:47:26 <maksim_> i don't know how to say this exactly correctly but i notice that Maybe, Reader, Writer, State are monadic in one type parameter i.e. the instantiation for Reader/Writer/State is m a rather than m, or m a b. is that always the case? are monads always monadic in one type parameter and fixed in the other?
07:47:48 <dminuoso> :k Monad
07:47:49 <lambdabot> (* -> *) -> Constraint
07:47:50 <ski> if `updateHere f z' applies `f' to all elements in the structure which logically corresponds to "here" (and which by precondition are assumed to be equal), then those calls to `f' won't be shared, i suppose ?
07:48:03 <maksim_> dminuoso, single kinded?
07:48:16 <maksim_> so the answer is yes?
07:48:22 <dminuoso> maksim_: Well, it takes one type argument - but it has to be of kind (* -> *)
07:48:28 <ski> hm, or you could carry `f x' around as you traverse, and just discard the old element. hmm
07:48:30 <dminuoso> maksim_: So `Maybe` is a Monad.
07:48:33 <dminuoso> maksim_: but `Either` is not.
07:48:38 <dminuoso> :k Either
07:48:39 <lambdabot> * -> * -> *
07:48:43 <dminuoso> maksim_: Neither is State
07:48:45 <dminuoso> :k State
07:48:46 <lambdabot> * -> * -> *
07:48:50 <ski> @kind Either Bool
07:48:51 <lambdabot> * -> *
07:48:57 <maksim_> yes partially applied
07:49:02 <fen> seems like it would just update the references at either side to the new version
07:49:06 <maksim_> so Either a for some a is monadic
07:49:16 <dminuoso> maksim_: It's a monad.
07:49:25 <ski> for every type `e', `Either e' is a monad
07:49:25 <dminuoso> The adjective "monadic" is rather fuzzy.
07:49:26 <maksim_> that's basically what i'm asking but you've answered it by showing me that the kind of a monad is * -> *
07:49:48 <maksim_> ok are there analogs to monads for * -> * -> * kinds?
07:49:55 <ski> no
07:50:00 <maksim_> interesting
07:50:03 <ski> monads are endofunctors
07:50:04 <fen> and these would never be used unless navigation left then right was used, which the point is it should not be. and if it is, then maybe its no slower anyway
07:50:06 <lavalike> it's not enough for a type to be of kind  * -> *  to be a monad (but maybe that goes without saying)
07:50:20 <maksim_> lavalike, yes i understand that
07:50:24 <dminuoso> maksim_: also note in particular that a Functor also exists for things of kind * -> *
07:50:47 <ski> fen : "the point is it should not be" -- in that case, why provide such navigation, if it's not to be used ?
07:50:52 <fen> left + right according to Int, which has inverse under (+) so (left . right) = id
07:50:53 <dminuoso> maksim_: In a particular way a functor is equipped with two tools: one to map a type to another, and one to map a function to another. 
07:51:21 <dminuoso> maksim_: Maybe is that first part, it maps Int to Maybe Int, Bool to Maybe Bool, etc.. and with fmap you can map functions `a -> b` to `Maybe a -> Maybe b`
07:51:24 <maksim_> so a bifunctor (for example) isn't an endofunctor because it goes from a 
07:51:46 <maksim_> here the line between haskell and cat theory gets a little blurry
07:51:56 <fen> ski: the problem is when the Comonad for a 2d grid kind of traverses one direction updating the orthogonal direction. so that up . left /= left . up
07:52:07 <fen> up . left = up . down . left . up
07:52:11 <maksim_> because i think in saying that "a Functor also exists for things of kind * -> *" i think you're talking about cat theory rather than haskell?
07:52:13 <ski> fen : yes
07:52:22 <fen> and these should be shared with this kind of "let" referencing
07:53:18 <ski> fen : i once did a text-adventure as a zipper. i carefully made sure to have no cycles in the data structure (otherwise i'd have gotten update anomalies (deletion "anomalies" didn't matter, for my purposes))
07:53:44 <fen> yeah, it would be easy to write snake
07:53:56 <fen> but anyway. there is ComonadStore
07:54:02 <ski> (walking along a bidirectional path would reverse that "pointer", in the zipper)
07:54:40 <fen> and it has indexed navigation as indexed view of the duplicated structure
07:54:52 <fen> so it needs to be the same kind of lazy regardless of path
07:55:01 <fen> for duplicate
07:56:13 <fen> there isnt really any need to use ComonadStore to do this, as indexed navigation and indexed view use the same update, so the navigation can be done directly. but its a good test to check that the lazyness in all equivalent paths same
07:56:33 <fen> same lazyness*
07:57:09 <fen> and this is to be done with sharing, not just making a cononical path using the GroupAct approach
07:57:33 <fen> not sure how those can be thought about reasonably either
07:58:37 <maksim_> what editor do you guys use? i've been using spacemacs but i'm so used to vim that's it's rough
07:58:53 <ski> Emacs
07:59:10 <ski> i think many use Vim as well
07:59:43 <maksim_> there's no intero analogue for vim though is there?
07:59:52 <maksim_> it's unfortunate because intero is actually really nice
08:00:02 * ski wouldn't know, sorry
08:02:16 * hackage IntervalMap 0.6.1.0 - Containers for intervals, with efficient search.  http://hackage.haskell.org/package/IntervalMap-0.6.1.0 (ChristophBreitkopf)
08:08:43 <Solonarv> intero is not bound to a specific editor, people have written haskell plugins that use intero for several editors
08:08:43 <maksim_> another dumb question: when i get an error like "couldn't match expected type with actual type" what is that referring to? the argument being passed to the function or the function itself being applied to the argument?
08:09:43 <Solonarv> if you google something like "intero vim plugin" you should find one, assuming it exists
08:10:01 <maksim_> Solonarv, yes i've googled around and they exist but they're not as polished as intero on emacs
08:11:11 <Solonarv> you could try using haskell-ide-engine instead, which uses the LSP
08:11:19 <maksim_> yes that works fairly well
08:11:25 <maksim_> at least on vscode
08:11:52 <MarcelineVQ> maksim_: best to use an actual error to ask about, in case what people are interpreting you meaning is different from what you do mean
08:12:22 <fizbin> Is there an easy way to find all the differences between two Data.Map maps? In particular, Data.Map.difference doesn't show me keys that are mapped to different values in the two maps.
08:12:33 <maksim_> MarcelineVQ, https://paste.rs/VVj.hs
08:12:52 <maksim_> i got that error by just trying to apply test to an integer
08:12:56 <maksim_> i know what the error is
08:13:11 <maksim_> i'm just trying to actually parse out the explanation
08:13:24 <maksim_> what is it exactly that excepted a type of `Integer -> t`
08:13:59 <maksim_> i feel like it's the integer 
08:15:09 <maksim_> but that seems backwards from how i imagine type checking works (functions expect correctly typed arguments rather than arguments expecting functions that match their types)
08:16:06 <Solonarv> you didn't paste the thing that triggered the error?
08:16:20 <maksim_> it's just `test 3`
08:16:25 <geekosaur> the error doesn't match what's there
08:16:27 <MarcelineVQ> only functions can be applied, as such applying test to an Integer means test must be a function  of   Integer -> sometype   when checking types we find out that isn't the case
08:16:34 <geekosaur> ou have to use runState
08:16:40 <maksim_> yes i know i have to use runState
08:16:46 <ski> maksim_ : "expected type" is what the context (around the use of `test' here) expects the subexpression to have as type
08:16:53 <geekosaur> or execState or evalState if you want only part of the result
08:17:05 <ski> maksim_ : "actual type" is the actual type of that subexpression (`test' here)
08:17:23 <Solonarv> the typechecker goes "hm, you're applying 'test' to an Integer, so I expect 'test' to be a function that accepts an Integer"
08:17:24 <maksim_> ski, what is the context? IO ?
08:17:47 <fen> its like there could be 2 different comonad instances, depending on if rows or columns were traversed first
08:17:50 <maksim_> tangentially: is there a way to see what types the compiler infers for all of my expressions?
08:17:52 <geekosaur> and something to remember here is that partial application exists, so an expression consisting of a function with a missing parameter shows up as a function
08:18:06 <Solonarv> but 'test' isn't an integer-accepting function! it's a 'State Int Int'!
08:18:11 <maksim_> yes
08:18:20 <ski> in the definition `x = test 3', the context is everything surrounding this occurance of `test', so like `x = (...) 3', counting everything except the subexpression i marked with `...'
08:18:35 <mitchellsalad__> Will GHC ever rewrite `const x y` as `x`?
08:18:50 <Solonarv> mitchellsalad__: I'd expect that to happen most of the time, actually
08:18:52 <mitchellsalad__> I'm wondering if `const x y` is a reliable way of preventing `y` from being garbage collected at least until `const x y` is
08:19:07 <Solonarv> no, it certainly isn't
08:19:26 <ski> clearly `...' is applied to an argument (`3') here, so the context expects `...' to have a function type. moreover, the argument type of that function type is expected to be equal to the type of `3' (which is defaulted to `Integer' here)
08:19:39 <maksim_> ski, yes
08:19:41 <maksim_> thank you
08:19:46 <ski> > ord False
08:19:48 <lambdabot>  error:
08:19:48 <lambdabot>      â€¢ Couldn't match expected type â€˜Charâ€™ with actual type â€˜Boolâ€™
08:19:48 <lambdabot>      â€¢ In the first argument of â€˜ordâ€™, namely â€˜Falseâ€™
08:19:49 <ski> @type ord
08:19:51 <lambdabot> Char -> Int
08:20:04 <ski> in this case, the context around the subexpression `False' is `ord (...)'
08:20:18 <geekosaur> hm, I have tat backwards as how it shows. give an extra paramete, the result of the function is shown as needing a unction type so it can be applied to the extra parameter
08:20:25 <ski> the expected type is `Char', because that's what the context (the call to `ord') expects `...' to have as type
08:20:37 <ski> the actual type is the type `Bool' of the actual subexpression `False'
08:21:04 <ski> maksim_ : ^ that's another example, where the subexpression in question wasn't the function, but the argument expression
08:21:09 <maksim_> yes
08:21:35 <maksim_> ski, how do i know which subexpression ghc is focusing on?
08:21:53 <maksim_> for example my error mentions the entire expression
08:21:55 <maksim_> test 0
08:22:11 <ski> yea, it's not that immediately obvious, i agree
08:22:21 <mitchellsalad__> Solonarv: a quick core dump suggests you are correct
08:22:34 <maksim_> it does say test is applied to only one argument ... but hence my confusion about whether the issue was with test or 0
08:22:37 <ski> in the above case, it says "In the first argument of â€˜ordâ€™, namely â€˜Falseâ€™" -- which seems to mean that the subexpression in question is that first argument
08:22:44 <maksim_> yes in ord False it's clear
08:23:19 <ski> but in your paste, it said instead "The function â€˜testâ€™ is applied to one argument," -- which didn't sound similar, but which apparently means that `test' itself is the subexpression in question
08:23:29 <maksim_> the other question i had would help: is there a way to get ghc to tell me what types it infers for all of my expressions
08:24:12 <mitchellsalad__> Solonarv: Thoughts on this line, which tries to prevent "const m r" from floating out side of the "\r -> ..." binding? https://github.com/HeinrichApfelmus/reactive-banana/blob/ee9d5a3962c2bc48782512f54a3ae14a7a3d5b4b/reactive-banana/src/Reactive/Banana/Internal/Combinators.hs#L161
08:24:45 <maksim_> reactive banana lol
08:24:48 <ski> (also, in general a type error doesn't arise at a single location, but rather as a mismatch between at least two different locations. the location at which the mismatch is *detected* might not be the place where you want to fix it. "type error slicing" would report all relevant locations)
08:24:56 <mitchellsalad__> maksim_: what about it?
08:24:57 <MarcelineVQ> maksim_: judicious use of _ can get you a lot of info
08:25:09 <maksim_> mitchellsalad__, just a funny name for a library
08:25:15 <maksim_> MarcelineVQ, yes type holes
08:25:22 <maksim_> ski, yes i've run into that as well before
08:25:26 <MarcelineVQ> all my expressions is likely to end in an unreadable mess though, since there's one expression in a haskell program, and a looooot of subexpressions
08:25:36 <mitchellsalad__> maksim_: oh, yeah. Its mascot is a cowboy banana.
08:25:40 <ski> unfortunately, i know of no type error slicing implementation for Haskell
08:25:49 <maksim_> MarcelineVQ, sure but for small functions it wouldn't be bad
08:26:21 <geekosaur> maksim_, there's -ddump-tc but it's not going to be very friendly. judicious use of _ (wrap an expression you are about in ( ... :: _) ) wll shwo you the type of just that
08:26:39 <ski>  ("Skalpel" <http://www.macs.hw.ac.uk/ultra/skalpel/> is one for SML. there's a demo on that page)
08:27:17 <maksim_> geekosaur, thanks
08:27:26 <Solonarv> mitchellsalad__: you could try defining a NOINLINE variant of const and see if that's better
08:27:38 <hexagoxel> mitchellsalad__: cache uses unsafePerformIO, iirc, so this does indeed have "an effect"
08:28:37 <mitchellsalad__> Solonarv: great idea
08:29:09 <Solonarv> it would at least make sure you still have a 'const x y' by the time you get to Core
08:32:24 <mitchellsalad__> Solonarv: Interesting... hrm... does it follow that this 'y', which exists in the optimized Core, will not be garbage collected until the closure containing this noinlined 'const x y' is?
08:32:40 <Solonarv> maybe?
08:32:44 <mitchellsalad__> :)
08:33:25 <mitchellsalad__> Well, thanks
08:34:22 <c_wraith> mitchellsalad__, ghc actually has some optimization in the garbage collector for that.
08:35:15 <c_wraith> in some cases the gc will cause the evaluation of functions that are known to only throw data away.
08:36:02 <mitchellsalad__> interesting!
08:36:36 <c_wraith> the classic example is a thunk representing (fst (x, y)), where the pair is known to not require evaluation to find the (,) constructor.
08:37:40 <Solonarv> perhaps 'keep x y = fst (lazy (x, y))' could work?
08:38:25 <MarcelineVQ> why does y need to stay around
08:38:25 <mitchellsalad__> My use case: I have a mutable collection of IO actions, and I want the IO actions to keep the collection itself alive (so that only an empty collection can be garbage collected). I was wondering if, instead of inserting `action`, inserting `const action collection` would suffice
08:38:28 <c_wraith> I worry that something else has gone wrong if you care about preventing the gc of an unused value.
08:38:43 <Solonarv> that is a good point
08:39:09 <Solonarv> mitchellsalad__: if nothing's referring to the collection, why do you need it to stay around?
08:40:30 <kuribas> how do you go from a typelevel ('Maybe Symbol) to typelevel Maybe String?
08:40:32 <ggole> Is there spooky logic-in-finalizers stuff?
08:40:34 <c_wraith> the main time it makes sense is when there's a finalizer on a value that you don't want to run prematurely, like with ForeignPtr values.
08:40:50 <ggole> You should avoid that sort of thing if at all possible.
08:40:54 <c_wraith> for those, touchForeignPtr was added specifically to prevent it.
08:41:45 <Solonarv> kuribas: I think you'd need a typeclass, or involve the 'singletons' library
08:41:48 <mitchellsalad__> Solonarv: I didn't explain myself properly, the collection is contained within another data structure, and it's /that/ data structure that I want the existence of any actions in the collection to keep alive.
08:42:08 <kuribas> Solonarv: I find it a bit weird their is no way to unlift datakinds
08:42:21 <c_wraith> kuribas, there currently are no type level Char values, so there's no type level String
08:42:32 <c_wraith> kuribas, there is work in progress to address that
08:42:38 <kuribas> ah cool
08:43:17 <c_wraith> I'm not sure when it's expected to be done, though
08:44:04 <c_wraith> mitchellsalad__, but why? are there finalizers you want to prevent from running?
08:45:07 <mitchellsalad__> c_wraith: that's right
08:45:27 <c_wraith> are they part of  ForeignPtr?
08:45:40 <mitchellsalad__> no, they're attached to an IORef#
08:45:51 <mitchellsalad__> Er, MutVar#, whatever's inside.
08:45:56 <c_wraith> my best advice is... don't do that. :P
08:46:12 <mitchellsalad__> Can't stop me.
08:46:47 <mitchellsalad__> I am aware of the subtlety between finalization/deadlock detection ordering... that doesn't apply to me here
08:46:54 <mitchellsalad__> Is there some other reason you advise against this?
08:47:53 <c_wraith> just that it makes things hard.
08:48:11 <c_wraith> but if you must do it, consider https://hackage.haskell.org/package/base-4.12.0.0/docs/GHC-Exts.html#v:touch-35-
08:49:02 <mitchellsalad__> This is for https://hackage.haskell.org/package/timer-wheel, a data structure with a background thread. It makes for a much cleaner API if you can just create the structure in IO, and GC controls when the background thread can die (since the wheel was dropped on the floor).
08:49:13 <mitchellsalad__> I'd rather not have a `withTimerWheel :: (TimerWheel -> IO a) -> IO a`
08:49:41 <mitchellsalad__> Ok now I'm scared. I don't know what `touch#` is and there aren't any docs
08:50:24 <c_wraith> well, you should be scared. that's my point. but touch# is the primitive touchForeignPtr is based on.
08:50:35 <c_wraith> see https://hackage.haskell.org/package/base-4.12.0.0/docs/src/GHC.ForeignPtr.html#touchForeignPtr
08:51:51 <c_wraith> it can certainly do what you need.
09:00:21 <maksim_> how do i unwrap Foo here: newtype Foo = Foo Int
09:00:33 <c_wraith> maksim_, pattern match.
09:00:41 <maksim_> c_wraith, no other way?
09:01:17 <ski>   newtype Foo = MkFoo {unFoo :: Int}
09:01:24 <ski> will automatically generate
09:01:31 <ski>   unFoo :: Foo -> Int
09:01:31 <maksim_> yes ha i'm trying to avoid runReader :)
09:01:33 <maksim_> as an exercise
09:01:33 <c_wraith> maksim_, you can write a function Tha does the pattern match for you. but in general, "pattern match" is the underlying operation that drives Haskell
09:01:39 <ski>   unFoo (Foo n) = n
09:01:41 <ski> for you
09:01:42 <c_wraith> *that does
09:01:47 <maksim_> yes
09:02:01 <maksim_> oh well pseudo code it is then :)
09:02:38 <MarcelineVQ> newtype Foo = Foo Int deriving (Eq, Ord, Num, Show)  unwrap? whatever for.   muahaahaha
09:03:25 <c_wraith> I mean, there's one other answer, but I'm hesitant to provide it.
09:03:32 <maksim_> c_wraith, please do
09:03:35 <c_wraith> it just makes many things worse.
09:03:41 <maksim_> this is just for an exercise anyway
09:03:48 <c_wraith> there's always coerce!
09:04:27 <geekosaur> an exercise is te worst time to play with internals
09:04:34 <maksim_> ah yea i saw that on SO
09:04:38 <geekosaur> in short, don't
09:05:02 <c_wraith> coerce will lead you down the road of ridiculously annoying type errors for eternity.
09:06:15 <c_wraith> pattern matching comes with type information to aid in inference. coerce comes with compiler-generated constraints that don't guide inference at all.
09:40:19 <mitchellsalad__> coerce @_ @Int MyNewtype
09:40:40 <mitchellsalad__> I mean myNewtype
09:42:15 <c_wraith> at that point, I see no advantage over pattern matching (or using a function that does it)
09:45:03 <Solonarv> using 'coerce' to wrap/unwrap a single layer of newtype is not really necessary, agreed
09:46:11 <Solonarv> it's mainly useful when you're wrapping/unwrapping in a container (i.e. 'coerce' instead of 'fmap unMyNewtype'), or in combination with foldMap
09:48:27 <mitchellsalad__> how about for `unwrapper . f` vs. `coerce f`?
09:49:54 <Solonarv> eh, shouldn't really make a difference afaik
09:50:46 <bollu> why is cata faster than regular recursion?
09:51:16 <Solonarv> it isn't automatically faster
09:52:04 <Solonarv> in the cases where it *is* faster, that's generally because it's friendlier to inlining and other optimizations
09:58:19 <c_wraith> and I'd expect it to not be faster most of the time.
10:05:55 * hackage snaplet-customauth 0.1.2 - Alternate authentication snaplet  http://hackage.haskell.org/package/snaplet-customauth-0.1.2 (kaol)
10:05:57 --- mode: glguy set +v govno
10:12:17 <MarcelineVQ> https://hackage.haskell.org/package/base-4.12.0.0/docs/Data-Complex.html should tha say Ord instance in the line about Complex Float?
10:13:21 <glguy> MarcelineVQ: It's implied by the sentence structure that the Ord instances are being discussed.
10:14:19 <MarcelineVQ> As in, should someone add Ord instances this would be the case?
10:17:03 <glguy> I suppose? I think the main issue with Complex Ord instance isn't that it inherits problems from other instances, but that the Ord class kind of sucks in that it's reused for both an arbitrary total ordering for things like Data.Map as well as a semantically interesting ordering for numbers
10:17:36 <glguy> It would be nice to have the instance for some cases and would be error prone for others. We should change the sentence to be about `Eq` and it would be less confusing
10:18:07 <MarcelineVQ> Alright thank you, sanity check completed hehe
10:20:17 <fresheyeball> hey folks 
10:20:25 <fresheyeball> so I am trying to show a friend how Set is not a Functor
10:20:34 <fresheyeball> and falsify it with the Functor Laws
10:20:43 <fresheyeball> Set.mapMonotonic does pass `fmap id = id`
10:20:56 <glguy> But has the wrong type
10:21:00 <glguy> err
10:21:03 <fresheyeball> and I can't seem to find values that falisfy it with `fmap f . fmap g = fmap (f . g)`
10:21:16 <glguy> fresheyeball: how about f and g are negate
10:21:23 <glguy> and we're dealing with Int
10:21:49 <fresheyeball> glguy: that passes
10:22:08 <fresheyeball> t g f ys = mapMonotonic f (mapMonotonic g ys) == mapMonotonic (f . g) ys
10:22:08 <glguy> No, it fails because you get an invalid set half-way through
10:22:27 <fresheyeball> t negate negate [1,2,3] = True
10:22:45 <fresheyeball> glguy: so that doesn't work
10:22:58 <glguy> Yeah, it accidentally works, make sure your test checks that the set is valid after each step
10:22:59 <Ariakenom> fresheyeball: fmap :: (String -> IO ()) -> Set String -> Set (IO ())
10:23:17 <fresheyeball> let's leave IO out please
10:23:19 <Ariakenom> fresheyeball: but IO () doesn't have Ord
10:23:21 <fresheyeball> I am trying to tech a noob
10:23:22 <glguy> valid :: Ord a => Set a -> Bool
10:23:47 <MarcelineVQ> fresheyeball: recall that a Set is of unique values
10:23:55 <fresheyeball> yes
10:24:10 <fresheyeball> I can't seem to find 2 functions and a set that falisify it though
10:24:19 <fresheyeball> > let t g f ys = mapMonotonic f (mapMonotonic g ys) == mapMonotonic (f . g) ys
10:24:22 <lambdabot>  <no location info>: error:
10:24:22 <lambdabot>      not an expression: â€˜let t g f ys = mapMonotonic f (mapMonotonic g ys) ==...
10:24:33 <fresheyeball> ugh, how we lambdabot this?
10:24:36 <glguy> fresheyeball: mapMonotonic doesn't work because it restricts to only working with monotonic functions
10:24:50 <glguy> fresheyeball: and Functor doesn't provide you that restriction
10:24:50 <fresheyeball> glguy: I know, but I want to prove it with an example
10:25:06 <fresheyeball> and I can't find an example
10:25:42 <fresheyeball> > let t g f ys = Data.Set.mapMonotonic f (Data.Set.mapMonotonic g ys) == Data.Set.mapMonotonic (f . g) ys
10:25:44 <lambdabot>  <no location info>: error:
10:25:44 <lambdabot>      not an expression: â€˜let t g f ys = Data.Set.mapMonotonic f (Data.Set.map...
10:25:48 <fresheyeball> grr
10:25:55 <glguy> fresheyeball: You need to use the 'valid' function in your test
10:26:01 <glguy> you're creating intermediate garbage values
10:26:09 <fresheyeball> glguy: look
10:26:11 <geekosaur> yoiu want @let for a definition, > takes expressions not definitions
10:26:23 <fresheyeball> I am trying to prove that mapMonotonic is not a valid implimentation for fmap
10:26:34 <glguy> Yeah, and you can do that by showing that it creates invalid sets
10:26:34 <glguy> done
10:26:37 <fresheyeball> to prove that I need to show a counter example to the Functor Composition law
10:26:55 <fresheyeball> no that is not good enough
10:27:00 <fresheyeball> because it relies on foldable
10:27:09 <Solonarv> simple: mapMonotonic requires that the function is monotonic. fmap cannot require anything of the function it receives
10:27:15 <glguy> Functor should work for all function arguments and this one doesn't
10:27:25 <fresheyeball> glguy: dude I KNOW
10:27:26 <glguy> and I don't konw where foldable came in
10:27:40 <fresheyeball> because showing the set is smaller does not mean the law is violated
10:27:44 <fresheyeball> smaller needs foldable
10:27:55 <Ariakenom> mapMonotonic is unsafe. fmap isn't. thus /=
10:28:13 <glguy> fresheyeball: OK, who said something about smaller?
10:28:22 <fresheyeball> look
10:28:36 <fresheyeball> please for now, stop explaining to me why fmap and mapMonotonic are different.
10:28:38 <fresheyeball> I know
10:28:46 <fresheyeball> I am trying to explain to a friend this very thing
10:28:59 <Ariakenom> ... which is why we're giving explanations?
10:29:04 <fresheyeball> and I want to do it by showing how `map f . map g = map (f . g)`
10:29:08 <fresheyeball> does not hold 
10:29:13 <fresheyeball> and to demonstrate that
10:29:21 <fresheyeball> I need 2 example functions and an example Set
10:29:25 <fresheyeball> and I can't find those examples
10:29:28 <Solonarv> you can't show it that way because it isn't true
10:29:41 <fresheyeball> right, I am trying to show it's NOT true
10:29:49 <fresheyeball> by having an example where it's NOT true
10:29:57 <Solonarv> you can't show 'map f . map g â‰  map (f . g)' because that is not true (AFAIK)
10:30:13 <fresheyeball> That statement is not true for Set
10:30:18 <fresheyeball> because Set is not a Functor
10:30:26 <fresheyeball> and we should be able to show that with an example
10:30:31 <glguy> fresheyeball: You're using the wrong approach to show that this is a bad implementation, so you won't be able to find the example
10:30:44 <fresheyeball> uh why not?
10:30:54 <fresheyeball> also seriously why can't we do this with lambdabot
10:31:06 <fresheyeball> if you can get my `t` function into lambdabot, I will show you what I mean
10:31:06 <geekosaur> I told you how to and why
10:31:17 <Solonarv> because mapMonotonic f . mapMonotonic g = mapMonotonic (f . g) is *true*
10:31:19 <geekosaur> your'e too busy arguing.
10:31:26 <Solonarv> so, good luck showing it's false
10:31:38 --- mode: glguy set +v Ferdirand
10:31:39 <fresheyeball> ok so if 
10:31:45 <geekosaur> my understanding is that monotonic is a *stronger* statement with a stronger precondition
10:31:58 <Ferdirand> i'm pretty sure you can write a fmap implementation for Set that respects the functor laws
10:32:04 <fresheyeball> lets say we defined fmap = mapMonotonic
10:32:09 <fresheyeball> we just broke fmap yes?
10:32:24 <Ferdirand> it's just that it type cannot fit the specification of haskell's Functor class because of the Ord constraint
10:32:27 <geekosaur> you mde a tightr precondition, yes
10:32:29 <glguy> yes, because now it can generate invalid maps for some inputs
10:32:35 <fresheyeball> ok cool
10:32:41 <Ferdirand> but you could define your own restricted functor class that only accepts Ord types
10:32:44 <geekosaur> that doesnt' mean it admits things it didn;'tbefore, it means it rejects things it used to admit
10:33:00 <fresheyeball> give me an example of 2 functions and a Set that prove that if we defined fmap = mapMonotonic, we have broken fmap
10:33:08 <geekosaur> so you're going in the wrong direction
10:33:11 <glguy> You don't need 2, you need one. negate
10:33:17 <fresheyeball> obviously this means that one of those functions must not be monotonic
10:33:19 <glguy> fmap negate will generate an invalid map
10:33:37 <Solonarv> try this:
10:33:37 <glguy> composition doesn't feature
10:34:00 <Solonarv> @let egSet = Data.Set.fromList [1,2,3]
10:34:02 <lambdabot>  Defined.
10:34:09 <fresheyeball> here
10:34:16 <Solonarv> > 3 `Data.Set.member` egSet
10:34:18 <lambdabot>  True
10:34:43 <Solonarv> > (-3) `Data.Set.member` (Data.Set.mapMonotonic negate egSet)
10:34:46 <lambdabot>  False
10:34:53 <fresheyeball> @let t f g xs = Data.Set.mapMonotonic f (Data.Set.mapMonotonic g xs) == Data.Set.mapMonotonic f xs
10:34:55 <lambdabot>  Defined.
10:34:58 <fresheyeball> there
10:35:07 <fresheyeball> > :t t
10:35:09 <lambdabot>  <hint>:1:1: error: parse error on input â€˜:â€™
10:35:24 <fresheyeball> @t t
10:35:25 <lambdabot> Maybe you meant: tell thank you thanks thesaurus thx tic-tac-toe ticker time todo todo-add todo-delete type v @ ? .
10:35:30 <Solonarv> do you see the problem? mapMonotonic produces a garbage set if the function isn't strictly nondecreaseing
10:35:34 <Ariakenom> Should mapMonotonic have a unsafe prefix?
10:35:42 <fresheyeball> Solonarv: yes I see the problem
10:35:47 <fresheyeball> here is what I want
10:35:54 <fresheyeball> please make `t` return False
10:35:57 <glguy> Ariakenom: No, it has monotonic in the name capturing the restriction
10:35:58 <fresheyeball> which we should be able to do
10:36:02 <glguy> fresheyeball: no, you can't
10:36:20 <fresheyeball> then Set is a Functor if the definition is just using the Functor laws
10:36:22 <glguy> (if you fixed t to use (f.g) in the second case)
10:36:34 <glguy> fresheyeball: No, fortunately that isn't enough
10:36:39 <crestfallen> hi I'm trying to check this type in ghci :
10:36:47 <fresheyeball> @let t f g xs = Data.Set.mapMonotonic f (Data.Set.mapMonotonic g xs) == Data.Set.mapMonotonic (f . g) xs
10:36:48 <lambdabot>  .L.hs:241:1: warning: [-Woverlapping-patterns]
10:36:48 <lambdabot>      Pattern match is redundant
10:36:49 <lambdabot>      In an equation for â€˜tâ€™: t f g xs = ...
10:36:54 <fresheyeball> @let t' f g xs = Data.Set.mapMonotonic f (Data.Set.mapMonotonic g xs) == Data.Set.mapMonotonic (f . g) xs
10:36:55 <lambdabot>  Defined.
10:36:56 <crestfallen> % :t ((<*>) @((->) _))
10:36:56 <yahb> crestfallen: (w -> a -> b) -> (w -> a) -> w -> b
10:37:02 <fresheyeball> glguy: good catch
10:37:13 <fresheyeball> lets get `t'` to return False
10:37:20 <glguy> fresheyeball: The law requires your function works for all functions and yours doesn't, it has a precondition of monotonicity
10:37:27 <crestfallen> so after importing Control.Applicative, I cannot use that command in ghci
10:37:38 <fresheyeball> glguy: I know, so lets get t' to return False by using a non-monotonic function
10:37:41 <Solonarv> crestfallen: you need to enable TypeApplications
10:37:47 <glguy> fresheyeball: you can't
10:38:07 <crestfallen> Solonarv, thanks let me try
10:38:08 <int-e> > t id id (S.fromList [0/0])
10:38:11 <lambdabot>  error:
10:38:11 <lambdabot>      Ambiguous occurrence â€˜tâ€™
10:38:11 <lambdabot>      It could refer to either â€˜Debug.SimpleReflect.tâ€™,
10:38:15 <int-e> > t' id id (S.fromList [0/0])
10:38:16 <fresheyeball> use `t'` please
10:38:17 <lambdabot>  False
10:38:18 <fresheyeball> ok
10:38:29 <fresheyeball> wonderful! 
10:38:33 <fresheyeball> int-e++
10:38:44 <Solonarv> that's just because the Eq instance for IEEE floats is broken
10:38:53 <fresheyeball> can I get an example using abs or mod?
10:38:56 <glguy> Solonarv: come on , fresheyeball is happy. let's move on
10:39:03 <Solonarv> > S.fromList [0/0] == S.fromList [0/0]
10:39:05 <lambdabot>  False
10:39:08 <Solonarv> see?
10:39:15 <int-e> > 0/0 == 0/0
10:39:18 <lambdabot>  False
10:39:21 <fresheyeball> Solonarv: I think that is not what I am looking for
10:39:24 <Ariakenom> Solonarv++
10:39:26 <fresheyeball> that looks like a happensance
10:39:29 <int-e> Solonarv: a broken Eq instance is the only way I see of doing this
10:39:34 <fresheyeball> ok
10:39:36 <Solonarv> int-e: I agree!
10:39:46 <fresheyeball> so if we can apply a function f to each member of a set
10:39:55 <EvanR> NaN /= NaN isnt broken... according to gospel
10:40:03 <fresheyeball> and satisfy both Functor Identity, and Functor Composition laws
10:40:07 <Solonarv> fresheyeball: just to make sure, do you know how Set is actually implemented?
10:40:13 <fresheyeball> then by what law is Set not a Functor?
10:40:25 <fresheyeball> Solonarv: it's complex
10:40:39 <Solonarv> by the law that says "if x is not garbage, then fmap f xs should also not be garbage"
10:40:51 <Ferdirand> it is a functor. a restricted one.
10:40:54 <Ariakenom> EvanR: is that true? as far as I could tell when I looked into it IEEE calls it a 4th ordering "unordered"
10:41:06 <EvanR> ordering...
10:41:07 <Ariakenom> (less, equal, greater, unordered)
10:41:15 <Solonarv> indeed; you can define a "restricted functor" type class, and Set can be an instance of that just fine
10:41:16 * hackage tldr 0.4.0.1 - Haskell tldr client  http://hackage.haskell.org/package/tldr-0.4.0.1 (psibi)
10:41:17 <Ferdirand> it's only unfortunate that haskell's default Functor class does not allow for restricted functors
10:41:24 <crestfallen> Solonarv, sorry I've never enabled anything. what is the command exactly please?
10:41:43 <Solonarv> crestfallen: :set -XTypeApplications
10:41:59 <EvanR> was responding to something about Eq
10:42:03 <crestfallen> thanks kindly your help the other day is paying off too Solonarv 
10:42:12 <fresheyeball> For the moment, lets pretend `mapMonotonic` was called, `mapUnsafe`
10:42:33 <fresheyeball> are we saying there is no Functor law broken by `mapUnsafe` as `fmap`?
10:42:54 <Solonarv> indeed, it does not break the laws for 'fmap'
10:43:27 <fresheyeball> Solonarv: so why is there no Functor instance for Set, given that mapUnsafe will satisfy the Functor laws?
10:43:39 <Solonarv> because the instance is bogus!
10:43:44 <Solonarv> you're getting back broken sets!
10:43:58 <fresheyeball> Solonarv: that is not true
10:44:05 <fresheyeball> we just sometimes get back smaller sets
10:44:05 <Solonarv> @let egSet' = mapMonotonic negate egSet
10:44:06 <lambdabot>  .L.hs:235:10: error:
10:44:06 <lambdabot>      â€¢ Variable not in scope:
10:44:07 <lambdabot>          mapMonotonic :: (Integer -> Integer) -> S.Set Integer -> t
10:44:16 <glguy> A user should expect to be able to provide any function. mapMonotonic doesn't check its argument because it only exists as an optimization where you've promised that your function is monotonic
10:44:19 <Solonarv> no, you never get back a smaller set
10:44:23 <int-e> > (-2) `S.member` S.mapMonotonic negate (S.fromList [1,2]) -- garbage
10:44:25 <lambdabot>  False
10:44:25 <crestfallen> one thing the @ char is required what do you call that in this case Solonarv ?
10:44:29 <Solonarv> which is in fact the problem
10:44:44 <Solonarv> crestfallen: idk, I just call it an '@'
10:44:58 <glguy> fresheyeball: You don't get smaller sets back, you get *invalid* ones back
10:45:02 <crestfallen> hah! thanks
10:45:10 <fresheyeball> I see
10:45:11 <glguy> one that don't behave like sets
10:45:14 <Solonarv> > egSet
10:45:16 <lambdabot>  fromList [1,2,3]
10:45:19 <fresheyeball> you mean we broke the underlying rep
10:45:41 <Solonarv> @let brokenSet1 = Data.Set.mapMonotonic (const 2) egSet
10:45:43 <lambdabot>  Defined.
10:45:46 <Solonarv> brokenSet1
10:46:01 <Solonarv> > brokenSet1
10:46:01 <fresheyeball> > brokenSet1
10:46:01 <EvanR> i wonder if anyone has figured out how to do something interesting with invalid sets caused by using the lib wrong
10:46:03 <lambdabot>  fromList [2,2,2]
10:46:03 <lambdabot>  fromList [2,2,2]
10:46:06 <fresheyeball> right
10:46:09 <EvanR> stupid set hacks
10:46:25 <fresheyeball> ok
10:46:32 <int-e> . o O ( irresponsibleMapMonotonic )
10:46:33 <fresheyeball> so I still need an example to demonstrate this
10:46:37 <fresheyeball> but point taken
10:46:57 <fresheyeball> if we imagine a version of `Data.Set.map` that does not have the Ord constraint
10:47:07 <fresheyeball> is there an example that we can falisfy the Functor laws?
10:47:20 <Solonarv> there can't be such a function
10:47:35 <Solonarv> in fact Data.Set.map *does* satisfy the laws for fmap
10:47:39 <koz_> MarcelineVQ: I gave it a once-over, but a lot of stuff I haven't read into deeply. Why do you ask?
10:48:03 <MarcelineVQ> koz_: Just wondering, read a bit last night and got stuck on essentialy the first question
10:48:12 <koz_> Which one?
10:48:17 <fresheyeball> Solonarv: ok, so how can I use the Functor laws to prove that Set is not a Functor?
10:48:20 <int-e> Broken Ord instances aside,  Set.map f . Set.map g == Set.map (f . g)  holds.
10:48:21 <koz_> (as in, what was it again?)
10:48:27 <MarcelineVQ> 1.4i I think
10:48:28 <MarcelineVQ> lemme see
10:48:34 <Solonarv> you can't! that's what we've been telling you all this time!
10:48:35 <fresheyeball> because just saying "we can't impliment it" is not satisfying 
10:48:46 <Ariakenom> You can say that a function written with only Functor in mind may break the Set.
10:48:51 <fresheyeball> we should be able to show that `map f . map g = map (f .g)` is false
10:49:23 <Solonarv> the reason is that 'fmap' must accept *any function* and produce a valid result, which isn't possible for Set
10:49:30 <EvanR> if you dont have an Ord instance, how would you construct the new set
10:49:35 <MarcelineVQ> koz_: Use Curryâ€“Howard to prove the exponent law that a b Ã— a c = a b+c . That is, provide a function of the type (b -> a) -> (c -> a) -> Either b c -> a   and one of   (Either b c -> a) -> (b -> a, c -> a)
10:49:36 <int-e> fresheyeball: You can define an "Ord" variant of Functor,  class FunctorOrd f where fmapOrd :: (Ord a, Ord b) => (a -> b) -> f a -> f b  and work with that
10:49:52 <fresheyeball> int-e: I think that IS a Functor though
10:50:01 <MarcelineVQ> erf, the superscript of the exponents didn't paste
10:50:16 <koz_> :t either
10:50:17 <lambdabot> (a -> c) -> (b -> c) -> Either a b -> c
10:50:21 <int-e> fresheyeball: but for a different category than the one captured by the Functor class
10:50:22 <Solonarv> or more generally: class CFunctor c f | f -> c where cfmap :: (c a, c b) => (a -> b) -> f a -> f b
10:50:33 <koz_> MarcelineVQ: ^ is half the answer right there.
10:51:09 <int-e> Solonarv: I was going to ask, is that defined anywhere?
10:51:10 <fresheyeball> I think `data OrdSet a = Ord a => OrdSet (Set a)`
10:51:12 <Solonarv> then you can write instance CFunctor Ord Set where cfmap = Data.Set.map
10:51:17 <fresheyeball> is a normal Haskell Functor no?
10:51:20 <Solonarv> int-e: feels like it might be somewhere
10:51:20 <int-e> fresheyeball: no
10:51:23 <Solonarv> fresheyeball: no
10:51:29 <lambdabot> no
10:51:34 <fresheyeball> ok, then I am confused
10:51:54 <glguy> fresheyeball: Try to implement the instance and see where you get stuck
10:51:57 <glguy> it's instructive
10:51:58 <MarcelineVQ> why though, why is the first function suggested either? the latter function fits the right side of the = but I don't see how the first fits the left side of =, if it's supposed to
10:52:07 <int-e> fresheyeball: try making a Functor instance for that and decipher the ensuing error(s).
10:52:35 <Solonarv> :t uncurry either
10:52:36 <lambdabot> (a -> c, b -> c) -> Either a b -> c
10:52:40 <Solonarv> MarcelineVQ: ^
10:53:11 <fresheyeball> int-e: ok
10:54:04 <int-e> fresheyeball: You'll find that for implementing  fmap (f :: a -> b) (os :: OrdSet a) :: OrdSet b, you'll be provided with an Ord instance for a, but you'll have to produce an Ord instance for b out of thin air.
10:54:26 <fresheyeball> int-e: ahhh that makes sense
10:54:44 <fresheyeball> I could do Invariant, but that's it
10:56:46 <Solonarv> aha! I found it: https://hackage.haskell.org/package/constrained-monads-0.5.0.0/docs/Control-Monad-Constrained.html#t:Functor
10:57:00 <int-e> Solonarv: thanks!
11:00:06 <fresheyeball> can I get an example of a type that is not a functor but where we can write a type checking instance of fmap.
11:01:58 <Solonarv> sure, one sec
11:03:14 <Solonarv> https://gist.github.com/Solonarv/391362e38cfd3f1d7198458ffd9deabe -- fresheyeball
11:03:44 <MarcelineVQ> Solonarv, koz_ : My issue, I think, is given (a^b) x (a^c) = a^(b+c) why is the question: provide  (b -> a) -> (c -> a) -> Either b c -> a  and  (Either b c -> a) -> (b -> a, c -> a)  and not provide  ((b -> a), (c -> a)) -> Either b c -> a  and (Either b c -> a) -> (b -> a, c -> a)  
11:03:52 <MarcelineVQ> which doesn't read so good in irc :X
11:04:36 <Solonarv> MarcelineVQ: because 'x -> y -> z' and '(x, y) -> z' are equivalent, and the former reads more nicely
11:05:01 <Solonarv> (the isomorphism is given by 'curry' and 'uncurry')
11:06:48 <fresheyeball> Solonarv++
11:06:55 <fresheyeball> Thank you
11:19:44 <MarcelineVQ> I see, thank you Solonarv, koz_  I had to consult exponent rules to make sense of the relations there. As functions it's pretty obvious stuff, but I wanted to relate that to the algebra form and didn't really recall the equivalencies of exponents
11:20:05 <koz_> MarcelineVQ: No worries.
11:21:49 <MarcelineVQ> Might have made more sense to not do two steps of form changing as the first question though hehe, comparatively the other questions are entirely direct. Actually, looking at them, the other two questions are about the components that would have solved this one, huh... strange order
11:26:41 <cemerick> question about how laziness interacts with (nested) pattern matching: if I have a definition like `foo a@A{b = B c d} = ...` how should I think about when those bindings are realized? i.e. if I use `c` and then `d` in the body of the function, will the access of `d` need to re-traverse a->b even though that was required by the access of `c`?
11:28:05 <Solonarv> no, it won't
11:28:30 <geekosaur> those bindings are strict and done "all at once". or did you mean with ~ ?
11:28:31 <koz_> MarcelineVQ: Prod isovector about it.
11:28:44 <koz_> By email or on the (rare) occasions they're around here.
11:29:10 <Solonarv> AFAIK that code turns into (roughly): 'foo a = case a of A{b = b'} -> case b' of B c d -> ...'
11:30:21 <MarcelineVQ> koz_: book is final form for the moment so it doesn't matter much just now but thank you, could be useful for a revision or edition update
11:30:36 <koz_> MarcelineVQ: It's a digital publication, and new releases are always possible.
11:30:50 <koz_> I'd advise letting isovector know anyway.
11:33:00 <cemerick> geekosaur: nope, as-is, thanks
11:33:08 <cemerick> Solonarv: that is very clarifying, thanks
11:33:17 <MarcelineVQ> dear isovector, some readers of yours are too dumb to remember a^b^c is a^(b*c). Consider asking the question you wrote about a^(b*c) before the one that uses it implicitly   Sincerely, some cool kid.   :>
11:33:34 <koz_> MarcelineVQ: Lol, yeah, email that.
11:33:51 <koz_> (also, good on you for supporting isovector)
11:35:35 <MarcelineVQ> Haskell is the most mature general programming language with sexy types so it would probably be a bad idea for people not to get this book
11:36:35 <koz_> Yeah, but it's still good to see.
11:36:50 <lavalike> MarcelineVQ: only one of the two possible 'a^b^c's is a^(b*c) too (:
11:36:52 <koz_> It makes a great companion to jle`'s Introduction to Singletons.
11:40:20 <govno> What usefull I can read about Haskell Type System?
11:40:49 <koz_> govno: What do you want to know?
11:41:11 <govno> koz_: System F
11:42:41 <monochrom> System F is not Haskell type system.
11:45:00 <geekosaur> Haskell's type system is Hindley-Milner with typeclasses. System Fc is used internally by ghc but is not exposed; although some extensions partially expose some aspects of it.
11:45:47 <Welkin> I thought it was system f-omega
11:46:00 <Solonarv> yeah I thought it was System FÏ‰
11:46:07 <geekosaur> hm, maybe
11:46:24 <govno> ok, thanks
11:46:50 <geekosaur> in any case, one thing in it that is not exposed is geenral type lambdas: they exist at the level of ghc core, but cannot be written in user code
11:47:23 <Welkin> MarcelineVQ: you are writing a book?
11:47:48 <MarcelineVQ> No, sandy maguire did
11:47:55 <monochrom> GHC is walking the fine line between getting closer to dependent typing and yet forbidding downright type lambda.
11:47:58 <Welkin> on what?
11:48:08 <MarcelineVQ> modern type-level programming in haskell
11:48:17 <Welkin> is there a link?
11:48:40 <MarcelineVQ> https://leanpub.com/thinking-with-types
11:48:44 <Welkin> type-level programming is one of the things I still have not gotten into yet with haskell
11:48:51 <Welkin> I haven't found a reason (yet)
11:49:17 <int-e> > 2^1^0 == 2^(1*0)
11:49:19 <lambdabot>  False
11:49:29 <int-e> > (2^1)^0 == 2^(1*0)
11:49:30 <monochrom> I visualize that as trying to swim without getting wet.
11:49:31 <lambdabot>  True
11:50:03 <Welkin> I have been programming just fine without it for some years now
11:53:16 <MarcelineVQ> int-e: thank you for that, I see I've made an error in bracketing, as well as reading the rule.  lavalike: I didn't understand your comment, were you hinting at what int-e did?
11:54:26 <lavalike> MarcelineVQ: no just saying that a^(b^c) /= (a^b)^c
11:57:22 <Welkin> (a^b)^c = a^(bc)
12:11:10 --- mode: glguy set +v fen
12:26:22 <Zemyla> Oh, I figured out why Stack was dying. I added swap space to my AWS instance, and it stopped.
12:28:46 * hackage unicode-transforms 0.3.5 - Unicode normalization  http://hackage.haskell.org/package/unicode-transforms-0.3.5 (harendra)
12:56:01 <fen> is accessing the nth value in an m-tuple (m >= n) faster than accessing the nth value in a list of length m ?
12:56:22 <geekosaur> yes
12:57:12 <geekosaur> you can think of a tuple as an anonymous record (anonymous product if we're being formal). lists are singly-linked lists
12:58:37 <Welkin> records *are* tuples :D
12:58:53 <Welkin> named tuples*
13:00:44 --- mode: glguy set +v ratzes
13:01:04 <monochrom> "Anonymous record" is usually anonymous on the record type itself but still have named fields.
13:02:03 <monochrom> So overall there is outer anonymity and inner anonymity.
13:02:45 <Zemyla> Why is Kleisli not a Functor?
13:03:08 <ratzes> Hi, if I want to use the CUDA compiler (nvcc) to generate c code to link into ghc, what would be the intended way to do that with Stack?
13:03:51 <lavalike> :t Kleisli
13:03:52 <lambdabot> (a -> m b) -> Kleisli m a b
13:04:02 <Zemyla> It really should be a Functor, Applicative, Alternative, Monad, MonadPlus, MonadFix, MonadZip, and so on.
13:06:07 <monochrom> Kleisli is quite the wrong kind.
13:06:24 <monochrom> Like, * -> * -> *, not * -> *
13:07:50 <lavalike> isn't it many of those things through https://www.stackage.org/haddock/lts-12.18/base-4.11.1.0/Control-Arrow.html#t:ArrowMonad
13:09:02 <Zemyla> It should have like instance Functor m => Functor (Kleisli m e).
13:09:11 <fen> Welkin. there are some choices of how to store references to edges of a graph at a node. one way is e.g. (Int -> Graph), another is [Graph] and finally a tuple of Graphs, which can be unwound at the datatype decleration, e.g. if 4 edges per node data Graph a = Graph {value :: a,up :: Graph a,down :: Graph  a,left :: Graph a,right :: Graph a}
13:10:15 <fen> it seems like these names give, like a tuple would, faster access than a list would
13:13:10 <fen> shame this only seems available for homogeneous graphs
13:13:48 <fen> why not have a O(1) version of list with names for every element?
13:14:21 <fen> data List a = List {list1 :: Maybe a,list2 :: Maybe a ... }
13:14:31 <fen> or is that wrong?
13:15:05 <Welkin> what
13:15:08 <fen> could have like ListN where this only went up to listN :: Maybe a }
13:15:25 <zachk> fen, lens maybe?
13:15:30 <zachk> @type _1 
13:15:31 <lambdabot> (Functor f, Field1 s t a b) => (a -> f b) -> s -> f t
13:15:32 <fen> Welkin: the Maybe is there in case it fails to contain an element
13:15:55 <fen> zachk: its not clear how that works
13:16:14 <fen> Welkin: or are you asking about the Graphs?
13:16:33 <zachk> fen like this:
13:16:35 <zachk> > (1,2) ^. _1
13:16:37 <lambdabot>  1
13:16:38 <zachk> > (1,2) ^. _2
13:16:40 <lambdabot>  2
13:16:42 <zachk> > (1,2) ^. _3
13:16:45 <lambdabot>  error:
13:16:45 <lambdabot>      â€¢ Could not deduce (Field3 (a0, b0) (a0, b0) b1 b1)
13:16:45 <lambdabot>        from the context: (Field3 (a, b) (a, b) b1 b1, Num b, Num a)
13:16:57 <zachk> > (1,2) ^? _3
13:16:59 <lambdabot>  error:
13:16:59 <lambdabot>      â€¢ Could not deduce (Field3 (a0, b0) (a0, b0) b1 b1)
13:16:59 <lambdabot>        from the context: (Field3 (a, b) (a, b) b1 b1, Num b, Num a)
13:17:57 <fen> not how its used, how its implemented
13:18:06 <zachk> > let x = [1,2,3] in (x ^? _1, x ^? _2, x ^? _3 , x ^? _4) 
13:18:08 <lambdabot>  error:
13:18:08 <lambdabot>      â€¢ Could not deduce (Field4 [a0] [a0] b3 b3)
13:18:08 <lambdabot>        from the context: (Field4 [a6] [a6] b3 b3, Num a6, Num a5, Num a4,
13:18:10 <geekosaur> fen, there's a Vector type if you want an indexed clection
13:18:14 <fen> no
13:18:14 <zachk> fen,  not sure 
13:18:24 <zachk> but it seems to fail if the field isn't available
13:18:32 <zachk> > let x = [1,2,3] in (x ^? _1, x ^? _2, x ^? _3) 
13:18:34 <lambdabot>  error:
13:18:35 <lambdabot>      â€¢ Could not deduce (Field3 [a0] [a0] b2 b2)
13:18:35 <lambdabot>        from the context: (Field3 [a4] [a4] b2 b2, Num a4, Num a3, Num a,
13:18:38 <zachk> I thought it didnt 
13:19:25 <fen> data Vector a = Vector {-# UNPACK #-} !Int                        {-# UNPACK #-} !Int                        {-# UNPACK #-} !(Array a)         deriving ( Typeable )
13:19:29 <fen> whats all that about?
13:19:36 <fen> are those !Int like names?
13:20:06 <koz_> Strict fields.
13:20:23 <fen> like records accessors?
13:20:32 <koz_> fen: No.
13:20:37 <fen> then how is it fast?
13:20:41 <zachk> > let x = [1,2,3] in (x ^? element 0, x ^? element 1, x ^? element 2 , x ^? element 3) 
13:20:44 <lambdabot>  (Just 1,Just 2,Just 3,Nothing)
13:20:47 <fen> or as fast as tuples
13:20:50 <koz_> fen: https://wiki.haskell.org/Performance/Data_types#Strict_fields
13:20:55 <zachk> > let x = (1,2,3) in (x ^? element 0, x ^? element 1, x ^? element 2 , x ^? element 3) 
13:20:58 <lambdabot>  error:
13:20:58 <lambdabot>      â€¢ Could not deduce (Traversable ((,,) Integer Integer))
13:20:58 <lambdabot>          arising from a use of â€˜elementâ€™
13:21:04 <zachk> guess that doesnt work :(
13:21:04 <fen> or even slightly relevant to this approach 
13:21:25 <fen> zachk, it also does nothing to explain how they are implemented
13:21:53 <zachk> not sure how lens are implemented, I just use them sometimes, there implementation can be quite complex
13:21:55 <fen> if its just a partial traversal like (!!) then its slow
13:22:18 <geekosaur> allocated size, current size, lower level array, all strict and unpacked into a linear format in memory (iirc)
13:22:23 <fen> (just suspecting this because Lens seems to be something to do with Traversal)
13:22:47 <fen> geekousar: does this mean its accessed as fast as a records names?
13:23:01 <geekosaur> yes
13:23:06 <fen> like, is that the information the compiler needs to find it in memory
13:23:18 <fen> and thats somehow what records names do
13:23:55 <c_wraith> it's not a complex process. you just have a table for each data type mapping selectors to offsets.
13:24:04 <c_wraith> easy for the compiler to build.
13:24:07 <fen> hmm, if thats the case it does seem fast. wonder what all the hype about structured access was about
13:24:17 <geekosaur> a named field (or an unnamed positional in a tuple) compiles to a constant offset. for a vector it has to compute the offset, but it can do that quickly; I think there's a slight slowdown to check that the index is valid
13:24:26 <fen> probably some misconception about trees of bounded balance 
13:25:03 <geekosaur> and lenses are rather more than just traversals.
13:26:21 <fen> but these _2 types are fast like records names and vectors?
13:26:35 <c_wraith> _2 is a value.
13:26:39 <fen> sorry not type
13:26:41 <fen> type thing
13:26:46 <fen> things
13:27:25 <fen> well, when they are used to direct an update..
13:27:29 <fen> no, a retrival
13:27:37 <c_wraith> they usually compile down to same thing as the direct obvious representation when enough information is available at compile time.
13:27:57 <fen> does that mean the zippers are slow
13:28:01 <fen> in comparison
13:28:26 <fen> like, there was hope that carving a path lazily through a high dimensional grid was fastest with zippers
13:28:37 <fen> maybe its just less memory intensive or something
13:28:41 <c_wraith> zippers do such different things that I'm not sure how to answer that.
13:29:08 <geekosaur> what e.g. _2 does is determined via typeclass; it'll be slow if the type is
13:29:24 <fen> just cant understand how this O(1) access could work, and if it does, how it could be improved upon
13:29:24 <geekosaur> type of the value it's applied to
13:29:44 <fen> ok, just wrt Vectors then
13:29:59 <c_wraith> a zipper of a vector is mostly useless.
13:30:09 <fen> no Zippers of lists
13:30:15 <fen> compared to vectors
13:30:28 <fen> (high dimensional nestings via type Indexed free for example)
13:30:31 <c_wraith> why would you do that comparison? they're entirely different.
13:30:34 <koz_> To echo c_wraith: There's not much point having zippers of vectors.
13:30:54 <koz_> They're a different structure, useful for different things.
13:31:01 <fen> not zippers of lists compared to zippers of vectors. zippers of lists compared to vectors 
13:31:11 <koz_> fen: That's an even more odd comparison.
13:31:13 <c_wraith> yes. why would you compare those?
13:31:21 <c_wraith> they are entirely different
13:31:28 <c_wraith> they share no use cases
13:31:30 <fen> ok. consider something like a high dimensional firework
13:31:31 <geekosaur> why do I feel like this is a theoretician being boggled by hardware implementation?
13:31:48 <koz_> fen: What is a 'high dimensional firework'?
13:31:59 <fen> it goes some way into the high dimensional space, then affects a ball around it
13:32:07 <monochrom> I only see an armchair philosopher.
13:32:29 <fen> a high dimensional armchair?
13:32:44 <hodapp> my armchair only has 4 dimensions, like God intended
13:32:55 <c_wraith> no, high dimensions mean it's hard to be far from the things I want to be far from.
13:33:18 <fen> the idea was that because the zipper has access to its immediately adjacent neighbours in each dimension, it can quickly update them 
13:33:19 <monochrom> That's why it's hard to be far from armchair philosophers!
13:33:24 <fen> (a stencil convolution)
13:33:52 <hodapp> but the more dimensions, the more and more likely that two randomly-chosen vectors are orthogonal
13:33:56 <hodapp> which explains why you make no bloody sense
13:34:06 <koz_> fen: If you're after stencils for arrays, massiv has support for that.
13:34:06 <fen> a vector would have to access very very many locations that are close, as the dimension incrases
13:34:11 <c_wraith> fen, it's not immediately obvious how to make zippers work in multiple dimensions.
13:34:17 <koz_> (and pretty efficient support at that)
13:34:52 <fen> c_wraith. preparing a presentation of that
13:34:59 <fen> there are a few ways to explore
13:35:31 <c_wraith> no, I mean - it's easy to create logical inconsistencies with them. it's hard to avoid it while retaining any efficiency
13:35:36 <zachk> fen, by vectors you mean like arrays? or Nat indexed linked lists?
13:35:49 <fen> currently working on extending this approach to higher dimensions; https://bpaste.net/show/66e90aa344f3
13:36:19 <c_wraith> yeah, that approach doesn't extend to a 2d grid.
13:36:20 <fen> either by nesting, or collapsing this nesting at the datatype. but that gives very many named accessors for the edges
13:36:42 <fen> eg in 2d data Graph a = Graph {value :: a,up :: Graph a,down :: Graph  a,left :: Graph a,right :: Graph a}
13:37:12 <fen> zachk: yeah, lets call them arrays
13:37:25 <fen> c_wraith. sure it does
13:37:27 <koz_> fen: If _arrays_ interest you, look up how massiv does stencilling.
13:37:30 <zachk> arrays are pretty fast, usually 
13:37:49 <koz_> Since performance for stuff like stencilling is a big part of massiv's whole raison d'etre.
13:37:54 <fen> koz_ does it just use zip like repa?
13:38:08 <koz_> fen: I haven't looked into it personally.
13:38:25 <koz_> I'm saying that your combination of (stated) concerns appears similar to massiv's.
13:38:34 <koz_> Thus, the approach taken by massiv might be of interest to you.
13:39:20 <fen> yeah, it uses arrays
13:39:41 <fen> its just like generating the indexes for the surrounding values and accessing the array there
13:39:52 <fen> that was the described approach above
13:39:56 <fen> nice to see it implemented
13:40:37 <fen> but how to tell without benchmarking (which is fairly opaque) how this would compare with the Graph way
13:40:41 <fen> of traversing edges
13:40:42 <c_wraith> fen, your Graph type is incompatible with that style of zipper.
13:40:48 <fen> to get to the surrounding values
13:40:54 <fen> c_wrait
13:40:56 <fen> sorry
13:41:07 <fen> not zipper
13:41:09 <fen> pointer 
13:41:15 <fen> just for a name
13:41:23 <c_wraith> your Graph type has tons of assumptions baked in, like up (down x) == x
13:41:24 <fen> it differs from nested zippers sure
13:41:38 <fen> its the "collapsed into the datatype" version
13:42:01 <fen> c_wraith. yes, thats how the cyclic referencing works
13:42:11 <c_wraith> that assumption isn't compatible with how zippers works.
13:42:17 <fen> its not zippers
13:42:23 <fen> its an alternative
13:42:50 <c_wraith> anyway - that type is basically useless in Haskell.
13:43:02 <c_wraith> you're better off with an array
13:43:02 <fen> where rather than using FreeN (type indexed Nat for depth of Free), this is expanded out into the datatype directly
13:43:12 <fen> how is it useless!?
13:43:15 <fen> its great
13:43:19 <c_wraith> immutability.
13:43:30 <fen> does cyclic boundary conditions great 
13:43:43 <fen> and its immutable! what more could you want!?
13:43:44 <c_wraith> if you want to change any node, you have to copy the whole graph
13:43:58 <c_wraith> you might as well just use an array
13:44:17 <fen> thats not so bad. you only need to update them as you access them
13:44:18 <fen> lazy
13:44:24 <c_wraith> same properties, but way more compact in memory and runtime efficient
13:44:38 <fen> and for eg comonad you update them all anyway. and then just access them as needed
13:44:57 <c_wraith> the performance characteristics there get very bad.
13:45:00 <koz_> Arrays can also be 'delayed' for even more efficiency if you need to do lotsa changes followed by lotsa queries.
13:45:08 <koz_> (this is something massiv _also_ does to fairly great effect)
13:45:08 <fen> only rarely actually making the full global update compute. actually, only when doing another full global update!
13:45:29 <Welkin> what is with warp? No matter what I do it won't use http2
13:45:49 <Welkin> I have ssl running now on the proxy
13:46:15 <c_wraith> http2 requires the server itself to do TLS
13:46:23 <Welkin> damn it
13:46:28 <Welkin> that is a huge pain
13:46:37 <Welkin> how do I even get that to work through a reverse proxy?
13:46:44 <Welkin> also, warp doesn't handle ssl
13:46:50 <c_wraith> it's fine to do http2 to the proxy and http1 to the app servers
13:46:56 <fen> c_wraith: your sure that accessing the same value many times at any point in an array is as fast as navigating a list to that value and just accessing it there many times
13:47:00 <c_wraith> (1.2)
13:47:02 <Welkin> that is what I am doing
13:47:13 <Welkin> but it is still responding with http1.1
13:47:43 <c_wraith> look at what the proxy needs, not what warp needs.
13:47:45 <fen> should I benchmark that?
13:47:55 <c_wraith> it's the thing using http2 or not.
13:48:00 <fen> or is the "firework" test needed?
13:48:33 <geekosaur> you've been told how it works. if that didn't make sense, benchmark it. then try to figure out why.
13:48:58 <Welkin> wow! all I had to do was add a `http2 directive to the nginx config
13:49:06 <fen> again. accessing a value at ANY point. over and over. compared to the "head" style zipper 
13:49:09 <Welkin> didn't know that
13:49:17 <Welkin> thanks c_wraith 
13:49:28 <fen> yes, what has been said seems to surgest the array version would actually be faster. as it wouldnt need to navigate the zipper
13:49:38 <c_wraith> Welkin, glad it was easy. :)
13:49:53 <geekosaur> [16 21:23:49] <geekosaur> a named field (or an unnamed positional in a tuple) compiles to a constant offset. for a vector it has to compute the offset, but it can do that quickly; I think there's a slight slowdown to check that the index is valid
13:50:08 <fen> and ok, so something else might make this repeated access be fast for another reason if its memoised
13:50:18 <fen> so the firework test seems the best way
13:50:47 <fen> as in high dimensions this has many *different* values which are close, so if there is any speed up it should be apparent then
13:52:08 <fen> if this ends up being about hardware it would be annoying as its difficult to understand
13:52:34 <fen> but it seems like these direct locations in memory require that
13:53:06 --- mode: glguy set +v stphrolland
13:53:15 <fen> might have to benchmark very carefully 
13:53:36 <fen> might be only a slight difference
13:53:49 <fen> but if it *is* faster. that would be great
13:53:57 <fen> and i cant see why it shouldnt be
13:54:17 <fen> now either thats a stupid mistake, or a misunderstanding or something
13:54:36 <fen> but at this point it seems like only a benchmark can tell
14:48:01 <buttons840> is there a way I can prevent a ForeignPtr from running it's finalizer?
14:48:15 <buttons840> its*
14:51:10 <buttons840> maybe stick the ForeignPtr in a global list? mine as well double down on leaking memory if I'm going to be doing it anyway
14:51:42 <dmj`> Anyone know off hand the hsc2hs syntax for converting an enum into a Haskell ADT automatically
14:52:16 <buttons840> dmj`: haven't done it myself, but you've seen https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/utils.html right?
14:52:21 <buttons840> #enum?
14:53:01 <buttons840> dmj`: there's also https://en.wikibooks.org/wiki/Haskell/FFI#Enumerations
14:54:54 <dmj`> buttons840: yes, have looked at both, the latter is for converting #define's into newtype over an Int, not generating a Haskell ADT from a typedef enum { fieldA = 0, fieldB } your_type;
14:56:02 <Solonarv> buttons840: 
14:56:12 <Solonarv> do you want to completely remove the finalizer, or delay it?
14:57:20 <buttons840> Solonarv: remove, or delay indefinately
14:58:11 <zachk> wow does building lens take awhile :(
14:58:14 <geekosaur> dmj`, I don't think there is one. c2hs may have one
14:58:47 <Solonarv> if you're in control of the finalizer being registered, you can just have it look at some reference cell and do-nothing
14:59:09 <dmj`> geekosaur: was afraid of that, I got excited because someone posted syntax from a presentation that looked like this
14:59:11 <dmj`> geekosaur: { #enum c_typedef as HaskellType {underscoreToCase} deriving (Show,Eq) }
14:59:27 <Solonarv> otherwise the cleanest solution is probably pattern-matching on the ForeignPtr and removing the finalizer manually?
14:59:32 <geekosaur> you might ask them if they have a custom template
14:59:32 <MarcelineVQ> didn't c_wraith say something about this subject of finalizers not hours ago
14:59:45 <zachk> is there anyway to "lock" a TVar so everyone else will block when trying to access it?
14:59:52 <geekosaur> they mentioned touchForeignPtr, which isn't indefinite delay
14:59:58 <buttons840> Solonarv: yeah, the problem is I have an "Image" struct from a C lib, and I have a nice loadImage function that sets up finalizers and everything works, except for one of the libs functions gets mad if I free an image passed to it
15:00:29 <dmj`> geekosaur: oh, from the docs, "this construct does not emit the type definition itself." 
15:00:30 <Solonarv> if that's the issue you should really be using touchForeignPtr
15:00:46 <Solonarv> (use it *after* the call to the function)
15:00:53 <geekosaur> yeh
15:02:06 <buttons840> Solonarv: yeah, I forked a thread that sleep (or "delays") and that sort of work until the timer runs out -- should I just turn the timer way up?
15:02:20 <buttons840> perhaps until after the heat death of the universe?
15:02:36 <Solonarv> I suppose that could work, but it doesn't really seem any better than sticking them in a global
15:03:04 <Solonarv> it's not like a long-running thread takes up /less/ memory than a list entry
15:03:17 <buttons840> Solonarv: true, i'll go with the global, it's simpler -- my next question though is should I use an IORef, and how can I "initialize" a global IORef?
15:03:48 <dmj`> geekosaur: yea, must be a custom template then
15:04:09 <Solonarv> you might be better off with an MVar or TVar than an IORef, but that shouldn't be hard to change anyway
15:04:46 <buttons840> Solonarv: nevermind, I guess IORef just takes a value with it's constructor, so I can just do like "evilGlobals = IORef []" at the top of a module?
15:04:56 <Solonarv> as for how to initialize a global ref:
15:04:56 <Solonarv> myGlobal = unsafePerformIO (newIORef initialValue)
15:04:56 <Solonarv> {-# NOINLINE myGlobal #-}
15:05:02 <Solonarv> there's no IORef constructor
15:05:21 <buttons840> you're right
15:05:47 <Solonarv> the NOINLINE is *necessary*, if that gets inlined you might end up creating separate IORefs
15:07:42 <buttons840> Solonarv: ty
15:16:30 <dmj`> geekosaur: it's actually c2hs
15:18:06 <dmj`> It seems there's no way to bind a C enum with Haskell sum using hsc2hs. 
15:20:16 * hackage multilinear 0.3.1.0 - Comprehensive and efficient (multi)linear algebra implementation.  http://hackage.haskell.org/package/multilinear-0.3.1.0 (ArturB)
15:31:56 <buttons840> Solonarv: it's not working, do you know if GHC will garbage collect the IORef once it can see that it wont be used anymore?
15:32:30 <Solonarv> maybe? I guess you could try sticking a usage at the end of 'main'
15:33:25 <buttons840> Solonarv: i'll try
15:35:25 <buttons840> Solonarv: yep, calling the setWindowIcon function with a completely different arg at the end (after the "main loop") fixes it -- i guess GHC saw that function was the only thing using the IORef and that function wasn't ever called again
15:37:45 <buttons840> can I prevent the garbage collection? I could fork a thread to keep the IORef around similar to what I was doing before? have a better idea?
15:38:37 <c_wraith> I'm confused how two people are asking essentially the same question but on totally different use cases today.
15:38:47 * Solonarv shrug
15:39:20 <buttons840> c_wraith: what question? mine?
15:39:48 <c_wraith> yep. someone else was asking how to keep a value from getting finalized too early before.
15:40:14 <buttons840> heh, yeah, seems like a weird request
15:41:33 <geekosaur> brainwaves >.>
15:41:47 <c_wraith> I'm sticking with "please try to find a different design if it's at all possible". and "if not, there's https://hackage.haskell.org/package/base-4.12.0.0/docs/GHC-Exts.html#v:touch-35- " and then "yes, that should scare you" 
15:42:50 <geekosaur> this sounds like gtk, so there probably isn't one; it exepcts C manual memory mismanagement
15:44:22 <buttons840> geekosaur: it's raylib....  running on gtk ;)
15:47:16 * hackage simple-log 0.9.10 - Simple log for Haskell  http://hackage.haskell.org/package/simple-log-0.9.10 (AlexandrRuchkin)
15:47:36 <buttons840> IMO if the ForeignPtr interface allows us to create ForeignPtrs without finalizers, and later add them, it should also provide a way to remove them -- not complaining, just wonder if anyone else thinks that's reasonable?
15:47:53 <buttons840> (remove finalizers that is)
15:49:00 <zachk> couldn't you just readd a finalizer that is a null finalizer?
15:49:37 <buttons840> zachk: good idea, i'm not sure, does adding a new finalizer overwrite what's already there?
15:49:50 <geekosaur> no, it adds
15:50:07 <geekosaur> youd' need a new api that gave you some kind of finalizer handle so you coudl refer to it for removal later
15:50:17 <geekosaur> and then convine libraries to give you that handle
15:50:27 <c_wraith> yeah, the problem is identifying which handler to remove
15:52:40 <buttons840> geekosaur: you could just clobber all the existing finalizers and trust that people don't abuse it, but i guess that's breaking a lot of the safety in the existing API
15:53:30 <buttons840> c_wraith: what is GHC.Exts? why did you suggested it related to this problem I'm asking about?
15:54:58 <geekosaur> it's the official location for ghc internals
15:55:21 <geekosaur> and what they pointed at was something exorted from it
15:56:22 <buttons840> oh right, "#v:touch-35" but my browser just goes to the top of the page
15:56:41 <geekosaur> the - at the end is part of it
15:56:58 <geekosaur> sadly it's not documented, at least not there
15:57:15 <geekosaur> it's the prmitive underlying touchForeignPtr
16:06:03 <buttons840> i will just malloc and leak the memory in my C wrapper, then I can leave my ForeignPtrs alone in Haskell -- if I want to leak memory, I should have though of C to begin with ;)
16:15:01 <zachk> how come when I cabal install, and reinstall a program, it modifies the file twice?
16:18:06 <koz_> What's the right way to unwrap a Down a? There doesn't seem to be anything like 'getDown'.
16:19:11 <koz_> (well, _other_ than 'coerce')
16:21:16 <MarcelineVQ> often it's used with an idiom that doesn't keep the wrapped value beyond the comparison, eg.   comparing
16:21:50 <MarcelineVQ> though getDown should exist for the sake of humor if for no other reason
16:21:52 <geekosaur> ^ and the constructor is exported, so pattern match
16:23:00 <ion> > case Down 42 of (Down x) -> x
16:23:02 <lambdabot>  42
16:24:04 <ion> > let getDown (Down x) = x in getDown "tonight"
16:24:06 <lambdabot>  error:
16:24:06 <lambdabot>      â€¢ Couldn't match expected type â€˜Down aâ€™ with actual type â€˜[Char]â€™
16:24:06 <lambdabot>      â€¢ In the first argument of â€˜getDownâ€™, namely â€˜"tonight"â€™
16:24:28 <ion> Whoops. Well, you see what I mean.
16:25:16 <hpc> do {a little dance}
16:26:55 <Solonarv> huh, it isn't 'newtype Down a = Down { getDown :: a}' ? that's weird
16:29:50 <ion> > (coerce :: Down Integer -> Integer) (Down 42)
16:29:52 <lambdabot>  error:
16:29:52 <lambdabot>      â€¢ Variable not in scope: coerce :: Down Integer -> Integer
16:29:52 <lambdabot>      â€¢ Perhaps you meant â€˜coercedâ€™ (imported from Control.Lens)
16:31:22 <Solonarv> @let import Data.Coerce
16:31:23 <lambdabot>  .L.hs:80:1: error:
16:31:23 <lambdabot>      Data.Coerce: Can't be safely imported!
16:31:23 <lambdabot>      The module itself isn't safe.
16:31:53 <Solonarv> Pah! Cowardly lambdabot
16:32:54 <Solonarv> % coerce @_ @Int (Down @Int 42)
16:32:54 <yahb> Solonarv: 42
16:33:53 <Solonarv> ion: see above
16:34:04 <Solonarv> lambdabot doesn't have coerce ;)
16:37:19 --- mode: glguy set +v fen
16:37:41 <fen> is the array memory address approach like C ?
16:37:57 <fen> or are there better ways that are more commonly used?
16:38:14 <Welkin> only for a CArray
16:38:25 <Welkin> Arrays are boxed
16:38:44 <fen> the haskell C interface does that memory access right?
16:38:55 <fen> after compilation
16:39:12 <Welkin> what?
16:39:24 <Welkin> here is what you need for working with c arrays https://hackage.haskell.org/package/base-4.12.0.0/docs/Foreign-Marshal-Array.html#v:mallocArray
16:40:01 <fen> all the memory allocation is done by the compiler usually right?
16:40:37 <fen> that link is for a direct interface to C arrays
16:40:42 <Welkin> it literally calls `malloc` in C
16:40:54 <Welkin> https://hackage.haskell.org/package/base-4.12.0.0/docs/src/Foreign.Marshal.Alloc.html#mallocBytes
16:41:16 <fen> but normally an array, or any other haskell code allocates memory by at the backend to C
16:41:34 <Welkin> no
16:41:45 <Welkin> haskell compiles to machine code
16:41:49 <fen> oh
16:41:59 <fen> so it does memory allocation directly
16:42:04 <fen> thanks
16:42:06 <Welkin> https://github.com/takenobu-hs/haskell-ghc-illustrated
16:42:10 <Welkin> there is a guide
16:42:35 <fen> hmm, so is the style of this allocation similar to C?
16:42:46 <Welkin> I have no idea
16:43:02 <Welkin> here is another tool https://github.com/quchen/stgi
16:43:10 <fen> in terms of calculating memory addresses etc
16:43:13 <Welkin> I haven't learned much about the compiler
16:43:24 <Welkin> there are guides and tools you can use to do so though, like those I posted
16:43:33 <Welkin> or reading the ghc user guide
16:44:32 <fen> yeah, garbage collection and the haskell heap and stuff is kind of complicated, and thankfully the high level paradigm saves the user from these considerations
16:46:54 <greymalkin> License question: Since haskell doesn't have truly (binary) shared libraries, what is the intended distribution mechanism for LGPL libraries?
16:47:10 <fen> but, it would be interesting if the arithmetic used to calculate addresses and whatever else is used in the interface to hardware memory is similar to the standard stylistic approach of C and underpinned arguments in favour of C based on perceived performance advantages from interfacing with memory directly
16:47:45 --- mode: glguy set +v fen_
16:48:32 <greymalkin> In other words, if there is no binary distribution of haskell programs in which different implementations of a library can be substituted, it seems that LGPL is no less restrictive than GPL.
16:48:48 <greymalkin> Unless you jump through two FFI hoops to turn it into a truly shared library...
16:49:11 <fen_> the reason for mentioning this is because Zippers seem to kind of do this arithmetic where eg lists serve instead of integers keeping track of positions. 
16:49:41 <fen_> maybe this style of approach has intrinsic performance advantages 
16:50:27 <fen_> derived just by avoiding such redundant information, such as carrying the length of a list as an integer 
16:50:29 <Solonarv> it definitely does; if you're using lists you have to do a /lot/ of pointer-chasing
16:51:06 <fen_> wait, thats confusing Solonarv. Zippers built on lists, and lists themselfs are being argued for
16:51:32 <Solonarv> oh, I meant specifically for indexing into the structure
16:51:36 <fen_> well, specifically, the use of lists in zippers
16:51:41 <Welkin> lists are not for indexing
16:51:54 <Welkin> they are used more often as a control structure (an iteration)
16:52:05 <fen_> no, but pointers made from partially traversed lists are
16:52:16 <Welkin> there are better data structures for indexing
16:52:19 <Welkin> like arrays, for one
16:52:37 <Welkin> or even Sequence, which is a 2-3 finger tree
16:52:45 <fen_> again, thats confusing since this is an argument against the indexing style of arrays specifically 
16:53:05 <Welkin> you mean a c-style array
16:53:07 <Solonarv> use of lists in zippers is a good idea because the fundamental operation is "move left/right", which translates to a cons and an uncons; the tails can be shared, so there's no copying needed
16:53:21 <Welkin> in many languages, like javascript, an array is just a record with integer keys
16:53:22 <fen_> the redundant integer arithmetic can be avoided by using the datatype itself to store positional information 
16:53:31 <Welkin> same with lua tables
16:53:49 <fen_> Welkin, arrays like haskells Map, being a key based lookup
16:53:53 <Solonarv> just to clarify: you do mean 'Zipper a = Zipper [a] a [a]', right?
16:54:07 <Welkin> fen_: IntMap or Map which is a balanced binary tree
16:54:10 <fen_> Solonarv. thats one of many ways to implement zippers
16:54:41 <Solonarv> right well you were talking about "lists vs arrays" so I assumed you were talking about the zipper that actually involves lists
16:54:59 <fen_> ([a],[a]) and ([[a]->[a]],[a]) and ([a],a,[a]) all work and are different styles 
16:55:08 <Welkin> my favorite use for lists is still implementing a queue
16:55:25 <fen_> there are even styles of zipper using continuations
16:55:45 <Solonarv> as in "why isn't it '(Vector a, a, Vector a)' instead of '([a], a, [a])'" - right?
16:55:49 <koz_> Welkin: Is that the one with two lists (one for each end)?
16:55:54 <Welkin> koz_: yes
16:56:11 <koz_> I'm still a bit amazed that in an immutable setting that really _is_ the best you can do.
16:56:16 <Welkin> I remember when I first came to haskell I thought "how would I make a queue? I don't have pointers"
16:56:34 <Welkin> koz_: the performance is amortized O(1)
16:56:37 <Welkin> it is quite amazing
16:56:45 <fen_> Solonarv. no definitely not. more like, why is Data.Map or Data.Array used when a zipper based on lists can retain positional information
16:56:47 <koz_> Welkin: Oh, I don't doubt the proof.
16:56:54 <koz_> I'm just kinda shocked that it's something _that_ obvious.
16:56:59 <Welkin> yes
16:57:19 <Welkin> I remember implementing lists in C with pointers and what a pain it was
16:57:19 <ski> (istr Okasaki has a variant where you don't need the amortization ?)
16:57:27 <fen_> (and the higher dimensional version of zippers obviously are needed to mimic the generality of keys, though it is restricted to keys of the form [Int])
16:57:29 <Welkin> and then in haskell, it's nothing
16:57:52 <Welkin> I mean implemeting queues using a list
16:57:56 <koz_> Welkin: My favourite is still what ADTs give you so trivially versus trying to do the same in everything without ADTs.
16:58:12 <koz_> Students that I teach are amazed that I can do, for example, heterogenous trees in like, 5 lines.
16:58:32 <koz_> Whereas they have to use visitors and inheritance and other vomit-inducing code-generating stuff.
16:58:37 <ski> heterogenous ?
16:58:47 <koz_> ski: Where you have different types of node.
16:58:51 <koz_> (ASTs for example)
16:59:10 <ski> oh (not sure i would call that heterogeous, but ok)
16:59:17 <koz_> ski: I can't think of a better term off-the-cuff.
16:59:23 <Welkin> I couldn't imagine using pointers to implement data structures any more
16:59:24 <ski> fair enough
16:59:28 <koz_> 'Stuff that looks like ASTs' is a bit long.
16:59:30 <Welkin> if there are no ADTs, I will pass
16:59:54 <ski> pattern-matching ftw
17:00:02 <koz_> I think I most recently did a 10-minute demonstration in response to 'how do we write code to intepret propositional formulae'.,
17:00:10 <koz_> The students watching were actually amazed.
17:00:20 <koz_> Because they tried to do the same in Java and just fell over before they even began.
17:00:54 <Solonarv> I should really learn a JVM language that has ADTs... (some of my projects are minecraft mods and therefore tied to the JVM)
17:01:01 <koz_> Solonarv: Scala I guess?
17:01:08 <Solonarv> ewwww scala
17:01:10 <koz_> Although I think some of the less functional JVM languages have them too.
17:01:14 <fen_> anyway its a question about haskell indexed containers versus haskell nested zipper based pointers
17:01:16 <koz_> (Ceylon maybe? Kotlin perhaps?)
17:01:24 <koz_> Eta's a thing too now.
17:01:27 <Welkin> Solonarv: you mean Frege? https://github.com/Frege/frege
17:01:29 <Solonarv> kotlin sounds like it'd have them
17:01:30 <hpc> oh hey, minecraft modding
17:01:42 <Welkin> or this https://eta-lang.org/
17:01:49 <koz_> Welkin: Yeah, Eta's neat.
17:01:52 <Welkin> it is haskell on the jvm (for real)
17:01:58 <Welkin> scala is a lie
17:01:59 <koz_> Mostly AFAICT.
17:02:07 <koz_> Scala is a rather different beast.
17:02:10 <koz_> (emphasis on 'beast')
17:02:16 <Welkin> it's java wearing new clothes
17:02:35 <hpc> Welkin: wow you weren't kidding
17:03:14 <Welkin> I was just looking for frege and found eta too
17:03:39 <koz_> How many languages are we up to now named after famous logicians?
17:03:48 <hpc> koz_: not enough
17:04:59 <Shockk> I have a quick question; I'm trying to add type signatures to my language, so I'm thinking about how to support this in my compiler; so,
17:05:02 <fen_> so... maybe there is a chance that by using the intrinsic shape retaining position instead of indexed locations, that some needless arithmetic can be avoided, and that as a result, nested zipper based pointers, or other fast lazy implementations of ComonadStore (such as higher dimensional cyclic graphs obtained by flattening the nested zipper pointers) can be used instead of explicitly indexed containers in haskell for a performance b
17:05:44 <koz_> Shockk: Are you asking about syntax or semantics?
17:05:46 <fen_> and that this also would be better than the similar approach taken by C style malloc arrays
17:05:57 <Solonarv> fen_: you might get a boost on 'extract' performance, but get a large hit everywhere else
17:06:19 <Shockk> straight after parsing, I have an AST of things like type signatures, definitions, functions with definitions in them and such, but I'm wondering if I should be lowering this to a *different* AST type that's more suitable for code gen, or if I should be trying to do my initial parsing in a way that's suitable for code gen?
17:06:41 <koz_> Shockk: Type checking is typically done at the samentic analysis step.
17:06:41 <fen_> just not sure if C has anything similar where the shape of (the derivative of) a container is used instead of indexed memory addresses 
17:06:51 <koz_> After you've gone past that (which code generation is _way_ past), type sigs are gone.
17:06:54 <fen_> Solonarv: what, how?
17:06:56 <Solonarv> having multiple intermediate ASTs is fairly common, afaik
17:07:05 <Shockk> ahh okay
17:07:08 <koz_> I think the nanopass approach even _encourages_ it.
17:07:19 <fen_> Solonarv: the design is for interacting nodes on a Graph
17:07:20 <koz_> Shockk: Are you familiar with the five-phase model of compilation?
17:07:35 <Shockk> I've heard of it but I haven't looked into it
17:07:42 <koz_> Shockk: _Do_.
17:07:46 <koz_> Like, seriously.
17:07:49 <Solonarv> fen_: "moving" your focus is just altering the pointer and you don't have to touch the backing data structure at all; you also have no risk of turning a finite cyclic graph into an infinite tree
17:07:59 <fen_> based on fast relative location handling using balanced partition trees
17:08:49 <Shockk> koz_: is this what you mean? https://www.tutorialspoint.com/compiler_design/compiler_design_phases_of_compiler.htm
17:08:50 <fen_> it does affect the datastructure, and its precisely those changes that serve to replace keys
17:09:07 <koz_> Shockk: That's a few more steps than usual.
17:09:12 <Shockk> oh right
17:09:22 <koz_> Typically it goes 'lexical analysis, syntactic analysis, semantic analysis, optimization, code generation'.
17:09:40 <fen_> oh, you meant just altering the key
17:09:49 <fen_> but the point is that this is redundant
17:09:57 <fen_> it does a needless calculation 
17:10:07 <Solonarv> arithmetic is cheap
17:10:18 <koz_> Especially on fixed-length integers.
17:10:26 <Solonarv> pointer math is not slow, /following/ pointers is
17:10:30 <koz_> And for large collections of data, memory movement costs can easily overwhelm calculation.
17:10:43 <fen_> but its the exact same calculation as changing the one hole context
17:10:44 <koz_> One big reason arrays are good is that the data has good locality if it's laid out well.
17:10:53 <Shockk> hmm I see; right now I'm doing lexical analysis, syntactic analysis, and code gen
17:10:57 <fen_> that is equivalent to the same arithmetic 
17:11:08 <koz_> Shockk: You can't really _do_ codegen until you've done all the analysis steps.
17:11:16 <Solonarv> equivalent semantics does not mean equivalent performance
17:11:37 <koz_> Unless you have a _very_ good idea of what your 'final' AST is gonna look like.
17:11:40 <Shockk> well I mean yes, so far I don't have any semantic analysis being done though
17:11:47 <Shockk> basically what you said yes
17:11:48 <fen_> (+1) is just like `right' for zippers for example
17:11:54 <fen_> its just the same
17:12:00 <fen_> in terms of performance
17:12:05 <fen_> its the same calculation!
17:12:16 <koz_> fen_: How is 'right' defined in your case?
17:13:23 <Shockk> so should I have a semantic analysis stage before code gen that takes the AST from the syntactic analysis, makes sure it's correct and does type checking and stuff, and lowers to another AST suitable for code gen?
17:14:00 <Shockk> (assuming no optimization stuff for now anyway)
17:14:04 <fen_> the simplest example is with a zipper like ([a],[a]); right (ys,xs) = fmap (\(x,xs') -> (x:ys,xs')) (uncons xs)
17:14:13 <fen_> :t uncons
17:14:14 <lambdabot> [a] -> Maybe (a, [a])
17:14:17 <Solonarv> Shockk: yup, that's about right
17:14:23 <koz_> Shockk: Usually yes, that's how you'd do it.
17:14:37 <koz_> Once you get past all the analysis, you usually try and have an AST that's amenable to optimization and codegen.
17:14:49 <koz_> This is _not_ necessarily the same as an AST that's amenable to semantic analysis.
17:15:07 <fen_> here, the length of ys serves as the Int position. and adding a value to it is like (+1) on the Int index
17:15:11 <Shockk> right I see
17:15:27 <Shockk> that makes a lot of sense, thanks
17:15:32 <koz_> fen_: fmap (\(x, xs') -> (x:ys, xs') is _nothing_ like (+ 1)
17:15:34 <Solonarv> fen_: again, just because they have the same meaning does not imply that they have the same performance characteristics
17:15:42 <koz_> Neither in terms of semantics nor in terms of performance.
17:15:45 <Shockk> I knew I had a hole in what I was doing but wasn't sure exactly what the best approach was to fox it
17:15:49 <Shockk> to fix i*
17:15:52 <fen_> obviously, taking the length of ys would be costly, but there is no need to do this, since the position is retained explicitly
17:16:03 <koz_> Shockk: Let me find you the nanopass compiler paper.
17:16:07 <koz_> You may find it insightful.
17:16:14 <Shockk> that'd be great, thanks
17:16:30 <koz_> Shockk: https://www.cs.indiana.edu/~dyb/pubs/nano-jfp.pdf
17:16:44 <koz_> It uses Scheme as a presentation language, but that shouldn't detract from the key ideas much.
17:17:09 <fen_> koz_ its more like \(ys,x:xs) -> (x:ys,xs) where the length of ys has been increased by 1
17:17:31 <fen_> Solonarv. Int is so much like [()] though, the performance should be the same
17:17:43 <koz_> fen_: That's _nothing_ like true.
17:17:44 <ski> (note this `fmap' is over `Maybe', not list or something ..)
17:18:11 <koz_> [()] is a pointer to a cons cell, Int is a machine integer.
17:18:15 <koz_> (modulo levity)
17:18:16 <Solonarv> the index+array version is 'data Zipper a = Zipper !Int (Vector a)' with 'right z@(Zipper pos xs) = let !pos' = pos+1 in if pos' < length xs then Zipper pos' xs else z'
17:18:17 <fen_> koz_ is that because of the binary representation of Ints?
17:18:31 <Solonarv> yes! that's why they're fast
17:18:32 <Shockk> koz_: something else; right now I have a hack workaround to avoid having to try and code-gen an empty tuple passed to a function; in semantic analysis should I just remove this empty tuple in that situation?
17:18:44 <Shockk> also this paper looks helpful, thanks
17:18:45 <koz_> Shockk: I don't think I quite understand.
17:18:55 <fen_> fast maybe, but unnecessary!
17:19:11 * ski . o O ( unnecessarily fast )
17:19:21 <fen_> no just not needed to retain positions
17:19:31 <Solonarv> I thought the whole point of your foray into whatever-it-is was to make fast zippers?
17:19:51 <Solonarv> er, s/zipper/whatever data structure you were making/
17:19:52 <fen_> take a traversal for instance. you dont do that by calculating the position of every value!
17:19:55 <Shockk> koz_: oh, err, well a simpler example is, like, an empty statement like:  `()`
17:20:04 <Shockk> just an empty tuple as its own statement, without any assignment or anything
17:20:14 <koz_> Why would you even have that be a thing though?
17:20:25 <Shockk> well I wouldn't but it's something that my language would syntactically support
17:20:31 <koz_> Why though?
17:20:31 <fen_> even if that were fast. direct recursion is faster, even though it moves values around instead of doing Integer arithmetic 
17:20:32 <Shockk> it supports bare values without an assignment
17:21:09 <Solonarv> fen_: if you want to fmap over the Vector-plus-index zipper you just fmap over the vector and pass the index along
17:21:15 <Solonarv> same for traverse, etc
17:21:22 <fen_> thats the slowest thing ever!
17:21:31 <Solonarv> how is fmapping over a Vector slow?
17:21:35 <Solonarv> where did you get that idea?
17:21:37 <Shockk> well in my language for example a function call is just a value without an assignment (which may have side-effects inside that function of course)
17:21:42 <fen_> slower than list
17:21:50 <Solonarv> again: where did you get that idea?
17:21:50 <Shockk> in the case where the function doesn't return anything, I represent that as it returning an empty tuple, ()
17:22:07 <fen_> does unnecessary integer math thats why its slower
17:22:20 <Welkin> cpu cycles are cheap
17:22:22 <geekosaur> Solonarv, I think we established earlier that fen_ hs no clue how Vector works
17:22:23 <Welkin> memory access is slow
17:22:24 <Solonarv> lists do "unnecessary" pointer chasing!
17:22:36 <Welkin> lists will have much worse performance than an unboxed array/vectr
17:22:38 <geekosaur> or that contiguouvalues + calculated index i always faster than pointer chasing per element
17:23:02 <geekosaur> thy ctally wanted to benchmarkit earlier because it was so inconceivable
17:23:02 <koz_> Shockk: So, just to be clear: this is something you do in the _source_ language, or is this something you get after some transformations you do to the source language?
17:23:10 <Shockk> I think what I'm asking is, in those cases, at the semantic analysis stage, should I just remove things like empty tuples that can't really be code genned, or should that be after semantic analysis as an optimization?
17:23:10 <fen_> geekosaur: following that, its now obvious *why* it should be faster not to use them
17:23:17 <Shockk> koz_: this is in the source language sorry
17:23:51 <Solonarv> what do you mean by "can't be codegenned"? generating code for a "do nothing" statement is very simple
17:23:55 <Solonarv> you just don't emit anything
17:24:02 <koz_> Yeah, I'd agree with Solonarv.
17:24:06 <Welkin> noop
17:24:08 <koz_> You'd just have your codegen ignore that.
17:24:10 <fen_> geekosaur: what is this contiugousness argument?
17:24:12 <Solonarv> or perhaps you emit a NOP instruction
17:24:21 <Welkin> count your cpu cycles and insert noop where necessary!
17:24:23 <Shockk> hmm oh, that's fair enough; I think my current code gen approach is wrong then
17:24:29 <koz_> Basically, I wouldn't worry excessively about codegen at your stage.
17:24:34 <koz_> _First_ get all your analysis straight.
17:24:44 <Shockk> right now it's designed for an AST to directly map to an emitted LLVM Operand
17:25:04 <Solonarv> fen_: if your data is next to each other in memory then you have a higher chance of it all ending up in the CPU cache, so the CPU doesn't have to fetch as often
17:25:05 <koz_> @pl \x -> x - 1
17:25:05 <lambdabot> subtract 1
17:25:12 <koz_> ... I ALWAYS FORGET THAT EXISTS
17:25:14 <koz_> :t subtract
17:25:15 <lambdabot> Num a => a -> a -> a
17:25:38 <fen_> but how is that something to do with this?
17:25:48 <geekosaur> that's only part of it. fen_ is using *lists*.
17:25:55 <Welkin> memory speed is the bottleneck for modern computing
17:26:04 <geekosaur> liked lists, tail is a pointer to the next list element instead of items next to each other
17:26:20 <Welkin> fetching from memory is 2 orders of magnitude slower than in cache, disk is 3 orders, network is 4 orders
17:26:33 <Solonarv> which means that to get the next list element, a CPU has to follow the pointer... and that's slow
17:26:36 <geekosaur> an addition to get something in the ame block of memory is always faster than dereferencing pointers to get to each successive element
17:26:51 <geekosaur> Iâ€¦don't know how to help you if this is not obvious
17:26:54 <fen_> ok, well if thats the main argument then surely being able to focus on one local region using stencils on Pointers should help partition a large grid nicely
17:27:05 <koz_> Suppose I have a function foo :: a -> b -> c -> d, and want to call foo with the _same_ a, but _different_ b, c, depending on a condition I check with an 'if'. Can I actually write this without doing something like 'if cond then foo x y z else foo x y' z''?
17:27:09 <geekosaur> ok, that emant noting to you.
17:27:14 <Solonarv> not if your data structure is made of a bunch of lists
17:28:16 <Solonarv> koz_: let f = foo x in if cond then f y z else f y' z' ?
17:28:28 <koz_> Solonarv: Thank you, shoulda known.
17:29:14 <fen_> like. 100 dimensional length 11 grid initially zeroed, set the central value = 1 and then apply a Gaussian blur. do this by moving the Pointer to the value 1 and changing the surrounding values using a weighted average
17:30:01 <ski>   foo x `uncurry` if cond then (y,z) else (y',z')  -- koz_
17:30:20 <Solonarv> I'm pretty sure that's faster with an array-and-pointer approach
17:30:23 <koz_> ski: Solonarv's scales better to more arugments, but yeah, works too.
17:31:03 <fen_> when does having to calculate all the indexes become slower than the fact the Pointer has immediate access to its surrounding values?
17:31:17 <koz_> Who says those surrounding values are useful in any way?
17:31:23 <Solonarv> the pointer does not have /immediate/ access to its surrounding values
17:31:30 <fen_> see the above bluring problem
17:31:31 <Solonarv> koz_: the fact that we're doing a gaussian blur
17:31:33 <koz_> You might have two lists' worth of cons cells randomly striped across each other in memory.
17:31:56 <fen_> well that would scupper things
17:32:03 <Solonarv> the pointer has access to its surrounding values *after dereferencing an address*
17:32:06 <koz_> fen_: And is _totally_ 100% possible.
17:32:22 <fen_> not for arrays or Map though??
17:32:25 <koz_> Unless you literally use memory pooling and ensure eveyrthing is placed exactly where you want and the OS doesn't mess with you.
17:32:46 <fen_> koz_ would hope the threading would actually *help* !
17:32:46 <koz_> An array _will_ have its elements contiguously placed.
17:32:56 <fen_> hmmm
17:33:00 <koz_> fen_: Threading ain't got nothing to do with it.
17:33:04 <Solonarv> yes for Map, Map is a binary tree - there is also pointer-following involved
17:33:06 <fen_> thats definatly the most serious consideration then
17:33:15 <koz_> When you use a linked structure, _any_ linked structure at all, you are essentially saying 'OS, please put these wherever'.
17:33:23 <koz_> Or rather 'memory allocator, please put these wherever'.
17:33:28 <fen_> ffs
17:33:35 <Solonarv> although it's only O(log size) deref's instead of O(size) like for lists
17:33:38 <koz_> At the Haskell level, you neither know of, nor have control over, this kind of detail.
17:33:38 <fen_> any way to mend that?
17:33:47 <koz_> fen_: Write C, use memory pooling, cry deeply,.
17:33:48 <Solonarv> yes - use a freaking array!
17:33:59 <koz_> Or what Solonarv's been trying to tell you for hours.
17:34:04 <koz_> (well, Solonarv and others)
17:34:14 <koz_> The fact that you don't know this kinda thing is also _deeply_ concerning.
17:34:18 <fen_> what if the Pointer has type level singleton indexes of its location so it can get the values
17:34:23 <fen_> (of the index)
17:34:24 <Solonarv> doesn't matter
17:34:29 <fen_> and then call to an array
17:34:30 <Solonarv> types don't *exist* at runtime
17:34:38 <fen_> (which it would be good if the compiler did anyway)
17:34:40 <Solonarv> so they certainly can't help performance!
17:35:05 <Welkin> some type information still exists (for Typeable)
17:35:12 <fen_> er, if the values are being obtained then certainly something exists at runtime
17:35:19 <fen_> (values of the index)
17:35:26 <fen_> (from it being a singleton)
17:35:40 <Solonarv> yes, but passing around a (Typeable (n :: Nat)) dictionary or whatever isn't any faster or more memory-efficient than passing around an Int
17:35:55 <Welkin> (cadddr '(((1 (2 3 (4 5))))))
17:35:58 <Welkin> are we writing lisp ow?
17:36:00 <Welkin> now?
17:36:19 <fen_> not sure about Typable, is that the same thing?
17:36:36 <Solonarv> close enough for the purposes of this discussion
17:36:41 <fen_> fair
17:36:47 <fen_> hmm
17:36:51 <fen_> this is troubling 
17:37:01 <Welkin> I don't understand your trouble
17:37:10 <Welkin> haskell takes care of all the details
17:37:10 <koz_> fen_: Use arrays, problem solved.
17:37:12 <Welkin> just use it
17:37:25 <koz_> Literally, massiv does _everything you seem to care about_.
17:37:26 <fen_> just as well i didnt spend any time on nested zippers 
17:37:31 <koz_> It even parallelized it for you!
17:37:34 <Welkin> yes
17:37:39 <koz_> s/parallelized/parallelizes/
17:37:40 <Welkin> have you learned about haskell's concurrency?
17:37:43 <Welkin> it's mindblowing
17:37:54 <fen_> or a generalised abstraction for graphs to support them
17:37:54 <Welkin> *And* parallelism
17:38:14 <fen_> is there anything worth using that for?
17:38:16 <fen_> ffs
17:39:12 <fen_> need to mend this contiguous thing
17:39:22 <fen_> then maybe they will be at least usable! 
17:39:24 <Welkin> type Graph = Vector [a]
17:39:27 <Welkin> there is your graph
17:39:46 <Welkin> type Graph a = Vector [a]
17:39:53 <koz_> Or, be clever, and use this: https://github.com/snowleopard/alga
17:40:15 <fen_> there is absolutely no way that the nested zipper pointers could be faster for any example?
17:40:56 <Welkin> koz_: nice library
17:41:01 <koz_> Welkin: I know right?
17:41:15 <koz_> It's based on some really neat theory too.
17:41:25 <koz_> Totally blew my mind, and I'm (supposedly) a data structure 'expert'.
17:41:33 <koz_> (or that's what my Master's degree says anyway...)
17:41:38 <Welkin> I've only done toy examples with graphs because I never actually use them for anything
17:41:48 <koz_> Welkin: Neither, to be honest.
17:41:54 <fen_> does it use arrays?
17:41:57 <Welkin> it's all lists/arrays and maps
17:41:57 <koz_> Most of what I work with these days, ironically, happen to be arrays.
17:42:01 <koz_> fen_: Read it and find out. :P
17:42:06 <fen_> sure
17:42:21 <fen_> but, is it based on this array fastness?
17:42:42 <koz_> fen_: Read it and find out. :P
17:42:44 <koz_> Seriously.
17:42:53 <koz_> You seem to care significantly about formalism and theory.
17:42:59 <koz_> This is an excellent study in how you can use that.
17:43:11 <fen_> look, just not wanting to have to benchmark EVERYTHING if someone could just say
17:43:28 <koz_> fen_: Who said anything about benching? Just have a read of the link and see.
17:45:08 <fen_> it uses Graph, which is Array
17:45:14 <fen_> http://hackage.haskell.org/package/containers-0.6.0.1/docs/Data-Graph.html
17:45:51 <fen_> http://hackage.haskell.org/package/algebraic-graphs-0.2/docs/Data-Graph-Typed.html
17:45:58 <koz_> Well, there you have it then.
17:46:10 <fen_> they are  King-Launchbury graphs apprently 
17:46:59 <fen_> so a graph library can use an Array backend, that seems like a fine way to get round this contiguousness problem
17:47:33 <fen_> but its not exactly inspiring that the whole contiguous datataype (Pointer) thing might be a bad idea
17:47:45 <koz_> fen_: Which is basically what we've all been trying to tell you. For hours.
17:47:51 <fen_> if it can at least be as fast it might just give a nice API
17:47:58 <fen_> or be more readable or something
17:48:04 <fen_> but it was supposed to be faster
17:48:18 <fen_> hours?
17:48:27 <koz_> Well, it _felt_ like hours. :P
17:48:32 <fen_> hmm
17:48:39 <koz_> I might be exaggerating (sp?) slightly.
17:48:55 <fen_> time flies! 
17:49:14 <fen_> when your having functional graphs
17:49:44 * ski . o O ( fruit flies around graphical functions ? )
17:51:02 <fen_> blast them from the sky!
17:53:08 <fen_> anyway, seems like making comonadic stencils from traversables only works for cartesian grids, so its not like that kind of API is relevant here
17:53:28 <fen_> ( King-Launchbury graphs)
17:56:49 <fen_> what about using type level binary instead of Nat?
17:57:06 <koz_> fen_: I think you _still_ fundamentally fail to see where the problem lies.
17:58:23 <fen_> this would be to get this index for the underlying array
17:59:06 <fen_> then the pointer navigation could still be used instead of Int arithmetic 
18:00:09 <fen_> and be as fast
18:02:26 <fen_> wasnt that the fundamental issue koz_ ?
18:16:36 * koz_ found another HLint bug woo.
18:22:49 <fen_> so summing an array should be faster than summing a list?
18:23:55 <koz_> fen_: In general yes.
18:24:00 <koz_> (not asymptotically though)
18:24:02 <koz_> (obviously)
18:24:27 <koz_> And this is before we bring in stuff like SIMD, which can be _hilariously_ faster for something like 'summing an array'.
18:25:15 <fen_> always?
18:25:42 <koz_> fen_: I will not make absolutist pronouncements of that form absent context.
18:26:03 <fen_> like, regardless of length
18:26:42 <fen_> thinking about how likely it is that it ends up scrambled in disparate memory
18:26:58 <koz_> With a list, it depends on about twenty things, none of which you have any control over.
18:27:12 <koz_> (at least not in Haskell without C FFI and very promiscuous knowledge of your OS's internals)
18:28:03 <fen_> can recursion still be used nicely on arrays to the same effect as recursive functions over lists (head tail recursion)
18:28:34 <Solonarv> eh, not really
18:29:01 <fen_> doesnt that ruin most of the higher order recursive functions intrinsic to haskell?
18:29:24 <koz_> fen_: You have to write a bit differently for arrays, but honestly, explicit recursion isn't encouraged anyway.
18:29:39 <koz_> If you can write stuff with (for example) folds, it'll likely be faster due to fusion, for example.
18:29:44 <fen_> no, normally its abstracted into generalised recursion schemes
18:29:49 <fen_> such as the foldable instance for list
18:30:03 <fen_> thats why summing was the question
18:30:05 <Solonarv> yes, and arrays/vectors also have a Foldable instance
18:30:10 <koz_> fen_: The Foldable instance for _lists_ does not contain any recursion schemes that I am aware of.
18:30:23 <fen_> yeah, but its not the same recursive abstraction that was being examplified
18:30:32 <koz_> The fact that two things provide the same API doesn't mean that the implementations are similar.
18:30:41 <koz_> You can define, for example, foldMap for both arrays and lists.
18:30:53 <koz_> The definitions will look different, but that's because they're different data structures.
18:31:14 <fen_> fmap f xs = case xs of [] -> []; (x:xs') -> f x : fmap f xs'
18:31:31 <fen_> that kind of recursion
18:32:06 <Solonarv> fmap â‰  recursion
18:32:25 <koz_> Recursion schemes /= what you just described either, for the record.
18:32:28 <fen_> obviously the instances are different. thats not the point. the utility of the foldable class isnt being questioned, but that of head tail recursion
18:32:43 <koz_> Head-tail recursion is of no real benefit with arrays.
18:32:43 <Solonarv> please explain what you mean by "head tail recursion"
18:32:56 <fen_> generalised recursion schemes - such as the foldable instance for list
18:33:05 <koz_> (and what Solonarv said, just for clarity)
18:33:23 <fen_> Solonarv: the definition of fmap above pattern matching on cons
18:33:32 <koz_> fen_: Are you aware of what 'recursion schemes' are in general parlance? Foldable is a _specialization_ of a particular recursion scheme.
18:33:43 <koz_> (that of a catamorphism to be precise)
18:33:43 <fen_> is an example of what is being referred to as a head tail recursion schem
18:33:52 <Solonarv> indeed, you can't pattern match on arrays
18:33:57 <Solonarv> why is this a problem?
18:34:31 <fen_> because if pattern matching is in general slower than the integer calculations and memory access of the Array version
18:34:48 <fen_> then it means focusing on recursion schemes is worse
18:35:13 <fen_> and it underpins even the foundational haskell teaching  
18:35:47 <koz_> fen_: Who, and where, have you found anyone teaching anyone else anything about arrays, in Haskell or any other language, using 'recursion schemes' or pattern matching?
18:35:55 <fen_> eh!?
18:36:02 <fen_> no, recursion schemes are taught
18:36:11 <fen_> but arrays that dont use them should be used
18:36:14 <fen_> thats the issue
18:36:24 <koz_> fen_: I don't even know how to _parse_ that statement.
18:36:33 <koz_> What do you mean by 'arrays that don't use them'?
18:36:43 <fen_> as arrays dont use them
18:36:50 <fen_> arrays, which dont use them
18:37:04 <fen_> ...
18:37:32 <Solonarv> I /think/ what fen_ is trying to express is: people are taught to use recursion schemes, but arrays can't use recursion schemes and are faster - so why are people being taught recursion schemes?
18:37:53 <koz_> Solonarv: 'recursion schemes' because fen_'s description and how most folks use the term don't line up, but OK.
18:38:14 <fen_> hmm, yes that, exactly, and worse, not just that they are taught, thats just one aspect of the scenario. libraries are developed around them!
18:38:42 <koz_> fen_: Uhhh, I don't think I agree with the second part.
18:38:53 <koz_> Nobody's developing libraries with 'recursion schemes' on top of arrays.
18:38:58 <koz_> (at least, nobody I'm aware of)
18:39:26 <Solonarv> I think it boils down to "why are people being taught the slow thing, instead of the fast thing"
18:39:39 <fen_> koz_ head tail recursion as used in the implementation of the foldable instance for list was as an *example* of a recursion scheme, particularly because of the fact it is a common introductory haskell lesson
18:40:09 <fen_> koz_ they are developing them *not* on top of arrays, and thats the issue
18:40:22 <fen_> not that developing recusion schemes for arrays is even possible
18:40:29 <koz_> fen_: I still don't understand what your issue is.
18:40:35 <koz_> Arrays and lists require different tools.
18:40:41 <fen_> koz: that recursion schemes are being developed
18:40:45 <Solonarv> the answer is: recursive ADTs are great for things that *aren't* flat homogenous data
18:40:46 <koz_> Some don't translate well semantically or performatively.
18:40:55 <koz_> How and why is this a problem?
18:41:20 <fen_> because its slower than just using arithmetic to handle indexs for arrays 
18:41:34 <koz_> fen_: That's assuming we can represent all data forever using arrays and nothing else.
18:41:36 <fen_> (maybe the recursion schemes are just for this arithmetic...)
18:41:38 <koz_> Nobody has made that claim anywhere.
18:41:59 <Solonarv> simple linked lists are not a particularly great *data* structure, but in combination with laziness they make a pretty good *control* structure
18:42:25 <koz_> Solonarv: I'm gonna be borrowing that phrase. :)
18:42:34 <Solonarv> I feel honored!
18:42:49 <fen_> the claim was that any datatype just scatters values in memory, placing them on various separate drives and networks etc, and that as a result, nothing except arrays was to be used for developing high performance haskell
18:43:13 <koz_> fen_: I certainly never made this claim.
18:43:22 <koz_> (_especially_ vis a vis 'drives and networks')
18:43:23 <fen_> its paraphrased
18:43:27 <koz_> (since that's a _whole_ other ball game)
18:43:28 <fen_> for effect
18:43:35 <geekosaur> whic is so very helpful
18:43:40 <koz_> fen_: Paraphrasing isn't invention. Nobody said that.
18:43:51 <geekosaur> especally since most of that is unsupported, unless you've come up with garbage from elsewhere
18:44:06 <fen_> they hide the values, and camels must be used to treck out to retrieve them
18:44:23 <geekosaur> ...
18:44:32 <koz_> What the actual Eff.
18:44:32 <Solonarv> the "memory" we were talking about earlier is RAM, by the way
18:44:40 <fen_> same thing
18:44:47 <koz_> fen_: Not by a _long_ shot.
18:45:00 <Solonarv> if you're only interested in dramatic effect, sure, same thing
18:45:00 <koz_> RAM, disk and _network_ are very different things, requiring different treatment.
18:45:19 <fen_> right, ok we are agreed
18:45:37 <fen_> control structures...
18:45:49 <fen_> but not fast
18:46:00 <fen_> there needs to be somewhere fix this
18:46:09 <Solonarv> well, what would a fix look like?
18:46:23 <koz_> And what precisely are you fixing to what end?
18:46:40 <fen_> the compiler being neat about contiguous placement in memory of contiguous datastructures
18:47:24 <fen_> to fix the issue of recursive datatypes being inferior to arrays for performant computations 
18:47:38 <Solonarv> it is, but recursive ADTs are not (and cannot be!) placed contiguously
18:47:58 <koz_> At least not with any hope of _staying_ that way.
18:48:02 <geekosaur> contiguous placeent isn't the issue. add-constant is faster than load-new-address-from-memory
18:48:30 <Solonarv> fen_: I feel like you might benefit from learning a bit of rust, tbh
18:48:54 <fen_> such jibes
18:49:08 <Solonarv> haskell education resources gloss over the low-level details because that simply isn't the focus of the langage
18:49:25 <fen_> it sets up a tradition of research
18:49:28 <fen_> and thinking
18:49:41 <fen_> in favour of recursive datatypes
18:49:47 <geekosaur> thinking from false starting principles doesn't accomplish much
18:50:04 <fen_> well, much fast
18:50:05 <koz_> fen_: I don't see how that makes any sense. We have libraries, and _good_ ones, for array-related work.
18:50:20 <Solonarv> recursive datatypes are *useful*; how else are you going to represent e.g. a JSON value?
18:50:33 <fen_> the problem is that the *only* good libraries *must* use arrays
18:50:35 <koz_> If such a 'tradition' existed, nobody would be bothering to make, or _want_, such libraries.
18:50:45 <koz_> fen_: _Nobody_ claimed this at any point ever.
18:50:57 <fen_> that has been gleaned 
18:51:18 <fen_> obviously, wrt certain applications
18:51:49 <fen_> but in this case, attempts have been made to derive a high performance recursive datatype
18:52:03 <fen_> which seems in essence to be a misguided goal
18:52:22 <Solonarv> if performance is a criterion of "good", then yes, in some domains you may need to use non-recursive datatypes
18:52:54 * dmwit . o O ( Look how fast I get the wrong answer! )
18:53:53 <Solonarv> "high performance recursive datatype" is not an oxymoron; "recursive datatype with good data locality" is, however, extremely hard or impossible
18:54:04 <fen_> the work was undertaken based on the understanding that the poor performance of list, the archetypal recursive datatype, could be alleviated by an alternative approach to indexing, for which zippers serve well
18:54:42 <Solonarv> right. linked lists are terrible at indexing; you can paper over this somewhat, but you can't make it go away
18:55:01 <fen_> dmwit: sure, its not the be all and end all, but it is certainly going to affect the competitivity of the datatype 
18:56:39 <fen_> Solonarv, not nesacarily the double linked recursive cofree zipper presented earlier, but e.g. the nested difference zipper; Free (\a -> ([[a]->[a]],[a]))
18:57:45 <Solonarv> difference lists still fall under "paper over it"
18:59:14 * ski . o O ( "Sacrificing the calf of flexibility on the altar of reliability" by Peter J. Denning in 1976 at <https://dl.acm.org/citation.cfm?id=807704> )
18:59:17 <fen_> the goal now is to either provide "good data locality" for that type, or somehow leverage its utility as a "control structure" to some useful ends 
19:00:11 <Solonarv> you can't "provide data locality" to a type, it's a property of the type (or rather, its representation)
19:00:43 <fen_> isnt it just an issue about ensuring the compiler dosent fed-ex it internationally?
19:01:21 <Solonarv> no! locality is a matter of how your data is laid out in memory
19:01:44 <Solonarv> broadly speaking: if stuff you'll need at the same time is close to each other, that's good
19:01:50 <Solonarv> if it's all over the place, that's bad
19:02:06 <koz_> The fancy term for this being 'good/bad spatial locality'.
19:03:28 <fen_> and apparently, despite the good adjacency of the nested difference zipper above. which serves as a higher dimensional cartesian grid zipper, it cannot be made to correspondingly ensure good spacial locality for its representation in memory
19:03:45 <ski> @remember Solonarv simple linked lists are not a particularly great *data* structure, but in combination with laziness they make a pretty good *control* structure
19:03:45 <lambdabot> Good to know.
19:03:56 <fen_> and so should be scrapped in favour of arrays
19:04:25 <fen_> ski: how can that be utilised?
19:04:41 <ski> sorry ?
19:04:46 <fen_> anyway its not a linked list...
19:05:11 <fen_> what is this idea of "control structures", and how can they be useful?
19:05:50 <Solonarv> in an typical imperative language, control structures are things like 'if/then/else', 'for/while', and so on
19:06:19 <ski> various kinds of iteration are control structures, as are coroutines, e.g.
19:06:34 <fen_> oh, so it is even that the euclidean zipper isnt even a good control structure?
19:06:41 * ski . o O ( "The anatomy of a loop: a story of scope and control" by Olin Shivers in 2005-09 at <http://www.ccs.neu.edu/home/shivers/citations.html#loop>,<http://lambda-the-ultimate.org/node/1026> )
19:07:25 <Solonarv> I'm not sure what kind of control structure it represents, let alone if that control structure is useful
19:07:34 <ab9rf> fen_: why are you passing positivist judgments on control structures, outside of any meaningful evaluatory context?
19:07:51 <fen_> im not sure how that happened, terribly sorry
19:08:05 * ski isn't even sure what "euclidean zipper" refers to, nor to what *end* it may or may not be appropriate as a control structure
19:08:33 <fen_> ski: imagine a big regular n dimensional grid with a one hole context 
19:08:45 <fen_> grid pointers
19:09:04 <fen_> higher dimensional cartesian / euclidean zippers
19:09:10 <davean> The comonad-ish thing?
19:09:17 <fen_> aye, thats the one
19:09:19 <ab9rf> i don't know what an euclidean zipper is. whether one could be useful as a control structure depends on what one is and what you're trying to use it for
19:09:56 <fen_> it was supposed to be for fast stencil convolutions for covnets and finite difference schemes for high order accurate fluid simulations
19:10:07 <ab9rf> the n-dimensional generalization of a zipper? i've seen some discussion of those
19:10:29 <ab9rf> might be useful for very specific applications
19:10:30 <fen_> and about how slow they are compared to arrays for the same applications?
19:10:44 <ab9rf> i've never had to write any such code, and so have no experience
19:11:09 <fen_> ab9rf: that probably wouldnt help at all
19:11:20 <ab9rf> fen_: considering how bad arrays are for some such purposes....
19:11:36 <ab9rf> fen_: many moons ago, i used to write graphics software
19:11:38 <fen_> nono, they are all parallel and contiguous and stuff, they are just great
19:11:43 <edwardk> ab9rf: kenny foner wrote a nice paper on using this for spreadsheet like workloads
19:12:36 <ab9rf> fen_: one of the ongoing problems we had was in efficiently applying transformations across Very Large pixmaps where the transformations relied on "nearby" pixels
19:12:41 <edwardk> https://www.schoolofhaskell.com/user/edwardk/cellular-automata stuck with 1d automata, but everything there works in n-dimensions
19:12:57 <fen_> ab9rf: such as bluring or edge detection?
19:12:58 <Solonarv> n-dimensional zippers are one thing, Fix (\a -> ([Endo [a]], [a])) is another...
19:13:03 <ab9rf> we couldn't juse use a "plain array" because our app was explicitly designed to support pixmaps too large to hold n memory
19:13:31 <ab9rf> fen_: blur was fairly easy. the troublesome ones were apps that had more flexible boundaries in how "nearby" they could search
19:14:00 <edwardk> ab9rf: mmap is your friend
19:14:15 <koz_> Hi edwardk!
19:14:17 <ab9rf> edwardk: not when you support images that are too lage to fit in your memory space
19:14:18 <koz_> Always good when you're here.
19:14:25 <edwardk> ab9rf: point =)
19:14:34 <edwardk> heya koz
19:14:45 <ab9rf> edwardk: also, mmap wasn't really a thing in 1998 :)
19:14:49 <fen_> ab9rf: well, nd list zippers would be perfect for that, except they tend to have bad memory allocation layouts
19:15:02 <fen_> apparently...
19:16:01 <edwardk> mmap was introduced in the 80s =)
19:16:10 <ab9rf> we had good code for convolutions that would sample from a fixed radius of the operation point
19:16:24 <ab9rf> edwardk: yes, but not impliemented reliably across multiple platforms.
19:16:56 <ab9rf> edwardk: iirc, we used mmap where it was avaiable but we could not rely on it
19:17:08 <edwardk> makes sense
19:17:27 <davean> ab9rf: what do you mean mmap wasn't a thing in 1998?
19:17:35 <ab9rf> edwardk: but this was 20 years ago and many of the details have escaped me
19:17:36 <fen_> ab9rf: just use indexes for everything, thats the word on the street
19:17:59 <ab9rf> davean: there were plenty of common platforms in 1998 that either did not have mmap or had a very stupid mmap
19:18:18 <davean> ab9rf: it had been in POSIX for ages, what platforms lacked it?
19:18:29 <koz_> davean: Windows probably. :P
19:18:31 <ab9rf> davean: i don't recall. but we supported OS/2 :)
19:18:33 <koz_> (a fact likely true to this day)
19:18:39 <ab9rf> my guess is, in fact, windows
19:18:43 <edwardk> hell, windows subsystem for linux failed the test for being better than a 'very stupid mmap' just a year or two ago ;)
19:18:57 <koz_> edwardk: Colour me _decidedly_ unsurprised.
19:18:59 <ab9rf> which has a mmap-like functioanlity, but it is Not Much Like POSIX mmap 
19:19:01 <davean> edwardk: It fails it today, at least its mtime update isn't POSIX compliant
19:19:19 <edwardk> davean: color _me_ decidedly unsurprised
19:19:22 <ab9rf> also, in 1998 we'd be talking Windows 95... which is even more insane
19:19:43 <davean> edwardk: someone ran some of my test cases on it ....
19:20:05 <davean> edwardk: I had a test branch labeled "even worse than linux?" it hit
19:20:09 <monochrom> In 1998 I was using OS/2 too!
19:20:45 <ab9rf> ah, es, i remember this issue
19:21:08 <fen_> it would be interesting to see if the 2d version of a linked list could be represented nicely in memory 
19:21:09 <ab9rf> mmap had different semantics in at least older UNIX variants
19:21:35 <ab9rf> i'm not sure when they converged, but i remember running into this in the early 90s
19:22:11 <edwardk> Fun fact: way back in 94 i wrote a bunch of software for OS/2 warp for bulletin boards, like a "FOSSIL" driver for running stock DOS BBS software over telnet.
19:22:28 <koz_> edwardk: Is there _anything_ you haven't at some point worked on or around?
19:22:32 <fen_> it would also be interesting to see any recursive datatype that could outperform arrays for 2d grid convolutions 
19:22:37 <koz_> Seriously, I've found C++ tutorials you wrote.
19:22:41 <edwardk> if you tell me i can work on or around it
19:22:41 <ab9rf> edwardk: ooh, i remember FOSSIL
19:22:51 <davean> edwardk: If what I saw from my test cases is any indication, I'm shocked anything runs correctly on WSL
19:22:58 <ab9rf> edwardk: i ran BBSes in the late 80s and early 90s :)
19:23:16 <ab9rf> i was a FidoNET echomail coordinator for a year or so
19:23:33 <edwardk> ab9rf: likewise. i worked with one of the guys who wrote TAG, and wrote a Renegade clone called Psychosis
19:23:51 <ab9rf> edwardk: that is, i think, after my time
19:24:12 <ab9rf> i was out of the fight-o-net scene by 1991
19:24:52 <edwardk> Paul Williams (the Tag guy) was the regional hub for 1:120 (detroit) for fidonet. He moved his board onto an ISDN connection off the internet so he could toss long distance email without long distance calls. Helped keep fidonet running for a couple of years at the end after the advent of PPP.
19:25:22 <edwardk> My BBS era was mostly late 80s to 95 or so
19:25:24 <ski> (hm, i suppose mmap doesn't support "modulo" (a la blits) ?)
19:26:25 <edwardk> At one point I was listed as 'co-sysop' on something like 20 BBSes in the detroit area, because all I did was dial in to things and upload random shareware projects and deal with fidonet drama, which was ridiculous.
19:26:42 <ab9rf> edwardk: i had an UUCP-over-TCP mail feed from a friend of mine in california in that timeframe
19:28:22 <edwardk> guessing from the nick you were or are still a HAM?
19:28:33 <ab9rf> edwardk: still are, although it's been years since i operated
19:28:43 <ab9rf> but i renewed my license earlier this year
19:28:43 <edwardk> makes sense
19:29:23 <ab9rf> i actually found my yaesu HT just the other day
19:29:36 <ab9rf> needs a new battery, the LiON it came with is dead as a dodo
19:30:19 <fen_> radio guys huh? got to get that photon-photon interaction (perturbation thoery?)
19:31:11 <fen_> something about that with shockwaves in plasma (magnetic fluid)
19:31:12 <ab9rf> unfortunately, pretty much all of the discretionary spending this year went to pay for my son's surgery, so no new toys for me
19:31:56 <edwardk> =/
19:32:08 <fen_> lazer resonance chemistry is pretty cool on that front too
19:32:18 <ab9rf> so anyhoo
19:32:35 <fen_> radio? whats going on with that radio?
19:33:30 <edwardk> i confess, i never had the tolerance for putting up with the background noise to do ham radio
19:33:48 <ab9rf> edwardk: i'm not that fond of voice modes. much prefer digital.
19:33:55 <koz_> ab9rf: Sorry to hear about your son.
19:34:17 <ab9rf> edwardk: that said, i did really well working 20 meters phone for field day one year :)
19:34:29 <ab9rf> koz_: oh, he's doing fine. 
19:34:37 <koz_> ab9rf: I'm glad to hear.
19:34:44 <koz_> The word 'surgery' sounds quite scary.
19:34:55 <edwardk> i used to be familiar with x.25 and ax.25 from doing frame relay for a while
19:35:09 <edwardk> but i kind of let it all go as irrelevant 20 years ago
19:35:42 <ab9rf> edwardk: i have a truly craptastic 2m mobile that i bought on ebay for $50 that i managed to get to work for AX.25
19:36:06 <ab9rf> edwardk: i've actually gotten QSL cards for contacts made via the ISS repeater
19:36:29 <edwardk> damnit, i don't want this hobby =P
19:36:49 * geekosaur does very little aside from skywarn when necessary any more
19:36:53 <edwardk> this way lies me sitting around talking to derek elkins about software designed radio code all day every day
19:37:17 <koz_> edwardk: Nooooo, we need you to keep reifying category theory into Haskell.
19:37:20 <ab9rf> if i had a better radio, i could do much better at that; if you can split your TX and RX under program control to compensate for doppler shift, you can get a much larger window through RS0ISS
19:38:09 <ab9rf> skywarn where i live is a total joke
19:38:36 <ab9rf> by the time the net gets itself up and running, the storm will be out of the area
19:39:14 <ab9rf> and even when they get it up and running, they have not been able to reliably maintain a liaison to actually get reports passed onto the WFO, so just who is that you're reporting to?
19:40:03 <ab9rf> i'm a registered NWS volunteer spotter, but ham radio is my fourth choice option for reporting, after NWS direct dial, internet (email/web), and 911
19:40:51 <geekosaur> sounds like akron area skywarn; greater cleveland area's a bit better organized
19:48:47 * ski misses ddarius
20:12:16 * hackage crackNum 2.3 - Crack various integer, floating-point data formats  http://hackage.haskell.org/package/crackNum-2.3 (LeventErkok)
20:13:17 <ion> That verb generally has a different connotation than "decompose".
20:26:35 <ab9rf> heh
21:16:16 * hackage email-validate 2.3.2.9 - Email address validation  http://hackage.haskell.org/package/email-validate-2.3.2.9 (GeorgePollard)
21:44:01 <edwardk> ski: i got him to come out to ICFP, and hooked him up with an interview (and now job) over with ought in the bay area, so he's doing okay.
21:44:17 <edwardk> ski: i want to get him to log back into IRC, but baby steps =)
21:50:08 <koz_> :t preview
21:50:09 <lambdabot> MonadReader s m => Getting (First a) s a -> m (Maybe a)
21:50:24 <koz_> :t review
21:50:26 <lambdabot> MonadReader b m => AReview t b -> m t
21:56:44 --- mode: glguy set +v Phil25
22:01:59 --- mode: glguy set -v Phil25
22:42:51 <uboa> hi
22:42:58 <uboa> i'm going to spend the rest of the night learning haskell uwu
22:56:29 <koz_> uboa: Good plan?
22:56:38 <User_> Why isn't haskel a top programiimng language?
22:59:20 <Gigabitten> first time compiling a complicated program
22:59:30 <Gigabitten> 1 like = 1 prayer for bugs that don't fill 10 screens
23:01:10 <uboa> if i'm a noob to haskell should i even bother worrying about stack yet?
23:01:45 <Gigabitten> 15 bugs let's go
23:03:12 <Axman6> if you have GHC installed then stack won't make life any easier (or harder really) - ghci and eventually cabal will get you most of what you need uboa. For me, I find stack is easier to start new projects with
23:03:31 <koz_> Hiya Axman6.
23:03:35 <Axman6> o/
23:03:37 <koz_> What did you do with Axmen0-5?
23:03:56 <uboa> Axman6, thanks
23:05:13 <Axman6> they were inferior prototypes destroyed due to their flaws - this model is the ideal model, much like WD-40 was the fortieth attempt at developing a water dispersant
23:05:30 <koz_> Water dispersal sure is hard!
23:05:42 <Axman6> the aim of the project was to develop the world's first Axe/Man hybrid
23:05:53 <uboa> i love axeman great store
