00:06:56 <johnnymacs> hash tables are so good you guys
00:07:03 <johnnymacs> I am so glad that emacs lisp has hash tables
00:10:49 <EvanR> i like HashMap
00:15:02 <f-a> what is the correct way of enabling event logging in a .cabal file?
00:15:19 <f-a> I put ghc-options: -eventlog, but see no event.something file created
00:20:40 <cocreature> f-a: you still need to pass "+RTS -l" to the executable when you run it
00:20:57 <f-a> thanks cocreature
00:21:05 <cocreature> f-a: see https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/runtime_control.html#rts-eventlog
00:22:49 <MacSlow> Is there a semantic difference between the terms "data constructor" and "value constructor"?
00:24:50 <EvanR> no, unless you want to make distinctions for values that dont seem to have normal constructors... like 9 'c' and functions
00:25:30 <EvanR> 9# 9##
00:25:40 <EvanR> but i dont think anybody does
00:26:24 <MacSlow> EvanR: ah ok... I thought them to be the same but was a bit confused when reading about "data" in different articles ... one using "data constructor" and the other "value constructor" for the right side of the equal sign
00:27:21 <EvanR> "value constructor" is weird to me
00:28:40 <f-a> MacSlow: usually the stressed difference is between data constuctor and type constructor
00:28:57 <f-a> if you have that clear, it's ok
00:29:05 <MacSlow> f-a: like in data Maybe a = Nothing | Just a ?!
00:29:41 <MacSlow> there Maybe a is the type constructor and Nothing and Just a are the data/value constructors
00:29:53 <f-a> yep
00:29:56 <MacSlow> that's my current understanding of it
00:32:51 <cocreature> MacSlow: fwiw if you want to be pedantic, "Maybe" is the type constructor. "Maybe a" is the result of applying the type constructor to one argument
00:33:34 <MacSlow> cocreature: hehe... I'm not that pedantic... (yet?) :)
00:34:22 <MacSlow> oh... "(,) 1 2" works to construct a pair
00:35:03 <cocreature> if you enable TupleSections you can also use "(1,) 2" and "(,2) 1" which can often come in handy
00:35:59 <MacSlow> cocreature: is that a special compiler/ghci directive one has to use?
00:36:51 <cocreature> MacSlow: yes it’s a GHC extension. you can enable it by adding "{-# LANGUAGE TupleSections #-}" to the top of your source file, use default-extensions in your cabal file or pass -XTupleSections to ghc
00:40:18 <MacSlow> not touched much of cabal yet.
00:40:35 <MacSlow> only used it to grab gloss
00:41:32 <MacSlow> but already saw that for compiling more than one file CMake does not work at all with ghc
00:41:51 <MacSlow> stack and cabal are the tools of choice, right
00:43:18 <cocreature> yep
01:54:33 <`Guest00000> are there any "expandable" languages like lisp but properly typed/functional like haskell?
01:55:33 <f-a> `Guest00000: I think something like lisp macros and static typing don't work well together
01:55:41 <`Guest00000> why?
01:56:54 <cocreature> `Guest00000: you might want to take a look at hacket which is an attempt to mix racket and Haskell
01:57:06 <f-a> because you end up with a giga-sum-type, which destroys a bit the benefits of static typing
02:13:44 <centril> cocreature: I don't know what it is with parens... but I just can't seem to get over lisps use of so much parens
02:14:05 <centril> maybe Haskell is good at teaching an aversion to parenthesis
02:15:48 <cocreature> dunno, I feel like the parenthesis are just a cheap excuse that people to avoid having to take a closer look at lisp than an actual problem when you write lisp
02:16:52 <f-a> I dislike significant whitespace so much, centril ;_;
02:17:08 <centril> cocreature: yes probably; but Haskell is also syntactic nirvana
02:17:30 <srk> centril: I'm making fun of people who think prehistoric C-like syntax is good and that we still need semicolons
02:17:53 <centril> except for  x :: t   instead of  x : t    ofc... that one is a bit sad.
02:17:54 <`Guest00000> why would you ever need the type of * ?
02:18:28 <`Guest00000> srk: but how else would we write many statements/definitions/bindings in one line?
02:18:33 <centril> f-a: why?  significant space ends all "where do we place {" debates and it is pretty darn readable
02:18:59 <centril> srk:   ;   is useful if you are using LambdaCase and want a short oneliner
02:19:09 * [exa] gently reminds code generators and universality of text interfaces
02:19:19 <srk> yeah but not in case of C :)
02:19:27 <srk> feels so redundant
02:19:46 <centril> srk: that sounds like a general statement about C
02:19:54 <srk> :D
02:20:49 <`Guest00000> let's think * has some type
02:20:59 <`Guest00000> because it would be comfortable to any term have one
02:21:07 <[exa]> centril: highly suggest reading this https://www.cl.cam.ac.uk/~srk31/research/papers/kell17some-preprint.pdf
02:21:14 <`Guest00000> but will you ever need to name it?
02:22:53 <centril> [exa]: oh boy
02:23:46 <centril> [exa]: I read the abstract and it seemed like I will disagree deeply, but I will read on
02:24:54 <[exa]> centril: haskell people are sometimes forgetting about the 'communicative design' as well (luckily not as much as java people... :D)
02:25:26 <srk> what does that even mean? :)
02:25:56 <centril> [exa]: can you define communicative design? it seems like "design for a language where you can communicate well for readability"
02:26:38 <`Guest00000> is there even such a notion as  f y -> (y :: T) ?
02:27:13 <centril> `Guest00000: not sure what you're on about
02:28:17 <centril> \f y -> y :: T   is a lambda term  :: _ -> T -> T
02:28:25 <[exa]> srk centril: the paper explains it well; it's about communicating with other stuff in the computer. Especially about packaging systems and libraries.
02:28:45 <centril> [exa]: oh, hardware control
02:28:49 <f-a> centril: (re: whitespace) I feel moving code around is more troublesome
02:28:55 <`Guest00000> centril: well, dep. types have dependent product  (x :: T) -> f x; i thought, can we have something in reverse?
02:30:53 <centril> `Guest00000: your notation is strange... I'd write a dependent product (sigma type) as: \exists x : t. (x, f x)
02:32:14 <`Guest00000> i mean dependent product in the sense pi, not sigma
02:32:50 <f-a> centril: but what I miss the most are this-kind-of-identifiers ;_;
02:33:53 <centril> [exa]: hardware control is not intrinsic to C the language, but C the ABI -- and Rust can interoperate with the C ABI and hardware just as well as C can... so C is a redundant, unsafe (as in littered with undefined behavior around each corner) language with no abstraction power that should just die.
02:35:12 <[exa]> centril: please read on :]
02:35:32 <centril> f-a: hmm... I never had any trouble moving code around in Haskell... Might I suggest avoiding a lot of where blocks?
02:35:32 <i-am-the-slime> Hello! Can I add some dependencies if I do "runhaskell"?
02:35:58 <centril> I think if you use a lot of top level functions and use modules as abstraction boundary you get an easier time
02:36:25 <centril> and testability (quickcheck) is a nice bonus.
02:36:31 <f-a> centril: I do indeed (ab)use where
02:36:57 <f-a> i-am-the-slime: if you install them via cabal install... otherwise, I don't think so
02:37:21 <centril> f-a: I only use 'where' blocks if I need to enforce call-by-need (e.g: evaluate expr once); and even then I prefer let bindings
02:37:33 <i-am-the-slime> f-a: Can you give an example of this?
02:37:39 <`Guest00000> the length function from Vect could be thus reverse-dependent
02:37:39 <i-am-the-slime> So in that folder I would say cabal-install?
02:37:43 <[exa]> centril: btw I agree with most of Rust (and hope most C people will be able to get rid of C and go to Rust), but there are things where it just sucks (esp. any kind of pointer juggling)
02:37:48 <f-a> i-am-the-slime: do you use cabal?
02:37:49 <`Guest00000> length :: Vect n t -> (n :: Nat)
02:37:50 <f-a> or stack?
02:38:20 <centril> f-a: I wrote up a coding style guideline for how I write Haskell a while back: https://github.com/DATX02-17-26/DATX02-17-26/blob/dev/docs/code-style.md
02:38:28 <i-am-the-slime> f-a: Whatever works. For this I only have one .hs file right now.
02:38:33 <f-a> thanks centril
02:39:01 <centril> =)
02:39:02 <f-a> i-am-the-slime: then, say you need the `random` package for System.Random
02:39:10 <f-a> $ cabal install random
02:39:19 <f-a> and runhaskell will "see" it
02:39:32 <f-a> centril: why is let/where needed for call-by-need?
02:39:34 <centril> [exa]: I admit I don't do much low level programming myself; but I'm working on proposals to improve Rust =)
02:40:00 <i-am-the-slime> f-a: Will that install the package globally? I hope it will work on nixos.
02:40:15 <f-a> centril: some opinionated choices in there :P
02:40:38 <f-a> i-am-the-slime: I have 0 experience with nix and nixos
02:40:49 <centril> f-a: I assume GHC optimization passes can mostly figure it out, but I prefer to be explicit about not duplicating evaluation
02:40:53 <f-a> but yeah, it will be installed in ~/.cabal, globally
02:40:54 <centril> and it makes code more readable too
02:41:05 <f-a> I see
02:41:34 <f-a> i-am-the-slime: nowadays I always make a small project and use cabal new-build
02:41:47 <centril> f-a: what I'm most opinionated about is that:  eval (Var v) = exp1 ; eval (Lit i) = exp2  is horrible style and that case expressions are better
02:41:52 <centril> especially LambdaCase
02:41:54 <i-am-the-slime> f-a: I'm happy doing that
02:41:58 <f-a> i-am-the-slime: if you have an .hs file, put it in a folder, cd into it and try `cabal init -n`
02:42:08 <i-am-the-slime> why not cabal new-build?
02:42:19 <centril> if there's one thing I'd deprecate it is the ability to write  eval (Var v) = exp1 ; eval (Lit i) = exp2
02:42:38 <f-a> i-am-the-slime: I do use cabal new-build. I don't understand the question
02:43:07 <i-am-the-slime> Ah I thought `cabal init -n` was different from new-build since it doesn't have "new-build" in it :D
02:43:24 <f-a> sorry, init is to init the project
02:43:34 <f-a> but the quickest way for you is:
02:43:44 <f-a> 1. install the dependencies globally with cabal install
02:43:47 <f-a> 2. runhaskell
02:44:48 <i-am-the-slime> I think I want to try cabal init -n way
02:45:07 <i-am-the-slime> Okay now I have some more files :D
02:45:13 <f-a> good
02:45:44 <f-a> open the .cabal file and insert whatever dependency you have in build-depends
02:46:08 <f-a> (under the executable section)
02:46:16 <i-am-the-slime> I think I want Lens
02:46:46 <i-am-the-slime> so do I just put the word lens, there, I guess
02:46:52 <f-a> i-am-the-slime: then `lens` it will be, yeah
02:47:06 <i-am-the-slime> Do I need a comma after base?
02:47:12 <f-a> yup
02:47:15 <f-a> base,
02:47:16 <f-a> lens
02:47:20 <i-am-the-slime> or before lens, I guess
02:47:43 <f-a> now: cabal new-build
02:48:03 <f-a> if you miss any dependency, cabal will complain
02:48:15 <i-am-the-slime> unknown package lens
02:48:19 <i-am-the-slime> maybe "Lens"?
02:48:31 <f-a> is it the frst time you run cabal?
02:48:37 <f-a> try: cabal update
02:48:55 <i-am-the-slime> downloading the latest package list from hackage.haskell.org
02:49:06 <f-a> good, wait until it is over
02:49:25 <i-am-the-slime> it is
02:49:29 <f-a> cabal new-build
02:49:35 <i-am-the-slime> looking better
02:49:45 <f-a> good
02:49:53 <i-am-the-slime> now we're building  the world I guess :D
02:50:04 <f-a> yup, lens is a bigass package
02:50:11 <i-am-the-slime> Maybe I should have gone with microlens
02:50:18 <f-a> yup, personally it is what I use
02:50:27 <i-am-the-slime> I wanted to replace a list element at an index
02:50:33 <f-a> =D
02:50:39 <f-a> ok, microlens would do
02:50:52 <f-a> i-am-the-slime: are you looking for a repl by any chance?
02:51:22 <i-am-the-slime> I don't think there are any usable ones
02:51:32 <f-a> ghci?
02:51:40 <f-a> ok, just wait for the compilation to be over
02:51:54 <i-am-the-slime> I started out with ghci
02:52:01 <i-am-the-slime> but then I needed a few functions
02:52:19 <f-a> from a library, of that you defined yourself?
02:52:45 <f-a> in any case
02:52:47 <f-a> > [1..10] & ix 4 .~ 1337
02:52:51 <lambdabot>  [1,2,3,4,1337,6,7,8,9,10]
02:52:55 <f-a> ^-- this is what you probably need
02:57:57 <ongy> cabal new-repl is a thing btw.
02:58:18 <f-a> once he finishes to build the world =D
03:02:39 <i-am-the-slime> Now
03:02:43 <i-am-the-slime> I want to run the thing
03:03:09 <f-a> i-am-the-slime: if you want to toy with it in the repl
03:03:11 <f-a> just type cabal new-repl
03:03:51 <ongy> if you built a binary, one of the last couple of lines in the new-build is `link <some long path>`, that's where your binary is. The path is a bit long and unintuitive though :/
03:04:05 <f-a> if you want to see the compiled, it is under dist-newstyle
03:05:10 <i-am-the-slime> Will the repl also work if not everything compiles?
03:05:28 <i-am-the-slime> For example the functions that do compile?
03:05:31 <i-am-the-slime> Probably not, right?
03:05:40 <ongy> probably not. I think it first tries to compile the package and fails if it can't
03:06:05 <f-a> nope, I have ghcid open in a tab (a thingie that autorecompiles on save) and the actual repl on the other
03:06:12 <f-a> so the repl is always 'live'
03:07:23 <i-am-the-slime> So then what's the advantage of a repl over just putting stuff in a main method with a usable editor? I never really got this
03:07:39 <f-a> you can declare small functions
03:07:42 <f-a> inspect types
03:07:43 <f-a> etc
03:08:11 <`Guest00000> debugging
03:08:52 <`Guest00000> can have IORef (IO ())  and setting it in the repl while running the program which invokes it
03:09:06 <`Guest00000> and set repeatedly *
03:09:11 <i-am-the-slime> Is there a trick like in purescript where I can write _blabla and then the compiler will tell me possible functions that I could use instead of _blabla
03:09:30 <f-a> i-am-the-slime: yup
03:09:32 <f-a> write _
03:09:36 <i-am-the-slime> Okay sounds like you all like it quite a bit so I might give it a shot actually (the repl_ing)
03:09:59 <f-a> > 1 + _
03:10:02 <lambdabot>  error:
03:10:02 <lambdabot>      • Found hole: _ :: a
03:10:02 <lambdabot>        Where: ‘a’ is a rigid type variable bound by
03:10:07 <i-am-the-slime> f-a: Does this look only for functions which are in scope?
03:10:31 <f-a> i-am-the-slime: yep, but you can load other modules with :load
03:10:32 <`Guest00000> REPLicate
03:10:47 <f-a> or :m +Some.Module.From.A.Library
03:10:51 <i-am-the-slime> It would be cool if it could hoogle it also
03:11:00 <f-a> there is a possiblity
03:11:03 <f-a> just install hoogle
03:11:20 <f-a> and then add :def hoogle \x -> return $ ":!hoogle -q --colour -n 10 \"" ++ x ++ "\""
03:11:27 <f-a> to your ~/.ghci
03:11:39 <reactormonk> I want to talk to ghci from haskell code, sending it commands and receiving the output. Just shell out?
03:12:02 <i-am-the-slime> f-a: But then I have to type in my hoogle query, don't I?
03:12:08 <f-a> http://ariis.it/static/articles/no-docs-hackage/page.html <-- last paragraph, i-am-the-slime
03:12:46 <f-a> i-am-the-slime: yeah
03:13:42 <i-am-the-slime> I was hoping to just have the machine work for me :D. In a perfect world, this would be some kind of autocomplete for these holes, I guess, right?
03:14:13 <i-am-the-slime> Just put in the hole and then show stuff that fits in order of "scope", "dependencies", "any other libraries"
03:15:21 <f-a> reactormonk: is it a small thing?
03:15:55 <reactormonk> f-a, define "small"
03:15:58 <f-a> :)
03:16:24 <reactormonk> f-a, imagine literate-haskell, pretty much. Maybe I'll add markdown support for that one
03:21:07 <f-a> reactormonk: I am pretty sure there was a thread on -cafe similar to your problem, but I cannot find it now :s
03:23:37 <reactormonk> f-a, https://hackage.haskell.org/package/ghci ?
03:26:58 <f-a> could be reactormonk , but alas I have never touched it
03:27:05 <f-a> and documentation seems... scant
03:28:47 <reactormonk> yeah
03:29:10 <reactormonk> Any personal recommendation libraries to shell out with?
03:29:52 <f-a> well, there is https://hackage.haskell.org/package/process
03:32:06 <reactormonk> Heh, should have come up with that one ^^
04:23:21 <michalrus> Hey, can I somehow do `GHC.TypeLits.KnownSymbol n => Proxy a -> n`?
04:24:02 <cocreature> michalrus: should that "a" be a "n"
04:24:12 <michalrus> Yess.
04:24:20 <michalrus> Ummm.
04:24:22 <cocreature> you can use symbolVal
04:24:45 <cocreature> although that will give you a String
04:25:37 <cocreature> types of kind Symbol don’t have values so Proxy n -> n doesn’t really make sense
04:25:44 <michalrus> Yes. :(
04:25:50 <michalrus> OK, clear.
04:26:03 <michalrus> Thank you. =)
04:27:30 <michalrus> And… hmm. Could I somehow do 'SomeDataType and have it change to "SomeDataType" (but a KnownSymbol, not a String)?
04:28:07 <michalrus> This is all at type level, so… perhaps could be possible?
04:28:17 <cocreature> not sure I understand your questin. there is SomeSymbol
04:28:53 <michalrus> “This type represents unknown type-level symbols.
04:29:00 <michalrus> But this one would be known.
04:29:19 <cocreature> can you provide an example of what you’re trying to do?
04:29:22 <michalrus> I could write: `type X = "Something"`. Could I achieve the same thing with some… `type X = 'Something`?
04:30:12 <michalrus> I have `data Something = Something`, and I’d like to get a compilation error when `data Something` is renamed and `type X = …` is forgotten.
04:31:21 <cocreature> you can make type synonyms for types created using DataKinds if that’s what you’re asking
04:31:45 <michalrus> OK, I’ll read about that. :]
04:31:50 <michalrus> Thank you.
04:33:20 <cocreature> there is not much to read about, it literally works like you suggested, i.e., 'type X = 'Something"
05:00:07 <ongy> can I get the address of a C struct over ffi? Or should I add a minimal C function that just reads the address and returns a pointer?
05:03:54 <geekosaur> not sure I understand the question... ffi only deals with struct pointers directly, not struct-as-value
05:04:27 <geekosaur> so, depending on what you are doing, either it already does what you want or you're pushing ffi well beyond its limits and will need C
05:05:08 <ongy> I have a struct with a static address and I need a Ptr to said struct. The C-function equivalent would be: get_ptr() {return &struct;}
05:06:03 <geekosaur> sounds like you just want Ptr then?
05:06:18 <geekosaur> and the FFI will do what you want, because it can only deal with struct pointers directly
05:06:58 <ongy> how do I declare that pointer? I'm just missing the syntax
05:07:07 <geekosaur> (Ptr () if you don't use the internals directly, Ptr HaskellType if you do --- and you'll need a Storable instance or hsc2hs goop)
05:07:28 <cocreature> foreign import ccall symbolName :: Ptr a
05:07:48 <ongy> so I just ccall import it, even when it's not a function? That makes things easy :)
05:07:54 <geekosaur> yes
05:08:50 <geekosaur> ...although you may want a dummy Haskell type instead of () if you want some type safety, granting thta as soon as you cross the FFI barrier you've lost most of that type safety anyway
05:09:13 <geekosaur> (even a newtype is sufficient for that though)
05:09:25 <ongy> I'm aware of the types. I just wasn't sure whether I can import likethis
05:09:32 <ongy> Thanks :)
05:09:54 <cocreature> the report mentions it explicitely iirc
05:10:16 <geekosaur> likewise reclaling that
05:10:31 <cocreature> “If a Haskell program needs to access a C variable bar of integer type, foreign import ccall "&" bar :: Ptr CInt must be used to obtain a pointer referring to the variable.”
05:10:45 <cocreature> afaik GHC doesn’t care about the "&" part
05:14:21 <cocreature> hm I might be misrembering that part. at least I can’t find it documented anywhere
05:14:26 <ongy> wait, "&" bar? not "&c_name" <haskellName>
05:15:05 <cocreature> if you omit the c_name it defaults to the haskell name
05:15:30 <ongy> ah
05:23:18 <geekosaur> it makes sense. function names are implicitly pointers in C, so you don't need to & them; and a C variable isn't under Haskell's management, so you'd expect to have to use it via a Ptr and Storable or #peek/#poke instead of some kind of direct reference
05:23:33 <geekosaur> (jhc might have bent that a bit)
05:24:47 <geekosaur> likewise I'd expect an FFI reference to a static C array to not need the &, because it's already a pointer as far as C is concerned
05:25:20 <ongy> yea, it makes sense. But last I searched for it, I explicitly looked for an import type for variable pointers, and couldn't find one
05:25:23 <geekosaur> and if you set it up that way, the FFI translation is mechanistic and doesn;t need to know how C types work
05:32:00 <ski> geekosaur : well, a pointer to an array is different from a pointer to the first element of the array
05:38:41 <geekosaur> ski, that... is one of the places where understanding how C thinks vs. how other languages think is fairly important
05:39:32 <ski> it appears that pointers to arrays are not included among the supported foreign types, though
05:41:29 <ski> (`Ptr a' is a foreign type that can be used in argument and result types of a `foreign' declaration, for a type `a' that's exported by `Foreign'. however, looking into the docs on the latter, they do say "A value of type Ptr a represents a pointer to an object, or an array of objects, which may be marshalled to or from Haskell values of type a."
05:42:02 <ski> however, it appears that when they say "array" there, they really mean that we have a pointer to the first element of such an array, not a pointer to the whole array
05:42:43 <ongy> The typcal int *, size_t style C "array" to pass to functions
05:44:47 <ski> i'm not sure whether, if one has an `extern' array `blah' of type `int[42]' say, that one wants to import into Haskell, one would use `foreign import ccall "&" blah :: Ptr CInt'. i think one shouldn't use `foreign import ccall blah :: Ptr CInt', at least, since they say that that would refer to a function `blah' of no arguments returning such a pointer
05:44:55 <ski> ongy : *nod*
05:46:28 <ski> (it would be nice if one could import it with a type like  Ptr (CArray '42 CInt)  though, as an alternative)
05:47:30 <ongy> I wish type level arithmetic was more common :)
05:48:40 <ski> hm, ideally, that `42' would also be imported, rather than hardcoded, perhaps through grovelling through a `*.h' interface
05:49:16 <ongy> .hsc with sizeof(array)/sizeof(array[0])
05:50:32 <ski> (by "grovelling", i mean getting access to the values of such constants that a C program would see, on the platform in question. one way of implementing this is generating a small C program that outputs the values, perhaps by outputting a small Haskell module, which could then be imported)
05:52:26 <ski> (Riastradh's "Scheme48 Groveller" at <http://mumble.net/~campbell/darcs/s48-grovel/README> is an example of grovelling)
05:54:50 <nshepperd1> Does one import a C variable "int* var" as Ptr (Ptr CInt)?
05:55:33 <ongy> either that or hsc/c2hs I guess
06:01:33 <cocreature> I wonder what it says about my Haskell coding style, that haskell-mode’s syntax highlighting regularly breaks
06:02:17 <ongy> highlighting breakes? o.0 I don't agree with some of the auto-indent things vim does, but highlights?
06:02:43 <cocreature> if I reload the file it usually works again but before that it often just seems to give up on parts of my code
06:05:13 <cocreature> somehow my emacs setup seems to be getting more and more fragile over time
06:14:41 <reactormonk> nshepperd1, that only looks like one indirection, so just Ptr CInt? Or is it Ptr by default?
06:18:52 <nshepperd1> It could be Ptr CInt if the pointer is a constant (maybe? Depending how this import "&" stuff works?), but you'll need Ptr (Ptr CInt) if you want to modify it to point somewhere else at least
06:20:12 <ongy> depends? I would expect int[] to work, but `int *` should get a second indirection
06:23:08 <kuribas> cocreature: my one has always been fragile... Especially with the newest ghc
06:25:19 <cocreature> kuribas: I’ve just switched to not using anything except for syntax highlighting because I feel like the problems caused by the rest sadly outweigh the benefits they bring
06:25:37 <kuribas> cocreature: like?
06:26:03 <cocreature> e.g. hie seems to cause emacs to hang quite often, intero is extremely annoying when it doesn’t work, …
06:26:21 <cocreature> I’ll see how it goes, I can always go back
06:28:43 <Patternmaster> 13:24 -!- ShakespeareFan00 [~Shakespea@88.97.96.89] has joined #wikipedia-en
06:28:44 <Patternmaster> 13:25 -!- Someone_ [4280960e@gateway/web/freenode/ip.66.128.150.14] has joined #wikipedia-en
06:28:46 <Patternmaster> 13:25 < ShakespeareFan00> Hi people
06:28:49 <Patternmaster> 13:25 < Someone_> Hello. ANyone interested in blocking a sockpuppet?
06:28:52 <Patternmaster> 13:25 < Vermont> Always interested
06:28:54 <Patternmaster> 13:25 < Someone_> It is an LTA
06:28:56 <Patternmaster> 13:26 < Someone_> https://en.wikipedia.org/w/index.php?title=Names_of_Vietnam&action=history
06:29:00 <Patternmaster> 13:26 < Someone_> the most recent one.
06:29:02 <Patternmaster> 13:26 -!- kghardlined [~jnogab@114.7.19.5] has joined #wikipedia-en
06:29:05 <Patternmaster> 13:26 < paladox> ShakespeareFan00 hi, it's snowing here.
06:29:07 <Patternmaster> 13:26 < paladox> according to https://www.netweather.tv/live-weather/radar
06:29:10 <Patternmaster> 13:27 -!- mmecor [~mmecor@196.203.219.9] has joined #wikipedia-en
06:29:46 <kuribas> cocreature: I find it nice when it works, but very annoying when it doesn't.
06:30:11 <kuribas> cocreature: especially the "do you want to kill the whole session, etc..."
06:30:26 <cocreature> kuribas: yeah and the stupid window keeps popping up
06:30:50 <kuribas> the flycheck window?
06:31:08 <cocreature> no the one intero opens when it fails to start
06:31:17 <kuribas> ah... I never used intero...
06:31:18 <Patternmaster> sorry Ctl-K when I meant Ctl-C
07:08:07 <centril> [exa]: so, I read the paper... and did not agree at all
07:09:52 <centril> in particular, unsafe Rust has the same communicativity as C does afaik -- but you can relegate unsafety to a small subset (say 1%) of your code and check those parts heavily.
07:17:31 <[exa]> centril: yeah, which is basically C++.
07:17:38 * [exa] ducks
07:30:51 <ertes> what's the current state of cyclic imports?  do we still stub files?
07:36:57 <geekosaur> yes
07:37:36 <geekosaur> changing that ... the hidden pits are filled with landmines
07:38:22 <geekosaur> (*nobody* likes hs-boot files. but so far all attempted designs for cures end up being worse than the disease)
07:40:31 <ertes> ok, thanks
07:40:49 <ertes> not terribly important, but if that ever goes away, i'd like to use cyclic imports
07:41:53 <sepakorayl> what was the paper called again ?
07:42:05 <ski> centril :
07:42:15 <sepakorayl> btw what can SML Functors do that is hard to express in haskell?
07:42:27 <ski> centril : "maybe Haskell is good at teaching an aversion to parenthesis" -- i think there's no reason for that
07:43:31 <ski> but then i try to more or less minimize my use of `$'. in general, brackets are nothing to be afraid of. in certain circumstances, one may prefer stuff like `(foo x . bar y z . baz t) (...)', and, rarely, using `$'
07:44:08 <geekosaur> sepakorayl, parameterized modules. see for example parsec/megaparsec token parsers, whcih currently require you to import and then make local bindings parameterizing the token parsers with your language definition; in SML, you parameterize the import and you're done
07:44:27 <ski> sepakorayl : easily swapping out one implementation of a basic data structure for another, in an implementation of some more involved abstraction. even using both instantiations of said abstraction, in the *same* program
07:45:19 <sepakorayl> can't type families achieve something similar ?
07:46:43 <geekosaur> that doesn;t solve the second issue that ski mentioned very well, unless you annotate every usage in such a way that it knows which one to use
07:46:54 <ski> sepakorayl : this can also help separation of concerns. the implementation of the abstraction can't see the implementation of the data structure that it is parameterized over. however, when you use an instantiation of the abstraction, on some particular implementation of the basic data structure, *you* can still access the representation of that basic data structure, inside the larger abstraction
07:48:16 <ski> (this would be a case of parametricity, i think)
08:09:30 <sternmull> i read https://en.wikibooks.org/wiki/Haskell/Denotational_semantics and don't understand why the statement after "That's another reason why we use a different symbol:" says that 1 and 2 are not equally defined. Can someone explain that?
08:14:15 <geekosaur> either we are looking a different things, or you really are lost. they are equally *defined*. they are not *equal*
08:16:34 <geekosaur> oh, hm. you are treating the underbar ("=") as talking about definedness, but as described there it is *value* equality. "either {\displaystyle b} b is more defined than {\displaystyle a} a or both are equal (and so have the same definedness)"
08:16:54 <geekosaur> the parenthetical at the end wouldn't b eneeded if it meant equally *defined*, therefore it must mean equal *value*
08:17:05 <sternmull> geekosaur: English is not my native language. But i reed it as: "1 <= 2" and "2 <= 1" are not true. (cant use the real symbols in here...). I understand that it is not talking about the real less-equal in the value-sense.
08:17:35 <geekosaur> sternmull, yes, you just proved my contention
08:17:54 <geekosaur> you are reading it as "less than or equally defined"
08:18:02 <geekosaur> it means "less defined or precisely equal"
08:18:03 <sternmull> yes
08:18:24 <sternmull> oh, ok. Have to reread that part.
08:18:32 <geekosaur> see the part I quoted
08:18:41 <geekosaur> (sadly TeXified for extra confusion)
08:19:06 <ab9rf> my native language is english and i had to read that part of that document three or four times before i understood it. :)
08:19:26 <geekosaur> so "1 [<=] 2" asserts that eitger 1 is less defined than 2, or 1 is *the same as* 2. not just "is as defined as"
08:19:45 <geekosaur> which is why they called it semantic approximation order
08:19:49 <geekosaur> instead of definedness order
08:20:22 <sternmull> so <= means "definedness" and equality means "exact same value"?
08:22:01 <geekosaur> in the semantic approximation view, yes
08:22:07 <geekosaur> it's a bit screwy
08:22:41 <geekosaur> it;s switching "languages" on you, as it were
08:22:59 <sternmull> it confuses me that two integer values (like 1, 2, 3) are not equally defined... but maybe it gets clear if i read on.
08:23:00 <Unode> anyone familiar with stack might know what is going on here? http://dpaste.com/0S99RWF
08:23:15 <geekosaur> you still read it backwards
08:23:20 <geekosaur> equally defined, but not equally valued
08:23:37 <geekosaur> see: A quick way to remember this is the sentence: " {\displaystyle 1} 1 and {\displaystyle 2} 2 are different in terms of information content but are equal in terms of information quantity"
08:24:09 <geekosaur> in [<=], the "<" is talking about definedness but the "=" is talking about value
08:24:12 <hpc> it helps to realize that definedness is a partial order, not a total order
08:24:59 <Axman6> (True,_|_) is as defined as (_|_,False) yeah?
08:25:04 <hpc> http://blog.ezyang.com/2010/12/hussling-haskell-types-into-hasse-diagrams/ is also really helpful
08:25:11 <hpc> Axman6: they are incomparable in definedness
08:25:19 <geekosaur> this is because if oen is less defined than the other, you can't talk about equal at all
08:25:34 <sternmull> but it also says "neither 1 ⊑ 2  nor 2 ⊑ 1 hold". Which i understand as "1 and 2 are not equally defined".
08:25:42 <geekosaur> er. you can;t talk about equality of value at all
08:26:00 <geekosaur> you do not understadn right
08:26:12 <geekosaur> I have been failing to explain that, though, so let someone else try
08:26:43 <geekosaur> neither "1 is either less defined than or exactly equal to 2" nor "2 is either less defined than or exactly equal to 1" hold
08:26:54 <sternmull> still thank you for trying. I will read it again and see if i get it.
08:27:06 <ackthet> wait, is ghc-mod dead?
08:27:20 <ackthet> ah https://github.com/DanielG/ghc-mod/issues/900
08:27:22 <hpc> sternmull: what you're having trouble with here is the key property of a partial order
08:28:34 <hpc> sternmull: to simplify it a bit, let's use Bool which has this diagram of definedness - http://blog.ezyang.com/img/haskell-hasse/bool.png
08:28:42 <hpc> the only valid comparisons of definedness are along the grey lines
08:28:53 <hpc> _|_ [<=] False
08:28:58 <hpc> _|_ [<=] True
08:29:04 <geekosaur> ghc-mod is always messed up by ghc upgrades
08:29:28 <ackthet> it seems like the devel is kinda tired of it
08:29:36 <ackthet> is there another similar tool recommended?
08:29:44 <geekosaur> not yet.
08:29:56 <ackthet> I see
08:30:10 <geekosaur> ghc has sprouted a new, more stable interface... but not backward compatible, and few tools can use it yet
08:30:50 <Axman6> haskell-ide-engine is coming along nicely, and has the support from several of the developers of the previously popular tools for that sort of thing
08:31:01 <geekosaur> ^ that, yes
08:31:34 <geekosaur> its a better approach than ghc-od to start with... but needs support ghc didn't have before
08:31:48 <geekosaur> so ghc-mod worked by pokking around inghc's guts, which is inherently nonportable
08:31:51 <sternmull> hpc: I think i almost got it. But i asked earlier 'so <= means "definedness" and equality means "exact same value"?' and it seems that was not the whole truth.
08:32:21 <geekosaur> sternmull, your reading keeps treating "exact same value" as "as defined as"
08:32:34 <geekosaur> which it specifically is not
08:33:04 <geekosaur> 1 != 2. 1 is as defined as 2, but that is not what [<=] talks about.
08:33:16 <Axman6> if you want to use haskell-ide-engine, I would recommend cloning the repo and using make - it compiles several versions with different GHCs and appends the ghc version. the VC Code integration provides a script for choosing the right one automatically
08:33:25 <hpc> ah, i see the issue here
08:33:27 <geekosaur> "less defined, or (equally defined and equally valued)"
08:33:30 <hpc> 1 and 2 are not as defined as each other
08:33:31 <ertes> when i'm 'fail'ing in Q, how do i show a Name to the user in a useful way, i.e. taking current imports into account the way GHC does?
08:33:42 <geekosaur> is what the page describes
08:34:09 <sternmull> but doesn't "equally valued" imply same "definedness"?
08:34:56 <hpc> sternmull: i think what they mean is, say in http://blog.ezyang.com/img/haskell-hasse/int.png
08:35:15 <hpc> all of the integers are "fully defined" in the sense that in that style of graph they're all at the same height
08:35:34 <hpc> but for the purpose of [<=], which follows the lines on the graph
08:35:43 <hpc> 1 and 2 are not comparably defined
08:36:21 <sternmull> But "1 [<=] 1" is true. Right?
08:36:24 <geekosaur> but the page hasn't gotten that far yet. Ive been using the page's terminology
08:36:25 <hpc> yes
08:36:30 <hpc> [<=] is reflexive
08:36:38 <Axman6> Ed Kmett's talks on propogators do a fairly good job talking about the join semi latice of definedness of haskell values roughly, and I have probably terribly misquoted most of that)
08:36:45 <hpc> there's only one node called 1 on the graph
08:36:48 <geekosaur> (it starts talking about that in the next section)
08:36:51 <hpc> and every node is [<=] to itself
08:37:19 <hpc> but because it's a partial order, some comparisons you would expect to be valid are missing
08:37:42 <hpc> Axman6's example with tuples is good to think about as well, it looks like http://blog.ezyang.com/img/haskell-hasse/tuple.png if you diagram it
08:38:51 <sternmull> geekosaur, hpc: Thanks i think i that helped.
08:39:10 <hpc> http://blog.ezyang.com/2010/12/hussling-haskell-types-into-hasse-diagrams/ is really helpful in visualizing it
08:39:10 <ertes> apropos definedness, i found this article to be very interesting: http://blog.sigfpe.com/2008/02/how-many-functions-are-there-from-to.html
08:39:12 <geekosaur> that page might need to be rethought, since what it really means dones't coe out until the next section
08:39:19 <geekosaur> *doesn't come out
08:39:24 <hpc> as well as its follow-up posts
08:39:59 <geekosaur> it's using a confusing approximation in the part you tripped over
08:40:21 <hpc> sternmull: and if you want to understand partial orderings in general, another good example is the "is subset of" operation
08:40:32 <hpc> {} is a subset of {x} and {} is a subset of {y}
08:40:40 <hpc> {x} and {y} are both subsets of {x,y}
08:40:44 <hpc> but neither is a subset of the other
08:42:19 <sternmull> hpc: I know the concept of partial ordering from associative containers in C++ where keys need to have partial order. But your subset example is indeed a helpful description of its nature.
08:42:56 <hpc> :D
08:44:20 <athan> Is there a QQ for building regexes? Like [regex|/^(\.)+(\?)$/|] or something?
08:46:30 <Axman6> yes
08:47:01 <Axman6> I think trying to remember where it is
08:47:29 <geekosaur> http://hackage.haskell.org/package/rex-0.5.2/docs/Text-Regex-PCRE-Rex.html
08:48:04 <Axman6> lens-regex also provides https://hackage.haskell.org/package/lens-regex-0.1.0/docs/Text-Regex-Quote.html
08:57:56 <athan> awesome, thank you!
09:10:22 <Tuplanolla> @index bzzt
09:10:22 <lambdabot> bzzt
09:17:32 <qeyoa> should module name be the same as file name ?
09:18:10 <ertes> is there a way to anti-quote in [d| |]?  or what's the proper way to refer to a Name or Type?
09:25:04 <Tuplanolla> Use a `$(splice)`, ertes?
09:25:47 <ertes> oh, i think i can't refer to a Name directly…  i have to reify it
09:26:30 <ertes> or do i?  TH is kinda confusing
09:30:06 <delYsid> Any suggestions for a package to read for a rainy saturday afternoon?
09:30:42 <Tuplanolla> @hackage kan-extensions
09:30:43 <lambdabot> http://hackage.haskell.org/package/kan-extensions
09:31:20 <ertes> Tuplanolla: that was the answer…  i just had to do a few transformations
09:31:21 <ertes> thanks
09:32:38 <ertes> $(pure (LitT (StrTyLit lName)))  -- is there a shorter way to write this?
09:32:47 <cocreature> qeyoa: yes with minor idfferences: replace dots in the module name by directory slashes and append .hs
09:36:07 <MarcelineVQ> ertes: what is lName?
09:36:53 <MarcelineVQ> oh just a string
09:36:56 <ertes> MarcelineVQ: lName :: String
09:36:59 <MarcelineVQ> strTyLit
09:37:13 <ertes> yeah…  i'm trying to write a TH function to generate lens labels for me
09:37:23 <MarcelineVQ> https://hackage.haskell.org/package/template-haskell-2.13.0.0/docs/Language-Haskell-TH-Lib.html#v:strTyLit
09:37:31 <ertes> oh
09:37:45 <ertes> MarcelineVQ: thanks
09:37:51 <MarcelineVQ> all the godo helpers are hidden away in Lib
09:37:53 <MarcelineVQ> *good
09:38:16 <ertes> yeah, i'm still in the discovery phase
09:38:36 <ertes> not overly fond of using TH in the first place, but i didn't like the generics-based approach for a few reasons
09:40:14 <MarcelineVQ> ertes: you'll want to be using   [|   |]  for a lot of stuff so be sure to look into that, the helpers make things shorter but [|   |]  can make things a lot shorter
09:42:04 <MarcelineVQ> the standard tut is here https://wiki.haskell.org/Template_Haskell
09:42:17 <ertes> MarcelineVQ: i'm using it, but don't i need to anti-quote to refer to names from outside?
09:42:39 <MarcelineVQ> I don't know what that is, but you can $( )  within [| |]
09:42:59 <ertes> MarcelineVQ: that's exactly what i'm doing and where my question came from =)
09:43:02 <MarcelineVQ> hehe
09:43:06 <MarcelineVQ> I'm not so good with terms
09:43:12 <ertes> <ertes> $(pure (LitT (StrTyLit lName)))  -- is there a shorter way to write this?
09:43:27 <ertes> ^ this is inside a [d| |]
09:43:55 <Tuplanolla> There are places where you can avoid the `$(splice)` notation, but I prefer to use it anyway to clue the reader into realizing it's a splice.
09:44:54 <Tuplanolla> Generating lenses, say.
09:46:02 <ertes> actually if there were "instance synonyms", i might not be doing any of this
09:48:28 <MarcelineVQ> was  $(strTyLit lName)  appropriate or is this still unsolved?
09:49:42 <ertes> MarcelineVQ: it was appropriate in my little experiment…  now i'm doing the real thing
09:49:57 <ertes> so prepare yourselves for a lot more stupid TH questions =)
09:50:23 <robstr_> I'm trying to parse a json with aeson, I want to store the value as _double_ but the server reply's it as string with quotes, whats the best way do resolve this ?
09:52:44 <EvanR> if that string is wrapped with something, modify the FromJSON code for that wrapper to interpret that field as a double and not a string
09:53:29 <EvanR> otherwise look at it as a Value, access that string, and parse it as a double
09:54:05 * EvanR imagines there are aeson lenses that do all this
09:54:15 <robstr_> EvanR: ty, I'm relative new and I'm using do syntax and {..}
09:55:02 <robstr_> EvanR: what library to look at for ignoring "
09:55:13 <EvanR> no what
09:57:03 <robstr_> i mean, `read "\"1\"" :: Int` is not possible I could drop the first and last char .. but there should be a better way, right ?
09:57:09 <EvanR> no
09:57:35 * EvanR installing aeson right now
09:58:39 <robstr_> I could do `tail . init` :P  ...
09:59:45 <ertes> this is turning out to be far more complicated than i thought…  at least if i want to handle types of kind other than *
10:00:15 <EvanR> robstr_: what type is "\"1\""
10:00:40 <EvanR> and where did it come from
10:01:05 <robstr_> It's one element from a json response
10:01:16 <EvanR> what type is it
10:01:56 <ertes> robstr_: so the JSON contains a decimal string representation of a number, and you want to parse it into an actual number?
10:02:22 <EvanR> yeah its trivial in aeson
10:02:31 <ertes> if yes, just decode twice
10:02:41 <EvanR> i just dont have it installed to test my answer
10:02:51 <tdammers> so for clarity, the JSON you're getting is something like this: {"number":"1.234"} ?
10:03:04 <robstr_> tdammers: right
10:03:16 <ertes> robstr_: are you using FromJSON?
10:03:24 <robstr_> yes
10:03:29 <EvanR> hence my unanswered question about the wrapper
10:03:51 <ertes> robstr_: are you writing the FromJSON instance by hand?
10:04:03 <robstr_> ertes: yes, i need to change names
10:04:18 <tdammers> then "remove the quotes" is not the right approach, because no matter what type you parse this into, there will be no actual quotes in the value
10:04:48 <tdammers> the quotes are part of JSON syntax, Aeson "removes" them while parsing, so the string value you get will not contain any quotes
10:05:13 <tdammers> (but if you `show` it, quotes will be added in order to match Haskell's string literal syntax)
10:05:19 <geekosaur> (and don't confuse the quotes ghci or lambdabot shows you --- the "external form" --- with the internal form)
10:05:29 <geekosaur> > "a"
10:05:31 <robstr_> Ok, aeson gives me: *** Exception: JsonHttpException "Error in $[0]['base']: expected Double, encountered String"
10:05:33 <lambdabot>  "a"
10:05:40 <ertes> robstr_: then just decode the string you get from the outer JSON
10:05:40 <geekosaur> > text "a"
10:05:44 <lambdabot>  a
10:06:07 <ertes> robstr_: Scientific has a FromJSON instance, so it should be fairly easy
10:06:09 <EvanR> JsonHttpException from aeson?
10:06:28 <ertes> robstr_: oh, in fact there is a function for this
10:06:42 <robstr_> EvanR: oh its from `req`
10:06:47 <ertes> robstr_: https://hackage.haskell.org/package/aeson-1.3.0.0/docs/Data-Aeson.html#v:withEmbeddedJSON
10:06:59 <EvanR> what...
10:08:22 <EvanR> decode "1" :: Maybe Double
10:08:26 <EvanR> Just 1.0
10:08:27 <EvanR> :|
10:08:30 <ertes> robstr_: so if (j :: Value) is of the form (String _), then you can do this:  withEmbeddedJSON "decimal number" pure j
10:09:13 <robstr_> ertes: thank you , I will try this
10:09:23 <ggVGc> decode "one" :: Maybe Int
10:09:29 <ggVGc> > decode "one" :: Maybe Int
10:09:32 <lambdabot>  error:
10:09:33 <lambdabot>      Variable not in scope: decode :: [Char] -> Maybe Int
10:10:32 <ertes> (the existence of withEmbeddedJSON kinda has a cynical overtone…  people fuck up JSON often enough that there is a predefined function to handle it)
10:24:07 <geekosaur> aka experienced programmer >.>
10:24:54 <maaku> I seem to recall a haskell-written CAD or 3d modeller, called "edges" or "wings" or something like that. Anyone know what I'm thinking of?
10:25:03 <maaku> google is being useless at the moment
10:26:49 <maaku> n/m found what I'm looking for
10:32:35 <srk> huh, curious about that as well :D
10:35:17 <phz_> peeps, how am I supposed to notify that I’ve updated a library here since the issue is closed? https://github.com/fpco/stackage/issues/3389
10:37:59 <cocreature> phz_: make a PR that lifts the constraints on your lib in build-constraints.yaml
10:38:59 <phz_> ok, that has changed
10:39:11 <phz_> I thought I wouldn’t get blacklisted that quick…
10:41:15 <phz_> cocreature: https://github.com/fpco/stackage/blob/master/build-constraints.yaml#L966
10:41:18 <phz_> it’s still there
10:41:34 <phz_> though I had a lot back in the old days
10:41:42 <phz_> like, al
10:42:15 <cocreature> phz_: https://github.com/fpco/stackage/blob/master/build-constraints.yaml#L3766
10:42:23 <cocreature> that’s the thing that needs to be removed
10:42:48 <phz_> cocreature: aren’t they gonna remove it at some time anyway?
10:43:18 <cocreature> sure you can also just wait
10:43:26 <phz_> great then
10:43:29 <cocreature> but I had the impression that you want to take action :)
10:43:38 <phz_> I was just surprised yeah
10:43:46 <phz_> I’ll try to see what’s happening with al
10:43:54 <phz_> IIRC, it was due to pkg-config on Ubuntu
10:43:57 <phz_> not my package :(
11:08:45 <ski> `Guest00000 : "is there even such a notion as  f y -> (y :: T) ?" -- symmetric to the usual dependent function type, which i'll write `(val x : T) -> ..x..' here, one can imagine a type `..c.. -> (cont c : T)', where `c' is a continuation of type `T'. also, mirror image to `(val x : T) /\ ..x..', one can have `(cont c : T) \/ ..c..', when if the user disproves `T', using `c' as refutation, then we'll prove `..c..', which may depend on that refutation/contin
11:09:07 <ski> er, cut off near ".., one can have `(cont c : T) \/ ..c..', when if the user disproves `T', using `c' as refutation, then we'll prove `..c..', which may depend on that refutation/continuation")
11:09:24 <ski> hm `length :: Vect n t -> (n :: Nat)' is an interesting example
11:10:00 <ski> it would seem that we can only call `length vec', if we know that `vec' has the length that this call will return ?
11:11:02 <EvanR> hah
11:11:09 <EvanR> backwards pi
11:11:17 <ski> (this might be useful in a reversible-in-the-sense-of-logic-programming setting, where you can feed an output to a function, solving `f x = y' for `x', getting all possible `x's which satisfy that. or feed both the output, and some inputs, getting back solutions for the remaining inputs)
11:15:31 <k0ral> Hello
11:16:03 <infinisil> k0ral: hi!
11:18:38 <k0ral> I am able to write `splitEven :: Vector (2*n) a -> (Vector n a, Vector n a)` and `splitOdd :: Vector (2*n + 1) a -> (Vector n a, Vector (n+1) a)` ; but how should I write a `split` function that works on either even or odd length vectors ?
11:25:43 <eschnett> k0ral: you would express this in a more generic way, using a signature such as "split :: Vector n a -> (Vector (Div n 2) a, Vector (n - Div n 2) a)"
11:26:14 <eschnett> k0ral: see also <https://hackage.haskell.org/package/ghc-typelits-extra-0.2.4/docs/GHC-TypeLits-Extra.html>
11:35:01 <k0ral> eschnett: is that considered hacky, or is it just a window into the future of Haskell/GHC ?
11:35:13 <k0ral> eschnett: I mean, seeing as an OPTIONS_GHC is required for that to work
11:35:54 <eschnett> k0ral: i'd consider it leading edge, in the direction of liquid haskell. this has future, but is not mainstream.
11:41:37 * ski can has past ?
11:48:18 <k0ral> eschnett: using your signature, I wrote `split vector = (Vector.take vector, Vector.drop vector)` but it fails to compile: using `take` leads to "Couldn't match type ‘n’ with ‘Div n 2 + m0’"
11:49:04 <k0ral> it looks like Vector.take fails to understand that `Vector n a` is the same as `Vector (Div n 2 + Div n 2) a`
11:49:29 <k0ral> hmmm and actually what I wrote above is incorrect for odd `n`
11:50:18 <k0ral> I would expect Vector.take to figure that `n` is `Div n 2 + p` for some p
11:50:35 <EvanR> just implemented (==) as not . different, where different doesnt inspect the whole structure
11:50:55 <EvanR> isnt this almost always faster? is that the default when deriving Eq ?
11:51:32 <k0ral> EvanR: how would `different` work without inspecting the whole structure ?
11:51:41 <EvanR> quite easily
11:51:47 <ski> EvanR : using concurrent evaluation ?
11:52:00 <EvanR> example
11:52:06 <EvanR> different Z (S _) = True
11:52:16 <EvanR> different Z Z = False
11:52:25 <EvanR> different (S x) (S y) = different x y
11:52:47 <k0ral> equal Z (S _) = False
11:52:55 <k0ral> equal Z Z = True
11:53:01 <k0ral> equal (S x) (S y) = equal x y
11:53:07 <k0ral> how is that less efficient ?
11:53:08 <EvanR> i stand corrected
11:53:33 <EvanR> for some reason i thought (==) had to look at the whole structure
11:53:38 <ski> better take something like a pair, a list, or a tree as example -- where you have multiple "sibling" components
11:54:03 <EvanR> yes i am looking at a tree, the runlength encoded natural
11:54:24 <ski> it'd be nice if both `apart (False,_|_) (True,_|_)' and `apart (_|_,False) (_|_,True)' yielded `True'
11:54:45 <EvanR> yeah theres that
11:55:09 <ski> also see "parallel or",`gustave',(Barry's) `majority'
11:56:00 <ski> (and `unamb',`lub', and possibly `lvish')
11:56:02 <MarcelineVQ> is this where lub comes in?
11:56:04 <MarcelineVQ> yay
11:56:21 <Tuplanolla> It seems to come up every day these days.
11:56:23 <EvanR> wait wait... if the algorithm calls to test if two things are "not equal" then surely i am not crazy in thinking "not (x == y)" is bad right
11:56:52 <EvanR> i.e. it would look through everything to make sure its equal, then negate the bool
11:57:13 <EvanR> and i want to actually test if they are different
11:57:24 <k0ral> x == y can return False early
11:57:34 <k0ral> as soon as one difference is found
11:57:38 <EvanR> oh
11:58:26 <EvanR> well at the very least a difference test saves me a not right
11:59:26 <k0ral> it just depends on the way the structure is traversed, you may just want to pick a smart traversal function that first inspects elements that are likely to be different
12:00:00 <k0ral> but I really don't see any difference (pun not intended) in performance between equal and not . different
12:00:06 <EvanR> so i am on the right track that equality test, "not" or not, performance rests on finding differences
12:00:17 <EvanR> soon
12:00:38 <EvanR> and verifying *is equal* will take the most work
12:01:12 <frerich> EvanR: For 'different' to be able to return False, it also has to traverse to the very end.
12:01:13 <EvanR> k0ral: the difference was, between doing different or not equal
12:01:21 <EvanR> not the other way around
12:01:45 <EvanR> different False, is equal , sure yeah ok all the same
12:01:47 <ski> in Oz, `x == y' can work even if part of `x' and `y' aren't determined yet. it'll just postpone that check, and carry on checking the rest. sometimes that suffices to determine the overall result. otherwise it blocks, waiting for another thread to instantiate `x' ior `y' enough for it to continue
12:02:06 <EvanR> that is handy
12:02:33 <EvanR> since i have this tree, i really do seem to have an opportunity to exploit some of this... maybe
12:02:44 <k0ral> whether you use "not . equal" or "different", it will be equally slow when you call it on identical elements, since a complete scan of the structure is required to prove it
12:02:47 <ski> (this is also related to coroutining support in Prolog systems, where it'd not be another thread, but rather this goal would delay itself until it becomes acticated later by such instantiation)
12:03:33 <EvanR> k0ral: this algorithm ends when it finally determines two things are equal, so thats good
12:03:42 <ski> (see e.g. the dif/2 predicate in Prolog. and freeze/2,when/2 in general)
12:04:43 <ski> (in anyway, i don't think there's an assymetry between `==' and `/=' here)
12:04:58 <EvanR> yes i see that now
12:05:11 <EvanR> but there is an assymetry between equal and different
12:05:26 <EvanR> equal True will be a lot of work, different True possibly not
12:06:02 <ski> i don't see how there would be an asymmetry between "equal" and "different"
12:06:12 <EvanR> the amount of work to get a True
12:06:23 <ski> one of them could be defined as calling the other, and then `not', or the other way around -- i don't see a difference
12:07:13 <EvanR> i think we all understand each other
12:07:22 <k0ral> eschnett: I had to add several type annotation (Constraints to be accurate) to ghave it work: split :: KnownNat n => KnownNat (Div n 2) => n ~ ((Div n 2) + p) => KnownNat p => Vector n a -> (Vector (Div n 2) a, Vector p a)` ; is that wrong ?
12:07:30 <EvanR> this is like the opposite of violent agreement
12:08:33 <EvanR> so... in the end i should not try to implement a difference test if it deriving Eq would do the same thing
12:10:06 <k0ral> EvanR: unless you want a specific way to traverse the structure to optimize early returns of "equal", I don't see the benefit :)
12:10:21 <EvanR> early returns of equal False
12:10:24 <EvanR> got it
12:10:39 <k0ral> yes, equal False :)
12:11:18 <EvanR> if you were trying to implement Eq Real though... you have to admit, the situation is pretty assymetric!
12:12:40 <frerich> EvanR: I believe it's only imaginable to have a type for which deciding equality is harder (or easier) than deciding inequality if you have some funny definition of equality.
12:14:08 <ski> hm, on a quick look in `monad-par' and `lvish', i don't find operations corresponding to Oz `==' and `\=='
12:15:10 <ski> EvanR : possibly
12:15:49 <ski> frerich : semi-deciding is, of course, another matter :)
12:43:18 <catern> idea: type-based syntax highlighting
12:43:59 <catern> with an effect system, you could highlight functions using specific effects in specific colors :)
12:44:18 <catern> bright red for mutation, radioactive green for exceptions, etc
12:46:16 <EvanR> wouldnt radioactive green make more sense for mutation, secret of the ooze and all
12:46:28 <EvanR> red for crashing exceptions
12:47:03 <EvanR> "crash and burn"
12:47:28 <joncfoo> aside: just want to point out that `hspec-expectations-pretty-diff` is pretty darn nice - super useful to see diffs for failed output in Hspec test failures :)
12:47:37 <catern> that's true, that's better
12:47:41 <EvanR> (in a presentation, i used radioactive sign for mutable cells, biohazard for other side effects)
12:47:59 <EvanR> er not biohazard, inverted pentagram
12:48:08 <Rembane> Metal.
12:49:04 <catern> possibly actual application of this: in a coroutine system, dangerous red (or whatever color) for functions that yield
12:49:26 <catern> then if you're doing the crazy thing of trying to do mutations atomically by not calling yielding functions
12:49:33 <catern> you can easily see when you mess it up :)
12:51:08 <catern> (I guess essentially this is just a question of how to surface effect-system-information to the programmer, since it doesn't appear in the surface syntax of a language)
12:53:29 <dede> !keep sami4ak
13:02:33 <nbro> Hi
13:04:56 <platz> thinking of 'scan' as a map with state i think is more intuitive than a "foldl with a list of succesive reduced values"
13:07:27 <EvanR> scanl looks like, show the state of folding over the first i items, at position i
13:07:33 <EvanR> so like a debug log
13:08:16 <EvanR> > scanl (-) 0 [1,2,3,4,5,6]
13:08:20 <lambdabot>  [0,-1,-3,-6,-10,-15,-21]
13:08:42 <EvanR> 0-1, -1-2, -3-3, ...
13:08:50 <platz> yes - my observation is since you result with the same number of output elements, it is rather like a map
13:09:12 <platz> but you have a stateful accumulator for each sucessive transformation from input to output
13:10:10 <EvanR> i get scared and confused thinking of "maps with state" because thats literally how i have seen people use map, by looking at globals from inside the visitor function
13:10:29 <EvanR> that or the execute I/O from in the function
13:10:54 <platz> in haskell?
13:11:01 <EvanR> no
13:11:08 <EvanR> in every other language
13:11:14 <platz> ah
13:12:22 <michalrus> Hey. Can I do this somehow for »any« constructor out there?
13:12:23 <michalrus> data AB = A | B ; type family ToKnownSymbol (a :: AB) where ToKnownSymbol 'A = "A" ; ToKnownSymbol 'B = "B"
13:12:35 <michalrus> / /cc cocreature (this was my previous question)
13:14:46 <cocreature> michalrus: by any constructor, you mean any constructor of a given type, e.g., AB in your example?
13:15:23 <michalrus> I meant any at all, but if a given one can be done, then fine, too! I need this for just one type.
13:16:05 <cocreature> I don’t think there is a solution here that doesn’t involve TH. if you’re lucky the singletons library might have something for this, otherwise you’re going to have to write it yourself
13:16:27 <michalrus> Okay. =] I’ll look at sigletons then, thanks!
13:17:23 <ternary> Can someone explain to me what exactly Criterion is doing in it's benchmarking? I have a function that executes in a handful of milliseconds normally, but when I benchmark it, it ends up taking several minutes
13:17:26 <ternary> However, despite it taking so long, the results claim the test only took a few milliseconds. About the same as when I ran it normally
13:17:34 <ski> platz : "since you result with the same number of output elements" -- but you don't, you get one more
13:18:03 <ski> @type mapAccumL
13:18:06 <lambdabot> Traversable t => (a -> b -> (a, c)) -> a -> t b -> (a, t c)
13:18:07 <ski> @type mapAccumR
13:18:09 <lambdabot> Traversable t => (a -> b -> (a, c)) -> a -> t b -> (a, t c)
13:19:54 <ski> > mapAccumL (\s n -> (s + n,s)) 100 [1,2,3,4]
13:19:57 <lambdabot>  (110,[100,101,103,106])
13:20:00 <ski> > mapAccumR (\s n -> (s + n,s)) 100 [1,2,3,4]
13:20:03 <lambdabot>  (110,[109,107,104,100])
13:20:11 <ski> > scanl (+) 100 [1,2,3,4]
13:20:14 <lambdabot>  [100,101,103,106,110]
13:20:15 <ski> > scanr (+) 100 [1,2,3,4]
13:20:18 <lambdabot>  [110,109,107,104,100]
13:21:06 <ski> `mapAccumL' is like `mapM' (or `traverse') on `State'
13:22:33 <platz> ah, yes i forgot the initial value is in the output too
13:22:38 <platz> thanks
13:25:53 <ski> ternary : it runs the test many times, and (presumably) divides the total time by the number of times, in order to average away fluke differences, getting better precision in the answer (hopefully closer to the true value)
13:26:14 <ski> EvanR : "in a presentation, i used .." -- syntactic salt ?
13:26:34 <EvanR> yeah
13:27:11 <ski> catern : in Mercury, you can declare impure (as well as `semipure') predicates and functions, but every call to them has to be prefixed by `impure' (resp. `semipure'). this is a case of syntactic salt
13:28:06 <ski> it is expected that this would typically only be used for FFI operations, before wrapping in a nicer declarative interface, or implementations like constraint solvers, which use impure stuff internally, but which still present a declarative interface
13:30:46 <catern> ski: yeah, there's lots of things like that, where some salt is special-cased into the language
13:30:49 <ternary> ski: The test claims to have taken ~23ms but the full running time of the process was ~232s. Are you saying it runs each test over 10000 times?
13:30:50 <ski> (`impure' means that you can't commute the call with any other call (even other pure calls, which may be a bit strange ..), while `semipure' ones commute with other `semipure' (as well as pure) calls, but not with `impure' ones. `impure' would typically modify state, while `semipure' would read mutable state. there could also be indeterminacy from randomness, scheduling, library operation not pinning down behaviour, &c.)
13:31:09 <ski> ternary : iirc, yes
13:31:33 <catern> ski: but with effect systems, you can generically represent any kind of effect or tag or things like that (e.g. impure in Mercury, or unsafe in Rust, or async in Python, could be reprsented with an effect)
13:31:36 <EvanR> reading = pure :(
13:31:49 <ski> no, semipure
13:32:00 <EvanR> read = not impure :(
13:32:28 <catern> ski: I'm musing about how information about these generic, extensible effects would be surfaced to the user so they can actually use that information, because (being generic) they don't show up in the syntax
13:33:42 <ternary> ski: Well that's good to know. I thought it was only doing things once since I have some benchmarks with very very inconsistent times. Thanks
13:34:08 <ski> catern : yes. it showing up in interfaces (types) is one thing, which helps. but just looking at a piece of code, it's nice to be able to see where side-effects can happen, without having to look up signatures, for ease of reasoning (including refactoring)
13:34:42 <EvanR> wait, not im confused salt is good?
13:34:57 <EvanR> i mixed that up with syntax saccharine
13:34:58 <ski> EvanR : i'm not sure what you meant by your previous remarks, could you elaborate ?
13:35:20 <EvanR> now*
13:35:28 <Tuplanolla> Do we really need syntactic sugar, salt and pepper?
13:35:40 * EvanR looks up salt
13:35:47 <ski> syntactic salt here is considered good, since it warns the reader that something strange/funny may be going on, and it's an extra hurdle for the writer to pass, making them think twice before introducing side-effects
13:36:27 <ski> (the term "unsafe" in various Haskell operations serves a similar (if a bit crude) purpose)
13:37:20 <EvanR> hence the renaming to promisePurePerformIO
13:37:37 <ski> yea (or just `promisePureIO')
13:39:39 <catern> to some degree you really want to avoid the syntactic salt for effects, though
13:39:40 <ski> catern : iirc, i talked with someone in #rust about some proposal for calling functions that return a `Maybe'/`option' result (or perhaps it was something like `Either', i don't recall exactly), adding a `?' to the call, which (a) implicitly unwraps the `Maybe', passing the `Just' contents to the surrounding context; (b) if there's a `Nothing', current local execution is aborted, and a `Nothing' is returned at some lexical outer level (such as a function bo
13:39:56 <catern> ski: cut off at "such as a function bo"
13:40:02 <ski> (hm, possibly cut off near "..; (b) if there's a `Nothing', current local execution is aborted, and a `Nothing' is returned at some lexical outer level (such as a function body boundary)")
13:40:26 <MarcelineVQ> ski: " .. (such as a function b"
13:40:39 <MarcelineVQ> so you got pretty far
13:40:40 <ski> in this case, the `?' annotation on the call would be the syntactic salt
13:41:21 <ski> (hmm .. i wonder whether where it cuts off depends on which server in the IRC network you're connected to ..)
13:42:15 <catern> ski: well... hmm. is that really syntactic salt? if you didn't have that '?' annotation, but everything behaved the same, you would just have exceptions
13:42:39 <ski> catern : anyway .. i suggested to them (a) to introduce an explicit boundary delimiter, not just rely on function body boundaries. this facilitates refactoring better, is more modular
13:42:56 <ski> catern : and (b) suggested allowing a similar construct for other effects
13:42:57 <catern> and I'm not sure I really want to say that the only difference between monadic and exceptional error handling is that the former has some syntactic salt? (hm)
13:43:28 <ski> catern : in this case, iirc, you'd not have exceptions, but type errors, since `Maybe T' doesn't match `T'
13:43:44 <catern> ski: sure, that's what I'm saying "if everything behaved the same"
13:43:53 <frerich> Is there a nice, short, accurate, maybe even common term for the set of states which can be reached from a given state? 'Continuations'? 'Descendants'? 'NextSomething'?
13:44:04 <catern> you'd have "automatic propogation of None" or something
13:44:21 <Tuplanolla> Transitive states, frerich?
13:45:11 <frerich> Tuplanolla: Yes, like in a directed acyclic graph.
13:45:29 <frerich> Tuplanolla: In graphs, I know the term 'adjacent'.
13:45:33 <ski> i'm not sure if by "monadic error handling" you mean using `do'-notation, or else the raw combinators (`(>>=)',`return',`(<|>)',`empty',`catchError',`throwError',`liftA2',&c.) .. or something else
13:45:37 <ski> catern ^
13:45:46 <catern> ski: I mean returning a Maybe
13:46:07 <ski> well, it matters not only what the types are, but how you express the computation
13:46:12 <catern> (or the equivalent in Rust for errors, which is "Result")
13:46:16 * ski nods
13:46:36 <frerich> Tuplanolla: In this case, it's more like in a chess game, i.e. I have a function which given a state of the board yields all possible next states. I wonder what t ocall that function. :-)
13:46:52 <ski> what i suggested in #rust i would not equate with ordinary monadic error handling, as currently done in Haskell
13:47:22 <ski> however, it's more or less a version of an alternative syntax for effects in Haskell, which i tentatively call "reflective syntax", which i've been pondering for a while :)
13:47:27 <catern> (also, to some extent I think you want to avoid syntactic salt for effects, because in some sense the entire *purpose* of effects (or do notation for monads) is to get rid of syntactic salt, since there is a very natural salt-y expression of effects: store-passing-style or continuation-passing-style or whatever)
13:47:48 <Tuplanolla> I already gave you the answer, frerich.
13:47:48 <catern> (s/effects/effect systems/)
13:48:08 <frerich> Tuplanolla: 'transitive states'? Oh, ok...
13:48:20 <jmcarthur> frerich: "frontier"?
13:48:28 <ski> what this does is it reintroduces side-effects, but only locally / within delimited lexical scopes. you still can't call a side-effectful function, you have to delimit/reify the side-effects lexically, into mere effects, at the call boundaries
13:48:54 <frerich> jmcarthur: That's kinda nice :-)
13:49:07 <jmcarthur> frerich: Do you mean all states reachable from here or only the immediate next states? If the latter, I've heard "frontier" used for it.
13:49:41 <ski> catern : "the entire *purpose* of effects (or do notation for monads) is to get rid of syntactic salt" -- well, to get rid of the *plumbing* of such effects sure, but not necessarily the actual direct (from a lexical standpoint) *sources* of effects
13:49:44 <frerich> jmcarthur: Yeah, the latter. I like 'frontier' because I'm also using this function to build a tree of all possible states, i.e. I'm iteratively extending the frontier. :-)
13:50:03 <jmcarthur> frerich: Yeah, this sounds like the right context for it then.
13:50:14 <frerich> jmcarthur: Thanks for that idea, I'll go with that!
13:50:59 <ski> catern : let's say we have some computation using idioms / applicative functors, like `foo (f (g x) (h x y)) <$> bar (f (g y) (h y z)) <*> baz (...)'
13:51:46 <ski> catern : now, let's say that we realize that we want to do effects in the `h' calls as well. now we need to not only annotate those `h' calls, but the entire *path* down to them, in the syntax tree !
13:52:13 <catern> yes, quite a drag
13:52:20 <ski> i want to avoid the latter part, writing something like (concrete syntax not decided, consider in flux) :
13:53:11 <ski> from
13:53:19 <ski>   [< foo (f (g x) (h x y)) [{ bar (f (g y) (h y z)) }] [{ baz (...) }] >]
13:53:24 <ski> we go to
13:53:44 <ski>   [< foo (f (g x) [{ h x y }]) [{ bar (f (g y) [{ h y z }]) }] [{ baz (...) }] >]
13:54:11 <ski> changing the result type of `h' from `Blah' to `I Blah', for the idiom `I' we're currently using
13:54:48 <ski> `[< ... >]' acts as the outer effect delimiter, outside of this, there's no side-effects
13:55:14 <ski> `[{ ... }]' marks places where we need to perform an effect
13:55:40 <ski> if they are nested (as i just realized is the case now, in the above example), then that requires `Monad'
13:55:51 <ski> otherwise, just `Applicative' is required
13:56:08 <ski> if there's just one `[{ ... }]', then only `Functor' is required
13:57:38 <catern> hmm... I admit I am slightly out of my depth here :)
13:57:53 <ski>   [< f [{ c }] >]  =  f <$> c  -- if the expression `f' has no `[{ ... }]'s in it
13:59:38 <ski> hm, i just realized that's somewhat a confused explanation. i should make up my mind as to whether i want to use meta-variables for expressions, or just object-variables (less generality, but hopefully conveying the gist). i'll settle for the former
14:00:07 <ski>   [< E E0 >]  =  E <$> [< E0 >]  -- if the expression `E' has no `[{ ... }]'s in it
14:00:11 <Ariakenom> ski, doesnt Idris have a similar ! syntax?
14:00:19 <ski>   [< E >]  =  pure E  -- if the expression `E' has no `[{ ... }]'s in it
14:00:53 <ski>   [< E E0 >]  =  [< E >] <*> [< E0 >]  -- general case, applies if none of the previous two cases apply
14:01:39 <ski>   [< let x0 = E0 in E >]  =  [< E0 >] >>= \x -> [< E >]  -- general case, you can imagine special cases
14:02:36 <ski>   [< if E then E0 else E1 >]  =  [< E >] >>= \b -> if b then [< E0 >] else [< E >]  -- general case, you can imagine special cases
14:03:17 <ski>   [< case E of {P0 -> E0; ...} >]  =  [< E >] >>= \case {P0 -> [< E0 >]; ...}  -- general case, you can imagine special cases
14:03:57 <ski> `[{ ... }]' (reflections) are not allowed directly inside lambda, without an intervening `[< ... >]' (reification)
14:05:01 <ski> Ariakenom : it has something like this, but it definitely lacks the outer delimiter. apart from that, i'm not sure to which extent it corresponds
14:06:15 <ski> in any way, the idea is that it must be syntactically/lexically apparent where side-effects can possibly come from, in an expression, so that one can safely use the usual modes of equational reasoning, refactoring, when there's no `[{ ... }]' present in a (sub)expression
14:06:37 <ski> (all reflections must be delimited by a reification)
14:07:32 <Ariakenom> http://docs.idris-lang.org/en/latest/tutorial/interfaces.html#notation
14:08:24 <ski> (.. then, it would be interesting to consider whether one could use an implementation strategy like in Andrzej Filinski's "Monadic Reflection" paper, using delimited continuations under the hood, to make programming using monads&idioms more efficient. i suspect that using such would avoid much need for `Codensity'/`ContT'/`Cont')
14:09:43 <Peaker> a PL design problem: I have "catch" which removes an exception type from a structural-sum-type.  Then I want to >>= it with a different action that may throw that same exception type.  This is of course a type error.  Manually lifting the computation before calling >>= is a bit ugly/messy :(  Did anyone use structural row-type sums for checked exceptions?
14:10:28 <ski> (fwiw, Idris didn't exist when i started thinking about this)
14:10:46 <ski> Ariakenom : ty, though i've read that before
14:11:40 <ski> idiom brackets were one source of inspiration for the idea. quasiquotation, as in the Lisps, and MetaML/MetaOCaml, another. and Filinski's two papers a third
14:11:52 <Ariakenom> I thought to be prudent and link to some related work
14:12:07 <catern> Peaker: this may be a naive comment, but isn't using row types the standard way to implement effect systems?
14:12:25 <Peaker> catern, it's one way, in PureScript, though they're throwing it away, perhaps also due to this reason
14:12:38 <ski> Ariakenom : no worry. just saying that i've run into `!' in Idris before, when talking about this :)
14:12:46 <Peaker> e.g:  trying to use >>= between:  DivByZero notMemberOf e => Exc +{| e} a    and  (a -> Exc +{ DivByZero: () | e } b)     <--- +{| e}  and +{ DivByZero: () | e }   cannot unify
14:13:29 <Peaker> +{ .. fields .. | column }   is notation for a sum type.  "fields" are just constructors with content type, and "| column"  represents more typed fields, named "column"
14:13:47 <EvanR> the handler cant throw the exception, or the next normal action
14:14:01 <EvanR> either way im wondering how thats justified
14:14:10 <Peaker> EvanR, you mean in my catch?
14:14:13 <EvanR> yes
14:14:21 <ski> (and then i've also connected these ideas to "equality chain proofs" going back at least to a paper by Lennart Augustsson on Cayenne, and this can be tied, i believe, to HoTT. there are also papers about "structural equational reasoning", which i think are relevant)
14:14:48 <Peaker> EvanR, yeah, for simplicity's sake lets say the handler doesn't throw anything. I just have something that handles the exception and thus removes it from the list of possible errors
14:15:12 <EvanR> oh, in the whole thing
14:15:22 <ski> Peaker : can you have the `catch' return a result that involves a row variable ?
14:15:31 <EvanR> but you said, removes it from the thing you >>= with
14:15:53 <EvanR> like the right arg cant throw it.. was confused
14:16:28 <Peaker> ski, Yeah
14:16:49 <Peaker> ski, tho my current type system cannot express a relationship between 2 row variables (I am thinking this will lead me to have to change that)
14:17:33 <ski> Peaker : iiuc, the problem is that the result of `catch' isn't polymorphic, insists on not being able to give a `DivByZero', rather than being polymmorphic about that
14:17:40 <ski> do i understand incorrectly ?
14:17:56 <Peaker> ski, yeah, that's exactly the issue
14:18:27 <ski> what's the current type of `catch' ?
14:18:30 <Peaker> though the polymorphic type that allows it from catch would need to say the new row variable is related to the old variable
14:18:44 <ski> (or if it's a construct, what's the inference rule ?)
14:18:56 <Peaker> sec, writing it down
14:22:41 <Peaker> I guess it would be: catch :: (a -> Exc b res) -> Exc a res -> Exc b res
14:24:55 <Peaker> Basically:   catch f = either f Right
14:25:43 <ski> hm, but that doesn't express that the result doesn't include the alternative `DivByZero', while the input does ?
14:26:57 <ski> hmm
14:27:18 <ski> oh, i suppose `a' is the exception type ?
14:27:46 * ski isn't sure how Peaker indicates which kind of exception to catch
14:27:50 <Peaker> Yeah, "a" and "b" are the exception types, and the err handler would handle DivByZero specifically
14:27:57 <Peaker> example here: https://github.com/lamdu/lamdu-calculus/blob/master/ExceptionMonad.md
14:28:15 <ski> or are you just reraising the exception in the Handler, if it wasn't what you were looking for ?
14:28:33 <Peaker> You'd do: catch (\case DivByZero -> #Success 0 ; other -> #Error other)
14:28:38 <Peaker> exactly
14:28:51 <ski> ok .. i'm not sure that's that good an idea
14:29:27 <Peaker> Well, the tags indicating which constructor aren't first-class entities. "case" and "inject" for sum types take them as AST parameters specially
14:30:08 <ski> mhm
14:30:58 <ski> from a "stacktrace" perspective, at least, one'd like not to reraise. so that it's easier to track down the actual source
14:32:00 <ski> (and ditto if one wants restartable conditions rather than exceptions. where handler code can resume the code that signalled the condition, so that the `raise'/`signal' call returns)
14:32:56 <ski> (several Lisp systems have such restarts. they could either be restarted programatically, or by the user in an interactor, if it percolates up to the toplevel)
14:34:14 <ski> anyway, fwiw, i tend to prefer an exception primitive that not only has a continuation for the exception case, but also one for the normal case
14:34:43 <ski>   catchBind :: MonadError e m => (e -> m o) -> (a -> m o) -> m a -> m o
14:35:07 <jmcarthur> This feature was introduced to OCaml a few releases back, and I really love it there.
14:35:40 <jmcarthur> That is, you can catch an exception as a pattern.
14:35:57 <EvanR> yes its easy to prove properties of programs written using such combinators, within each callback you have your "proof context" clearly modified somehow
14:36:08 <ski> see the paper "Exceptional Syntax" by Nick Benton,Andrew Kennedy in 2001 at <http://lambda-the-ultimate.org/node/1193>
14:36:27 <jmcarthur> match f () with Normal_constructor _ -> ... | exception My_exception -> ...
14:36:34 <ski> jmcarthur : oh, you can write `match ... with raise MyExn v -> | _ -> ...' now ?
14:36:48 <ski> great :)
14:37:14 <jmcarthur> The upcoming algebraic effects will take this further.
14:37:20 <ski> (though i would prefer `raise' over `exception' being used in the pattern)
14:38:03 <jmcarthur> exception was already a keyword
14:38:41 <ski> see <http://ambassadortothecomputers.blogspot.se/2010/09/reading-camlp4-part-11-syntax.html> for more
14:39:21 <ski> oh, for some reason i assumed `raise <expr>' was a syntactic form
14:39:36 <frerich> ski: Every second evening I see you dishing out half a dozen links to various papers and blog spots. Do you just remember them or do you have some sort of library of papers to grep in? :-)
14:40:58 <EvanR> i would like to subscribe to that newsletter
14:41:27 <ski> frerich : er .. most of them i just recall, i suppose
14:41:56 <ski> (recall-on-the-spot, not as in being able to spit them out in unrelated situations)
14:42:53 <ski> (also, i fear that my internal store of paper references may be smaller than you think, i repeating some of them more often than you think)
14:43:29 <ski> frerich : .. of course, some of them are just stuff which i've recently seen somewhere
14:44:01 <ski> (i wouldn't be surprised if i have referred to "Polymorphic Type Inference" more than a hundred times, by now)
14:44:40 <Rembane> ski: Which paper is that?
14:45:53 <pavolzetor> hello, how do I get an active block in IRBuilder (llvm-hs-pure)?
14:46:04 <ski> @where polymorphic-type-inference
14:46:05 <lambdabot> "Polymorphic Type Inference" by Michael I. Schwartzbach in 1995-03 at <https://cs.au.dk/~mis/typeinf.p(s|df)>,<http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.57.1493>
14:46:09 <pavolzetor> I am having trouble inserting phi nodes for nested if/else
14:46:19 <ski> Rembane : i finally got around to putting it in `where' ..
14:46:24 <Rembane> ski: Thank you!
14:46:25 <ski> (a few days ago)
14:46:32 <Rembane> ski: Well worth your time it seems.
14:47:25 <ski> frerich : actually, <https://web.archive.org/web/20161027043544/http://mjambon.com/extend-ocaml-syntax.html#lettry> is probably a better link describing rationale for `let try' (or the alternative form that jmcarthur mentioned). i didn't find that first, because apparently it has 404:ed
14:47:53 <pavolzetor> I could add a dummy block but that seems rather hackish
14:47:55 <ski> the paper also describes more theoretical advantages to something like `catchBind' as a primitive over `catch'
14:48:12 <pavolzetor> cocreature, any ideas?
14:48:13 <ski> namely, the former is more modular, over refactorings of code
14:49:04 <ski> Rembane : you may also want to check out
14:49:08 <ski> @where on-understanding
14:49:08 <lambdabot> "On Understanding Types, Data Abstraction, and Polymorphism" by Luca Cardelli,Peter Wegner in 1985-12 at <http://lucacardelli.name/Papers/OnUnderstanding.A4.pdf>
14:49:16 <jmcarthur> ski: Another blog post with some motivation https://blog.janestreet.com/pattern-matching-and-exception-handling-unite/
14:49:21 <jmcarthur> frerich: ^^
14:49:50 <Rembane> ski: Thank you! Now I have things to read for the rest of the month.
14:50:00 <ski> jmcarthur : ty :)
14:50:39 <Peaker> ski, Even the "either" type combinator for catch suffers from the same unification. For the polymorphism about whether DivByZero is inside the result of catch, yet still having uncaught exceptions in the list for result of catch, I need catchRes = insideExc + more   where the "more" represents the DivByZero again (or others)
14:51:12 <Peaker> ski, (where catchRes is the row type for exception list of the result action of catch, and "insideExc" are the row representing the exceptions of the caught action)
14:51:35 <Peaker> ski, but my type system doesn't have this kind of disjoint union (a = b + c) for type variables, and I am not sure what adding it would entail
14:52:26 <Peaker> I'd need a new primitive operation, I think, like:  reinject :: (b = a ++ c) => +{| a} -> +{| b}
14:53:59 <ski> Peaker : hm, i'm not quite sure to which parts "the result action of catch" and "the caught action" refer to
14:54:02 <Peaker> (that's equivalent to doing: \case Cons1 -> inj Cons1, \case Cons2 -> inj Cons2, ... \case ConsN -> inj ConsN, absurd -- except without having a specific list of constructors)
14:54:26 <Peaker> resultAction = catch caughtAction ...
14:54:29 <ski> Peaker : as an alternative to `a = b + c', perhaps you could have `a =< b'
14:55:09 <Peaker> ski, I guess so - since I don't really have anything of type "c" anywhere
14:56:23 <Peaker> the dual of this for records would be.  delFields :: {| b} -> {| a}   that removes some fields -- useful
14:57:05 <ski>   catch_DivZero :: Exn c0 a -> Exn +{DivZero | c1} a -> Exn +{| c0 + c1} a
14:58:01 <picknick> I've heard that there's a new way to reduce gc latency with compact regions, has anyone tried using it with games yet? if so how did it work out?
14:58:32 <ski>   catchBind_DivZero :: Exn c0 a -> (a -> Exn c0 o) -> Exn +{DivZero | c1} a -> Exn +{| c0 + c1} o
14:59:30 <ski> though you don't have label polymorphism
15:00:38 <ski> i suppose you need an isomorphism between `Exn +{l : t | c} a' and `Exn c (Either t a)', for any label `l' ?
15:00:47 <Peaker> ski: i think the type above is still problematic
15:00:53 <pikajude> how do I un-hide the ghc module in ghci
15:00:53 <ski> yes ?
15:00:55 <pikajude> I've forgotten
15:01:19 <lifter> Is this an appropriate place to ask about conduit?
15:01:34 <Peaker> ski, "c1" is known not to contain "DivByZero". Maybe "c0" there is also known not to contain it. In that case -- you cannot bind the result of catch to an action that contains it
15:01:52 <Peaker> ski, maybe it's not a problem if "c1" doesn't end up with such constraints in genreal
15:02:01 <Peaker> (oops, if "c0")
15:03:21 <Peaker> ski, Yeah, that isomorphism seems like what "catch" is applying here
15:03:44 <pikajude> also, is there a way to compare a ghc Name.Name with a template-haskell one
15:03:55 <ski> lifter : i think so
15:05:06 <ski> Peaker : i think `c0' should be allowed to include `DivZero' there, no ?
15:05:24 <ski> i don't know whether you require a disjointness constraint in order to be able to form `c0 + c1'
15:05:38 <ski> (also, is `+' commutative ?)
15:08:03 <ski> Peaker : well, if you have such an isomorphism, then `either' and ordinary `(>>=)' would suffice, no ?
15:09:00 <ski> Peaker : hm, perhaps you'd want to be able to go between `Exn +{| c0 + c1} a' and `Exn c0 (Exn c1 a)', given an explicit enumeration of the labels in `c1' say ..
15:09:22 <MacSlow> Greetings everyone!
15:09:36 <ski> hello MacSlow
15:11:09 <MacSlow> hey ski
15:13:00 <lifter> I have a text file of words 236,000 words, one per line. I want to use sqlite to insert each word into a database table. My Haskell program reads the file using Data.Text.IO.readFile and then inserts the words into the table using a "insert into ... values ..." statement. But somewhere I'm running out of memory. Can I use conduit to do this?
15:13:35 <clever> lifter: are you using String or Text?
15:13:51 <reactormonk> Anyone got Monad m => m a -> (a -> Bool) -> m [a] lying around? There's whileM, but that doesn't quite fit.
15:13:57 <ski> centril : "what I'm most opinionated about is that:  eval (Var v) = exp1 ; eval (Lit i) = exp2  is horrible style and that case expressions are better","if there's one thing I'd deprecate it is the ability to write  eval (Var v) = exp1 ; eval (Lit i) = exp2" -- hehe, i strongly feel opposed to this :)
15:14:21 <lifter> clever: Text (strict).
15:14:24 <reactormonk> ... I'll go use hoogle next time
15:14:33 <clever> lifter: try Lazy Text instead and see if it helps
15:15:21 <clever> lifter: also, are you inserting several rows in a single query, one each, or all?
15:15:30 <jmcarthur> It seems unlikely that a file that small (by my guess) should cause you to run out of memory. Perhaps the issue is with the insertion.
15:15:31 <ski> centril : the reason is that, to the greatest extent possible, i wasnt to allow being able to use "equational reasoning" in an immediate way. if we have `length [] = 0' and `length (x:xs) = 1 + length xs', then it's easier (a) to understand the bits in isolation; and (b) to individually apply these as rewrite (e.g. refactoring, or proving) rules, forwards or backwards
15:15:46 <reactormonk> Nah, the one I found doesn't work.
15:15:55 <ski> centril : while with using `case' or `\case', that's not as syntactically apparent
15:16:04 <ski> s/wasnt/want/
15:16:21 <lifter> clever: Every row is inserted via a single sql statement.
15:17:00 <jmcarthur> lifter: Is your code short enough to put on lpaste.net?
15:17:35 <Peaker> ski, I can't use that isomorphism, there's no mechanism that we have to allow it (not sure what would be a good one)
15:17:36 <ski> centril : mitigating factors here are overlapping patterns, and especially the top-to-bottom checking of branches. these together mean that one can't as often as one'd like understand a defining equation in isolation, as a true statement about the operation being defined
15:17:58 <jmcarthur> lifter: (I'm sad about clever's suggestion to use lazy Text, but if it helps then that at least narrows it down a lot.)
15:18:07 <Peaker> ski, we have a big open issue of how to write generalized code on top of our structural types - given that constructor/field labels are not first class
15:18:23 <Peaker> (making labels first class would make type inference a big problem)
15:19:00 <ski> centril : i think it would be interesting if matching switched from "top-to-bottom" to "most specific first", if one can reasonably specify that in a way that one can fit it in head, and it's useful enough when reasoning about code
15:19:12 <lifter> jmcarthur: Hmm, the actual operation is spread across a few functions in a few modules, I don't know if it would be easily pastable into a pastebin.
15:20:29 <ski> centril : one could also have a special pattern (distinct from `_') for "everything else", which is to be used for "catch-all" situations (partial or total) (namely, when there's a more specific case which might override the current one)
15:20:32 <lifter> clever: jmcarthur: You know what's interesting is that this was working for a few years until just recently, I think the only thing that changed is I updated my stack resolver from lts-10.5 (I think?) to lst-11.0
15:20:54 <ski> centril : that would serve as a warning to the reader that that case can't be understood in isolation of the other cases
15:21:54 <ski> Peaker : well, you could add special syntactic constructions which mention labels, in place of first-class functions which can't be passed them as arguments ?
15:22:09 <jmcarthur> lifter: That is very interesting.
15:22:18 <jmcarthur> And sad.
15:22:23 <Peaker> ski, yeah, but which ones?
15:22:48 <Peaker> ski, "Case" and "RecordExtend" are basically such syntactic constructions
15:24:07 <ski> Peaker : *nod*, that's the question
15:24:25 <ski> Peaker : the above two isomorphisms i mentioned could possibly be candidates
15:25:11 <ski> (i'm assuming `Exn c a' amounts to `Either +{| c} a', yes ?)
15:25:29 <Rembane> lifter: Does it still work if you recompile it with the old resolver?
15:25:29 <lifter> So if I change from strict to lazy text, I would use the lazy version of text's "readFile", which returns a lazy text result, so every downstream operation should work with lazy text, right
15:25:37 <lifter> Rembane: Yeah.
15:25:50 * ski is probably mangling the distinction between variant types, and row types, a bit, they realize
15:26:04 <Peaker> ski, That would be: Exn +{| c} a  (the exception type can have concrete sum type in there)
15:26:09 <Rembane> lifter: Check. Then we have found a regression. Exciting.
15:26:36 <ski> Peaker : hm, so what's the difference between `Exn' and `Either' ? just different names ?
15:27:16 <Peaker> ski, Yeah, we don't really have "Either", since structural types seem to make it kind of redundant
15:27:25 <Peaker> (we can define a nominal Either type, we don't yet tho)
15:27:38 <lifter> Rembane: I'm not sure how to interpret that.
15:27:41 <ski> so is `Exn' built-in ?
15:28:11 <Rembane> lifter: I'm just sad that things break for you.
15:28:15 <Peaker> ski, We want to actually have it be an effect row in our general "Mut" monad (kinda like PureScript's effect rows)
15:28:29 <Peaker> ski, but you can write a pure exception monad manually, doesn't have to be built-in
15:28:51 <ski> mhm
15:28:52 <lifter> Rembane: Ah ok, thanks.
15:29:12 * ski idly wonders whether effect rows in PureScript are ordered or not
15:29:13 <Peaker> so a slightly-more-disciplined IO monad, where the exceptions are also checked (hopefully)
15:29:32 <Peaker> ski, I'm pretty sure the row mechanism there is like Lamdu's, so they shouldn't be
15:30:04 <ski> hm, some effects doesn't commute, like continuations
15:30:43 <ski> (continuations is like a litmus test for an effect system with a claim to be general, to me)
15:32:38 <amartya> \quit
15:32:44 <amartya> \part
15:33:03 <Peaker> ski, Yes -- we're not anti-transformer stacks
15:33:27 <Peaker> ski, but instead of "IO" and "ST s" and such, we have just "Mut" - which is meant to replace them and not transformers. We'll hopefully have ContT/etc
15:34:05 <Peaker> ski, and if it does replace "IO" and "ST s", it seems nice to track the existence of "File" effects, "Network" effects, checked exceptions, etc. Truly, it's just a preliminary experiment at this point
15:34:12 <ski> not-transformers is ok, as long as i can still express the useful functionality of them
15:34:28 <Peaker> (PureScript are throwing away their effect rows soon, with a big breaking change, so their experiment result is "Nope")
15:34:49 * ski , yesterday, heard a talk (in person) by SPJ about Linear Haskell
15:35:23 <Peaker> cool, is it planned soon?  Along with dependent types -- Haskell seems to become all the things to all the peoples? :)
15:35:29 <Peaker> (Not to mention backpack)
15:35:30 <Rembane> ski: Was it a good talk?
15:35:42 <Rembane> Is backpack the ML functors in Haskell?
15:35:42 <ski> they were discussing a non-standard version of `(>>=)' (not along the indexed monad direction), which they were hoping someone could generalize
15:35:49 <Rembane> Cool
15:35:57 <ski> Rembane : yes, i think so. only an hour though
15:36:03 <ski> Rembane : iiuc, yes
15:36:08 <Rembane> ski: Ach. I could listen to him forever.
15:36:10 * ski has yet to look at BackPack
15:36:58 <Peaker> Rembane, Backpack is ML functors iiuc
15:37:18 <Rembane> Peaker: Nice!
15:37:27 <Rembane> So Haskell implements all the things. Like CLISP or C#. :D
15:37:30 <ski>   (>>=) :: IO p a -> (a -o IO q b) ->_p IO q b  -- iirc
15:38:03 * ski wonders whether that was right
15:38:17 <Rembane> With the lollipop operator?
15:39:18 <Peaker> What is "-o" and "->_p" ?
15:41:30 <ski> oh, it should be
15:41:50 <ski>   (>>=) :: IO p a -o (a ->_p IO q b) -o IO q b
15:42:06 <haskelll> Where is the java channel
15:42:27 <ski> `-o' is linear implication, as in linear logic, `->' is ordinary implication, which can use its argument/assumption as many times as it wants
15:42:58 <ski> `A -> B' can be defined as `!A -o B', where `!', "of course", explicitly gives license to use the input as many times as we like
15:44:10 <ski> in the (talk and) paper (which i'm looking at rn), they define `-o' as `->_1', and `->' as `->_omega', and they allow "linearity variables", here `p' and `q', which ranges over `1' and `omega', and which an operation can be polymorphic in
15:45:09 <ski> `IO p a' means an I/O action whose result (of type `a') must be consumed as many times as the linearity parameter `p' indicates
15:45:30 <ski> in the paper, `!A' is written `Unrestricted A'
15:46:06 <barrucadu> What paper is this?  The idea of a "linearity variable" sounds familiar, but I can't think where I've seen it before.
15:46:43 <ski> one interesting thing, which i hadn't seen before is that if you have a function with signature `f :: A -o B', then this doesn't mean that when calling `f', you can only call it with a "linear input", getting a "linear output" back
15:47:07 <ski> it means that *if* the result `f x' is consumed exactly once, then the input `x' is consumed exactly once
15:47:12 <ski> so, they redefine e.g.
15:47:18 <ski>   data Maybe a :: *
15:47:19 <ski>     where
15:47:40 <ski>     Nothing ::      Maybe a
15:47:48 <ski>     Just    :: a -o Maybe a
15:48:21 <ski> since to consume `Just x' once, we have to consume `x' once (consumption must descend)
15:49:42 <ski> (this is unlike uniqueness systems, like Clean and Mercury, and i think Rust is not that dissimilar, in which you get a guarantee that you have a unique reference to something (it hasn't been duplicated in the past), which enables update-in-place, as opposed to a linear system which ensures (will not duplicate in the future))
15:50:19 <ski> anyway, `Unrestricted' is still defined like
15:50:27 <ski>   data Unrestricted a :: *
15:50:30 <ski>     where
15:50:40 <ski>     Share :: a -> Unrestricted a
15:50:57 <ski> using the "intuitionistic" implication `->' still, and *not* the linear `-o' one
15:51:02 <ski> iow, this amounts to
15:51:07 <ski>   Share :: !a -o Unrestricted a
15:51:21 <ski> and so `Unrestricted a' is isomorphic to `!a'
15:52:08 <ski> (disregarding extra bottoms, anyway. perhaps one'd like `Unrestricted' to behave as a "GADT `newtype'" instead)
15:52:30 <ski> (there's also the question of how nontermination, and exceptions/partiality tie in to "consumption" and linearity)
15:53:28 <ski> anyway, one interesting thing that SPJ talked about, which i hadn't heard about or realized before, is that you can still (apparently) express this uniqueness-like constraint of "haven't been duplicated in the past", by using `Unrestricted' in combination with CPS !
15:54:54 <ski>   newMArray :: Int -> (MArray a -o Unrestricted b) -o b
15:54:57 <ski> is an example of that
15:55:12 <ski> there's also
15:55:25 <ski>   write :: MArray a -o (Int,a) -> MArray a
15:55:43 <ski>   read :: MArray a -o Int -> (MArray a,Unrestricted a)
15:55:56 <ski>   freeze :: MArray a -o Unrestricted (Array a)
15:57:13 <ski> so, these operations get a reference to a mutable array. the `-o' arrow for those argument types doesn't ensure that we get the only reference to the mutable array, but it ensures that we can't duplicate *our* reference to it, within the implementation of these operations
15:58:00 <ski> and, if we make our own operation, which calls these on a mutable array obtained as an argument via `-o', then as soon as we pass it to one of these operations, we can't use it anymore
15:58:43 <ski> therefore `write' and `read' "returns it back to us", in the result (conceptually a new array is returned in the `write' case, the idea being to use update-in-place on the old one)
15:59:26 <[exa]> linearity yay!
15:59:47 <ski> however, to get hold of an initial `MArray a', the only way is to use `newMArray' to which we pass a callback, that callback isn't allowed to duplicate its reference, *and*, it's only allowed to return a result that *can* be duplicated !
16:00:20 <ski> therefore, we can be sure that it doesn't leak out of the callback, and that we indeed have the unique reference in `write', so we can do update-in-place
16:01:07 <barrucadu> That feels similar to the runST trick
16:01:49 <ski> but, unlike in a uniqueness tracking system (where, to use SPJ's terminology, "uniqueness is attached to kinds" (i'm not quite sure that's an accurate way to think of what Clean or Mercury does, but let's accept it for the moment)), here linearity is attached to function arrows
16:02:32 <ski> (SPJ consistently talked about "linearity", not mentioning "uniqueness", instead talking about two distinct flavors of "linearity". however, i prefer the term "uniqueness" for what Clean and Mercury, and i think, Rust, does)
16:02:50 <ski> (also, this confusion already occured in a Wadler paper, iirc)
16:03:30 <ski> (iirc, edwardk was the one who explained the difference to me, in terms of "hasn't been duplicated in the past" vs. "won't be duplicated in the future")
16:04:55 <ski> (oh, which reminds me that a linear type system also forbids one to throw away garbage. if that is allowed, then it's called affine. while i'm not sure there's a similar distinction for uniqueness : "hasn't been discarded in the past" seems to be a precondition to obtaining a reference to a value in the first place ?)
16:06:12 <ski> (systems like Clean and Mercury do insert calls to the GC (CTGC, Compile-Time GC, so to speak), if a unique object is not passed on, but discarded, and there happens not to be any relevant construction nearby which could reuse the block of memory (perhaps reusing the values of some fields))
16:07:03 <ski> anyway, to wrap up the above example, `freeze' would be used eventually, inside the callback to `newMArray', to freeze the mutable array (safely) into an ordinary immutable array
16:07:59 <ski> so, instead of having an operation `Int -> [(Int,a)] -> Array a' as a primitive (or implemented using `unsafeFreeze'), we can now implement it safely, in terms of lower-level, but still safe, operations
16:10:08 <ski> barrucadu : the paper i'm looking at, which i suppose may be the latest on this is "Linear Haskell: Practical Linearity in a Higher-Order Polymorphic Language" by Jean-Philippe Bernardy,Mathieu Boespflug,Ryan R. Newton,Simon Peyton Jones,Arnaud Spiwack in 2018-10 at <https://www.microsoft.com/en-us/research/wp-content/uploads/2017/03/hlt.pdf>
16:10:22 <ski> (i'm not sure whether the slides i saw are online)
16:10:47 <pikajude> is there a library for codegen of actual source files
16:10:51 <pikajude> rather than template haskell
16:13:38 <ski> barrucadu : well, the `runST' trick is about different State Threads not getting entangled with each other, no ?
16:14:19 <ski> (while the update-in-place guarantees are given by `ST' being an abstract data type, similar to the case for `IO' with `IORef',`IOArray')
16:14:57 <ski> (because Haskell doesn't have uniqueness (nor linearity), so that we can't expose the internal state-passing safely, unlike in Clean and Mercury)
16:15:22 <barrucadu> Yeah, and they both use a kind of quantification to prevent things leaking, `(LinearThing -o Unrestricted foo) -> foo` and `(forall s. ST s a) -> a`
16:15:29 <ski> anyway, backtracking to
16:15:30 <ski>   (>>=) :: IO p a -o (a ->_p IO q b) -o IO q b
16:15:50 <ski> the question is what generalization of monads could capture this (and perhaps other examples ?)
16:16:03 <ski> compare this with the index monad case, in which we instead have
16:16:43 <ski>   (>>=) :: IndexedMonad mix => mix i j a -> (a -> mix j k b) -> mix i k b
16:17:23 <ski> (example instances of `IndexedMonad' being `Cont2 o p a' which is `(a -> p) -> o', and `State2 s t a' which is `s -> (t,a)')
16:17:31 <ski> `join' here becomes
16:17:53 <ski>   join :: IndexedMonad mix => mix i j (mix j k b) -> mix i k b
16:18:08 <ski> iow, we have a natural transformation `mix i j . mix j k -> mix i k'
16:21:17 <ski> (that looks formally similar to `(.) : Hom(B,C) * Hom(A,B) -> Hom(A,C)', composition in a category. an `Ab'-category is a category in which the `Hom'-classes are abelian groups, and where composition is `(.) : Hom(B,C) (*) Hom(A,B) -> Hom(A,C)', identity `id : Z -> Hom(A,A)', where `(*)' is tensor product and `Z' is the group of integers)
16:23:24 <ski> (an `Ab'-category is a category that is "enriched" over the monoidal category `Ab'. it seems that for indexed monads, we want a category enriched over the monoidal category of endofunctors ? (cf. how monads are monoid objects in the monoidal category of endofunctor))
16:23:49 <ski> anyway, i wonder what `join' in the linear Haskell case here would be ?
16:23:56 <EvanR> is there ever a good reason to use data instead of a newtype, if you can
16:24:28 <ski> EvanR : perhaps if you expect there to be more alternatives in the future. not sure
16:25:03 <ski> (not sure about which direction the approximations would go, or perhaps they go both ways, so that it's incomparable in general)
16:26:43 <ski> hm, one interesting thing, which i asked about, was whether they had inequality constraints on these linearity parameters `p',`q'
16:27:29 <ski> and apparently they don't, since it seemed they never needed them, instead using "lub" and "glb", iiuc
16:27:44 <ski> hm, so i suppose maybe
16:27:59 <ski>   join :: IO p (IO q a) -> IO (p + q) a  -- ?
16:28:43 <ski> (GADT `newtype' is something which i've pondered before actually)
16:29:22 <EvanR> you cant do that now?
16:29:27 <ski> but now i recall that i wanted to be able to have multiple alternatives in a "GADT `newtype'", on that occasion, but with disjoint indices
16:29:33 <ski> EvanR : not to my knowledge
16:30:14 <ski> what i mentiones above, for `Unrestricted' would be to have a GADT `newtype', that still has only a single data constructor
16:30:26 <ski> currently, one can't have `newtype' existentials either
16:30:48 <ski> with the type-erasing implementation, i think that should work, as long as there's no constraints
16:31:48 <ski> however, if we want to keep open the room for implementations which do type-passing (e.g. to do more accurate GC, or to enable more unboxing of stuff), then perhaps we should not allow even unconstrained existential `newtype'
16:32:27 <ski> anyway, the earlier idea i had for "GADT `newtype'", which allowed multiple data constructors, was to be able to have e.g.
16:33:11 <ski>   newtype Vector a :: Nat -> *
16:33:13 <ski>     where
16:33:35 <ski>     Nil :: () -> Vector a Zero
16:33:52 <ski>     Cons :: (a,Vector a n) -> Vector a (Succ n)
16:34:18 <ski> the idea being that one can't match on a value of type `Vector a n', until one knows whether `n' is `Zero' or `Succ m' for some `m'
16:34:44 <johnw> ski: interesting idea
16:35:10 <johnw> so you can't ever actually have a value that might be one or the other; you always have to know, so no need for runtime differentiation
16:35:42 <ski> (so this aspect is unlike how GADTs work, so perhaps i shouldn't call this "GADT `newtype'" .. iirc in some dependently typed languages this is how indexed data types work, you have to determine the index before you can match on the data)
16:35:59 <EvanR> its like they are not the same type
16:36:04 <ski> right, some additional information must be present, which divulges `n'
16:36:05 <johnw> or, matching on the data infers the index
16:37:01 <ski> i'm also thinking that there may be a relation here between these two different uses of indices of the data type, and the `in' vs `out' mode of a predicate in logic programming
16:37:37 <ski> (one can encode a logic programming predicate as an inductive, often indexed, data type)
16:38:23 <ski> johnw : "matching on the data infers the index" would be how "our" GADTs in GHC works
16:39:04 <ski> (and, iiuc, i think this is also how indexed inductive data types in Agda2 works. perhaps Agda1 was different, i don't recall, and i don't have a working installation of Alfa to try with atm)
16:39:06 <johnw> ski: I just know that in Coq, I don't match on the index first, I match on the value and then the index is decided by doing so
16:39:14 * ski nods
16:39:56 <ski> johnw : however, what if you define the data type, not as an indexed inductive one, but as a recursive function on the index, which returns a type ?
16:40:14 <johnw> ski: good question, because I do use those as well
16:40:22 <johnw> I can't recall at the moment
16:40:36 <johnw> I think in that case you'd be right
16:40:56 <johnw> otherwise, I couldn't do beta reduction on the type-forming function
16:42:30 <ski> (in Agda1 you could say `Vector :: * -> Nat -> * = \(a :: *) -> \(n :: Nat) -> case n of {Zero -> data {@Nil :: _ a Zero}; Succ n -> data {@Cons :: Vector a n -> _ a n}}', if i recall the syntax correctly)
16:44:58 <ski> (er, that should be `_ a (Succ n)' .. perhaps this was the situation in which one should use `idata' rather than `data'. at least i recall `idata' was used for defining `Equal :: (a :: *) -> a -> a -> * = \(a :: *) -> \(x :: a) -> idata {@Refl :: _ a}' .. something like that)
16:46:24 <ski> then .. there's also the notion of GADT record type, where some record fields may only exist when the indices are specific things
16:46:47 <ski> a nice example of this would be something like
16:47:14 <ski>   record Apart (A : Set) (a : A) : A -> Set
16:47:17 <ski>     where
16:47:32 <ski>     Irrefl :: Apart A a a -> Void
16:47:45 <ski> (s/::/:/)
16:48:37 <ski> so, the `Irrefl' field in a value of type `Apart A x y' only exists if `x' and `y' are equal. in that case it contains an inhabitant of the empty type `Void'
16:48:53 <ski> otherwise, if `x' and `y' are apart, then `Apart A x y' amounts to the unit type
16:50:19 <ski> (hm, but if equalities of types can have multiple proofs, then perhaps apartness of types should also be able to have multiple proofs ? apartness of streams/sequences, and of functions, would naturally be able to have multiple proofs, anyway)
16:52:43 <johnw> this is all to work around the lack of logic in types, yes?
16:52:47 <johnw> no type-level if
16:52:52 <johnw> (or case)
16:53:09 <ski> hm, i don't know ?
16:53:16 <ski> (elaborate ?)
16:53:50 <johnw> i mean, these special GADTs are creating variant types based on the indices provided, such that the differences only exist at compile-time, right?
16:54:07 <johnw> whereas right now, type families are the only such variants we have
16:54:41 <ski> hm, i was just wondering to which extent one could emulate this with families, and how much or little awkward it would be
16:54:54 <johnw> do I need a newtype GADT, for example, or could I use a type family to indicate variant types based on the index?
16:54:59 <ski> (say closed families)
16:55:29 <ski> i'm not sure
16:55:44 <ski> perhaps a `newtype' family would be enough
16:56:12 <johnw> oh I see, you want to specialize type classes on it too... it seems like that would need even more features to work
16:56:25 <johnw> but I guess if we have data families, it makes sense
16:56:30 <ski> specialize type classes on it ?
16:56:41 <johnw> Why do you want newtype?
16:56:51 <johnw> don't we already have newtype families?
16:57:11 <ski> to avoid tagging if the output types of the data constructors are all disjoint
16:57:21 <johnw> "Although, a data family is introduced with the keyword "data", a data family instance can use either data or newtype."
16:57:36 <ski> while having the restriction that some external data will have to determine the indices for us
16:57:46 * ski nods to johnw
16:58:39 <ertes> with GHC generics how do i find the path to a field of a particular name?  the naive approach leads me to two overlapping instances for (:*:)
16:58:45 <ski> "don't we already have newtype families?" -- i think so. the question is to which extent the same thing can be emulated using them, and how much or little awkward it would be in practice
16:59:04 <ertes> so i think i need to encode the path using some type-level information
17:00:33 <ski> i suppose with the indices restriction, we could use that to partition the data constructors into disjoint sets, mutually non-overlapping in result types
17:00:55 <ertes> naive approach is with these instances:  instance (C' sym f) => C' sym (f :*: g);  instance (C' sym g) => C' sym (f :*: g)
17:02:16 <ski> (hm, but perhaps we can't always do such a partition in a satisfying way, overlap not necessarily being transitive)
17:03:19 <ski> (if we could partition, then for each class with one data constructor, we could use `newtype' semantics, and use `data' semantics for the others (while still insisting on knowing indices before being able to match))
17:05:01 <ski> johnw : it seems to me that with the family encoding, we must already have partitioned into classes .. but i think if each class has a single member (finest equivalence relation), which was what the original idea above was about, then we probably can always encode using families
17:05:33 <ski> (however, we need to invent new data type names for each equivalence class, which could be awkward ..)
17:07:30 <ski> (or else reuse existing type (so `type' family), but then we lose the data constructor, and also injectivity, i think)
17:12:40 <lyxia> ertes: compute the path with type families
17:14:39 <ertes> lyxia: that's what i've been trying, but i can't figure out a way to do it without doing anything dynamic
17:15:43 <lyxia> ertes: http://lpaste.net/363435
17:16:06 <ertes> d'oh!  Maybe!  of course
17:16:11 <ertes> thank you
17:16:34 <lyxia> yw
17:33:58 <EvanR> ski: any idea how to pronounce paul tarau ?
17:35:44 <delYsid> Is there something like sepBy that folds using the separator?
17:36:32 <delYsid> For parsing/folding a list of things with infix functions as separators?
17:37:30 <EvanR> delYsid: you mean like chain1 and chainl1 ?
17:37:43 <EvanR> er chainl
17:38:09 <cozachk> @type chain1
17:38:11 <lambdabot> error: Variable not in scope: chain1
17:38:53 <EvanR> chainl :: Parser a -> Parser (a -> a -> a) -> a -> Parser a
17:39:37 <ski> EvanR : er, with german pronounciation ?
17:39:47 <EvanR> thats what i was going with
17:40:09 * ski is just guessing, hadn't realized there could perhaps be alternative pronounciations
17:40:41 <EvanR> delYsid: carefully nested chainl's and chainr's can parse operators which have precedence
17:40:59 <EvanR> ski: ive definitely looked silly trying to use german pronunciation for everything
17:41:44 <ski> (i've run into papers by him a long time ago, in the context of logic programming. e.g. using continuations in Bin-Prolog, and also implications goals for making a temporary assumption to the knowledge base. however, i ran into those FP papers by him only a couple of years ago)
17:42:20 <EvanR> i saw the one where it constructed those "catalan" graphs in prolog
17:42:22 * ski should get back to using delimited continuations for parsing operator precedence, sometime ..
17:42:37 <ski> not sure i've seen that paper
17:42:46 <delYsid> EvanR: Cool, thanks!
17:44:05 <EvanR> hrm maybe i made that up
17:44:19 <ski> what is a catalan graph ?
17:44:22 <EvanR> but there is this... http://www.cse.unt.edu/~tarau/research/2013/serpro.pdf but in haskell
17:44:33 <EvanR> i tried to figure out what that is, but ended up only getting stuff by tarau
17:44:42 <ski> o'12
17:45:05 <EvanR> i think hes being idiosyncratic, referring to the relationship between size of trees and catalan numbers
17:46:57 <ski> hm, ".. we sepaarate the symbol content and the syntactic skeleton of a term that we serialize compactly .." reminds me of an exercise of applying CPS at type level, that i did
17:55:30 <ski> yuck, copy_term/2 :)
17:59:11 <EvanR> is that imperative or
17:59:54 <ski> i suppose it would be akin to `isForced :: a -> Bool' in Haskell
18:00:16 <ski> iow, it observes instantiation state, and so isn't a declarative/logical predicate
18:01:10 <ski> the conjunction `A = 0,copy_term(f(A),B)' isn't the same as the commutation `copy_term(f(A),B),A = 0' of it
18:01:41 <ski> the former will have the solution `A = 0,B = f(0)', the latter the solution `A = 0,B = f(C)'
18:02:24 <EvanR> gross
18:02:30 <ski> indeed
18:02:44 <EvanR> i dont know logic programming but it sounds gross
18:02:56 <EvanR> is commutation used during runtime?
18:02:57 <ski> the issue is that it makes a copy of a structure, including the sharing patterns of variables *inside* that structure, while severing all sharing between the copy and other datastructures
18:04:18 <anwar> Hi, I'm a haskell beginner and I want to import Control.Monad.Trans.Except to my source file but the compiler throws an error saying "Could not find module `Control.Monad.Trans.Maybe'". I thought my GHC installtion already comes with trans
18:04:23 <ski> so `S = g(2,A),copy_term(f(A,1,A),B)' will result in `S = g(2,A),B = f(C,1,C)', `C' being a fresh logic variable, where `g(2,A)' no longer shares (`A') with `f(C,1,C)', which it shared with the input `f(A,1,A)'
18:05:54 <ski> if `A' is later instantiated (say to `3'), then all sharing components of data structures will automatically be instantiated to the same thing. so `S' will become `g(2,3)', and the input to copy_term/2 will become `f(3,1,3)', but `B' will still be `f(C,1,C)', not `f(3,1,3)', since the copy was made before `A' was instantiated
18:06:28 <EvanR> how would that not cause chaos
18:07:15 <boj> anwar: did you install haskell platform, or something else?
18:07:23 <ski> copy_term/2 is typically used when meta-programming (if representing bound variables in the object language with logic variables in Prolog, rather than with ground / fully instantiated structures, like `var(0)',`var(1)',&c.), and for other hacks like my hack to simulate lambda expressions, to get anonymous predicates
18:07:55 <ski> (Ulrich Neumerkel came up with the same idea, and made it into a library. i wouldn't be surprised if other people had thought of it before, as well)
18:08:36 <anwar> boj: I don't know, or rather I can't recall how I installed haskell on my machine. How can I find that?
18:08:55 <ski> EvanR : "is commutation used during runtime?","how would that not cause chaos" -- no, for this, and other reasons, Prolog optimizers don't have that much leeway. Mercury, which is declarative, has more freedom here to use logical equivalences to rearrange code
18:08:56 <boj> anwar: you'll want to make sure  you added "transformers" to your cabal file's build-depends section
18:09:05 <boj> anwar: i am not sure to be honest
18:09:47 <ski> one nice thing in Prolog is when a predicate one has implemented can be run "both forwards and backwards", giving the behaviour of multiple functions for the cost (one single declarative implementation) of one
18:10:22 <anwar> boj: yes, I added it in the .cabal file and ran `stack build` and command just ran fine
18:10:30 <ski> one example of this is the append/3 predicate, which can do the job of `(++)',`stripPrefix',`stripSuffix',`splits :: [a] -> [([a],[a])]'
18:11:28 <anwar> boj: ohh I see it now, it works now :P
18:11:33 <ski> however, because Prolog won't reorder conjuncts, sometimes one'd like one ordering for the "forward" mode, and another for the "backward" .. and so one has to resort to checking the instantiation state, to be able to select the correct ordering at run-time
18:11:46 <boj> anwar: cool :)
18:12:08 <ski> (and checking instantiation state is in general a non-declarative operation, so one has to be careful when doing this, to not introduce impurities at the interface level of the predicate)
18:12:26 <anwar> boj: I had to refresh my session in VSCode to fetch the new libraries, thanks bro
18:13:21 <ski> Mercury, otoh, would usually do this by itself (as soon as you've declared which modes of the predicate you'd like), compiling each mode separately, and statically selecting which mode of a predicate call to use (so no run-time selection)
18:14:42 <ski> (Mercury compiles to more efficient code than Prolog can, at least for the most part. however, Mercury is also less flexible, in that the compiler has to statically know the allowed instantiation patterns of variables at each step in time, for all the modes of predicates (and functions))
18:16:34 <ski> EvanR : so, the answer to "how would that not cause chaos" is : by being very careful with how one uses such non-declarative features in Prolog, and by Prolog implementations not daring to use many logical equivalences, causing less flexibility (and sometimes less efficiency) than one could otherwise hope to attain
18:16:59 <EvanR> i see
18:19:11 <ski> (still, if you haven't learned logic programming, i would still suggest learning Prolog. most resources are for Prolog, even if one would want to use a more modern language like Mercury, with static typing, algebraic data types, parametric polymorphism (lacking parametricity, unfortunately), type classes, existentials, static mode, inst and determinism checking, &c.)
18:20:08 <ski> (also, it's easier to understand what the static mode, inst and determinism checking is yelling about, if one has already suffered the consequences of not having that in Prolog)
18:20:59 <ski> see "Re: Mercury in academic teaching?" by Richard A. O'Keefe in 2006-10-(09|10) at <http://www.mercurylang.org/list-archives/users/2006-October/004000.html>,<http://www.mercurylang.org/list-archives/users/2006-October/004011.html> for some info on that, and also some interesting more general opinion
18:24:05 <ski> hm, i suppose i should also mention the logic programming language Oz, which is used by the CTM book
18:24:08 <ski> @where CTM
18:24:08 <lambdabot> "Concepts, Techniques, and Models of Computer Programming", by Peter Van Roy,Seif Haridi, at <http://www.info.ucl.ac.be/~pvr/book.html>
18:24:50 <ski> (which in some ways resembles SICP, though it also covers logic programming, and declarative concurrency with dataflow variables (cf. `monad-par'))
18:25:45 <ski> there is also Twelf, which can be seen as a dependently typed logic programming language (you can search for proofs, just as you can search for solutions for variables in a call to a predicate)
18:27:47 <ski> and there's lambdaProlog and lolli (and some more like Lygon, Forum, ..), lambdaProlog uses lambda expressions (HOAS) to represent bound variables in object lanaguage, which is more sensible than what's often used in Prolog. you can match on lambda expressions, but lambdas can't compute arbitrary stuff, you need predicates for computation
18:28:54 <ski> lolli is similar to lambdaProlog, but is based on (intuitionistic) linear logic. Lygon is based on classical linear logic, Forum (iirc), meant for specifications of protocols ?, is based on the multiplicative fragment of linear logic, iirc
18:30:01 <ski> all these actually uses fragments of the respective logics (just like Prolog does, Prolog uses the Horn clause fragment. lambdaProlog uses the heriditary Harrop formula fragment)
18:31:03 <ski> then, if one mentioned Logic Programming, one should also mention Constraint Programming, in particular the fruitful combination Constraint Logic Programming (CLP)
18:32:31 <ski> ordinary prolog can be said to use equality constraints over trees (like algebraic data types, except un(i)typed). CLP adds more interesting constraints, like equalities, inequalities, and disequalities over arithmetic expressions, using integers, rationals, or real numbers (represented by floating-point numbers)
18:32:47 <pikajude> is there a way to get cabal new-run to rebuild the executable if the source files have changed?
18:33:43 <ski> so, one can state a problem (like mortgage) in terms of constraints (which may be generated dynamically by a running program), the constraint solver will simplify these as much as it can, and present the simplified state (projected down using existential quantifications) as answer
18:34:54 <ski> (there are also constraint solvers for probability reasoning, and for more abstract constraints (cf. "chemical molecule reactions", Linda tuple-space). ECL^{i}PS^{E} is a prolog system that's intended mainly for CLP)
18:35:02 * ski will stop the LP expose here ..
18:56:05 <jchia_1> how can i get the file for a stack resolver or at least check which ghc version it uses?
18:57:47 <mud> jchia_1: Is the website an acceptable way? There's a list of mappings essentially on the the front page, or you can go to the one for a particular resolver and it'll say right there
18:59:21 <lyxia> on GHC 8.4, it seems (a ~ b) no longer implies (a == b) ~ 'True...
18:59:57 <lyxia> oh because it has a clause (f a == g b) = f == g && a == b   ...
19:01:28 <jchia_1> mud: Yes, I can find the GHC version there.
20:37:38 <Xal> without going all in with a parser combinator library, is there a good haskell equivalent to scanf?
20:43:52 <lyxia> There's a nice one with GADTs but I don't know any existing implementations in Haskell.
20:45:54 <Xal> I just wanna parse a few integers separated by dashes, is that too much to ask?
20:46:04 <Xal> What's the best tool for this?
20:46:28 <lyxia> parser combinators.
20:51:28 <mud> Xal: There's really not a high barrier of entry to parser combinators. The simple things like that are genuinely simple to do with them.
20:52:53 <Xal> lyxia: alright, which one should I use for a one-off String parsing deal
20:53:06 <Xal> I don't really feel like introducing 400 dependencies
20:53:41 <mud> megaparsec is a decent choice. I have no idea what its transitive dependencies look like though, I don't tend to care much about that usually
20:54:13 <lyxia> Xal: there's a parser library in base
20:57:13 <lyxia> I wonder whether that's any good
20:58:08 <lyxia> maybe not.
21:01:53 <lyxia> sorry I can't find anything truly lightweight
21:02:22 <lyxia> with the amount of monadic parser tutorials one would think there's a basic String parsing library out there
21:05:03 <lyxia> parsec seems to have the fewest deps
21:05:11 <regina> Seems like the types in the Prelude.hs are wrong for sin, asin, and sqrt …
21:05:38 <sl2c> lyxia: ReadP is a basic string parser I think but also you don't want to use String
21:05:46 <regina> There’s no way sin and asin should have the same type signature, for instance…
21:06:23 <sl2c> :t sin
21:06:25 <lambdabot> Floating a => a -> a
21:06:50 <regina> sl2c: exactly. sin can’t output –3 for example
21:07:36 <sl2c> I don't think Haskell has numeric types with bounds
21:07:50 <regina> they should be writeable though, yes?
21:07:54 <fakenullie> Which library is in base?
21:07:55 <regina> I’m reading RWH chapter 6.
21:10:04 <fakenullie> Most floating point functions in base are just direct calls into glibc or something afaik
21:11:09 <fakenullie> That's why there's nan
21:17:18 <amalloy> Xal: a few integers separated by dashes? you can do that easily enough with span/break and read
21:17:53 <Xal> amalloy: yeah, but it's got some optional components and a few forms
21:17:59 <Xal> regex looks very tempting
21:18:03 <Xal> but it feels... firty
21:18:05 <Xal> dirty*
21:19:46 <regina> fakenullie: interesting
21:24:47 <fakenullie> Xal: parser combinators are type safe regexps
21:27:03 <fakenullie> With sane syntax
21:31:39 <mud> And actually composable
21:34:43 <regina> some of the new API’s to regex — eg rex in R — are composable too
21:36:34 <ski> regina : a function is allowed to not output all the values in its codomain / result type
21:37:48 <regina> ski: and shouldn’t or at least couldn’t this be noted in the types?
21:38:37 <regina> yeah I think technically the difference between range or image and codomain is what’s used vs what might have been allowed to be used
21:38:45 <ski> it could be noted, but isn't required. sometimes refrained from, because the image is hard to describe nicely
21:39:08 <regina> the normal thing from maths class is just that sin: [–∞,+∞] -> [–∞,+∞] is a valid function definition but a bad one.
21:39:14 <ski> otoh, the function not being defined on all of its domain / argument type, is a worse offended
21:39:22 <ski> s/offended/offender/
21:39:31 <regina> and acos is then accepting undefined inputs
21:40:04 <ski> well, one could say
21:40:20 <ski>   cos : ℝ ⟶ ℝ
21:40:28 <ski> or restrict the codomain as
21:41:32 <ski>   cos : ℝ ⟶ {x : ℝ | -1 ≤ x ∧ x ≤ 1}
21:41:43 <regina> yes and the second one is a better function
21:41:57 <ski> or even "corestrict" the domain also (or instead) as
21:42:58 <ski>   cos : ℝ ∕ 2·π·ℝ ⟶ {x : ℝ | -1 ≤ x ∧ x ≤ 1}
21:43:36 <ski> where by ⌜ℝ ∕ 2·π·ℝ⌝ i mean the real numbers, "modulo ⌜2·π⌝"
21:44:49 <ski> (which does not mean that we restrict the domain to a half-open interval of length ⌜2·π⌝, such as ⌜{x : ℝ | 0 ≤ x ∧ x < 2·π}⌝ or ⌜{x : ℝ | -π < x ∧ x ≤ π}⌝)
21:44:50 <regina> yes
21:45:51 <ski> anyway, one could argue that the arcus cosinus function really should be typed as
21:45:58 <ski>   cos⁻¹ : {x : ℝ | -1 ≤ x ∧ x ≤ 1} ⟶ ℝ ∕ 2·π·ℝ
21:47:21 <ski> e.g. ⌜cos⁻¹ 1⌝, on this view, doesn't give ⌜0⌝, nor ⌜2·π⌝ or ⌜4·π⌝ or ⌜-2·π⌝, &c., but is "ambiguous" among all these
21:47:45 <ski> and we can express that by saying that it gives ⌜0⌝, up to congruence modulo ⌜2·π⌝
21:48:26 <ski> "the second one is a better function" -- depends on how one wants to use it
21:48:35 <regina> ski: yes, exactly
21:48:37 <ski> strictly speaking
21:48:38 <ski>   cos : ℝ ⟶ ℝ
21:48:43 <ski> is a different function from
21:48:49 <ski>   cos : ℝ ⟶ {x : ℝ | -1 ≤ x ∧ x ≤ 1}
21:48:59 <ski> the first is not surjective, but the second is surjective
21:49:03 <regina> ski: no, there is no good use of a cos with a codomain that’s so much larger than the used range / image…
21:49:31 <ski> both of these are not injective, and
21:49:33 <ski>   cos : ℝ ∕ 2·π·ℝ ⟶ {x : ℝ | -1 ≤ x ∧ x ≤ 1}
21:50:10 <ski> is still not injective. however if we use tangens instead, then the formulation of that corresponding to this last form would be injective, while the two previous forms wouldn't be injective
21:50:59 <ski> if we want to express the function ⌜x ↦ x + cos x⌝, then we have no use for restricting the codomain of ⌜cos⌝
21:53:12 <regina> “we”?
21:54:19 <regina> fakenullie: glibc/math/w_sqrt* do check for positivity.
21:54:53 <ski> regina : anyway, if you want more precise domains (no partial functions), more closely fitting codomains (less slack, smaller part of the codomain which isn't in the image), or less non-injectivity (using a quotient type for the domain in order to make the preimage of points in the codomain be smaller, in the limit, singletons), then you should probably look into dependent typing, such as Agda or Coq
21:55:13 <ski> regina : colloquial "we"
21:55:30 <regina> ski: yeah, those and idris – and liquid haskell – all sounded interestig for this kidn of reason
21:55:43 * ski nods
21:55:53 <regina> still seems doable as far as ch 6 RWH
21:59:37 <ski> > acos (-1)
21:59:41 <lambdabot>  3.141592653589793
21:59:43 <ski> > acos (-2)
21:59:47 <lambdabot>  NaN
22:01:49 <ski> anyway, if we use division or square root, say, then we need dependent function type, or dependent pair type, to express the type of functions using those
22:02:04 <ski>   f (x,y) = x / (x - y)
22:03:20 <ski>   f : {(x,y) : ℝ² | x ≠ y} ⟶ ℝ
22:03:41 <EvanR> should it be not equal, or x "apart from" y
22:03:49 <andportnoy> ski: jeez can you do that in Haskell?
22:03:57 <ski> (subset comprehension is a kind of dependent pair construction)
22:04:35 <ski>   f : (x :) ℝ ⊗ {y : ℝ | x ≠ y} ⟶ ℝ  -- alternative formulation of the same thing
22:04:35 <EvanR> exists (specific) separation greater than zero between x and y
22:05:14 <ski> EvanR : i was using the convention to denote apartness with ⌜≠⌝, fwiw
22:05:24 <EvanR> interesting
22:05:29 <ski>   f x y = x / (x - y)
22:05:45 <ski>   f : (x :) ℝ ⟶ {y : ℝ | x ≠ y} ⟶ ℝ  -- dependent function type
22:06:02 <EvanR> these arrows arent showing up well :(
22:06:20 <MarcelineVQ> is the a differenc between  (x :) ℝ   (x : ℝ)
22:06:26 <andportnoy> pardon for intruding here
22:06:32 <EvanR> https://imgur.com/a/xNLLR
22:06:45 <andportnoy> but are these arrows and inequality signs legal operators in Haskell?
22:06:56 <andportnoy> I'm a total novice in Haskell
22:07:26 <ski> (otherwise apartness would often be denoted by `#' or ⌜⋕⌝)
22:08:37 <ski> MarcelineVQ : i tend to use the ⌜(x :) σ⌝ in dependent types, like ⌜(x :) σ ⊗ τ⌝ and ⌜(x :) σ ⟶ τ⌝, to emphasize that the types are the "main part", and the naming of the value "an afterthought"
22:09:08 <EvanR> ⌜(x :) σ ⟶ τ⌝ here... its not clear that x is sigma or the whole thing
22:09:19 <regina> ski: ⊗?
22:09:25 <ski> as that convention stands, ⌜(x :) σ⌝ on its own means nothing (though one can consider extensions of the interpretation which would allow more meaning)
22:09:29 <EvanR> is the* sigma
22:10:42 <ski> regina : what is usually written as ⌜×⌝, cartesian product, in math, but in personal notes i usually prefer writing ⌜⊗⌝, when i want to emphasize its multiplicative ("tensor") aspects, such as for the dependent pair type
22:11:19 <ski> EvanR : yea, i assume that ⌜(x :)⌝ binds tighter than ⌜⟶⌝ and ⌜⊗⌝, in order to write less brackets
22:13:01 <regina> ski: ⋕?
22:13:37 <ski>   f : (x :) ℝ ⟶ (y :) ℝ ⟶ x ≠ y ⇒ ℝ  -- might be yet another formulation of the curried ⌜f⌝
22:14:24 <ski> regina : apartness, <https://ncatlab.org/nlab/show/apartness+relation>
22:15:44 <ski> apartness on the real numbers turns out to be nicer, computationally, than equality on the real numbers
22:16:45 <regina> ski: understood, thx
22:16:58 <EvanR> i find myself coding reals again as we speak
22:17:10 <ski> two real numbers being apart means that there's at least some positive distance inbetween them
22:17:15 <EvanR> taking some pointers from this slideshow http://www.cs.ru.nl/~spitters/presentation_reals.pdf
22:17:27 <ski> if two real numbers are apart, then either one is less than the other, or vice versa
22:17:28 <regina> does the type world have a notion fo bilinearity?
22:17:51 <ski> otoh, we can't compute that any two pair of real numbers either are equal, or one is less than the other, or vice versa
22:18:06 <ski> nor can we compute that either one is at most the other, or vice versa
22:18:14 <regina> EvanR: that way lies madness
22:18:29 <ski> regina : a bilinear transformation is just a linear transformation from the tensor space
22:18:44 <regina> ski: I know. how does that relate to types?
22:21:09 <ski>   (·) : ℝ ⊗ V ⟶ V  -- is scalar multiplication on a vector space ⌜V⌝
22:25:00 <ski>   ((v′,v) ↦ ⟨v′ | v⟩) : V⁺ ⊗ V ⟶ V  -- is "dot" multiplication of a covector in ⌜V⁺⌝ (sorry, couldn't find a superscript star), and a vector in ⌜V⌝
22:26:04 <ski> also matrix-vector multiplication, covector-matrix multiplication, and matrix-matrix multiplication have a tensor vector space as domain
22:27:59 <regina> right…
22:28:05 <regina> but what about in type land?
22:28:07 <ski> (⌜V⁺⌝ was meant to denote the dual space of ⌜V⌝, iow the space of linear functional of type ⌜V ⟶ ℝ⌝, sometimes written as ⌜V ⊸ ℝ⌝)
22:28:29 <ski> well, i'm not sure what you're asking about. afaiac, these are types
22:30:44 <ski> you can construct ⌜U ⊗ V⌝ as a quotient type over linear combinations of pairs from the cartesian product ⌜U × V⌝
22:30:44 <regina> ski: say you have two types A and B. what is it that makes you want to use ⊗?
22:31:15 <regina> ski: ah ok. any examples? (not from multilin. alg.)
22:31:44 <ski> well, if we ignore vector spaces, then the reason is linear logic, as developed by Jean-Yves Girard and others (LaFont,Blass,&c.)
22:32:35 <ski> in linear logic, assumptions are replaced by resources
22:32:49 <ski> you can use an assumption as many times as you like in a proof, in ordinary logic
22:32:56 <ski> in linear logic, you must use it exactly once
22:33:47 <ski> however, there's now two kinds of conjunction, the additive conjunction, aka "external choice" or "with", and the multiplicative conjunction, aka "tensor", or "both"
22:35:13 <ski> the multiplicative conjunction of ⌜A⌝ and ⌜B⌝ is written ⌜A ⊗ B⌝. a proof of this consists of a proof of ⌜A⌝ and a proof of ⌜B⌝ (as per the usual Brouwer-Heyting-Kolmogorov interpretation of proofs. also cf. Curry-Howard correspondence)
22:35:51 <ski> to use a resource of type ⌜A ⊗ B⌝, we must use both the subresource of type ⌜A⌝, and the subresource of type ⌜B⌝
22:36:06 <ski> (can, and must)
22:38:16 <ski> however, regarding the additive conjunction of ⌜A⌝ and ⌜B⌝, written ⌜A × B⌝ or ⌜A & B⌝, to use a resource of this type means to make a choice of whether we want to use the ⌜A⌝ or the ⌜B⌝ parts. conceptually, these parts are potential, and aren't materialized until we make the choice of which of them we want
22:39:15 <ski> therefore, to *construct* a value of type ⌜A × B⌝, we're allowed to "use the same resources" when producing the ⌜A⌝ part, as when producing the ⌜B⌝ part, seemingly violating the restriction that we can't use a resource more than once
22:39:59 <ski> but this is only apparent, for in fact only one of these two potential constructions will be realized, and so we will end up with only one use of the resources we used
22:41:19 <ski> then, there's an additive disjunction of ⌜A⌝ and ⌜B⌝, written (by me) ⌜A + B⌝ (Girard writes ⌜A ⊕ B⌝, with the intent to draw analogy to direct sum), aka "internal choice", or "either .. or"
22:42:11 <ski> to produce a resource/value of this type, we must make a choice to either produce an ⌜A⌝, or to produce a ⌜B⌝, and then use the available resources ("assumptions") to produce that value/resource
22:43:02 <ski> to consume such a resource, we must provide two cases, one for the ⌜A⌝ case, and one to the ⌜B⌝ case. in the first branch, we'll be passed a resource of type ⌜A⌝, in the second, one of type ⌜B⌝
22:43:25 <ski> we don't make the choice, the choice is made for use. we can use the same resources in both branches, since only one of them will be realized
22:44:30 <ski> if a Café offers "coffee or tea", then they're offering you ⌜coffee × tea⌝, external choice, they don't make the choice. you, the receiver of the "with" resource makes the choice
22:45:48 <ski> if a Restaurant says that "on thursdays, the standard meal is either soup or pie", then that probably means that they'll decide which to serve on each thrusday, so internal choice, ⌜soup + pie⌝
22:46:32 <ski> if they serve you both a main course and a dessert, then you get to consume both, so that's ⌜main_course ⊗ dessert⌝
22:47:32 <ski> there's a linear implication, ⌜A ⊸ B⌝, "lolli". to use such a resource, we must feed it a resource of type ⌜A⌝, and we obtain a resource of type ⌜B⌝ in exchange
22:48:06 <ski> if we can buy a muffin for a dollar, then that can be expressed as ⌜dollar ⊸ muffin⌝
22:48:46 <ski> since we can't use resources twice, we can't prove ⌜dollar ⊸ (muffin ⊗ dollar)⌝
22:49:21 <ski> ⌜A ⊗ B⌝ express "aggregation of resources", placing two resources beside each other, and wrapping them together in a container
22:50:39 <ski> the tensor, the multiplicative conjunction, has a neutral element, multiplicative truth, which i write as ⌜⊤⌝ (Girard writes ⌜1⌝), it corresponds to a container with zero things inside, "no information"
22:51:01 <ski> ⌜⊤ ⊸ X⌝ is equivalent to ⌜X⌝, for any resource ⌜X⌝
22:51:30 <ski> since we can't discard resources, we can't prove ⌜garbage ⊸ ⊤⌝
22:51:57 <johnw> ski: How do you type ⌜?
22:52:19 <ski> the resource ⌜! X⌝ ("of course ⌜X⌝") is an explicit licence to use ⌜X⌝ as many times as we please
22:52:22 <johnw> For me it would be \cul
22:53:11 <ski> so, the proof of ⌜dollar ⊸ muffin⌝ above is probably really a proof of ⌜! (dollar ⊸ muffin)⌝, iow we can buy as many ⌜muffin⌝s as we like, provided we have enough ⌜dollar⌝s
22:54:17 <ski> there is a neutral element for the additive conjunction, which I write as ⌜1⌝ (Girard writes ⌜⊤⌝), "erase", it can be thought of as a garbage dump
22:54:53 <ski> we can always prove ⌜X ⊸ ⊤⌝, for any ⌜X⌝ .. but then we can't get rid of ⌜⊤⌝ ..
22:55:05 <ski> er, sorry that should say
22:55:10 <ski> we can always prove ⌜X ⊸ w⌝, for any ⌜X⌝ .. but then we can't get rid of ⌜w⌝ ..
22:55:18 * ski sighs, tries once more
22:55:23 <ski> we can always prove ⌜X ⊸ 1⌝, for any ⌜X⌝ .. but then we can't get rid of ⌜1⌝ ..
22:56:14 <ski> finally, there's a neutral element of additive disjunction, written ⌜0⌝, which is impossible to prove, and therefore we can prove ⌜0 ⊸ X⌝ for all ⌜X⌝
22:56:37 <ski> that's all combinators of propositional, intuitionistic, linear logic
22:57:21 <ski> there's also a classical version, which has three more connectives, being "multiplicative disjunction" aka "par", its neutral element "bot", and a dual connective to "of course", "why not"
22:57:45 <ski> these are a bit harder to give an intuitive grasp for
22:58:15 <ski> however, while getting ⌜! pizza⌝ means that you can consume as many ⌜pizza⌝s as *you* like
22:59:01 <ski> if you get ⌜? pizza⌝ ("why not pizza"), that means that you're forced to consume as many ⌜pizza⌝s as whoever is providing you with them likes/wants
23:01:04 <ski> multiplicative disjunction, which I write ⌜A ⊕ B⌝ (Girard writes ⌜A ⅋ B⌝), roughly means that you're playing two parallel games ⌜A⌝ and ⌜B⌝, or alternatively living in two parallel universes, however there is communication going on between them, you can use resources obtained in one, while acting in the other
23:01:43 <ski> its neutral element, "bot", ⌜⊥⌝ means the end of such a minigame / universe. it collapses/dies
23:02:18 <ski> regina : and that's a brief explanation of all connectives of classical propositional linear logic
23:02:49 <ski> johnw : "How do you type ⌜?" -- i copy it :)
23:04:07 <ski> regina : anyway, if you think about multiplying a scalar with a vector, then that is an operation that in some sense uses both of its arguments (which can be combined into a single argument in a tensor space)
23:04:55 <ski> regina : otoh, a polynomial may use its input as many times (at least finitely many) as it likes, so one would probably need to describe it using ⌜!⌝,"of course"
23:06:06 <ski> (i suppose ⌜(! ℝ) ⊸ ℝ⌝ should be the vector space of polynomials, hmm)
23:10:25 <ski> otoh, one could think of vector addition as in some sense "alternative use" (cf. superposition), so vector addition over ⌜V⌝ could be described as living in ⌜(V × V) ⊸ V⌝. however, this is the same as ⌜(V + V) ⊸ V⌝ (finitely indexed product and finitely index coproduct coincides in the category of vector spaces)
23:18:17 <ski> hmm, would ⌜! ℝ⌝ be the vector space of formal power series ?
23:22:58 * EvanR head implode
