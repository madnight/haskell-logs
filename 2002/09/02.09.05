00:06:49 --- mode: card.freenode.net set +o Heffalump
02:59:15 <Heffalump> hmm, having to compile GHC twice on a slow machine is annoying
06:44:09 * tmoertel needs some coffee
07:27:11 <Knuth> hi again, every1
07:27:17 <Heffalump> hi
07:27:21 <shapr> y0
07:27:28 <Heffalump> that's a rather grand nickname :-)
07:27:32 <Knuth> i have a (prolly silly) question
07:27:42 <ski> hi
07:27:52 <shapr> hi ski
07:27:55 <Knuth> Heffalump: so everyone says. still, i _can_ understand _part_ of taocp :)
07:28:04 <Knuth> hello ski
07:28:04 <ski> shapr : hi !
07:28:28 * Heffalump hasn't actually read any of taocp.
07:28:30 <Heffalump> I ought to.
07:28:41 <Knuth> taocp rulez
07:28:43 <Knuth> ok so:
07:28:45 <Knuth> i write this:
07:28:46 <ski> me neither
07:28:49 <Knuth> module Main where
07:28:57 <Knuth> a :: Int
07:29:01 <Knuth> oops
07:29:06 <Knuth> i mean a :: [Int]
07:29:17 <Knuth> a = [1..50]
07:29:22 <Knuth> when i say product a
07:29:24 <Knuth> it says 0
07:29:29 <Knuth> and when i say sum a
07:29:32 <Knuth> i get the right answer
07:29:43 <Heffalump> you're sure it's [1..50] not [0..50] ?
07:29:56 <Knuth> yup
07:30:00 <Heffalump> oh, overflow
07:30:05 <Heffalump> 50! > the range of an Int
07:30:13 <Heffalump> try a :: [Integer]
07:30:13 <Knuth> oh
07:30:36 <Knuth> worked like a charm :)
07:30:37 <Heffalump> and probably 50! has enough 2s in it to make it a multiple of 2^32 or whatever
07:30:44 <ski> product [1..50] :: Integer  --> 30414093201713378043612608166064768844377641568960512000000000000
07:30:49 <Knuth> yup
07:30:53 <Heffalump> indeed.
07:31:00 <Knuth> whats the diff between Int and Integer
07:31:13 <ski> Integer is _all_ integers
07:31:25 <Knuth> wow. rulez.
07:31:26 <ski> Int is just a (probably finite) subset
07:31:31 <Knuth> sux.
07:31:49 <Knuth> dont tell me haskell implies no limit!
07:32:05 <Heffalump> Int is only guaranteed to be 2^29
07:32:20 <ski> (minBound,maxBound) :: (Int,Int) --> (-2147483648,2147483647)  in hugs at uni
07:32:28 <Heffalump> there's no limit, but they obviously won't be as efficient as Int
07:32:35 <Heffalump> ski: the report says 2^29
07:32:38 <Heffalump> not really sure why
07:33:01 <Knuth> damn i like haskell :)
07:33:16 <ski> perhaps to make room for tag bits so we don't have to heap allocate on a 32 bit machine ?
07:33:18 <ChilliX> 2^29 so that stupid tagged implementation can still do with a 32bit word
07:33:24 <Heffalump> ah, right
07:33:33 <jewel> which one is that?
07:33:54 <Heffalump> hangon, what are tag bits needed for?
07:34:05 <Knuth> one last thing...is there any _good_ tutorial on haskell out there
07:34:06 <ski> i don't know, probably GHC does something like that, not totally sure
07:34:15 <ChilliX> so that you can distiniguish pointers from non-pointers
07:34:15 <Knuth> i dont like gentle intro to haskell
07:34:25 <Heffalump> ah, right
07:34:28 <ChilliX> GHC doesn't use tags
07:34:33 <ski> ok
07:34:44 <ChilliX> it's based on the STGM, which means Spineless *Tagless* G-machine
07:34:46 <ski> then maybe nhc or hbc does
07:34:52 <ski> :)
07:35:12 <ChilliX> tags are a thing of old
07:35:38 <ChilliX> the Haskell FFI requires sized integral types: Int8, Int16, Int32, Int64 etc
07:35:46 <ChilliX> if you use tags, you are screwed
07:36:00 <ChilliX> (as in have to waste memory)
07:36:49 * Knuth feels neglected :.(...
07:36:54 <Knuth> lol
07:37:32 <ski> There are a couple of different ways of implementing polymorphism
07:38:00 <Heffalump> what does the Spineless bit mean?
07:38:05 <Knuth> never mind, i'll find one. thanx for the help dudes
07:38:11 <Knuth> l8er
07:38:50 <ChilliX> the original G-machine stored all to be evaluated expression as graphs
07:39:28 <ChilliX> the path from the root of the graph to the currently processed function application node was called the spine
07:39:30 <ski> but it didn't have a spine (~stack frames ?)
07:39:32 <ski> ?
07:39:39 <ChilliX> which roughtly corresponds to a call stack
07:39:43 <Heffalump> ah, ok.
07:40:00 <ChilliX> the STGM just uses a normal stack
07:40:11 <ChilliX> and doesn't build all of the graph
07:40:18 <ChilliX> which makes it much faster
07:40:53 <Heffalump> btw, do you have some good examples of situations where laziness really helps programming?
07:41:19 <Heffalump> The best example I know of is encoding attribute grammars in a "single pass" functional program (quotes since it's not really single pass)
07:41:23 <Heffalump> but I'd like some more
07:41:24 <ski> (I remember reading a paper about a lang called "Tigre", that implemented the spine with the normal stack (the nodes were executed and then modified !))
07:41:32 <ChilliX> I have examples where it helps, but personally I find the biggest advantage
07:41:46 <ChilliX> in the way I structure my programs
07:41:50 <Heffalump> obviously tying the knot in recursive environments in a compiler/interpreter is useful too
07:42:43 <ChilliX> eg, I can write a function like assert in Haskell and know that the condition is not evaluated when I switch asseratuions off
07:42:56 <ChilliX> we used laziness in the ICFP submission
07:43:24 <ChilliX> the subsumption routine has the signature
07:43:29 <ChilliX> type Strategy = StdGen -> World -> Maybe (Priority, Cmd, Maybe Plan)
07:43:37 <ChilliX> subsumption :: StdGen -> World -> [Strategy] -> (Bid, Cmd, Maybe Plan)
07:44:04 <Heffalump> so you look for the first strategy that returns a result?
07:44:09 <ChilliX> I could then simply apply all strategies in a list comprehension and fold over it
07:44:12 <ChilliX> exatcly
07:44:22 <ChilliX> sure, you could reformulate that to work in a strict language
07:44:27 <ChilliX> it;s justa  lot uglier
07:44:50 <Heffalump> I think I'm mostly looking for examples that would require major structural changes to code, rather than local ones
07:44:56 <ChilliX> so most of the time, I think, laziness is just a convenience
07:45:22 <ChilliX> in my self-optimising parser library I use laziness in an essential way
07:45:41 <ChilliX> basically, from a set of combinators that describe the regular expressions
07:45:59 <ChilliX> at runtime a DFA is constructed (using the standard regexp -> DFA algos)
07:46:17 <ChilliX> the construction of the DFA is triggered by the lexing routine while it goes
07:46:32 <ChilliX> so, if the input requeires only part of the DFA, only part of it is computed
07:46:33 <ski> Someone (don't remember whom) made a parser (i think) that had both ordinary state propagation and backwards-in-time state propagation (two different kinds of state i.e.) . They relied on laziness on implementing the monad
07:46:47 <ChilliX> doing this without laziness would be a big pain
07:46:55 <tmoertel> much uglier w/o laziness: fibs = 1 : 1 : zipWith (+) fibs (tail fibs)
07:47:12 <Heffalump> atm I'm writing a lot of standard ML and using explicit laziness where I need it
07:47:34 <ChilliX> Heffalump: the point is: you can always do without laziness
07:47:43 <ChilliX> afterall, you could write it all in assembler
07:47:52 <ChilliX> it's just often much nicer with laziness
07:47:52 <Heffalump> sure
07:47:56 <ski> theoretically
07:48:31 <tmoertel> i'd say that the "it's often much nicer w/ laziness" theory has been demonstrated in practice
07:48:31 <ChilliX> there is a paper in JFP by Lennart A et al
07:48:59 <ChilliX> where they implement efficient updating arrays in a purely functional style, which makes essential use of laziness
07:49:46 <ChilliX> another one: the flattening transformation (although we have strict arrays) is better behaved in lazy languages
07:50:02 <ChilliX> as indexing into an array of complex structures is computed incrementally
07:50:45 <ChilliX> wavefront algos can be nicely done with laziness (essential a more complicated version of fibs above)
07:51:12 <ChilliX> my ports library for concurrent programming makes essential use of laziness
07:51:18 <ChilliX> (I need infinite event streams)
07:51:43 <ChilliX> Heffalump: how long do you want to go on ;-)
07:51:48 <Heffalump> that'll do :-)
07:51:56 <ChilliX> :-)
07:52:27 <Heffalump> Julian said he thinks laziness is more hassle than it's worth
07:53:03 <Heffalump> and I was sort of semi-convinced cos I couldn't think of that many situations where I needed it very widely - i.e. wouldn't be happy enough with some explicit laziness in certain places
07:53:37 <ChilliX> you want to have both laziness and strictness in your programs
07:53:56 <ChilliX> I managed to get both Martin Odersky and RObert Harper to agree to this!
07:54:19 <ChilliX> (to be honest, Bob didn't need much convincing, he had arrived at this conclusion before)
07:54:40 <ChilliX> I haven't tried Xavier Leroy yet
07:54:50 <ChilliX> the question, then, is only what the default is
07:54:52 <Heffalump> strictness to avoid space leaks, presumably?
07:54:58 <Heffalump> I'd like a per-module setting.
07:55:18 <ChilliX> I really like it much more fine-grain
07:55:38 <Heffalump> I think a strict default would make sense, except that it'd discourage people from getting used to laziness and realising where it really can help them.
07:55:54 <ChilliX> from a sematical point of view, laziness is the natural default
07:56:05 <ChilliX> as it makes equational reasoning easier
07:56:05 <Heffalump> could something like deepSeq be treated as a primitive, so it forced strict semantics inside it?
07:56:24 <ChilliX> have you seens http://research.microsoft.com/Users/simonpj/papers/optimistic/
07:56:39 <ChilliX> the only reason you want strictness is for efficiency
07:56:56 <ChilliX> and in a few cases for semantic reasons (eg, foreign calls)
07:57:58 <Heffalump> yeah, I've seen it, but it doesn't have the advantages you'd get from a programmer saying "use strict semantics here"
07:58:16 <Heffalump> (assuming you mean the Ennals/PJ paper at the HW)
07:58:34 <ChilliX> Ennals/PJ is a POPL submission
07:58:43 <Heffalump> oh, getting confused then
07:58:46 <ChilliX> Jan-Willhelm Maessen has a similar paper at HW
07:58:48 * Heffalump checks the URL
07:58:55 <Heffalump> ah, Eager Haskell
07:59:00 <ChilliX> right
07:59:14 <ChilliX> it doesn;t solver the problem of allowing the programmer to specify
07:59:34 <ChilliX> but it promises to erradicate quite a lot of reasons for using strictness in the first place
08:00:04 <ChilliX> what remains, probably doesn't need much more sophisticated support then what we have in haskell with strict data structure arguments and seq
08:00:47 <Heffalump> chillix: re eradicating reasons, it depends if the programmer wants a /guarantee/ of strict behaviour or not
08:01:54 <ChilliX> sure, but how often do you want this guarantee beyond something like the optimistic eval model
08:02:16 <Heffalump> errm, doesn't that back out if it does what it considers to be too much work?
08:02:29 <ChilliX> true
08:02:37 <ski> if you want to merge bottoms of product, i.e. Complex
08:02:38 <Heffalump> so if you make your code take a little bit longer to run, suddenly the behaviour switches over
08:03:00 <ChilliX> it does this decision on a very fine-grained level
08:03:19 <ChilliX> so, I don't think that something like this would easily happen
08:03:19 <ChilliX> (but I may be wrong)
08:03:23 <ChilliX> (this is pretty new stuff)
08:04:09 <ChilliX> may point was more that optimistic eval promises to solve a substantial part of the strictness to prevent space leaks and speed up a program problem
08:04:23 <ChilliX> we still want strictness primitives in the language
08:04:35 <Heffalump> but again, you could get nondeterministic space leaks caused by decisions deep inside the optimistic evaluator
08:04:39 <Heffalump> yeah.
08:04:55 <ChilliX> but maybe we don't need quite as much as without optimistic eval
08:05:43 <Heffalump> I'm not convinced by that, because I don't think optimistic eval can ever offer guarantees of strict behaviour without violating lazy semantics
08:06:34 <ChilliX> but look at the tail recursion problem
08:07:06 <ChilliX> where people think they write an efficient function only so that that function generates a list of suspensions as long as the recursion deep
08:07:18 <ChilliX> in each recursive step, there is only very little extra work to do
08:07:26 <ChilliX> so optim eval will *always* do it
08:07:36 <ChilliX> independent of how deep your recursion goes
08:07:42 <ChilliX> in a particular program run
08:07:51 <ChilliX> so it gives you some guarantess
08:08:07 <ChilliX> whether that is sufficient in a wide range of practical situations remains to be seen
08:08:25 <ski> but if we map a big computation over a long list ?
08:08:51 <ChilliX> where is the harm in doing that lazily?
08:09:55 <ski> well, if we have "tail-recursion" which builds supsension, because the mapped computation is sufficiently big so that optimistic eval will back out ..
08:10:42 <ChilliX> which in the ideal cases means that the long list will at no point in time be fully unrolled in memory: big plus, I think
08:10:42 <ski> or foldl or whatever ..
08:11:03 <Heffalump> well, you'd hope deforestation would deal with the long list being unrolled in memory anyway
08:11:22 <ski> but didn't optimistic eval back out of too big computation ?
08:11:30 <ChilliX> if you look at how often GHC does deforestation, that hope is quickly destroyed
08:11:35 <Heffalump> :-)
08:11:38 <ChilliX> try sum [1..1000]
08:11:44 <ChilliX> and see how it is not deforested
08:11:48 <Heffalump> really? that sucks.
08:11:57 <Heffalump> I'd assumed simple things that use primitives would be
08:12:07 <Heffalump> or is sum defined using foldl?
08:12:08 <ChilliX> then, try to rewrite it so that is becomes desforested and suddenly you see that you really don't want this
08:12:32 <ChilliX> sum is *for optimisation reasons* implemented by explicit recursion
08:12:38 <ChilliX> therefore it doesn't deforest
08:12:48 <ski> eh ?
08:12:57 <Heffalump> sumupto 0 = 0
08:13:18 <Heffalump> sumupto (n+1) = ((n+1) +) $! sumupto n
08:13:25 <Heffalump> is what I'd hope for in the final code, I think
08:13:38 <Heffalump> (or equivalent thereof)
08:13:55 <Heffalump> or really sumupto m@(n+1) = (m+) $! sumupto n
08:13:59 <ski> (or accumulator-version perhaps (strict))
08:14:10 <Heffalump> oh, yes.
08:14:15 <Heffalump> my version builds a huge stack.
08:14:23 <Heffalump> now I see why you don't want deforestation.
08:14:54 <ChilliX> GHC defines
08:14:56 <ski> would deforestation do the non-acc version ?
08:14:59 <ChilliX> sum	l	= sum' l 0
08:14:59 <ChilliX>   where
08:14:59 <ChilliX>     sum' []     a = a
08:14:59 <ChilliX>     sum' (x:xs) a = sum' xs (a+x)
08:15:34 <Heffalump> makes sense.
08:15:34 <ChilliX> and GHC is clever enough to figure out that sum' is strict in it's second argument
08:16:02 <Heffalump> eek, gotta go
08:16:05 * Heffalump disappears
08:16:12 <Ig> Only if + is strict, no?
08:16:17 * ChilliX sees Heffalump running away....
08:16:32 <ski> + *is* strict for the primitive numbers at least
08:16:35 <ChilliX> yes
08:16:56 <Ig> + is not guaranteed to be strict for Num a => a though
08:17:20 <ChilliX> Ig: well, there is also that following line above the definition: {-# SPECIALISE sum     :: [Int] -> Int #-}
08:17:25 <ChilliX> :-)
08:17:42 <ski> well if you do an instance Num () then all ops would just return () so they wouln't need to look at arguments
08:18:03 <ski> or ?
08:18:33 <ChilliX> sure, but therefore you specialise the overloaded function for types where you know GHC can optimise further
08:18:54 <ChilliX> anyway, I need to sleep now
08:19:00 <ChilliX> it's already late here
08:19:01 <ChilliX> 'night
08:19:02 <Ig> Night
08:19:08 <ski> night
08:19:50 <tmoertel> night
09:11:14 <shreya_> Can someone help me with an integral, after spending 5 hours trying to do it, I give up!
09:11:18 <shreya_> I can't think of what to do
09:11:26 <shapr> what's the problem?
09:11:28 <ski> what is the prob ?
09:11:56 <shreya_> ok
09:12:24 <shreya_> S[(x - 3)/(x^2 + 2x + 4)^2].dx
09:12:30 <shreya_> How would I go about solving it
09:12:48 <shreya_> I have tried all sorts of substitutions, dividing into partial fractions, etc.. but I can't get it
09:13:24 <shapr> oh, thought you were trying to cast from an Integral to something else, I can't help you with that, sorry :-(
09:13:31 <Ig> It's not pretty
09:14:00 <Ig> You'll probably going to want to do a trig substitution, which is pretty much confirmed by maple having arctan in the result
09:14:14 <shreya_> ok
09:14:40 <shreya_> this is going to get yucky, I can sense it
09:14:53 <shreya_> The bottom has order of magnitude x^4 and top x^1
09:15:04 <Ig> Hmmm, where is this from?
09:15:09 <shreya_> That means it'll probably have ~3 terms :/
09:15:40 <shreya_> It's just a question in my book, no answers though, but I am really trying to work out how to solve it, rather than what the answer is
09:17:19 <Ig> OK, so you can't have gone wrong further up then  :-)
09:17:54 * Ig hasn't done this for long enough to provide any more advice
09:19:11 <shreya_> alright
09:44:36 <Heffalump> S = an integral sign?
09:45:02 <ski> yes, it was
09:45:37 <Heffalump> hmm.
09:50:00 <jewel> a trig substitution?
09:50:31 <Heffalump> I think I can see roughly how to do it, but it's messy.
09:50:36 * tmoertel just finished code for turn-by-turn analysis of robot games
09:50:55 <Heffalump> first transform x to remove the 2x term in the denominator
09:51:28 <Heffalump> hmm, what's the integral of 1/(1+x^4) ?
09:53:38 <tmoertel> (not pleasant)
09:53:38 <jewel> shreya_: has the book
09:55:54 * tmoertel moves to computer w/ Mathematica installed . . .
09:56:31 <Heffalump> :-)
09:56:47 * Heffalump starts scribbling seriously
09:59:20 <tmoertel_mma> after simplification:
09:59:22 <tmoertel_mma> (1/(4*Sqrt[2]))*(-2*ArcTan[1 - Sqrt[2]*x] +
09:59:40 <tmoertel_mma> 2*ArcTan[1 + Sqrt[2]*x] - Log[-1 + (Sqrt[2] - x)*x] +
09:59:46 <tmoertel_mma> Log[1 + x*(Sqrt[2] + x)])
09:59:49 <tmoertel_mma> (end)
09:59:51 <Heffalump> but how did it get there?
10:00:16 <tmoertel_mma> with much computing ;-)
10:00:17 * Heffalump needs to know how to integrate 1/(1+y^2)^2
10:00:19 <Heffalump> :-)
10:01:44 <Heffalump> oh well, shopping calls.
10:05:17 * tmoertel moves on to bot-games visualiation code . . .
11:32:27 <PMode> hi
11:32:43 <PMode> has someone tried to compile some HOpenGL examples under the debian unstable packages?
11:50:56 * tmoertel finishes first draft of VisualizeGameASCII
11:55:35 <toadx> ERRnosuchshare (You specified an invalid share name)
11:55:50 <toadx> oops, wrong channel ;p
12:12:26 <PMode> ciao
13:52:50 <^Arag0rn^> hey all
13:52:57 <Heffalump> hello
13:53:09 <^Arag0rn^> ltns 
13:53:10 <^Arag0rn^> sup?
13:53:16 <Heffalump> not much
13:54:19 <^Arag0rn^> no great haskell new or sumthing? :P
13:54:38 <Heffalump> not really
13:54:51 <Heffalump> several ICFP entries, including one entirely from people on this channel
13:55:04 <^Arag0rn^> ICFP?
13:55:28 <Heffalump> International Conference on Functional Programming - they run a programming contest each year
13:55:44 * shapr boings
13:56:05 <^Arag0rn^> ah cool
13:56:08 <^Arag0rn^> and i missed it :P
13:56:14 <shapr> aragorn: http://www.haskell.org/ for Haskell stuff.
13:56:34 <^Arag0rn^> lol shapr
13:58:56 <^Arag0rn^> a quick look learns me that there is still no network socket thingy for hugs :P
13:59:09 <shapr> ghc has network stuff
13:59:27 <^Arag0rn^> notice the word hugs in my line :)
13:59:32 <Heffalump> I think people are working on hugs libraries atm
14:02:52 <^Arag0rn^> yeah...same as half a year ago :)
14:03:14 <shapr> aragorn: why not use the ghc stuff?
14:06:07 <^Arag0rn^> cause i like hugs and interpreters
14:06:21 <shapr> what about GHCi ?
14:08:28 <^Arag0rn^> :P
14:09:00 <shapr> it's an interpreter!
14:09:24 <^Arag0rn^> whats wrong with hugs? :P
14:09:37 <Heffalump> it's slower than ghci, although it loads things faster
14:10:14 <shapr> aragorn: hugs doesn't have socket stuff? ghc does?
14:10:26 <^Arag0rn^> lol
14:10:29 <^Arag0rn^> okay, u won shapr
14:11:16 <shapr> if you want haskell + sockets, ghc can do it, hugs cannot.
14:11:25 <shapr> I dunno about nhc though.
14:12:37 <^Arag0rn^> what i want is more time :P
14:13:38 <^Arag0rn^> i have not enough time to do my uni stuff, work stuff, java stuff and haskell stuff 2 :P
14:14:21 <^Arag0rn^> so i will get into haskell again when java's fisniehd
14:18:19 <shapr> I like Haskell.
14:18:23 <shapr> it's elegant.
14:18:35 * tmoertel peeks into #haskell
14:18:41 <shapr> hi tmoertel!
14:18:47 <tmoertel> hi shapr!
14:19:02 <shapr> how's the bot visualization project going?
14:19:19 <tmoertel> pretty good
14:19:25 <tmoertel> i have the analysis part mostly finished
14:19:28 <shapr> cool
14:19:31 <tmoertel> and I'm starting the visualization pieces
14:19:53 <tmoertel> I can create ASCII playbacks of games right now
14:20:12 <tmoertel> next is curses and Postscript
14:20:18 <shapr> what's the url again?
14:20:32 <shapr> aha http://tea.moertel.com/~thor/ravt/
14:20:33 <tmoertel> it's http://tea.moertel.com/~thor/ravt/
14:20:44 <tmoertel> urls in stereo!
14:21:02 --- topic: set to 'in-progress ICFP 2002 visualization toolkit http://tea.moertel.com/~thor/ravt/ || We put the Funk in Funktion || See logs @ http://tunes.org/~nef/logs/haskell/ || parr 0.2.3 released http://www.cse.unsw.edu.au/~chak/nepal/' by shapr
14:21:16 <shapr> is it okay if I put that url in the topic for general info purposes?
14:21:19 <tmoertel> thanks!
14:21:24 <tmoertel> yes
14:21:38 * tmoertel gets telephone . . .
14:22:41 * tmoertel is back
14:23:16 <Heffalump> tmoertel: hmm. I'm not convinced by that answer from mathematica.
14:23:37 <tmoertel> heffalump: for the 1/(x^4) ?
14:23:42 <shapr> we could ask the emacs derivative solver
14:23:50 <tmoertel> s/x^4/1+x^4/
14:23:50 <Heffalump> well, the whole expression
14:23:58 * Ig can see what maple thinks if you want
14:24:00 <Heffalump> oh, I said 1/(1+x^2)^2
14:24:09 <Heffalump> I think I changed it half-way through
14:24:26 * tmoertel fires up mma again . . .
14:24:26 <Heffalump> oh, after you gave the answer :-)
14:24:28 <Heffalump> doh.
14:24:46 <Heffalump> anyway. I can integrate that nasty thing shreya gave now, though I'd rather not work out the answer :-)
14:26:58 <tmoertel_mma> hef: Integrate[1/(1+x^2)^2, x] yields (x/(1 + x^2) + ArcTan[x])/2
14:27:21 <Heffalump> yeah, that's roughly what I get (didn't bother pushing the substitutions back through)
14:27:46 <tmoertel> any other calcs while I have Mma fired up? ;-)
14:28:04 <Heffalump> naah :-)
14:28:30 <Heffalump> I was just trying to work out /how/ to do that original integral manually, I don't actually care about the answer
14:29:35 <tmoertel> you could always write a Haskell module Math.SymbolicIntegration
14:29:48 <Heffalump> I don't know what the algorithms one uses for that are
14:29:52 <Heffalump> I suppose I could look them up
14:30:16 <tmoertel> (make sure you include an integrateWithExplanation function)
14:30:19 <tmoertel> ;-)
14:30:35 <Heffalump> :-)
14:30:44 <Heffalump> that'd be cool.
14:32:18 * shapr grins
14:32:21 <shapr> that would be nifty
14:35:44 <shapr> I don't even know what a derivative is... I want to take some math classes just so I can figure out what the heck you guys are talking about.
14:37:38 * tmoertel posts RAVT 0.3 tarball (has first VisualizeGameASCII impl) . . .
14:38:02 <shapr> w00
14:38:55 --- topic: set to 'ICFP 2002 Robot Analysis and Visualization Toolkit 0.3 http://tea.moertel.com/~thor/ravt/ || We put the Funk in Funktion || See logs @ http://tunes.org/~nef/logs/haskell/ || parr 0.2.3 released http://www.cse.unsw.edu.au/~chak/nepal/' by shapr
14:51:41 * shapr bounces
15:36:20 * shapr bounces more
15:38:16 <dark> hello :)
15:38:54 <shapr> hi dark, what's up?
15:39:15 <dark> Not much... I'm hoping to get to sleep soon.
15:40:40 <shapr> me too
15:40:45 <shapr> dentist tomorrow :-/
15:41:21 <shapr> I'm going to greece in a week! yay!
15:41:53 <shapr> hi dblack
15:45:32 <dark> shapr: Better not bring any games :)
15:54:06 <toadx> is ghc considered a game?
15:56:58 <shapr> dark: I have to get this debian laptop setup so I'll have at least something in greece.
15:57:17 <shapr> http://www24.brinkster.com/srineet/para/para.html
15:57:23 <shapr> paratrooper in Haskell
16:02:07 <toadx> man is that uglp
16:02:10 <toadx> ugly
16:02:19 <toadx> its so ugly i can't even type straight
16:03:10 <toadx> the code is pretty though
16:05:22 <toadx> I should start using that LaTeX+Haskell stuff
16:06:13 <dark> Is there an Obfuscated Haskell contest?
16:06:37 <toadx> dark: yes, just take any of your code and give it to a C programmer :p
16:07:59 * shapr grins
16:08:20 <dark> toadx: Do you mean that Haskell is obfuscated by definition?
16:08:35 <dark> I know that it takes a while to learn how to read Haskell code.
16:08:59 <shapr> I agree with that.
16:09:08 <shapr> especially named pattern matches
16:09:13 <shapr> those had me confused for a long time.
16:09:34 <dark> And incantations like "withRWS f m = RWS $ \r s -> uncurry (runRWS m) (f r s)" still frighten me.
16:09:38 <toadx> dark: I just meant that its hard to read if you have never seen haskell before. Additionally, most C programmers think they can learn any new language in 'a day'
16:10:36 <shapr> uncurry is for turning (a, b) -> c  into a -> b -> c right?
16:11:05 * toadx has no idea
16:11:17 <dark> shapr: Right.
16:11:22 <shapr> uncurry :: forall a b c. (a -> b -> c) -> (a, b) -> c
16:11:27 <dark> shapr: But when I first saw that code, I didn't even know what $ did :)
16:11:35 <shapr> curry unpacks the tuple
16:11:40 <shapr> uncurry packs the tuple.
16:11:49 <shapr> yah, $ is confusing at first
16:12:18 <shapr> Haskell lambdas are so terse they were extremely confusing to me at first
16:12:22 <shapr> I didn't realize it was just lambda
16:12:52 <shapr> map (\x -> x + 1) [1..5]
16:13:35 <Ig> They're pretty much exactly as terse as written lambda calculus lambdas  :-)
16:15:06 * shapr hopes for ghc5.04.1 to be released very soon.
16:15:08 <jlk> where is a site on source for haskell?
16:15:15 <shapr> jlk: haskell.org
16:15:53 <jlk> k
16:16:03 <shapr> jlk: you could look at some of the source I've written: http://purl.org/net/shapr/src/haskell/
16:16:15 <jlk> hey thanks.
16:16:21 <shapr> sure, np
16:17:44 <jlk> which text editors do you guys use?
16:17:51 <toadx> emacs
16:17:55 <shapr> emacs
16:18:27 <dark> nvi
16:18:42 <jlk> code appears clean..
16:19:00 <shapr> jlk: my code is actually pretty ugly, I'm still a n00b
16:19:12 <jlk> shapr, i see you like blue too.
16:19:12 <shapr> Ig and Heffalump write nice code.
16:19:16 <shapr> blue?
16:19:20 <dark> I have a feeling that Haskell is like Go in that respect.  Everyone's a newbie :)
16:19:27 <jlk> for colors.
16:19:27 <shapr> jlk: oh, that's my emacs color-theme
16:19:36 <jlk> gimme.
16:19:38 <Ig> vim!
16:19:39 <jlk> :)
16:19:41 <shapr> jlk: you using emacs?
16:19:52 <jlk> i have it.
16:19:56 <shapr> dark: there are some Haskell non-newbies on this channel.
16:19:58 <dark> Ig: vim is the emacs of vi :)
16:20:00 * Ig hopes shapr didn't think all the code I wrote during the ICFP was good  :-)
16:20:08 <jlk> I've just been busy working. :(
16:20:14 * Ig tries to grok that
16:20:25 <shapr> jlk: I've been busy working also, but I've still learned Haskell!
16:20:43 <shapr> My woman bought me two Haskell books nearly one year ago, and I've been hooked ever since.
16:21:41 <jlk> I got into it 2 years go. But the tutorial left me with little impression. :( 
16:22:01 <jlk> After 1 1/2 of prolog & lisp. I'm ready.
16:22:23 <jlk> it is the ibm tutorial by the way I think.
16:22:25 <shapr> I'd like to learn Mercury, then hopefully I'd be able to transfer that understanding into Prolog.
16:22:38 <shapr> read the YAHT haskell tutorial by Hal Daume III et al
16:22:48 <jlk> Prolog is similar to erlang
16:23:06 <shapr> erlang has some structural differences though...
16:23:17 <shapr> ability to upgrade a running program impresses me.
16:23:20 <jlk> I am. Half way through it since yesterday. quick n easy. 
16:23:22 <shapr> I wish GHC could do that.
16:23:38 <dark> shapr: Heh, GHC seems to have gone entirely in the other direction.
16:23:54 <shapr> very much so.
16:24:05 <jlk> shapr, i'll bet pretty major ones too.
16:24:17 <shapr> I like the recent thread on ghc-users about dynamic linking
16:24:22 <jlk> I gave it a glance.
16:24:55 <jlk> Frankly I don't even know if it has unification.
16:25:24 <shapr> hm, what's the latest version of Popen?
16:25:48 * shapr goes off to get the latest version
16:26:28 <shapr> aha, 1.0
16:27:29 * shapr installs popenhs 1.0
16:28:26 <Ig> Presumably erlang isn't lazy?
16:29:12 <shapr> I don't think it is, but I'm not sure.
16:29:38 <shapr> has anyone here tried to install the latest version of hIDE ?
16:29:47 <shapr> it wants something called "dot" which I've never heard of...
16:29:50 <Ig> Hmmm, upgrading running lazy programs sounds interesting
16:30:01 <jlk> if so.. lisp is kinda like it. However, there are trick you do with lambda functions to avoid function incorporation at runtime.
16:30:03 <Ig> Could be graphviz it wants?
16:31:46 <shapr> Ig: nope, I have that installed.
16:32:50 <Ig> Dunno then
16:35:45 * shapr reinstalls gtk+hs
16:37:09 * shapr tries to get the latest gtk+hs out of gnome.org cvs
16:40:04 <jlk> bye guys.
16:42:44 <shapr> hi Pseudonym
16:42:45 <shapr> what's up?
16:42:56 <Pseudonym> G'day.
16:42:59 <Pseudonym> Not much.
16:45:51 <shapr> stoopid cvs question, I'd like to get gtk+hs from cvs.gnome.org, and not the entire gnome dir, what's wrong with this: "cvs -z3 checkout /gnome/gtk+hs" ?
16:47:14 <Pseudonym> Does it not work?
16:48:06 <Pseudonym> Oh!
16:48:08 <shapr> says it can't find the module
16:48:10 <Pseudonym> It's the leading slash.
16:48:22 <Pseudonym> You should remove that. :-)
16:48:42 <shapr> cvs server: modules file missing directory for module gnome/gtk+hs
16:48:44 <shapr> what's that mean?
16:49:53 <Pseudonym> Hang on.
16:50:09 * shapr reads cvsbook
16:50:53 <Pseudonym> Looks like gtk+hs is a module all by itself.
16:51:04 <Pseudonym> You should just be able to check out gtk+hs
16:51:10 <shapr> oh, cool
16:51:23 <shapr> wow, how did you find that out?
16:51:30 <Pseudonym> http://cvs.gnome.org/bonsai/rview.cgi?cvsroot=/cvs/gnome
16:51:55 <Pseudonym> Damn there's a lot of modules there.
16:52:30 <shapr> more than I've ever seen.
16:52:35 <Pseudonym> Of course that may depend on some other modules.
16:52:40 * Pseudonym nods
16:53:03 <Pseudonym> Either the gnome people are really productive or there's a lot of gnome versions of "hello world" checked in.
16:53:48 * shapr laughs
16:53:48 <Ig> Last time I looked (a while ago admittedly) there were a lot of "hello world" class mail clients
16:53:57 <Pseudonym> That's true.
16:54:19 <Pseudonym> Skud posted something on advogato about this some time ago, back when she was a freshmeat moderator.
16:54:45 <Pseudonym> http://www.advogato.org/article/157.html
16:54:46 <Pseudonym> That's the one.
16:55:22 * shapr installs latest version of c2hs
16:55:37 <shapr> next, I'll install the latest gtk+hs, then finally, the latest hIDE
17:00:12 <shapr> I sincerely hope this dynamic linking thread on ghc-users ends with a hIDE that can be self-scripted with Haskell
17:00:37 * Pseudonym hasn't been following it, unfortunately
17:00:54 <Pseudonym> I have actually half-considered writing an ASN.1 compiler for Haskell.
17:01:01 <shapr> someone already did it
17:01:05 <Jerub> heya shapr.
17:01:08 <Pseudonym> Did they? Where?
17:01:08 <shapr> Dominic Steinitz I think his name is.
17:01:14 <Pseudonym> Oooh.
17:01:16 <shapr> I have a copy of it lying around here somewhere.
17:01:17 <Pseudonym> Must look that up.
17:01:29 <Pseudonym> Does it do XER?
17:01:29 <shapr> it's far more elegant than any ASN.1 compilers I've had to deal with before.
17:01:35 <shapr> I don't think so,
17:01:38 * Pseudonym nods
17:01:42 <shapr> iirc, it only DER and BER
17:01:44 <Pseudonym> I need at least BER and XER.
17:01:56 <shapr> it was specifically written for an LDAP client.
17:02:01 <shapr> XER is pure insanity :-)
17:02:05 <shapr> hi Jerub
17:02:12 <Pseudonym> XER is great, because it's human-hackable.
17:02:23 <shapr> yah, but why use ASN.1 at all?
17:02:36 <Pseudonym> When you're debugging ASN.1 protocols, there's nothing like it.  You make a packet, you compile it to BER and then send that.
17:02:45 <shapr> imho, ASN.1 gets my vote for "technology most likely written by minions of Cthulhu"
17:03:04 * Pseudonym is paid to hack Z39.50 for a living, so has to use ASN.1
17:03:22 <shapr> I spent six months or being paid to write an RFC3161 implementation in Java
17:03:27 <Pseudonym> You've got to remember that ASN.1 is 15 or so years old now.
17:03:31 <shapr> yah, I remember.
17:03:34 <Pseudonym> And that's just the standard.
17:03:41 <shapr> whech standard? :-)
17:03:47 <shapr> asn.1 has so many :-/
17:04:08 <Pseudonym> True. :-)
17:04:20 <Jerub> shapr: Thanks to the mindbending I recieved here, I hope to get a high distinction in my haskell subject :)
17:04:21 <shapr> dominic was recently trying to get SVG working in HaXml, I wonder whether he got that working.
17:04:36 <shapr> Jerub: we bent your mind? :-)
17:05:03 <shapr> Z39.50 is exciting stuff.
17:05:43 <Pseudonym> It is.  The trouble is, the spec is so vague.
17:05:48 <shapr> I was doing PKI, aka X.509 stuff.
17:05:50 <Pseudonym> Just about everything is "the server may do this".
17:05:59 <shapr> my sanity barely survived
17:06:06 <Pseudonym> Oh, fun.
17:06:54 * shapr think of PKCS12 and shudders
17:07:08 <Jerub> shapr: well. yeah. haskell is a mindbending experience.
17:07:16 <Pseudonym> You should look at the OpenPGP spec.  If ASN.1 is written by minions of Cthulu, PGP is written by high priests of the Church of Fortran.
17:07:17 <shapr> Jerub: it's mindstretching.
17:07:34 <Jerub> shapr: and apparently, everyone had trouble with layout errors. I didn't. I think my 'pythonesque' indentation seriously helped.
17:07:39 <Pseudonym> The joke in the working group was that PGP was a format that was Huffman coded by hand.
17:07:47 <shapr> doesn't PGP use PKCS12 keystores with all the other assorted X.509 trimmings?
17:08:01 <Pseudonym> Uhm... not in the open spec.
17:08:06 <shapr> Jerub: yah, python sets you up nicely for significant whitespace.
17:08:12 <Pseudonym> It uses PKCS encoding for the large integers, but that's about it.
17:08:39 <shapr> hm, I'm almost tempted to look at the spec.
17:08:43 <Pseudonym> Ah, but Fortran 77 and Occam were there first.
17:08:50 <Pseudonym> In the significant whitespace department, that is.
17:09:21 <Pseudonym> Occam I think was the first to use indentation to identify block structure.
17:09:21 <shapr> gtk+hs has the option for something called IDoc, any clues what it is?
17:09:41 <Jerub> Pseudonym: fortran 77 had significant whitespace?
17:09:46 <Pseudonym> Yes.
17:09:53 <Jerub> Pseudonym: I remember reading that fortran removed all whitespace before parsing.
17:09:55 <dark> It had significant column numbering, I think that's different :)
17:10:11 <shapr> ColorForth uses color in a similar way that python uses whitespace.. I wonder what else could fit into that list of nifty significance.
17:10:12 <Pseudonym> Well, perhaps "significant layout" is more accurate.
17:10:32 <shapr> RPG did that too, didn't it?
17:10:34 <Pseudonym> Comments start with a C in column 1, code must be indented by some spaces etc.
17:10:35 <Jerub> Pseudonym: the example cited was a for loop, and a variable assignment, that were 1 character different.
17:11:04 <Pseudonym> That sounds about right.
17:11:10 <Pseudonym> Fortran is nontrivial to parse.
17:11:23 <Jerub> because parsing technology was poorly understood at the time.
17:11:28 <Pseudonym> Right.
17:11:49 <shapr> oh, IDoc must be one of the haskell doc programs.
17:12:00 <Pseudonym> Oh, like Haddock?
17:12:50 <dark> On the other hand, parsers had to be much simpler to fit in the available space... so why did they make it hairy to parse?
17:13:12 <Pseudonym> Good question.
17:13:28 <shapr> dark: they might be saying that about Haskell in years to come.
17:13:35 <dark> I guess a lot of hairiness could have come from optimization decisions.
17:13:50 <dark> Where the actual effect on the language was an accidental side effect :)
17:13:56 <Pseudonym> I think it's because languages at the time used implementation-defined semantics.
17:14:15 <Pseudonym> "A Fortran program means whatever this compiler thinks it means."
17:14:29 <dark> Pseudonym: You mean like Perl?
17:14:32 <Pseudonym> These are the days before denotational semantics.
17:14:38 * Pseudonym laughs
17:14:46 <Jerub> dark: exactly.
17:15:10 <Pseudonym> Like Pascal in the early years, too.
17:15:21 <dark> Well Pascal suffered badly from the one-pass straitjacket :)
17:15:39 <Pseudonym> Pascal compilers were damn fast on slow machines, though.
17:15:57 <Pseudonym> Could be worse.  Could have been Algol.
17:16:11 <dark> At my university they perversely implemented a five-pass Pascal compiler.
17:16:12 * Pseudonym shudders at the thought of two-level grammars
17:16:15 <Jerub> dark: I remember having major troubles with the function declaration before use semantic when I was a beginner pascal programmer.
17:16:19 <dark> I always wondered what the point was :)
17:16:28 <Pseudonym> That's not perverse.  There's nothing in Pascal semantics which requires a one-pass compiler.
17:16:28 <dark> Pseudonym: You mean like C? :-)
17:16:41 <Pseudonym> The point is it's _possible_ to implement as a one-pass compiler.
17:16:51 <Pseudonym> Two-level grammars are evil.
17:16:56 <Pseudonym> No, no like C.
17:17:01 <dark> Pseudonym: I call it perverse because Pascal goes through contortions to allow a one-pass compiler, and then they went and used five.
17:17:24 <dark> Pseudonym: Isn't the combination of cpp and C a two-level grammar? 
17:17:24 <Pseudonym> Nah.  If you need the optimisation, you'll use five passes.
17:17:29 <Pseudonym> No.
17:17:36 <Pseudonym> Two-level grammar is more evil than that.
17:17:53 <Pseudonym> Algol's syntax has a grammar.  This grammar is the language generated by another grammar.
17:18:04 <dark> Jerub: I still write my programs that way :)
17:18:06 <Jerub> shapr: have you had a chance to look at the MetaEnvironment?
17:19:44 <dark> Pseudonym: Hmm is this like SGML DTDs?
17:20:08 <shapr> Jerub: nah, haven't gotten it working yet.
17:20:19 <shapr> Jerub: I wish...
17:20:26 <shapr> Jerub: my spare time has been very limited lately.
17:20:29 <Pseudonym> dark: Kind of, only not quite.
17:20:57 <Jerub> shapr: I downloaded and compiled it.
17:21:04 <Jerub> shapr: there are a few more tools I've had a crack at.
17:21:28 <Pseudonym> Imagine a grammar which parses well-formed rules in the Algol grammar.
17:21:41 <Jerub> Lapis is very interesting (google "lapis 0.99") - allows simultaneous editing. (Want to change every instance of a variable to spelt differently in an entire file?)
17:21:46 <Pseudonym> This is not like it parses yacc rules.
17:22:03 <Pseudonym> It parses something BNF rules for Algol only.
17:22:30 <Jerub> shapr: and FermaT (WSL) is really cool. Decompiled some hand written ASM including self modifying code to a human readable spec that wasn't more than 12 lines.
17:23:43 <shapr> Jerub: we need to collect a list of those tools.
17:24:58 <Jerub> shapr: Done and done. I'm doing research on the weekend (case studies on these tools) including useability analysis.
17:25:23 * tmoertel is back from dinner w/ friend
17:26:03 <shapr> Jerub: have you posted your writings online somewhere?
17:26:09 <shapr> I'd be very interested in seeing them.
17:26:13 <Jerub> shapr: I will be.
17:26:36 <Jerub> shapr: I'm not doing every tool I've researched/discovered, just a few. But I plan to collate some of this material.
17:26:52 <Jerub> I might do a project on it later this semester.
17:32:32 * tmoertel finishes reading the log of what I missed
17:32:53 <tmoertel> btw, dot is part of graphviz (referring way up in the log)
17:32:56 <shapr> oh
17:32:58 <shapr> hmm
17:33:43 <tmoertel> it's a preprocessor for generating directed graphs
17:34:35 <shapr> spiffy
17:35:43 <Jerub> tmoertel: yeah, I found that by reading the sourcetree.
17:36:16 <Jerub> oh, we're talking about MetaEnvironment right?
17:36:24 <shapr> I'm talking about hIDE
17:36:30 <shapr> Ig: ok, you were right :-)
17:36:41 <shapr> hm, looks like hIDE needs haddock-0.4 also
17:36:45 <tmoertel> Are any of the Strafunski folks working on hIDE?
17:37:06 <shapr> nah, hIDE is just one guy.
17:37:23 <shapr> I think MetaEnvironment has some connections with Strafunski
17:37:43 <Jerub> because graphviz is a build depend of MetaEnvironment
17:38:10 <shapr> aha, this is idoc: http://www.cse.unsw.edu.au/~chak/haskell/idoc/
17:38:15 <shapr> also by chakravarty
17:38:34 <shapr> he writes a lot of useful code.
17:39:34 <shapr> hi Xoltar, looking for Haskell info?
17:39:46 <Chilli> aehhmmm...thanks....
17:40:23 <shapr> Chilli: you're welcome :-)
17:40:44 <Chilli> :-)
17:40:49 <Jerub> Chilli: morning.
17:41:12 <Chilli> Morning Jerub
17:41:16 <shapr> crap, some random error while compiling the latest hIDE
17:41:27 <shapr> bah, I'll just go to sleep.
17:41:29 <shapr> g'night all.
17:41:33 <Pseudonym> Night.
17:41:38 <tmoertel> 'night
17:41:40 <Chilli> g'night shapr
17:42:01 <Chilli> and I haven't even had a coffee yet
17:42:11 <Chilli> but I think, I'll settle for a tea at the moment
17:42:16 * tmoertel just finished a tea, too
17:42:43 <Jerub> I had a coffee for breakfast.
17:42:51 <Jerub> and a hash brown at maccas for an after breakfast snack
17:43:11 <Jerub> (theres a maccas in the bris.qld.au central train station now)
17:43:17 <tmoertel> i'm more of a double ristretto-for-breakfast kind of guy
17:43:40 * Pseudonym usually has a liquid breakfast
17:43:46 <Pseudonym> Long macchiato on the run.
17:44:13 <Jerub> Usually I have porridge or jaffles.
17:47:06 <Xoltar> shapr: Hi there. Just hanging out, really. Anyone here working with HaskellDirect?
17:50:25 <Chilli> tmoertel: I am sure your stomach will love your for that ;-)
17:50:42 <engstad> Who's winning ICFP 2002?
17:50:50 <tmoertel> chili: it sure gets me going
17:51:01 <Heffalump> engstad: no results until the conference
17:51:07 <engstad> *nod*
17:51:28 <Pseudonym> Do we get to know who's eliminated in the early rounds, though?
17:51:33 <Heffalump> nafaik
17:52:34 <engstad> Btw, does anyone know a good introduction to back-tracking monads and/or material dealing with CLP (constraints-based logic programming)?
17:52:43 <Ig> Do we know there will be eliminations in early rounds?
17:52:53 <Pseudonym> The guy who works one cubicle across from me was in a Mercury team some time ago and seems to remember they gave out some information about early eliminations early, but admits his memory might be fuzzy.
17:53:11 <tmoertel> (they did last year)
17:53:27 <Pseudonym> engstad: Yes.
17:53:36 <Pseudonym> Both, in fact.
17:54:12 <Pseudonym> http://www.informatik.uni-bonn.de/~ralf/publications.html#P12
17:54:27 <Pseudonym> There's an implementation in http://sourceforge.net/projects/hfl
17:54:51 <Xoltar> G'night, all
17:54:56 <Pseudonym> Night.
17:55:03 <Pseudonym> As for CLP...
17:55:08 <Pseudonym> What do you want to know?
17:55:44 <Pseudonym> There's a book by Peter Stuckety and Kim Marriott called "Programming with Constraints: an introduction" which isn't bad.
17:55:49 <engstad> Pseu: Implementations in Haskell?
17:56:13 <Pseudonym> You mean implementations of CLP?
17:56:26 <Pseudonym> Uhm... not that I know of.
17:56:50 <Pseudonym> The Ralf Hinze paper is about nondeterminism monad transformers, and the HFL thing has my implementation.
17:57:05 <Pseudonym> I've also got a half-tested version with soft cut.
17:57:25 <Jerub> Chilli: Do you know of the SVRC?
17:57:46 <Chilli> Jerub: at least, I don't know what to make of the acronym...
17:58:33 <Jerub> Chilli: "Software Verification and Research Centre" - bunch of academics at UQ who do alot of functional programming.
17:58:42 <Pseudonym> engstad: Can I ask what you want it for?
17:59:12 <Chilli> Jerub: ah, ok, I know that there is such a Centre, but I actually didn't know they were doing FP
17:59:42 <Chilli> What are they doing?
17:59:46 <Chilli> FPish, I mean
17:59:50 <Jerub> Chilli: I saw some research that came out of it recently.
18:00:03 <Chilli> about?
18:00:10 <Jerub> for "Total Functional Programming" I think it was, a language they're using/developing.
18:00:22 <Chilli> sounds scary ;-)
18:00:26 <Jerub> making types/variables/data defined in terms of functional programming.
18:00:32 <Pseudonym> Sounds like the Borg.
18:00:41 <Chilli> Pseudonym: :-)
18:00:55 <Jerub> i.e. 2 defined in terms of 1+1.
18:00:56 <Pseudonym> All Your Functional Programs Are Belong To Us.
18:00:58 <Jerub> that kind of thing.
18:01:08 <Chilli> Jerub: sounds like higher-order logic and curry howard stuff to me
18:01:21 <Pseudonym> Surely 2 would be 1+1+0.
18:01:28 <Pseudonym> Or succ(succ(0)).
18:01:34 <Pseudonym> We don't need no steenking 1's.
18:02:05 <Jerub> I can't remember the syntax.
18:02:29 <Chilli> Jerub: are they using this to specify programs and then possibley derive implementations?
18:02:58 <Jerub> Chilli: I'm not sure. just the concept of programming without data is scaring me.
18:03:17 <Pseudonym> Well that's part of the thing of lambda calculus.
18:03:19 <engstad> Pseud: Sorry, boss at my desk for a while.
18:03:24 <Pseudonym> You don't need data structures, really.
18:03:34 <Pseudonym> engstad: fair enough
18:04:09 <engstad> Pseud: Well, I've made a lot of progress using Facile (CLP) w/ Ocaml, I was just wondering if there was Haskell equivalents.
18:05:01 <Pseudonym> I don't think anyone has written any general-purpose constraint solving engines for Haskell.  At least not yet.
18:05:20 <Pseudonym> I was thinking about this a couple of weeks ago, actually.  Does the O'Caml thingy use destructive update?
18:05:41 <engstad> Destructive update? Meaning mutable structures?
18:05:45 <Pseudonym> Yes.
18:05:49 <engstad> Yes.
18:05:53 <Pseudonym> In particular, mutable constraint variables.
18:06:01 <Chilli> Some of the Melbourne guys have tied some sort of solvers up to Haskell using the FFI
18:06:05 <engstad> Hmm..
18:06:12 * Pseudonym can believe that
18:06:13 <engstad> Hold on, let me check Facile.
18:06:18 <Chilli> I think, it was a solver for Boolean constraints
18:06:18 <Pseudonym> I bet I know who was responsible too.
18:06:28 <engstad> I would think you would need to be able to back-track.
18:06:40 <Pseudonym> Not just that, I think you need trailing.
18:06:58 <Chilli> and I think, it was Glenn...hmm. can't remeber his surname right now
18:07:01 <engstad> Trailing?
18:07:02 <Pseudonym> Backtracking is easy.  check out the Ralf Hinze paper.
18:07:06 <Chilli> maybe together with Martin Sulzmann
18:07:13 <Pseudonym> Yes.  Stuff which can be undone on backtrack.
18:07:48 <Chilli> they have used the thing to do various forms of type checking and program analysis
18:08:13 <Pseudonym> The "trail" is a stack in the Warren Abstract Machine (Prolog abstract machine, it vaguely resembles the G machine) which undoes variable bindings on backtracking.
18:08:28 <engstad> Ah, ok, I see.
18:10:14 <Pseudonym> OK, I was wrong.  I thought it was Harald Sondergaard.
18:10:15 <engstad> Yes, it's using a trail (Facile).
18:10:21 <Pseudonym> Right.
18:10:44 <Chilli> they are in Harald's group
18:10:47 <engstad> type stack = Level of lev | Empty | Trail of (unit->unit) * stack
18:11:04 <Pseudonym> Yup.
18:12:38 <Pseudonym> In Haskell, it'd be more something like this:
18:12:39 <engstad> I guess I should be reading more about the inner workings of CLP. Perhaps implement it in Haskell?
18:13:00 <Pseudonym> class MonadTrail m where
18:13:18 <Pseudonym> Hang on, I need an editor.
18:13:23 <engstad> ;-)
18:13:36 <Pseudonym> Ah, I won't bother.  What you need is a trailing monad transformer, where the trail operations work on the inner monad.
18:14:06 <Pseudonym> You should check out that book by Peter Stuckey and Kim Marriott.
18:14:26 <Pseudonym> http://www.cs.mu.oz.au/~pjs/book/book.html
18:16:07 <engstad> Thanks. Allthough I have quite some experience _using_ CLP by now, but _implementing_ a CLP system is a whole different thing.
18:16:56 <Pseudonym> pjs used a draft of the book for the honours subject I did with him.  It does go into the implementation.
18:17:28 <engstad> Oh, okey. :-) Then perhaps I should order it...
18:17:38 <Pseudonym> Or see if your local university library has a copy.
18:17:53 <engstad> Hmm... library?? 
18:18:05 <engstad> I haven't set my foot in a library in this country. :-)
18:18:10 <Pseudonym> If you want to do constraint solving over floats, by the way, you should also brush up on the simplex algorithm.
18:18:36 <engstad> Heh, remember that from my Control Theory classes in school...
18:19:04 <Pseudonym> Well that's basically the algorithm you use to test the satisfiability of inequality constraints.
18:19:10 <engstad> But no, my domain variables are integer intervals.
18:19:15 <Pseudonym> Oh, OK.
18:19:27 <Pseudonym> Unfortunately you're now in the land of NP hardness.
18:19:45 <Pseudonym> The simplex method is polynomial, integers are not.
18:20:01 <engstad> I know. But as I once read, even though it is NP hard,it doesn't mean that it is _impossible_ to solve. Think of a chess program.
18:20:10 <Pseudonym> The guy at Melbourne who was working on integers was Warwick Harvey.
18:20:36 <Pseudonym> Let me see...
18:20:44 <Pseudonym> http://www-icparc.doc.ic.ac.uk/~wh/publications/
18:20:54 <engstad> Cewl.. I'll check it out.
18:21:03 <Pseudonym> Look at his PhD thesis.
18:21:06 <Pseudonym> If it's there.
18:21:12 <Pseudonym> You might want to mail him and ask for a copy.
18:21:34 <engstad> "Integer constraint solving methods."
18:21:35 <engstad> ?
18:21:40 <Pseudonym> Yup.
18:21:58 <Pseudonym> A title more useful to your problem you're unlikely to find, methinks.
18:22:02 <engstad> Gzipped postscript. (Also through citeseer.)
18:22:23 <engstad> Citeseer is _soo_ useful.
18:22:29 <Pseudonym> Indeed it is.
18:22:52 <engstad> I must have been reading > 100 articles the last month... :-)
18:23:15 <Pseudonym> Unfortunately, there are too many interesting things in the world.
18:23:58 <engstad> True..
19:07:11 <Chilli> jens: are you around?
19:08:34 <jens> Chilli: i am actually :)
19:08:46 <jens> what's up?
19:09:30 <Chilli> in the c2hs contributors list, do you want to have your ne.jp or your redhat email adress?
19:09:50 <Chilli> (I am about to make a binary release)
19:10:56 <jens> actually i'd prefer petersen@haskell.org i think (forwarding address)
19:11:04 <Chilli> ok, fine
19:11:15 <jens> also i'm changing isp this month so...
19:11:21 <jens> thanks for asking :-)
19:11:30 <Jerub> I'm going to have to change isps. :(
19:11:37 <Chilli> thanks for contributing :-)
19:11:47 <jens> cheers
19:21:30 <Pseudonym> In or out, dude, make up your mind.
19:21:45 <Igloo> Sorry  :-/
19:21:51 <Pseudonym> :-)
19:21:53 <shreya> Haha
19:22:03 * Igloo is fighting many battles and mostly losing
20:11:10 * tmoertel finished colorization and bot stats to VisualizeGameASCII
20:11:21 <tmoertel> s/finished/finished adding/
20:37:48 <tmoertel> the icfp contest game server code has been released: http://icfpcontest.cse.ogi.edu/simulator/
21:32:50 <Chilli> cool!
22:03:59 <jens> and the code looks quite nice too :-)
23:44:51 * tmoertel heads off to bed
23:44:54 <tmoertel> g'night, all!
23:47:36 <Pseudonym> Night.
