00:26:09 <inkedmn> jig b!
00:26:11 <inkedmn> ;)
00:44:43 * seth is away: Don't say anything until I get back.
05:53:38 <jak_home> #join #kde-users
05:54:11 <Heffalump> nah, you don't want to do that
05:54:31 <jak_home> lol
05:54:40 <jak_home> that's better
05:54:56 <jak_home> trying out dark's latest
05:55:12 <jak_home> /tmp/ghc1955.hc: In function `SDLziBareziCDROM_zzdwccall9_fast2':
05:55:12 <jak_home> /tmp/ghc1955.hc:1659: warning: assignment discards qualifiers from pointer target ty
05:55:19 <jak_home> naughty
05:55:21 <jak_home> :)
15:17:01 <clausen> is there a good data structure that supports insertions and deletions effiently?
15:17:05 <clausen> (O(log n) ?)
15:17:09 <clausen> (worst case time?)
15:17:26 <whee> FiniteMap? heh
15:17:39 <clausen> does it support insertions and deletions?
15:17:59 <whee> it's basically a hash
15:18:06 <ibid> no
15:18:09 <ibid> it's a tree
15:18:50 <ibid> hashes are fairly hard to implement efficiently in a purely functional context
15:18:51 <whee> implementation might be a tree, but the interface is hashlike
15:19:05 <clausen> it looks array-like
15:19:05 <ibid> hash is an implementation, not an interface
15:19:07 <dark> I think you mean dictionarylike, you perlvert :)
15:19:10 <clausen> random access lists
15:19:18 <whee> dark: haven't used perl in five years :)
15:19:23 <ibid> do not spread the confusion
15:19:53 <clausen> skew binary random access lists seem to be good for random update
15:19:56 <clausen> but they don't support insertions
15:19:57 <ibid> clausen: FiniteMap is fairly similar to C++'s std::map, if you happen to know it
15:20:15 <ibid> clausen: it uses some balanced tree algorithm, generally
15:20:18 <clausen> yep
15:20:29 <clausen> like I said, it doesn't seem to support insertions/deletions
15:20:33 <ibid> it does
15:20:41 <ibid> addToFM
15:20:56 <ibid> there is also a deletion function, i forget which
15:21:03 <clausen> http://www.xoltar.org/languages/haskell/skinning_haskell.html
15:21:12 <clausen> grrr, it doesn't show a constructor!
15:21:16 * clausen looks for source
15:21:17 <dark> I don't think there are any performance guarantees about it, though.
15:21:29 <ibid> of course, it's a persistent data structure, so you'll get to keep the old version as well as the new
15:21:51 <ibid> dark: trees are generally O(log n) operating on a single element, but the laziness may skew it, yeah
15:21:57 <ibid> clausen: why do you need a constructor?
15:22:09 <ibid> clausen: use emptyFM or listToFM
15:22:51 <Igloo> Arrays are O(1) if you know what size you want, aren't they?
15:23:00 <ibid> for access
15:23:03 <ibid> not for update
15:23:09 <ibid> at least purely functional arrays
15:23:10 <Igloo> And in fact will be O(1) ammortised regardless i think
15:23:32 <Igloo> I think the implementations "cheat" for the Array module, BICBW
15:23:41 <Igloo> s/module/library/
15:24:07 <ibid> they may use some tree-based implementation that gives you O(log n), but you cannot have persistent arrays with O(1)
15:24:25 <ibid> at least without major hackery
15:25:22 <clausen> I want to see how it "grows"
15:25:22 <clausen> (reading the source for a constructor often helps)
15:25:22 <clausen> anyway, it looks like you're right
15:25:22 <clausen> :)
15:25:22 <Igloo> Huh? You just implement them as something resembling C arrays
15:25:35 <clausen> -- A finite map implementation, derived from the paper:
15:25:35 <clausen> -- 	   /Efficient sets: a balancing act/, S. Adams,
15:25:35 <clausen> -- 	   Journal of functional programming 3(4) Oct 1993, pp553-562
15:25:42 * clausen looks for the paper
15:25:43 <ibid> Igloo: C arrays are not persistent
15:26:15 <Igloo> Oh, I'm assuming you aren't changing one "copy"
15:26:39 <ibid> Igloo: "without major hackery" :-)
15:26:40 <clausen> Igloo: no
15:26:44 <clausen> Igloo: O(log n)
15:26:49 <clausen> Igloo: but you can have random-access lists
15:26:57 <Igloo> clausen: What is?
15:27:03 <ibid> Igloo: for that you'd need some intelligence in the compiler, to detect that you don't use the old copy anymore
15:27:12 <clausen> arrays have to be O(log n) lookup in haskell
15:27:27 <clausen> (I'm agreeing with ibid)
15:28:13 <clausen> (you could effectively have large tuples, if you know the size of them
15:28:20 <clausen> and that's constant time... but that's "cheating", hehe)
15:28:46 <kev> hmmm... how's that translate to ram then? as in, do ram chips have circuitry that does O(log ramsize) operations?
15:28:48 <ibid> clausen: you *can* get constant-time lookup for arrays... but then update is linear :-)
15:29:20 <ibid> kev: the arrays are most likely implemented as balanced trees, for O(log n) access and update
15:29:30 <clausen> ibid: really?!  how?  (got a ref?)
15:29:32 <kev> ibid: I mean as in computer hardware
15:29:39 <ibid> clausen: how to which one?
15:29:50 <clausen> constant-time lookup
15:30:15 <clausen> you could use C arrays, and copy the whole thing
15:30:18 <ibid> clausen: fairly simple: just use C arrays with copy-on-write semantics
15:30:20 <clausen> but you couldn't implement that in haskell
15:30:20 <kev> given that with asm you can do constant time reads and writes
15:30:25 <clausen> ibid: exactly
15:30:29 <ibid> clausen: no, but the compiler can
15:30:42 <clausen> yeah, ok
15:30:44 <ibid> clausen: i thought we were talking about the standard library Array module
15:31:34 * clausen is talking "in theory"
15:32:06 <ibid> clausen: actually, i'd assume some sort of unsafePerformIO with the IO monad might allow you to do that :-) (but uPIO is nonstandard and - well - unsafe)
15:32:36 <clausen> GRRRR
15:32:48 <clausen> the journal of functional programming doesn't have papers online before 97
15:33:14 <ibid> clausen: i'm talking about how one can implement such a beast using whatever means necessary under the hood but still presenting the Array interface
15:33:53 * clausen too
15:33:57 <clausen> I wasn't thinking straight
15:34:15 <ibid> with those constraints, O(1) lookup and O(n) update is quite possible
15:34:35 <ibid> however, O(1) lookup and O(1) update is probably very hard to do right
15:35:02 <ibid> some sort of hash table with history stack might do that trick for most use cases...
15:35:12 <clausen> I think it's impossible
15:35:23 <clausen> I'm not sure how you'd prove it
15:35:23 <kev> hmmm... okay, so I guess that standard ram chips do take O(lg n) operations
15:35:23 * ibid too
15:35:26 <ibid> but never say never :-)
15:35:36 <clausen> well, I'd like to prove it's impossible
15:35:41 <clausen> that way I can say never ;)
15:35:47 <clausen> (this is *really* important to me, hehe)
15:35:55 <clausen> (my honours thesis might depend on it...)
15:35:57 <ibid> assuming your proof is valid, of course :-)
15:36:02 <clausen> :P
15:36:18 <kev> clausen: I dread to think how nasty that proof would be
15:36:20 <ibid> well, you might want to look up the point Okasaki makes in the introduction (or whatever) of his book
15:36:21 <clausen> aren't there already proofs that persistent data structures are slower for some things?
15:36:39 * clausen grabs it
15:36:40 <ibid> about theoretical lower limits for performance of pure functional data structures
15:37:06 * ibid should be writing his master's thesis
15:37:19 <ibid> it's due monday :-)
15:37:26 <clausen> !
15:37:30 <kev> there's certainly loads of stuff on things like the minimum numbers of comparisons needed to sort stuff, but they rely on a lot of assumptions
15:37:54 <ibid> clausen: well, if i want to graduate this year :-)
15:38:03 <ibid> kev: not talking about that
15:39:28 <clausen> I can't find where he mentions it
15:39:35 <clausen> okasaki is a pain to navigate as a reference
15:41:15 <ibid> somewhere in the beginning
15:41:19 <ibid> my copy is at work
15:41:26 <ibid> maybe the preface
15:41:55 <ibid> what's a honours thesis?
15:42:24 <clausen> ah, found it!
15:42:32 <clausen> it's an .au thing
15:42:42 <clausen> it's a small research project
15:42:45 <clausen> (1/2 a year)
15:42:59 <clausen> (well, over a year, but you have some coursework in parrallel)
15:43:25 <clausen> I should be able to remember it, because it's a very funny contexxt
15:43:34 <clausen> the papers he cites are "Bag" and "Pip"
15:43:37 <clausen> (bagpipe :)
15:43:42 <clausen> and he talks about a dancing bear
15:43:44 <clausen> hehe
15:45:27 <ibid> where does it fit in the education chart?
15:46:02 <clausen> in .au, most courses are 3 years
15:46:15 <clausen> the "honours year" is optional
15:46:26 <clausen> (but is usually a prerequisite to post-grad studies)
15:46:33 <clausen> then you can usually do a PhD
15:46:52 <ibid> what's the prerequisite for the 3 years before honours?
15:55:27 <clausen> ibid: high school
15:56:56 <spank> hi
15:58:23 <spank> thanks to everyone who helped me and my friends with our college assignment last week, btw
15:58:42 <clausen> spank: are you goulash?
15:59:03 <spank> heh no
15:59:07 <clausen> (there was a portuguese girl here a few days ago)
15:59:11 <clausen> (her nick was goulash)
15:59:19 <spank> hmm.
15:59:20 <ibid> clausen: okay, honours is then roughly equivalent to our masters (although the "intended" duration for masters is about 5 years)
15:59:26 <clausen> we have masters here too
15:59:35 <clausen> the intended duration is 2 years
15:59:37 <ibid> what's that then
15:59:42 <ibid> 2 years from what basis?
15:59:45 <clausen> you can do honours, then masters if you want
15:59:49 <spank> well i'm portuguese too
15:59:54 <clausen> spank: I noticed ;)
16:00:00 <spank> oh :)
16:00:08 <clausen> she studies in the universidade de évora
16:00:09 <ibid> so your masters is 6 years from high school?
16:00:20 <clausen> ibid: you start masters 4 years from high school
16:00:28 <ibid> ok
16:00:32 <clausen> (or phd 4 years from high school... your choice)
16:00:38 <spank> yeah; i heard they learn haskell there too
16:00:54 <ibid> out here, you almost always get masters before heading for the intended 4 years of phd
16:00:55 <spank> i study at universidade do minho
16:01:14 <clausen> ibid: ouch!
16:01:15 <ibid> so phd is intended to be 9 years since from school
16:01:24 <clausen> ibid: start or finish of phd?
16:01:31 <ibid> 9 years to finish
16:01:42 <clausen> here, it's about 7
16:01:53 <clausen> (3 + 1 + 3)
16:02:15 <ibid> usuall one takes msc (5 years), then licentiate (~1-2 years), then phd (additional 2-3 years)
16:02:18 <clausen> spank: are évora and minho close?
16:02:43 <clausen> so, what do you do straight after school?
16:02:52 <ibid> masters, usually
16:02:57 <clausen> ah
16:02:58 <spank> not by any chance :)
16:03:10 <clausen> spank: huh?  (I speak pt_BR, btw)
16:03:25 <ibid> there is an option for bachelors (3 years) but it is rare to take it
16:03:36 <clausen> ah, everyone takes bachelors here
16:03:42 <clausen> it is the only optoin
16:03:42 <spank> like
16:04:16 <spank> minho is in a corner of portugal and évora is in another :)
16:04:18 <ibid> essentially, if you have completed everything except the master's thesis and the advabnced studies in your major, you can get the bsc
16:04:21 <ibid> i did that
16:04:25 <clausen> spank: aha
16:04:47 * clausen wants to go to portugal... see if it's the same as .br!
16:05:08 <spank> never been to brazil
16:05:50 <ibid> about the only things i know about portuguese is from orson scott card books ;-)
16:05:59 <clausen> ?
16:06:01 <ibid> ie. not a lot
16:06:11 <clausen> orson scott?
16:06:18 <ibid> orson (scott card)
16:06:33 <ibid> orson scott card is a rather well-known sf author
16:06:37 <clausen> ah
16:06:56 <ibid> books include ender's game, speaker for the dead, the alvin maker books etc
16:07:23 <emu> ibid: haha, same here =)
16:07:29 <ibid> http://www.hatrack.com/
16:07:58 <ibid> emu: sftd and xenocide and children of the mind? :-)
16:08:04 <emu> yep
16:08:08 <emu> good books
16:08:09 <ibid> fine books
16:09:13 <ibid> read shadow books yet?
16:09:25 <emu> ender's shadow?
16:09:29 <ibid> yeah
16:09:32 <ibid> and the others
16:09:44 <ibid> the shadow of the hegemon
16:09:48 <ibid> shadow puppets
16:11:19 <emu> just the first one
16:11:23 <emu> a long time ago
16:11:32 <emu> never got around to the newer ones yet
16:13:11 <ibid> the first one is the best of them imho
16:13:16 <ibid> but the others aren't bad
19:58:45 * seth is away: Don't say anything until I get back.
20:08:21 * seth is back (gone 00:09:36)
20:08:47 <whee> is there a way to have haskell expand list comprehensions to 'standard' map/fold/*?
20:11:51 <seth> Not sure I understand the question.
20:12:32 <whee> I'm wondering if haskell does some sort of transformation with list comprehensions so that they're expressed in terms of other things
20:13:02 <seth> If it does internally, it isn't accessible.  I don't know of any language constructs like that.
20:16:18 <whee> rewriting list comprehensions in a language that doesn't support them is turning out to be painful. heh
21:16:09 <whee> is there a lex that goes along with happy?
21:16:38 <seth> No, but a lexer in Haskell is much easier than one in another language.
21:16:47 <seth> Plus there are some good ones you can download.
21:16:56 <seth> But no program as in lex/yacc.
21:22:31 <SyntaxPolice__> whee: actually, there is a lex that goes with happy now :)
21:22:45 <SyntaxPolice__> its called Alex and you can get it from my home page, or in the latest CVS version of Happy
21:23:08 <whee> hooray
21:23:14 <seth> SyntaxPolice__: I didn't know that.  What is the URL of your home page?
21:23:20 <SyntaxPolice__> http://www.syntaxpolice.org/~ijones/alex/
21:23:40 <SyntaxPolice__> we're working on integrating them better, but for now it is quite useful. I use it every day
21:23:41 <seth> Does it support different input states, like flex?
21:23:51 <SyntaxPolice__> unfortunitely, Alex takes a lot of heap space.
21:24:12 <whee> what are the usual methods of lexing?
21:24:27 <SyntaxPolice__> seth: I'm not sure what different input states means.
21:24:49 <SyntaxPolice__> you could probably make something that takes some state into account by using the monadic lexer feature
21:25:15 <seth> SyntaxPolice__: Flex has this feature, that I used in some of my language implementations, that essentially let's you have multiple sets of lexing rules, and specify with a token the "next state".
21:25:20 <SyntaxPolice__> whee: with Alex, you specify a regular expression and a function to call that should probaby return pieces of a tree or tokens or whatever.
21:25:36 <whee> right, but without Alex, what would I use?
21:25:57 <SyntaxPolice__> seth: Not sure tha I understand still.  Got a pointer?
21:25:58 <seth> The happy docs and sample progs have info on that.
21:26:24 <SyntaxPolice__> I have the "Lex and yacc" book by O'Reilly, so I can look that up and try to answer your question on monday.
21:26:40 <seth> SyntaxPolice__: I can email you a .l file that uses the feature.  You won't find it in O'Reilly because it is a flex extension.
21:26:43 <SyntaxPolice__> it has informatoin on different implenentations like lex.
21:26:47 <SyntaxPolice__> flex I mean
21:27:11 <seth> SyntaxPolice__: I don't think they cover this feature, but there may be a newer edition than the one I've seen.
21:27:12 <SyntaxPolice__> seth: could be.  Sure, please send me your file, I would be curious, since I plan to do some hacking on alex within the next few weeks.
21:27:56 <seth> SyntaxPolice__: Great.  Let me know if you can use any help with this; it's an important thing for me.
21:28:26 <SyntaxPolice__> seth: cool.  I'm very interested in this project since I think there is a need for it, and Alex is a nice piece of software thats been sort of neglected.
21:28:41 <SyntaxPolice__> I've been given permission from work to work on it during work too, so thats nice.
21:31:42 <SyntaxPolice__> OK off to bed. talk to you folks later.
21:31:53 <seth> I sent the file.  Look at the lines that start with <DelimOrSep>
21:32:30 <SyntaxPolice__> Cool, thanks.  Good night.
21:33:35 <seth> Night.
23:58:37 <Verbophob> Hello
