00:00:16 <Pseudonym> The "choicepoint" operation is there to fake a "clause".
00:01:27 <ski> mm
00:02:24 <Pseudonym> BTW, softcut is actually not actually the cut operation, but most prologs which support soft cut actually wrap it in an if-then-else.
00:02:32 * Pseudonym saw no reason to be different
00:05:31 <Heffalump> oh, of course (re choicepoint)
00:23:52 <Pseudonym> Gotta go.  Bye
00:43:19 <ski> bye
03:00:18 <shapr> boing
03:05:37 <shapr> this is a really great book: http://www.iro.umontreal.ca/~lapalme/Algorithms-functional.html
03:05:46 <pesco> Hey shapr.
03:05:46 <shapr> I read fifty pages of it last night, I love it.
03:05:50 <shapr> hi pesco, what's up?
03:06:36 <pesco> shapr: I've written my very first raytracer! Don't get excited though, it's nothing fancy, it renders light volumes, not geometry.
03:07:04 <shapr> wow, awesome!
03:07:07 <shapr> I'm excited!
03:07:08 <shapr> :)
03:07:19 <shapr> I've never written any kind of raytracer at all, so I'm very impressed.
03:07:26 <shapr> do you have the source online?
03:07:50 <pesco> Have a look at the screenshot at http://www.haquebright.de/, entitled "Metaball variations"
03:08:16 <pesco> No, I don't but I'll put them up for you if you like.
03:08:39 <shapr> wow, that's cool!
03:09:49 <pesco> I think so, too. BTW, can you precisely explain to me what gamma correction is?
03:09:50 <shapr> btw, you should try color-theme.el from emacswiki.org it's very pleasing on the eyes
03:10:07 <shapr> gamma correction just means changing the brightness depending on your monitor/eyes
03:10:35 <shapr> for example, one of my monitors here requires that I turn the brightness down and contrast up to get  a less fuzzy picture
03:10:40 <pesco> Changing the brightness in which whay?
03:10:51 <shapr> so any programs that I run on that monitor I need to turn gamma correction way way up
03:11:10 <pesco> ah hm.
03:12:10 <shapr> for one of my other monitors, I have to turn gamma correction down on all programs, but brightness up on the monitor
03:12:25 <pesco> Hm.
03:12:35 <shapr> http://emacswiki.org/cgi-bin/wiki.pl?ColorTheme
03:12:49 <shapr> if you don't mind putting up the source to your ray tracer, I would enjoy reading it.
03:13:04 <pesco> OK, I'll do so in a minute.
03:13:23 <shapr> oh, I was reading about accumArray last night, that's awesome!
03:13:39 <shapr> I think I could use that to write a haskell version of the Paul Graham spamstat in no time flat.
03:14:23 <shapr> accumArray is perfect for histograms, and I think it's also good for bayesian statistics
03:14:33 <shapr> and maybe for markov chains, but I don't know enough about them
03:28:33 <shapr> brb
08:57:23 <dark> My curses module now says "This is blue" in blue :-)
08:57:49 * dark gazes contentedly before deciding to rewrite the whole thing.
08:59:16 <Igloo> lol
09:00:26 <Igloo> I was about to point you at what I did with curses, but I seem to have lost it
09:00:41 <dark> Igloo: I think you already sent it to me, I've been using your code as an example :)
09:00:59 <Igloo> Ah, http://urchin.earth.li/~ian/Hetris/
09:01:25 <Igloo> Oh, this is what I was actually using it for, though. Don't know if it'll be useful or not though  :-)
09:11:54 <dark> Igloo: You're doing very different things with it :)  I need multiple text windows, including pop-ups.
09:12:41 <dark> And I want to hide all the curses nastiness in the Curses module, so I only export generic functions such as write_string and touch_window.
09:13:34 <Igloo> Ah, yes, OK. I was just wanting a direct intreface to the simple bits  :-)
09:22:33 <dark> GHC uses this trick to define a hidden global variable:
09:22:33 <dark> theStdGen :: IORef StdGen
09:22:33 <dark> theStdGen  = unsafePerformIO $ do
09:22:33 <dark>    rng <- mkStdRNG 0
09:22:33 <dark>    newIORef rng
09:23:02 <dark> Then it has getStdGen and setStdGen functions (of IO type) that do readIORef and writeIORef on theStdGen.
09:23:35 <dark> I wonder if that approach is safe to use in my own code.  I'm getting tired of passing a cursesRef around everywhere, and in any case I would like to enforce that there's only one of it.
09:25:06 <dark> (Currently I'm "enforcing" this by having init_curses return it)
09:25:18 <Igloo> What's the cursesRef for?
09:26:59 <dark> It contains my visible_window and touched_window lists :)
09:27:40 <Igloo> Ah, right
09:29:16 <dark> I guess I could put the cursesRef inside the Windows type, so that touch_window gets it for free.
09:29:25 <Igloo> Hmmm, I think to be able to handle errors and debugging info nicely I'm going to have to change the type of pretty much every function in my code
09:29:36 <dark> Igloo: Sounds bad :)  Why?
09:29:54 <dark> Igloo: Do you mean changing every return type to (a, [Warning]) ?
09:30:05 <Igloo> Because it's going to want to be passed all teh way back up to the top
09:30:17 <Igloo> Something like that, yeah
09:31:04 <dark> This seems to be a common problem.
09:31:08 <Igloo> Actually, most of the places were I can get an error have return types IO a anyway, so maybe it won't be too bad
09:31:52 <dark> I did that for my ELF parser, and one effect (due to lazy evaluation) was that printing out the warnings would make it go "look for trouble", and warn about parts of the code that it never needed to parse.
09:32:28 <dark> Igloo: If you have an IO return type, then you could use exceptions.
09:33:19 <Igloo> Hmmm, exceptions might be sensible for the errors. The debugging info might be better of being hPutStrLns though
09:33:29 <dark> Yeah.
09:33:53 <dark> So far I'm ignoring nearly all errors with the Curses module.  I just don't know how to report an error if the display isn't working :-)
09:35:18 <Igloo> I wasn't sure if you were meant to call endwin if initscr failed or not  :-)
09:35:42 <dark> Heh... it's worse than that.  initscr doesn't fail, it aborts the program.
09:35:42 <Igloo> And if endwin fails then it all gets very confusing
09:36:18 <dark> If you want to avoid that, you'll have to use newterm.
09:36:57 <Igloo> Oh, so it does
09:37:07 <Igloo> I was just going on "Routines that return pointers always return NULL on error"
09:37:37 <dark> Maybe I'll make a throwIfERR operator :)
09:38:01 <dark> Though, in most cases I'd want to continue.  Who really cares if keypad() fails?  Or even delwin().
09:38:38 <dark> Logging warnings would be nice, so that the user knows what's going on if every key makes a beep :)
09:38:49 <Igloo> Hmmm  :-)
09:39:39 <dark> But if init_colors fails I don't want to abort, I should instead make an annotation in my handy cursesRef variable :)
09:41:06 <dark> Hmm that would make color_to_attr an IO type, though.
09:42:11 <Igloo> Hmmm, Haskell '98 exceptions seem to be a bit limited
09:43:02 <Igloo> In fact, I don't seem to be able to transmit information through them
09:43:59 <eivuokko> typeinformation?
09:44:28 <Igloo> Any information
09:44:47 <Igloo> I can make IOErrors only by supplying a String, and can't even get that String out again
09:50:59 <dark> System.IO.Error exports ioeGetErrorString in GHC.  I don't know how standard it is.
09:52:23 <Igloo> Oh, so there is. I hate how all the IO stuff is split between the report and library report
09:52:29 <dark> Yep, it's in haskell98's IO module.
09:53:08 <dark> I find stuff by doing a recursive grep in the library directory of the ghc sources :)  Then I work backwards to figure out if a standard module exports it.
09:53:17 <Igloo> It'd be nicer to be able to have an arbitrary type, though
09:53:22 <Igloo> lol
09:53:58 <dark> Yeah but isn't that a limitation of Haskell itself?  If the exception allows information of an arbitrary type, then it becomes a type constructor rather than a simple type.
09:54:44 <dark> That would make it hard to define catch, which needs to be able to catch any of several exceptions.
09:57:38 <Igloo> Hmmm, true
11:07:13 <jewel> is shapr around?
18:37:14 <dark> I performed a featurectomy on Curses.hsc, it's much nicer now :)
18:47:30 <dark> I realized I was being silly with my full-scale support for arbitrary stacks of windows, since I'm only going to have three or four of them anyway, and I'll never do weird things like drawing on a window that's behind another window.
19:17:34 <dark> Hmm I recently discovered that in a do sequence you can do "x <- return expr" rather than fiddling with let.  I wonder if it gets optimized the same though.
19:20:09 <Heffalump> they're equivalent semantically
19:21:09 <Heffalump> I suspect let is more likely to be efficient, since using <- return would require appropriate inlining of >>= and return to get the same effect
19:23:12 <Pseudonym> It depends on the monad, methinks.
19:23:33 <Pseudonym> Some might do some very nontrivial stuff in the >>= and return, like, say, gathering statistics on the number of >>= operations.
19:23:34 <Heffalump> oh, if your monad doesn't obey the monad laws (specifically "return a >>= k
19:23:34 <Heffalump> =
19:23:39 <Heffalump> k a"
19:23:49 <Heffalump> then they won't even be semantically equivalent
19:23:53 <Pseudonym> Well that's true.
19:24:08 <Heffalump> gathering statistics on >>= is a good case in point :-)
19:24:09 <Pseudonym> I think we can assume that monads obey monad laws, otherwise arbitrary stuff may break.
19:24:38 <Heffalump> not really, since the translation of do syntax is actually defined by the report
19:24:45 <Pseudonym> True.
19:24:56 <Heffalump> and actually most real Haskell monads violate the monad laws (with respect to seq)
19:25:01 <Pseudonym> I'm not sure how x <- return expr is less clear than let x = expr, though.
19:25:09 <Pseudonym> Oh, seq breaks everything.
19:25:14 <Heffalump> I think it is far less clear
19:25:20 <Pseudonym> Well, almost everything.
19:25:26 <Pseudonym> OK, lots of things.
19:25:30 <Heffalump> :-)
19:26:21 * Pseudonym is hacking Edison at the moment
19:26:31 <Heffalump> filling in features?
19:26:33 <Pseudonym> Updating it to use post-98 features like fundeps.
19:26:46 <Heffalump> ah, ok
19:26:56 <Pseudonym> Filling in implementations comes next.
19:26:58 <Heffalump> does it use mptcs then?
19:27:06 <Pseudonym> Yes.  Always did.
19:27:27 <Heffalump> oh, of course, collections
19:27:31 <Heffalump> obvious candidate for fundeps
19:27:44 <Pseudonym> Yup.  Sequences don't work, though.
19:28:01 <Pseudonym> Sequences need to support operations like map and concat.
19:28:34 <Pseudonym> Associations can only support them partially, but that's okay.
19:28:54 <Pseudonym> The nicest optimisations on associations are generally specialising for a specific kind of key.
19:29:02 <Pseudonym> Say, using tries to store strings,
19:29:22 <Pseudonym> Something I plan to implement, incidentally.
19:32:47 <Pseudonym> AFK
19:33:17 <Heffalump> is there a name for the datastructure you get if you extend the concept of a trie from being a tree to a DAG?
19:35:26 <Heffalump> oh well, bedtime, I'll read logs to see if anyone replied :-)
19:37:30 <dark> About the return thing... I find it more readable because let messes up the indentation, and then you get nested do and stuff :)
19:38:58 <Heffalump> are all your variables on the left-hand side of <- one letter?
19:40:23 <dark> No... sometimes they're two :)  Why?
19:41:33 <Heffalump> just wondering how you get consistend indentation for <- anyway
19:42:27 <Heffalump> s/end/ent/
19:43:34 <dark> Well, one version looks like this:
19:43:40 <dark> new_mapview map origin area = do
19:43:40 <dark>   w <- new_window area
19:43:40 <dark>   mv <- return (MapView map w area origin)
19:43:40 <dark>   draw_map mv
19:43:40 <dark>   return mv
19:43:44 <dark> And the other version looks like this:
19:43:50 <dark> new_mapview map origin area = do
19:43:51 <dark>   w <- new_window area
19:43:51 <dark>   let mv = MapView map w area origin 
19:43:51 <dark>     in do
19:43:52 <dark>       draw_map mv
19:43:53 <dark>       return mv
19:43:56 <Heffalump> errm, you don't need in
19:44:02 <Heffalump> let inside a do has special syntax
19:44:07 <Heffalump> new_mapview map origin area = do
19:44:07 <dark> It does?
19:44:12 <Heffalump>    w <- new_window area
19:44:17 <Heffalump>   let mv = MapView map w area origin
19:44:22 <Heffalump>   draw_map mv
19:44:27 <Heffalump>   return mv
19:44:32 <dark> Oh :)
19:44:35 <Heffalump> with correct indentation for the first w <-
19:44:45 <Heffalump> the two are equivalent, of course
19:45:00 <Heffalump> (modulo some applications of the >>= associativity law)
19:46:26 <dark> This wasn't in the Gentle Introduction :)
19:48:51 <dark> Thanks for the tip.
19:48:56 <Heffalump> oh, hangon, they do do slightly different things
19:48:59 <Heffalump> I'd never realised before
19:49:03 <Heffalump> True <- return (x /= y)
19:49:35 <Heffalump> if the pattern match fails, it actually calls the monad operation fail rather than causing an outright error
19:49:44 <dark> Cool :)
19:50:45 <Heffalump> ok, so the two forms of let are equivalent
19:51:17 <dark> Okay, it's bedtime now :)
19:51:28 <Heffalump> but the difference between let pat = expr and pat <- return expr is firstly the behaviour in the case of pattern match failure
19:51:38 <Heffalump> and secondly the polymorphism
19:52:10 <dark> Laziness, too, I guess.
19:52:26 <Heffalump> no
19:52:41 <Heffalump> well, hangon, possibly of the pattern match
19:52:43 * Heffalump thinks about it
19:52:56 <dark> if something then use pat else return ()
19:53:02 <Heffalump> ok, possibly, depending exactly on what >>= does for that monad
19:53:06 <dark> If that's the next statement, does evaluation of expr depend on something?
19:53:40 <Heffalump> hmm, yes, you're right I think
19:53:50 <dark> It might be lazy in both cases though :)
19:53:53 <Heffalump> oh, no, not of expr
19:54:12 <Heffalump> if you just have a variable on the LHS, then in either case expr won't be evaluated
19:54:44 <Heffalump> if you're binding a more complex pattern, then expr will be evaluated to WHNF immediately in both cases, assuming a sane >>= operator (I think)
19:55:24 <dark> Hmm... I don't think it would be, in the let form.
19:55:42 <Heffalump> if the LHS is a variable or a more complex pattern?
19:55:43 <dark> After all, if you write it out with "let ... in" then there isn't any monad operation until the if has resolved to a "return ()"
19:56:41 <dark> I thought let pattern matches weren't resolved until a variable from the pattern was used.  
19:56:52 <Heffalump> oh, you're right
19:57:05 * Heffalump never knew that either
19:57:59 <dark> Tricky laziness :)
19:58:03 * Heffalump decides he doesn't understand the semantics of let ... in
19:59:11 <Heffalump> ah. hm. irrefutable patterns all over the place
20:00:28 <dark> It's even more bedtime now :)  'night.
20:00:32 <Heffalump> night
20:00:49 <Heffalump> hmph. tricky stuff, this Haskell business
20:06:42 * Heffalump decides he understands the semantics but isn't entirely sure of implementation and goes to bed
21:49:59 <dwltr> {^window list}
22:19:16 <xxPete> anyone alive?
22:45:09 <Pseudonym> Might as well ask if you've got a question.
22:48:00 <xxPete> Okies, I'm wondering if anyone has playd with optimal binary search trees
22:48:17 <Pseudonym> Yes...
22:48:47 <xxPete> sweet, I'm trying to write some haskell code to do it, but I'm having trouble understanding the algorithm
22:49:32 <Pseudonym> First off, could you define "optimal"?  You mean as balanced as possible, or is it also subject to things like commonly used items being near the root of the tree?
22:50:00 <xxPete> the function will be passed a list of nodes and the probability the nodes will accessed
22:50:19 <Pseudonym> OK.
22:51:20 <xxPete> I've got an ugly looking Algorithm on paper in front of me that I don't really understand.
22:51:34 <Pseudonym> Which algorithm is it?  Splay insert?
22:51:34 <xxPete> I don't suppose you'd be able to explain in english what I'm going to have to do
22:52:13 <xxPete> Hmm, it odesn't give it a name.  I'll write it as best I can without all the funky wingdings
22:52:41 <xxPete> Cij = mini<=k<=j)  (C{
22:54:10 <Pseudonym> Perhaps it would be easier if you told me the authors of the paper./
22:54:25 <xxPete> Cij = min{<=k<=j}  (C{i,k-1} + C{k+1,j}) + E{l=i,j}(p{l})
22:55:33 <xxPete> the curly braces are for limits
22:55:50 <Pseudonym> OK
22:56:05 <Pseudonym> What's C and E?
22:56:06 <xxPete> I don't know who wrote the algorithm sorry
22:56:21 <Pseudonym> Isn't there a name and/or title on the paper?
22:57:29 <Pseudonym> Can I ask, BTW, what you're planning to use it for?  Or is it just the intellectual exercise?
22:57:29 <Pseudonym> Not many people need optimal bsts.
22:57:29 * Pseudonym is kinda curious
22:57:31 <xxPete> C{ij} is the minimum value of the average search times for a tree made of nodes i to j
22:57:45 <xxPete> I grabbed it from uni lecture notes
22:58:32 <xxPete> I don't need an optimal bst either, I think the lecturer thinks it's a good example of the power of functional programming
22:58:50 <Pseudonym> Oh, OK.
22:59:00 <Pseudonym> What's E?
22:59:06 <xxPete> Oh.. & E is the best I could do for that funky E looking symbol that means the sum of
22:59:24 <Pseudonym> Right.
23:00:08 <Pseudonym> So what you're looking at here is really a dynamic programming problem, but the lecturer probably just wants you to write it in the trivial bottom-up way.
23:00:40 <Pseudonym> So you're given a list of pairs, right?
23:00:44 <xxPete> Umm, I'm not entirely sure what you mean
23:00:48 <xxPete> yes that's right
23:00:59 <Pseudonym> (Ord k) => [(k,Float)] or something
23:01:35 <xxPete> yeah well be callng by obst [i...j] [Pi....Pj]
23:01:53 <Pseudonym> Hrm.  OK.
23:02:20 <Pseudonym> So what you need to do is write a function which is basically:
23:02:26 <xxPete> and just printing the results to the screen
23:02:39 <Pseudonym> c :: Int -> Int -> Float
23:02:56 <Pseudonym> Where c i j in Haskell corresponds to C{i,j} in your notation.
23:03:07 <Pseudonym> You may need to pass c some other information, like the probabilities.
23:03:20 <xxPete> I've been given a function type of
23:03:48 <xxPete> obst :: [Int] -> [Float] -> (BinTree Int, Float)
23:03:54 <Pseudonym> Right.
23:03:57 <Pseudonym> OK.
23:04:21 <xxPete> where data BinTree a = empty | Node a (BinTree a) (BinTree a)
23:04:25 <Pseudonym> The first return value is the tree and the second is C{0,N-1}
23:04:35 <xxPete> deriving Show
23:04:42 <Pseudonym> where N is the length of the lists.
23:05:06 <xxPete> that sound right to me
23:05:11 <Pseudonym> OK.
23:05:31 <xxPete> the float is the Value of Cij I think
23:06:41 <Pseudonym> Well it's C{i,j} where i and j are the minimum and maximum indices of the lists.
23:06:41 <Pseudonym> C{0,N-1} basically.
23:06:41 <Pseudonym> where N is the number of elements.
23:06:41 <Pseudonym> Do you understand the definition of Cij above?
23:08:58 <xxPete> Cij is the minimum average number of accesses to find a given key in the search tree
23:09:12 <xxPete> with keys i to j
23:09:14 <Pseudonym> Yes.  Do you understand that equation you quoted, though?
23:09:30 <xxPete> that's where i run into snags =)
23:09:35 <Pseudonym> Ah.
23:10:05 <Pseudonym> OK.  Let me help you grok the equation.  You should be able to cut the code once you understand it.
23:10:18 <xxPete> hard to implement an algorithm you don't understand =)
23:10:23 <Pseudonym> Understood.
23:11:04 <Pseudonym> Assuming that p(i) is the probability that element i will be accessed...
23:11:26 <Pseudonym> Then C{i,j} is the number of accesses which are required, on average, to access a single element.
23:11:28 <Pseudonym> Right?
23:11:46 <Pseudonym> In an optimal tree, that is.
23:11:49 <xxPete> yep
23:11:53 <Pseudonym> OK.
23:12:06 <Pseudonym> Forget that you saw that equation.  How would you go about determining that number?
23:13:05 <xxPete> add (probability of getting accessed * probes to access) / n
23:13:19 <Pseudonym> For each element, right.
23:13:22 <xxPete> yep
23:13:27 <xxPete> for n elements
23:14:11 <Pseudonym> Right.
23:14:12 <Pseudonym> How would you compute the number of probes to access, though?
23:14:15 <Pseudonym> Let's start with a simpler case, perhaps.
23:14:21 <Pseudonym> What's C{i,i}?
23:14:51 <xxPete> hmm
23:14:56 <xxPete> I'm thinking 1
23:15:05 <xxPete> i'm not sure whether i should factor in prob
23:15:21 <xxPete> could be meant to be just Pi
23:15:23 <Pseudonym> It goes to the question of what C{i,i} actually os.
23:15:38 <Pseudonym> In fact it's p(i) under the definition you were given.
23:16:10 <Pseudonym> It's the probability of accessing the single item times the number of probes you need to access it (namely 1).
23:16:26 <Pseudonym> OK?
23:16:45 <xxPete> yeah, Pi would be 1 as well though??
23:16:51 <xxPete> for a tree with only 1 element?
23:16:57 <Pseudonym> No.  That's not the definition of C.
23:17:44 <Pseudonym> What you're thinking of is C{i,j} divided by the sum of the individual probabilities of every element between i and j.
23:17:52 <xxPete> hmm... C is the smallest of the average number of comparisons.. right?
23:18:05 <Pseudonym> Kind of.  Let's go in baby steps, ok?
23:18:15 <xxPete> kk
23:18:18 <Pseudonym> :-)
23:18:41 <Pseudonym> OK.  Suppose you had a binary tree (Node x l r)
23:19:01 <Pseudonym> Moreover, suppose you knew the value of C{i,j} for both l and r.  Call it c_l and c_r.
23:19:09 <Pseudonym> What's the value of C for the tree?
23:19:38 <xxPete> so the node is x
23:19:42 <xxPete> and it has 2 subtrees
23:19:46 <Pseudonym> x is the item in the node.
23:19:49 <xxPete> or nodes running of it... l & r
23:19:55 <xxPete> right?
23:20:00 <Pseudonym> Right.
23:20:04 <Pseudonym> That's the type you gave me.
23:20:08 <xxPete> kk
23:20:41 <xxPete> I'm struggling here....
23:21:20 <xxPete> c_l + c_r ?
23:21:28 <Pseudonym> That's part of it.
23:21:44 <Pseudonym> c_l is the average number of accesses required to access an element of l.
23:22:04 <xxPete> yep
23:22:41 <Pseudonym> So how many accesses will you need to access an element of l when it's a subnode of (Node x l r)?
23:22:46 <xxPete> (c_l + c_r) /2 + 1?
23:23:09 <Pseudonym> Remember you've added another level to the tree here.
23:23:36 <Pseudonym> The height of (Node x l r) is one more than the larger of the height of l and the height of r.
23:23:38 <xxPete> that's y I figured I should add 1
23:23:49 <Pseudonym> Yes, except it's not quite as simple as that.
23:23:53 <xxPete> for 1 more probe to get to the tree in the first place
23:24:42 <Pseudonym> You need to weight the 1 by the probability that the probed node is in l.
23:25:21 <xxPete> hmm
23:25:23 <Pseudonym> It's c_l plus the sum of the probabilities of the elements in l.
23:25:40 <Pseudonym> Where you're implicitly multiplying the probabilities by 1.
23:25:57 <Pseudonym> Do you see that?
23:26:10 <xxPete> gimmie sec, gotta wrap my head around it
23:26:35 <Pseudonym> OK.  Let's introduce the notation that P{i,j} is the sum of p(i) + ... + p(j).
23:27:09 <xxPete> ok, c-l was worked out in the first place by Sum(prob * access) /n
23:27:15 <Pseudonym> Right.
23:27:30 <Pseudonym> Actually, let'
23:27:30 <xxPete> and will now be Sum(prob * (access +1) / n
23:27:51 <Pseudonym> Let's also introduce the notation A{i} = the distance from the root of the tree to element i.
23:27:51 <xxPete> or Sum((prob * access) + (1 * prob)) / n
23:28:38 <xxPete> or Sum(prob * access) /n + 1* prob
23:28:42 <Pseudonym> Then c_l = sum { A{i} * p{i} }
23:28:52 <xxPete> so for each level you add 1 * prob... is that right?
23:29:01 <Pseudonym> We need to compute sum { (A{i} + 1) * p{i} }
23:29:13 <Pseudonym> That is, c_l + sum { p{i} }
23:29:24 <xxPete> sorry prob in the last will become the sum of all probs
23:29:44 <xxPete> Sum(prob * access) /n + 1 * Sum(prob) is probably more accurate
23:30:15 <Pseudonym> That looks right.
23:30:34 <Pseudonym> The point is that it's c_l plus the sum of the probabilities of everything in l.
23:30:42 <Pseudonym> Do you believe me on that?
23:30:49 <xxPete> yeah, i'm happy with it now
23:30:52 <Pseudonym> OK.
23:30:54 <xxPete> just took me a little while =)
23:31:00 <Pseudonym> Same reasoning for the right subtree.
23:31:05 <xxPete> yep
23:31:17 <Pseudonym> Then you need to factor in the cost of accessing the root.
23:31:23 <Pseudonym> Which is...?
23:31:45 <xxPete> Px
23:31:50 <Pseudonym> Correct.
23:31:57 <Pseudonym> Add them all up and what do you get?
23:32:44 <Pseudonym> Actually it's p{k} where k is the _index_ of the root of the tree.
23:32:47 <xxPete> Px + (c_l + Pl) + (c_r + Pr)
23:32:49 <Pseudonym> In fact x is the _element_ there.
23:33:29 <xxPete> yeah that's what I mean Px being the prob that element x will be accesses
23:33:35 <Pseudonym> Right.
23:33:43 <xxPete> whereas Pl is the probabilty a value in the l subtree will be accessed
23:33:44 <Pseudonym> Well let's call it k for reasons which will become clear soon.
23:33:45 <Pseudonym> You've
23:33:53 <xxPete> and Pr is the prob the r subtree will accesssed
23:34:11 <Pseudonym> You've got c_l + c_r + P{k} + P{i,k-1} + P{k+1,j}
23:34:13 <Pseudonym> Do you agree?
23:34:35 <xxPete> yep
23:34:47 <Pseudonym> OK.  Sum the three probabilities at the end, what do you get?
23:35:42 <xxPete> as in Px + Pl + Pr = 1?
23:35:47 <Pseudonym> No.
23:35:55 <Pseudonym> It's P{i,j}
23:36:10 <Pseudonym> We're working over a subset of the whole dataset.
23:36:10 <xxPete> ah...
23:36:14 <xxPete> yes, gotcha
23:36:17 <Pseudonym> OK.
23:36:59 <xxPete> but it would = 1 once we got to the top of the tree right?
23:37:14 <Pseudonym> That's right.
23:37:20 <Pseudonym> So let's recap.
23:37:24 <xxPete> k, I can follow that
23:37:44 <Pseudonym> C{i,j} is really the "cost" of accessing an element in an optimal subtree consisting of the elements i..j.
23:38:36 <Pseudonym> If k is some index between i and j, then the cost of accessing an element in a subtree with k as the root is C{i,k-1} + C{k+1,j} + P{i,j}
23:38:39 <Pseudonym> Do you agree?
23:39:15 <Pseudonym> C{i,k-1} is the cost of the left subtree and C{k+1,j} is the cost of the right subtree.
23:39:20 <xxPete> hold up
23:41:35 <xxPete> ok that is because P{i,k-1} + P{k} + P{k+1,j} = P{i,j} right
23:41:39 <Pseudonym> Right.
23:41:52 <xxPete> kk then I'm still with you
23:41:58 <Pseudonym> OK.
23:41:58 <xxPete> althought my brain is hurting a little =)_
23:42:06 <Pseudonym> Only one more step.
23:42:11 <Pseudonym> What's C{i,j}, then?
23:42:15 <Pseudonym> Of ot
23:42:37 <Pseudonym> If it's the cost of the "optimal" tree, how would you compute it?
23:43:14 <xxPete> keep moving k
23:43:17 <xxPete> store the value
23:43:22 <xxPete> and then pick the smallest
23:43:31 <xxPete> as in move k from i to j
23:44:10 <xxPete> hmmm
23:44:30 <xxPete> this is looking recursive..... and they specifically said not to use recursion.
23:45:21 <xxPete> I'm going to need to optimise each subtree everytime...
23:47:39 <xxPete> I have to admit that even thoguh I think I understand the algorithm now I'm still not sure how to code it =)
23:48:05 <Pseudonym> No recursion?
23:48:18 <Pseudonym> Damn.  I thought this was about the power of functional programming. :-)
23:48:40 <Pseudonym> BTW, look at the original equation (I fixed the typo):
23:48:46 <Pseudonym> C{i,j} = min{i<=k<=j}  (C{i,k-1} + C{k+1,j}) + sum{l=i,j}(p{l})
23:48:50 <Pseudonym> Understand what it says now?
23:49:32 <xxPete> Yeah, I got it sorted now
23:50:06 <xxPete> Hmm, there's a note here that says using a recursive funtion to compute the values of C wiould take an exponential time
23:50:28 <xxPete> they suggest storing the values of C in a table and reusing them in subsequent computations
23:50:51 <Pseudonym> Right.  So they do want dynamic programming.
23:51:11 <Pseudonym> Or possibly memoisation.
23:51:23 <xxPete> Dynamic Programming would be it
23:51:37 <Pseudonym> Do you know what is meant by "dynamic programming"?
23:51:43 <xxPete> not really
23:52:11 <Pseudonym> Basically it means exactly what they said: Compute a table of the possible C{i,j} values.
23:52:38 <xxPete> I'm thinking along the lines of if(defined(Cij)) return Cij else return calculate(Cij)
23:52:43 <Pseudonym> Start with the base cases C{i,i} and from that compute C{i,i+1} and from that compute C{i,i+2} and so on until you have C{0,N-1}.
23:53:22 <xxPete> ok, that makes sense
23:53:32 <Pseudonym> Implementing the table will be fun.
23:53:48 <xxPete> My brain is going to be hurting a whole lot more before this is all over I'm thinking
23:53:59 <Pseudonym> On the other hand, do consider that you have lazy evaluation.
23:54:13 <Pseudonym> You don't actually have to compute those things in order as such.
23:55:10 <xxPete> I don't?
23:55:54 <Pseudonym> No.
23:56:38 <Pseudonym> I'll give you a hint.
23:56:52 <xxPete> bring it on
23:56:54 <Pseudonym> There's a lot of P{i,j} computation going on.
23:57:29 <Pseudonym> One way to compute that is to compute P{j,0} - P{i-1,0}.
23:57:40 <Pseudonym> Sorry, P{0,j} - P{0,i-1}.
23:57:55 <Pseudonym> Do you see that?
23:58:51 <xxPete> yeah I thikn so
23:59:37 <Pseudonym> OK.  So let's actually make a function p :: Int -> Float where p i = P{0,i}
