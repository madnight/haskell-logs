00:30:15 * hackage vector-sized 1.4.1.0 - Size tagged vectors  https://hackage.haskell.org/package/vector-sized-1.4.1.0 (jophish)
00:35:45 * hackage extensions 0.0.0.0 - Parse Haskell Language Extensions  https://hackage.haskell.org/package/extensions-0.0.0.0 (vrom911)
01:02:17 <tdammers> I like to read "=>" as "we can go to". E.g., "Semigroup a => Monoid (Optional a)" means "we can go from a Semigroup instance for a to a Monoid instance for (Optional a)"
01:02:29 <tdammers> and the instance declaration that follows tells you how to go there
01:02:51 <tdammers> (or maybe "given a Semigroup instance for a, we can gain a Monoid instance for (Optional a)"
01:02:53 <tdammers> )
01:19:15 * hackage ghc-prim 0.6.1 - GHC primitives  https://hackage.haskell.org/package/ghc-prim-0.6.1 (HerbertValerioRiedel)
01:38:12 <peutri> is there a way to bend lazy evaluation to perform a (<) on a stream of converging bounds?
01:38:16 <peutri> more formally:
01:38:40 <peutri> i've got a known rational x, an unknown rational y
01:39:06 <peutri> an infinite stream of [yLo,yHi] converging but probably never reaching y by binary search
01:39:49 <peutri> I'd like a non-wasteful way to compare x to y, producing as much more binary search as is needed to decide whether x < y
01:40:02 <peutri> (repeated for many values of x)
01:40:15 <alp> reminds me a bit of http://math.andrej.com/2008/11/21/a-haskell-monad-for-infinite-search-in-finite-time/
01:40:33 <peutri> i can do it with unsafe refs, i can do it with threaded state
01:41:05 <peutri> i can do it with the infinite bounds list traversing it from the start every time
01:41:26 <peutri> is there a "better" way?
01:42:53 <merijn> "plug it into an SMT solver" :p
01:43:00 <peutri> lol
01:43:56 <merijn> I'm not even really joking, if your problem becomes big enough you're unlikely to outperform some industry SMT solver with fancy algorithmics unless you're willing to invest ages :)
01:44:30 <peutri> it's a simple binary search
01:45:24 <peutri> i "feel" like i can't have O(1) access for free, but O(N) is a bit much, I figure I should at least be able to reach O(logN) via some recursive slowdown kind of approach
01:45:58 <peutri> and O(1) impure made safe by a library when I'll feel dirty enough
01:46:49 <peutri> oh I'm getting inspired
01:47:05 <peutri> (thanks for the link BTW alp, I *am* reading it)
01:51:15 * hackage base 4.13.0.0 - Basic libraries  https://hackage.haskell.org/package/base-4.13.0.0 (HerbertValerioRiedel)
01:53:12 <merijn> \o/
01:53:54 <MarcelineVQ> merijn: ib habbenin
01:55:12 <Uniaika> sclv, bgamari: looks like we may about to do something: https://twitter.com/AskAtlassian/status/1257132745504878592
01:56:47 <sclv> Thanks!
02:01:41 <Uniaika> üëç
02:06:50 <d2ci8xc5> damn quickCheck is so cool
02:09:06 <Uniaika> yes it is
02:09:23 <C87> lyxia I think I've got the interchange (Law 4 for applicative functors) now.  But I'm afraid I'm missing something because the way I've worked it out, "pure x" is not nothing.
02:09:56 <C87> Basically, I think it's saying: "applying a function to a value is the same as applying a pipe-to-value to a function".
02:09:58 <merijn> C87: Define "nothing" :)
02:10:50 <C87> merijn not my words, lyxia's.  That's why I think I'm missing something -- I don't come up with "bottom" or None or unit or any other variety of nothing.
02:12:31 <merijn> C87: That's not what people mean with "nothing". Usually an Applicative describes some type of effect "pure" is the noop effect. So "pure 5 :: IO Int" does no IO. "pure 5 :: Parser Int" parses no text, "pure 5 :: Maybe Int" does no shortcutting, etc.
02:12:35 <C87> Or in other words, the interchange law is saying: "Don't screw with the function input you're given, either".
02:13:37 <merijn> C87: pure/return are "identity effects" for >> and *> so "someAct >> pure 5" does the exact same effects as "someAct", it only effects the "result" value
02:15:53 <maerwald> Uniaika: is it?
02:17:26 <maerwald> Last time I used it, it threw an error on a financial computation that was simulated to have started in 1865. No one could figure out why.
02:18:07 <MarcelineVQ> I'm pretty sure time started in 1970
02:18:12 <maerwald> so we removed the property test =)
02:18:50 <peutri> sounds more like a problem in "no one" than "quickcheck"
02:19:14 * hackage packcheck 0.5.0 - Universal build and CI testing for Haskell packages  https://hackage.haskell.org/package/packcheck-0.5.0 (harendra)
02:19:17 <maerwald> peutri: It is a problem with quickcheck, because you can barely write a sensible generator
02:19:26 <maerwald> Just write unit tests for complicated stuff.
02:19:28 <peutri> that wasn't in the initial story!
02:20:24 <maerwald> you need data with sensible semantics
02:20:28 <maerwald> Not just random data
02:20:30 <peutri> sensible generation *is* where I've had the hardes time with quickcheck, though
02:20:41 <maerwald> Generating *sensible* data is extremely hard
02:20:50 <maerwald> And a waste of time imo (you could invest in unit tests instead)
02:21:01 <d2ci8xc5> quickcheck does edge case generation right? 
02:21:09 <merijn> d2ci8xc5: Depends on the generator
02:21:10 <maerwald> edge cases are not about sensible data
02:21:14 <peutri> not to the point of weird bugs, but at least to the point of striking a good balance between "direct generation" and "fuzzy generation with filtering"
02:21:22 <C87> merijn: Alright, I think I get you.  But why's that important from the perspective of interchange?
02:21:26 <maerwald> Encoding what sensible data is basically requires you to write a DSL first
02:21:30 <d2ci8xc5> I guess it kind of is fuzzing
02:22:18 <maerwald> you want a database with state X, where state X has properties A, B, C. Not a database with randomly filled values of all their types
02:22:32 <merijn> C87: Interchange is a result of "pure" being the "identity effect", it basically says "whether you first do pure and then an effect, or vice versa, the result is the same"
02:23:32 <d2ci8xc5> I just lvoe that this language expresses intent through its parameters
02:23:34 <d2ci8xc5> *love
02:24:14 * hackage roundtrip 0.2.0.6 - Bidirectional (de-)serialization  https://hackage.haskell.org/package/roundtrip-0.2.0.6 (StefanWehr)
02:24:23 <maerwald> d2ci8xc5: what do you mean?
02:25:18 <maerwald> readFile :: String -> IO ()
02:25:28 <maerwald> I have a thousand questions about this function
02:27:19 <maerwald> (it could read a filename from a hardcoded directory and then send it to stdeerr, or...)
02:27:21 <C87> merijn: the way I see it, though, the pure _doesn't_ happen first anyway.  It's locked within the function.  So the effect always happens first, then the "pure" (i.e. affect-nothing) occurs.
02:27:54 <merijn> C87: Wait, why is "pure" locked in the function?
02:28:25 <merijn> That's not how <*> works or is defined
02:28:38 <C87> Because (\f -> f x) requires the f for the application to take place, surely?
02:29:07 <merijn> C87: So? That lambda is *inside* the applicative, why would pure factor in there?
02:29:40 <merijn> :t (<*>)
02:29:42 <lambdabot> Applicative f => f (a -> b) -> f a -> f b
02:30:15 * hackage hw-json-standard-cursor 0.2.3.1 - Memory efficient JSON parser  https://hackage.haskell.org/package/hw-json-standard-cursor-0.2.3.1 (haskellworks)
02:30:32 <merijn> the effects of 'f' happen left-to-right, completely independently of what happens with functions/values inside
02:30:33 <C87> Phrased that poorly - we can do it outside of applicative or monads entirely as far as the control flow goes, I think.  Correct me if not.  But wouldn't "myFunction value = (\f -> f value) myFunction" be precisely equivalent?
02:31:06 <C87> as far as control flow goes?
02:31:35 <merijn> C87: "pure ($y)" give you an 'Applicative f => f ((a -> b) -> b)', you need to evaluate f's effects (i.e. the effect of pure) before you can access the function
02:32:09 <merijn> C87: But then you're leaving out pure entirely
02:32:30 <merijn> C87: The interchange law is talking about the effects (or lack thereof) of pure, not about whatever function you use
02:35:29 <C87> merijn: ($ y) gives you a ((a -> b) -> b); but that final "b" can never be obtained without supplying the (a -> b) ... isn't it so?
02:36:20 <merijn> C87: yeah, but that say nothing about whether pure does or does not have an effect on interchange
02:37:14 <merijn> Imagine if someone implemented 'pure' for IO as 'pure x = fmap (const x) (putStrLn "Hello!")'
02:37:15 <C87> merijn: Yes - but stay with me, maybe I'll get it ... now, the only way to obtain the final "b" is through application of the "a -> b", right?
02:37:22 <merijn> Yes
02:38:25 <merijn> That imaginary 'pure' for IO isn't lawful, as interchange doesn't hold (because it'll print "Hello" before or after the effects of 'u', depending on which version you use
02:41:24 <C87> So interchange is basically about pure, and not much to do with <*> at all, then?
02:41:39 <merijn> C87: It's the interaction of pure and <*>
02:42:16 <C87> In the law, the "f (a -> b)" is the "u" value on the RHS in "pure ($ x) <*> u".
02:43:41 <C87> What aspect of the interaction is it about?  'cos ... right now it looks like it's about left-to-right application, which would be weird...
02:44:26 <merijn> C87: The fact that pure doesn't affect the resulting effects, regardless of which side of <*> it is on
02:46:54 <C87> Then, as someone who implements <*>, are there no additional semantics that need to be adhered to?
02:47:28 <merijn> The other laws
02:47:46 <C87> Well, I suppose there would be one.  You can't mess with your "f a".  Wouldn't that break interchange, if you did that?
02:55:58 <C87> merijn: thanks for the perspective & a "Haskelly" way of thinking about it, it's much to think about, greatly appreciated ^_^
02:57:02 <polyphem> Hi , when i try to compile this (https://pastebin.com/JJf0NhLB) code i get this error (https://pastebin.com/GhLiUM51) . Cone someone please explain why ?
02:57:18 <polyphem> s/Cone/Can/
02:58:23 <merijn> polyphem: You're do block isn't indented enough
02:58:40 <merijn> polyphem: It needs to be indented further than the 'g' from go'
02:58:44 <polyphem> merijn: wait i try
02:59:08 <merijn> polyphem: See also: https://en.wikibooks.org/wiki/Haskell/Indentation
03:01:19 <polyphem> merijn: Thanks .... it worked
03:01:33 <polyphem> silly me :)
03:07:53 <d2ci8xc5> https://pastebin.com/ttZjA4DS
03:08:26 <d2ci8xc5> I have the following instance of Arbitrary, how are the types being inferred at a <- arbitrary and b <- arbitrary
03:08:49 <d2ci8xc5> also what's the best way to share a block of code here generally?
03:12:56 <merijn> d2ci8xc5: They're inferred from Two
03:14:33 <d2ci8xc5> so it will see my return to be Gen (Two a b) and it will specify the arbitrary values from that?
03:14:48 <merijn> d2ci8xc5: That one is general still
03:15:34 <d2ci8xc5> so it will see my return to be Gen (Two Int Int) and it will specify the arbitrary values to a :: Gen int and b :: Gen Int from (return Two Int Int)
03:15:38 <merijn> d2ci8xc5: But if you use that generator somewhere to generate, for instance "Two Char Int", then it will propagate those types to the use of arbitrary in the instance of Two to generate a Char and Int
03:15:41 <d2ci8xc5> for example &
03:15:46 <merijn> d2ci8xc5: Yeah
03:20:15 * hackage releaser 0.3.0.0 - Automation of Haskell package release process  https://hackage.haskell.org/package/releaser-0.3.0.0 (domenkozar)
03:52:15 * hackage zsdd 0.2.0.0 - Zero-Suppressed and Reduced Decision Diagrams  https://hackage.haskell.org/package/zsdd-0.2.0.0 (eddiejones)
04:16:44 * hackage record-dot-preprocessor 0.2.4 - Preprocessor to allow record.field syntax  https://hackage.haskell.org/package/record-dot-preprocessor-0.2.4 (NeilMitchell)
04:39:48 <syd> Hi peeps. What would you use to write a docs site?
04:43:43 <tdammers> syd: more context needed. what kind of docs are we talking? what's the process/workflow for getting these "docs" into the site? who is the target audience for writing those docs? who will read this? where do you want to host it? etc. etc.
04:43:51 <syd> tdammers:
04:44:01 <syd> I'm looking at smos.cs-syd.eu and I want to add more docs
04:44:08 <syd> but I'm running into limitations of what I have made already
04:44:15 <syd> no multiple levels of things in the menu for example
04:44:23 <syd> I need it to look half-decent
04:44:34 <syd> I'm very technical and the docs are rather technical
04:44:39 <syd> I'll host it myself
04:45:00 <syd> Right now I'm thinking of just building a little yesod site
04:45:03 <tdammers> mmh, OK
04:45:33 <tdammers> I'd roll with some kind of static site generator, probably roll-your-own based on a bunch of off-the-shelf libraries to address individual concerns
04:45:46 <tdammers> yesod is *way* overkill for this
04:46:20 <tdammers> hakyll is probably fine, so I'm wondering what exact limitations you've been running into
04:46:27 <syd> I agree with you. I'm currently using hakyll
04:46:50 <syd> I was thinking of using yesod so that when smos runs in the browser, I can serve it up via the same site in the docs to let people try.
04:47:04 <syd> the problems I'm running into is that the menu bar only has one level of nesting
04:47:09 <syd> so I can't have sub-pages
04:47:14 <syd> that's more of a css problem
04:47:20 <syd> so I'm really looking for a nice theme I guess
04:47:46 <tdammers> I'll just shamelessly plug my own thing here: https://sprinkles.tobiasdammers.nl/ - full freedom with HTML structure, CSS, etc. May or may not fit your bill better than hakyll.
04:48:10 <tdammers> Though I don't think Sprinkles will serve the "comes with a nice theme out of the box" requirement
04:48:31 <MarcelineVQ> sphinx is a popular option these days
04:49:03 <tdammers> it is, though it's more geared towards "just the documentation", and it's even more opinionated than hakyll
04:49:25 <MarcelineVQ> it has its limitations, just thought I should mentioning it :>
04:49:40 <tdammers> personally I'm not a huge fan of RST, fwiw, but if you think RST is a honking great idea, then you'll love sphinx
04:49:46 <MarcelineVQ> its search feature in particular does not impress me
04:49:58 <MarcelineVQ> To be very nice about it.
04:50:04 <tdammers> hmhm
04:50:30 <tdammers> then again, I think for a site like this, the best way to do "search" is "generate HTML that is search engine friendly, and let google do its thing"
04:50:39 <MarcelineVQ> ye
04:51:08 <tdammers> I don't think you should go any further than having a properly structured tree-shaped table of contents, proper cross-links, and a comprehensive and accurate alphabetic index
04:51:28 <tdammers> (and also a no-bullshit HTML structure to begin with, with proper use of HTML tags and all that)
04:52:17 <zincy__> Extensible effects instead of monad transformers in production?
04:52:29 <zincy__> Why?
04:52:55 <tdammers> zincy__: haven't seen it in the wild, but I can see the appeal
04:53:13 <timCF> Hello everyone! Need help of lens experts to make chunk of code more pretty. Basically looking for applicative-style lens-based oneliner which will replace this function https://gist.github.com/tim2CF/da8b0d4bed8d18d279c661cbf337dc12
04:53:14 <tdammers> for starters, avoiding the performance issues associated with transformers
04:53:53 <tdammers> I think the main appeal is that with a good extensible-effects implementation, effects compose more like sets and less like lists
04:54:04 <syd> tdammers and MarcelineVQ thanks!
04:54:07 <tdammers> monad transformer stacks are basically type-level lists of effects
04:54:18 <tdammers> but conceptually, you want them to be more like sets
04:54:51 <tdammers> the MonadSomething typeclasses kind of lift most of the ordering constraints, but we're still basically doing a type-level list search
04:55:41 <zincy__> tdammers: So extensible effects are like modifiable monad transformers?
04:55:53 <tdammers> no, I wouldn't say that
04:56:15 <zincy__> Seems like the basic extensible effects systems are just type level lists of effects
04:56:38 <tdammers> consider StateT. it models threading state through computations, passed around as implicit arguments and return values
04:56:58 <tdammers> we can wrap any monad stack in StateT, which is the type-level equivalent of consing a value onto a list
04:57:18 <tdammers> so essentially our stack now says "A State effect, followed by the rest of the stack"
04:57:26 <tdammers> StateT : m, if you will
04:57:59 <zincy__> Yep
04:58:29 <tdammers> we can drill into our stack by peeling off the StateT (e.g. with runStateT or whatever), but the stack is still fundamentally ordered
04:59:03 <tdammers> sometimes that is useful, e.g. when dealing with short-circuiting transformers like ExceptT, where stacking order matters
04:59:25 <tdammers> but more commonly, the notion of "combining" effects is a more orthogonal one, kind of like an "and also" relationship
04:59:32 <ski> `StateT s . ContT o' is not `ContT o . StateT s', doesn't commute
04:59:54 <tdammers> "thread this state through the computation, AND ALSO be able to throw quasi-exceptions"
05:00:36 <zincy__> So are we discussing "effects systems" or "extensible effects systems"
05:00:44 <ski> @unmtl StateT sr (ContT o m) a
05:00:44 <lambdabot> sr -> (a -> sr -> m o) -> m o
05:00:47 <ski> @unmtl ContT o (ReaderT sr m) a
05:00:48 <lambdabot> (a -> sr -> m o) -> sr -> m o
05:00:50 <zincy__> As in does this apply to just the former
05:01:10 <syd> zincy__ stick with IO or ReaderT something IO if you can
05:01:17 <tdammers> well, what makes an effects system extensible? MTL is basically extensible too
05:02:25 <zincy__> Extensible would mean can you change the range of effects at runtime?
05:02:44 <tdammers> not necessarily runtime, but on the consumer side
05:03:03 <zincy__> Ah ok thanks
05:03:32 <zincy__> So basically one reason to get away from transformers is because you don't want ordered effects
05:03:52 <tdammers> yes
05:04:41 <tdammers> one unfortunate aspect of transformers is that while we can use the MonadXXXX typeclasses to dig into the stack without explicit unpacking, this only works for one instance per stack
05:05:12 <tdammers> e.g. ReaderT Int (ReaderT Int IO), we can only use the MonadReader instance for one of those Ints
05:07:25 <merijn> hmm, does anyone have a less awkward way of writing record updates on records returned from a function? Stuff like "(proc exeName myArgs) { .. }" looks kinda awkward, imo
05:07:49 <ski> EvanR,MarcelineVQ : yea, the arrow in `class' declarations always felt a bit strange, to me
05:08:16 <tdammers> also, we can't easily write a function that is polymorphic over an effect, and adds one effect iff it's not already in the stack, that is, we can't have type-level set unions of effects, only type-level list consing.
05:08:35 <ski> tdammers : yea. i think it could be nice if one could label the individual transformer (or whatever) components
05:09:04 <solonarv> merijn: lens :p
05:09:15 <solonarv> or I guess you could just introduce a variable
05:09:18 <ski> (and then tell operations like `ask' the label, with `TypeApplications' maybe)
05:09:26 <merijn> solonarv: How, exactly, does lens make that better?
05:10:05 <tdammers> like, we can't easily say :: Eff m () -> Eff n () -> Eff (m <> n) () -- in the sense of "the resulting action may exhibit the effects of both of its operands"
05:10:23 <tdammers> and that kind of gets in the way of being more specific about the various effects allowed by IO
05:11:25 <MarcelineVQ> ski: yeah, like,   "semigroup implies monoid"   well, not to me, but monoid surely implies semigroup
05:14:07 <MarcelineVQ> Still, as just notation whose only meaning is "has superclass" there's really no problem, it's only when we try to explain it in other ways that trouble starts, seems like
05:20:35 <ski> it seems to me one could attempt to think of it in two ways. (a) as an implication, and then surely the arrow is pointing in the wrong direction. (b) as a presupposition, that `Ord a' isn't meaningful (rather than not holding), in case `Eq a' doesn't hold
05:21:48 <ski> (in the latter case, we could think of it as something like `Ord :: (a :: *) -> Eq a => *')
05:24:15 <MarcelineVQ> "a presupposition" dare we say, a constraint? :>
05:25:55 <ski> (i'm also thinking that, possibly, it would've been better to list the constraints on an `instance' declaration, and the superclasses on a `class' declaration, after, rather than before the main "`class' head". like how in Prolog the predicate which the clause belongs to is mentioned first .. would make it easier to quickly (v)grep for a specific `class' or `instance' declaration, much of the time)
05:26:51 <ski> so, `instance Eq [a] <= Eq a where ...', and so on
05:27:44 <ski> (but then i also prefer that in Prolog (and Erlang,Mercury), the ordering operation `<=' is spelled `=<')
05:32:53 <merijn> mmm, I have a datatype with a Rank2 function field. Now I have a function that matches on this datatype but never uses said field and GHC complaints about undeducible constraint and I don't understand why...
05:33:18 <ski> MarcelineVQ : hm, i suppose so. `Ord :: (a :: *) -> Eq a => *' being equivalent to `forall (a :: *). Ord a :: Eq a => *', and in turn to `forall (a :: *). Eq a => (Ord a :: *)'. similarly for `sort :: forall (a :: *). Ord a => [a] -> [a]',`forall (a :: *). sort @a :: Ord a => [a] -> [a]',`forall (a :: *). Ord a => (sort @a :: [a] -> [a])',`forall (a :: *). Ord a => (xs :: [a]) => (sort @a xs :: [a])'
05:34:00 * ski 's reminded of C-style signatures
05:34:52 <solonarv> merijn: honestly, I don't know whether lens would actually improve that.
05:35:23 <solonarv> you could do: proc exeName myArgs & field1 .~ foo & field2 .~ bar
05:35:42 <solonarv> or: proc exeName myArgs &~ do field1 .= foo; field2 .= bar
05:37:23 <merijn> Alternative challenge, help my figure out a pleasing way to line-wrap this to 80 characters :p https://paste.debian.net/1144894/
05:37:34 <merijn> Maybe I should just use GADTSyntax...
05:37:45 * solonarv polishes their <|>
05:38:08 <solonarv> GADTSyntax actually won't help much here
05:38:22 <merijn> solonarv: Yes it would!
05:38:35 <solonarv> then use it! :p
05:40:14 <merijn> solonarv: I guess it's too late to keep the number of extensions minimal anyway >.>
05:41:16 <merijn> Ah wait, no it wouldn't help that much >.>
05:41:39 <solonarv> I am at once happy and sad to be proven right
05:42:36 <merijn> The parentheses make linewrapping look ugly :\
06:03:53 <lyxia> type synonym it!
06:04:42 <lyxia> so you only have to linebreak it once
06:26:44 * hackage integer-gmp 1.0.3.0 - Integer library based on GMP  https://hackage.haskell.org/package/integer-gmp-1.0.3.0 (HerbertValerioRiedel)
06:40:21 <maralorn> phadej: Is there a reason that you haven‚Äòt released cabal-fmt 0.1.3 to hackage? 0.1.2 is broken in nixpkgs, but I think 0.1.3 should work. Great tool btw.
07:03:44 * hackage spectral-clustering 0.3.2.2 - Library for spectral clustering.  https://hackage.haskell.org/package/spectral-clustering-0.3.2.2 (GregorySchwartz)
07:06:07 <sm[m]> syd: sphinx does provides some solid themes (eg rtd) and a context aware site and/or page contents sidebar on every page, which is not that easy to find elsewhere. It can be used with markdown
07:07:18 <sm[m]> syd: among the Haskell options, also check out shakebook
07:15:45 * hackage intro 0.7.0.0 - Safe and minimal prelude  https://hackage.haskell.org/package/intro-0.7.0.0 (minad)
07:33:44 <hseg> am having trouble profiling my code
07:34:00 <hseg> am using cabal, have "profiling: True" in my cabal.project.local
07:34:46 <hseg> and all combinations i've tried to pass -prof +RTS -p to ghc have given me weird results
07:35:11 <merijn> hseg: You need to pass "+RTS -p" to your executable
07:35:15 <merijn> Not to GHC
07:36:20 <hseg> ok
07:37:02 <hseg> RTS complains the program wasn't compiled with -prof
07:38:13 <hseg> on a hunch, trying cabal build && cabal run component +RTS -p
07:38:53 <hseg> ... and the RTS still complains
07:39:00 <merijn> If you use "cabal run "ou need to do "cabal run component -- +RTS -p" or you will try to profile cabal, not your executable
07:39:53 <hseg> ok, now it works!
07:40:45 <hseg> also, i note you need to use cabal run for this -- cabal test won't pass flags to the program properly
07:44:06 <hseg> (btw, reason i'm profiling is that my test suite is taking more than an hour to run
07:44:08 <hseg> )
07:44:49 <hseg> discards aren't to blame here, no case in the heavier properties is discarded
08:12:41 <tcdal> I have built my library and executable with -fno-prof-auto -fno-prof-cafs. After running the executable, the profiling report is full of information about functions from other libraries. I am not interested in that. How can I switch that off? I would only like to see my own manually-defined cost centers in the profiling report.
08:22:49 <tomjaguarpaw> Why does cabal say "These modules are needed for compilation but not listed in your .cabal file's other-modules:" when those modules *are* in other-modules?
08:23:59 <merijn> tomjaguarpaw: Probably because you have library and executable sources in the same directory?
08:24:08 <tomjaguarpaw> Hmm, what does that mean?
08:24:26 <tomjaguarpaw> Yes, my executable top-level is Main.hs
08:24:34 <tomjaguarpaw> in the same directory as the library files
08:24:37 <tomjaguarpaw> Is that not permitted?
08:24:47 <merijn> tomjaguarpaw: It's problematic
08:25:08 <merijn> tomjaguarpaw: The problem is that GHC tries to find imported modules on the file system *first*, then it looks in the package database
08:25:51 <merijn> tomjaguarpaw: So, if you have them in the same directory, then instead of linking against your library (as intended) it will re-find and re-compile when you executable imports them (since it can find them!)
08:26:13 <tomjaguarpaw> Ah that's interesting
08:26:13 <merijn> tomjaguarpaw: And then cabal goes "wait, these modules linked into your executable aren't listed as other-modules of your executable!"
08:26:42 <tomjaguarpaw> How does it deduce that?
08:27:01 <merijn> tomjaguarpaw: For small programs I usually put the library modules in ./src/ and the executable(s) in .
08:27:07 <merijn> That solves the issue with minimal effort
08:27:38 <tomjaguarpaw> Yes, that sounds sensible
08:28:10 <tomjaguarpaw> Is there anything that cabal could do to make the message clearer?
08:29:02 <merijn> I'm not really sure, I suspect it might be harder than it seems :)
08:29:48 <tomjaguarpaw> Likely, otherwise it would probably have already been done :)
08:30:11 <tomjaguarpaw> Seems that GHC needs an option like "These are all the files from the source tree that I want you to even consider looking at"
08:30:50 <merijn> But even that can be tricky :)
08:30:53 <tomjaguarpaw> By the way, I only see this behaviour in the REPL.  v2-install and v2-build pass off without complaint.
08:30:58 <tomjaguarpaw> Oh, in what way?
08:31:26 <merijn> tomjaguarpaw: Because files might be generated from other files, could be literate Haskell (so .lhs)
08:31:39 <merijn> In general, there is no standardisation of how module names map to files
08:32:06 <tomjaguarpaw> This all sounds like a recipe for a confusing error message in May 2020
08:32:27 <phadej> Cabal knows quite well, but not well enough (dirty trackiness, which is broken in corner cases)
08:32:59 <phadej> tomjaguarpaw: GHC uses --make which is idea from '90ies (I guess)
08:33:15 <phadej> ie having build system inside the compiler
08:33:40 <tomjaguarpaw> Do you mean that Cabal invokes GHC with --make?
08:34:24 <phadej> yes, there aren't reasonable alternative
08:36:12 <tomjaguarpaw> Ah I see.  So GHC itself decides how it finds and builds dependencies of whatever its overall build target is.
08:36:47 <phadej> Cabal cannot build dependency graph without compilers's help; ghc -M gives some info, but doesn't tell dynamic dependencies (e.g. TH qAddDeowndentFile)
08:37:42 <tomjaguarpaw> Interesting
08:38:49 <tomjaguarpaw> Doesn't Cabal do out-of-tree builds?  i.e. it copies the source files to a temporary location where the build is run from.  
08:39:02 <tomjaguarpaw> That may explain why I don't get the error for v2-build and v2-install but I do get it for v2-repl
08:39:58 <sm[m]> tcdal: we'd need to see your exact build command
08:41:01 <tcdal> sm[m]: $ cabal new-run --enable-profiling --project-file=cabal.project.dev-sh --RTS -- exe:myexec +RTS -N1 -T -p -RTS
08:41:50 <tcdal> the project contains a library and a (tiny) executable that invokes that library
08:45:31 <siraben> I'm trying to call a Haskell function from C, but I have this build error http://ix.io/2kMp
08:46:04 <siraben> Seems quite tricky, has anyone done this with Nix?
08:47:21 <maralorn> When my primary target is having my libraries work with nixpkgs it‚Äòd be better to not provide upper bounds for my dependencies, right?
08:49:14 <siraben> I'm trying to get it to work on macOS, if anyone has managed to call Haskell from C on mac.
08:52:10 <wavemode> Never done it before, but are you sure "-lHSrts" is the name of the library? Not "-lHSrts-ghc8.8.3"?
08:56:12 <siraben> wavemode:  Ok, so I included the rts folder looks like a bunch of undefined symbols now http://ix.io/2kMv
08:58:28 <pie_> I'm pretty sure theres a standard way to do this but I dont see it, how do I map [] -> Nothing, [ ... ] -> Just [...] ?
08:58:38 <pie_> (xpost from -beginners)
08:59:47 <siraben> Ooh a natural transformation from [] to Maybe
09:00:33 <siraben> @pl foo l [] = case l of { [] -> Nothing ; l -> Just l }
09:00:33 <lambdabot> (line 1, column 11):
09:00:33 <lambdabot> unexpected " "
09:00:33 <lambdabot> expecting operator
09:00:40 <siraben> OOps
09:00:47 <siraben> .pl foo l = case l of { [] -> Nothing ; l -> Just l }
09:01:10 <siraben> @pl foo l = case l of { [] -> Nothing ; l -> Just l }
09:01:10 <lambdabot> (line 1, column 19):
09:01:10 <lambdabot> unexpected '{'
09:01:10 <lambdabot> expecting variable, "(", operator or end of input
09:01:45 <siraben> Wow so it doesn't do that form of case.
09:01:53 <wavemode> siraben: now that just looks like you're missing SDL, libgmp and libffi. If you have them you would link to them via -lSDL -lgmp -lffi
09:02:05 <tcdal> pie_: perhaps you are looking for Data.List.NonEmpty.nonEmpty
09:04:00 <siraben> wavemode:  Thanks! That worked.
09:04:28 <pie_> tcdal: hmm that looks right i think
09:04:35 <pie_> i guess what i want is kind of weird because;
09:04:41 <sm[m]> tcdal: in cabal new-run --help I see a lot of different profiling flags, explained in the manual hopefully. I'm guessing some different configuration of those might fix the issue
09:04:44 <pie_> <pie_> hmm i guess that is kind of weird because you dont get Just [...] from `Nothing : ...`
09:04:44 <pie_> <pie_> or maybe  `... : Nothing`, I always get that backwards
09:04:47 <sm[m]> also I don't know what --RTS is
09:05:57 <pie_> but I'd have to have a NonEmpty for that
09:06:14 <pie_> not that the conversion is hard but i hoped theres a concise way to do this :P
09:07:09 <pie_> doh nevermind it takes a [a]
09:07:53 <tcdal> sm[m]: Thanks. I just had another look, and there's one I didn't try yet. fingers crossed!
09:08:08 <pie_> but it gives me a nonempty so meh
09:09:27 <pie_> it kind of looks like i should be looking in the direction of semigroup
09:10:15 <pie_> or monoid
09:13:53 <pie_> this looks related https://old.reddit.com/r/haskellquestions/comments/6irwz3/just_mempty_nothing/
09:15:25 <tcdal> sm[m]: unfortunately even with --profiling-detail=none, I still get all kinds of profiling information from dependencies, which I don't want to see. so I have no idea how to get only profiling information from the functions I explicitly gave cost centre annotations.
09:17:22 <dsal> The boolean blindness is real.  I had my mqtt client encode the session bit as a boolean in connack flags result.  One value means an existing session was reused and another means a new session was created.
09:19:09 <fog> I have a usecase for MVector, but I'm not sure which PrimMonad I need to work with...
09:19:56 <fog> I dont really understand how it works, but I have a large matrix and I want to change random elements according to some pdf
09:20:22 <fog> and I gather that MVector could be good for this, because of its fast random modification
09:21:03 <fog> normally lists dont need to have a monad to work within specified, how do I know which to use?
09:22:20 <pie_> I think my problem is mempty gives me [] but I dont have something that will give me Just ...
09:22:23 <fog> it gives the options of RealWorld and ST
09:22:42 <fog> pie_ what about return or pure?
09:22:56 <fog> :t pure @Maybe
09:22:57 <lambdabot> error:
09:22:57 <lambdabot>     Pattern syntax in expression context: pure@Maybe
09:22:57 <lambdabot>     Did you mean to enable TypeApplications?
09:23:03 <fog> % :t pure @Maybe
09:23:04 <yahb> fog: a -> Maybe a
09:23:24 <fog> % pure @Maybe 0
09:23:24 <yahb> fog: Just 0
09:24:33 <fog> how does MVector use ST? or Realworld for that matter...
09:25:55 <pie_> fog: that might work
09:26:14 <pie_> i originally tried starting in that direction but too clueless :D
09:28:49 <fog> also, are there type annotated versions for the length?
09:29:19 <fog> so that the unsafe read/write/modify can be used safely... 
09:29:38 <fog> and so a matrix can be constructed without having rows the wrong length
09:30:29 <pie_> you mean having the length in the type like the usual dependent typing examples?
09:30:48 <fog> so i can use type-safety to build up matrices by (++) 
09:30:58 <fog> pie_ yeah
09:31:07 <fog> i would guess its the kind of thing a lib would have
09:31:18 <fog> just a phantom nat
09:31:37 <pie_> well it should be doable in theory since haskell has its type level programming , idk much about liquid haskell
09:32:28 <dsal> fog: when it doubt, ST
09:32:30 <fog> yeah, its just a newtype wrapper with an extra type parameter, but if there is a library it would save defining it
09:32:42 <fog> dsal: do you know why?
09:32:53 <dsal> Do I know why what?
09:32:59 <fog> whats it actually doing with the ST
09:33:17 <dsal> It's basically the same as IO
09:33:50 <fog> i can kind of understand how IO could work, like you could have it write to the disk to some filename, and read it in, or some more sophisticated version of this
09:34:07 <dsal> It doesn't write to disk, it just gives you  mutable memory to work with.
09:34:27 <dsal> ST is sort of like a subset of IO you can use outside of IO.
09:34:32 <fog> yeah, i just mean for the sake of intuition - but i have no idea about ST
09:35:03 <dsal> You can think of ST like a smaller IO.
09:35:13 <fog> is it just like a pure version of IO, so you dont have to use unsafePerformIO to retrive things from the monadic context?
09:35:34 <dsal> Well, *it* uses unsafePerformIO in a, you know, safe way.
09:35:48 <dsal> But you can't retrieve IO things made inside it.
09:35:50 <pie_> uh what. he was talking about sizes for matrices or did i miss something
09:36:10 <fog> pie_, i had 2 questions...
09:36:12 <pie_> ah
09:36:34 <pie_> s/he/they/
09:36:35 <fog> so how do I use it with random numbers that im getting from IO
09:37:01 <pie_> well either you break purity or you put the thing using the numbers into the io context
09:37:41 <sm[m]> tcdal: the profiling flags are complicated and I don't know cabal's well. But you saw that --enable-profiling enables library profiling also ? Did you try --disable-library-profiling ? Is the problem that you want to see your own library, but can't do that without seeing deps as well ?
09:37:42 <fog> yeah, im happy to work within IO, but if I want ST for the vectors, then is there something to do with monad transformer stacks?
09:37:53 <pie_> ST does some type stuff that prevents you from letting impure values escape from the scope, or something like that IIRC
09:37:56 <dsal> The other nice thing to think about when using ST is that wherever you *use* ST, you'll present as a pure function, so it'd work just like any other pure function.
09:38:20 <pie_> dsal: can you actually use st if you do IO
09:38:41 <pie_> or at least your program cant depend on results of that io
09:38:45 <dsal> You can use ST anywhere you'd use a pure function.
09:38:59 <pie_> ewll, nevermind, idk what im talking aout
09:39:01 <dsal> Yeah, just think of it a small, virtualized RealWorld you can spin up wherever.
09:39:13 <dsal> If you're already in IO, you could just do that, but it might get blurry.
09:39:16 <sm[m]> tcdal: I'm just throwing out ideas since you got no other answer.. I'm more familiar with stack
09:39:38 <pie_> we live in someone else's  ST
09:39:48 <fog> id rather only go to IO when i use random numbers
09:40:06 <fog> and actually, they seem slow, so i was going to make a fast pure version just moding against a big prime
09:40:14 <dsal> You don't really go to IO.  You can leave it, but you can't go back in.
09:40:55 <fog> how do mutable vectors work with let bound re-use?
09:41:19 <fog> normally if i do something that takes a few seconds in ghci, and then i do it again it returns instantly the second time
09:41:26 <fog> i guess it kind of keeps it in memory somehow
09:41:29 <dsal> Just give it a go.  It's not that confusing.  You make a mutable vector and then you mutate it.
09:41:45 <syd> tdammers and MarcelineVQ I did end up writing a little yesod server
09:41:53 <syd> I'll show you once I get it deployed. Should only be a few minuts
09:42:02 <sm[m]> tcdal: maybe the answer is to study https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/profiling.html and pass more detailed ghc profiling flags with --ghc-options
09:42:16 <fog> yeah, but if it was full of random numbers that I had already caluclated it, can I reference them again really fast?
09:42:51 <fog> i guess im worried that while the random access is fast, it might be slower than letting ghc memoise them
09:43:11 <dsal> I'm not sure what you mean.  Worrying is less productive than testing, though
09:43:49 <dsal> The few times I've used an mvector, I had regular vector code and did an mvector version and raced them.
09:44:49 <fog> well if i said like, calculate the 100th pseudo-random number, and it had to basically unroll all the intermidiate ones to get to it, if i then asked it to return the n<100th random number, it would have already calculated it, so i would expect it to return instantly 
09:45:15 <fog> but if i put them all into an MVector, then it might take longer to read from that index... idk
09:46:12 <fog> i guess the plus side is the GC isnt going to behave unexpectedly and delete the momoised reference
09:46:56 <fog> fine, better to have repeatability and robustness to scale changes 
09:47:00 <dsal> I'm not very familiar with these magical weak references and auto-memoization.
09:47:29 <fog> yeah, idk either, probably best to leave that to the pros
09:47:32 <dsal> What you're describing doesn't sound like a mutable vector, though.  It sounds more like a vector or something.
09:48:01 <WilliamHamilton> is it possible to have a `DEPRECATED` pragma for a pattern?
09:50:07 <fog> dsal: well they are in a matrix, and they are supposed to be converging to some image. so there is a white noise matrix which gets shuffled and then added to the update - but when its close to convergence, its better to only update few of the values, otherwise the updates get rejected because its unlikely they will only tweak the correct nodes
09:50:41 <WilliamHamilton> it is possible, never mind!
09:50:45 * hackage lzo 0.1.1.2 - minilzo bundled for Haskell  https://hackage.haskell.org/package/lzo-0.1.1.2 (vmchale)
09:51:09 <dsal> fog: do you have it working with a regular vector?
09:51:16 <fog> well, list
09:51:20 <fog> its super slow
09:51:45 <fog> i had it being traversed and just not making an update according to a random threshold, but when only like 1% are being updated its too wasteful
09:52:38 <dsal> Oh, well, try a vector or finger tree or whatever.  If that works and is fast enough, then good.  If it doesn't work, then you can decide whether a mutable vector will help.
09:52:55 <fog> i just need the fast random update
09:53:15 <fog> i heard mutable vectors were good for that as you dont need to freeze and thaw them between each update
09:53:45 <fog> i figure i would have a pdf to select which nodes to update
09:54:25 <dsal> You don't have to freeze and thaw to do an update of most data structures.
09:54:45 <dsal> Pretty much anything you use that isn't list will be a significant improvement.
09:54:47 <fog> i mean, if they are immutable they get coppied each time you update them right?
09:55:14 <dsal> There are a bunch of assumptions in that question.
09:55:29 <fog> well, i guess its MVector vs Vector
09:55:44 <dsal> Right now, it's anything at all vs. List.
09:56:01 <dsal> Seq or Vector or IntMap or something would likely be a huge improvement even if you didn't try the immutable variants.
09:56:21 <fog> surely its possible to conjecture as to which is preferable for this use case? 
09:57:36 <fog> i guess my concern is that since mutable vectors exist, and i have some perhaps unfounded reason for thinking they might be better - that i would like to understand this enough to actually be able to tell
09:58:13 <dsal> So, the right thing to do, and the thing I've done every time I've used a mutable vector, is to implement it with regular vector, mutable vector, IntMap, Seq, etc... and see which one you like the most.
09:58:42 <fog> not to understand how they work?
09:58:49 <fog> :(
09:59:27 <fog> I never heard of Seq, whats that good for?
09:59:29 <dsal> I'm not sure I understand what you mean.  I know the conceptual differences between them.  There are tradeoffs in each case.
10:00:02 <dsal> Seq is a finger tree.  The description at the top tells you where it might be better:  https://hackage.haskell.org/package/containers-0.6.2.1/docs/Data-Sequence.html
10:00:12 <fog> i guess its those tradeoffs which are the main concern
10:00:24 <fog> thanks!
10:01:31 <fog> ah, a partitioning with log access! nice
10:01:34 <dsal> "mutable is the fastest" isn't necessarily a great pick unless it's a super hot part of your program.  mutable vectors aren't exactly super fun to work with and the mutability isn't visible outside of the mutation box.
10:01:51 <fog> at least i can understand how Seq works...
10:02:03 <fog> MVector seems like voodoo atm
10:03:06 <dsal> It's a different way of thinking.  I guess the important thing is that the mutation only occurs inside the designated area.  From the outside, you can't prove there ever was mutable state.
10:03:43 <fog> well the entire program is just updating progressively sparser coverings over the matrix - so leveraging more and more over random access on convergence 
10:03:47 <fog> random update*
10:05:00 <fog> i mean, most of the work is done over the mutable thing, updating like 1% of the values before having to integrate over the whole thing before the next itteration to see if the update is an improvment
10:06:03 <fog> i guess you would need to freeze it before that... its an inner product with the target
10:06:11 <dsal> It would probably help if you didn't think of mutating a data structure as "copying the whole thing."  That's not how efficient data structures are implemented.
10:07:20 <fog> well i suppose i could just gather up all the updates, and traverse over the whole thing delivering them at the right position when its encountered
10:07:47 <fog> i guess im worried i normally do too much traverse, and that the mutable approach could be a better alternative
10:08:19 <dsal> You could've tested the various options in the time you spent worrying about it.  :)
10:09:07 <fog> well, i was only asking to try and understand more about how mutability actually works
10:09:54 <dsal> mutability works by giving you memory you can manipulate in sequence with operations that allow you to read and write to those locations.  It's basically just IORefs.
10:10:24 <fog> right, like C
10:10:29 <dsal> si
10:10:52 <fog> i guess that has some nice interaction with RAM
10:11:07 <dsal> Except in Haskell, you don't generally have to think about sequencing things.  Suddenly having to worry about all that feels weird in Haskell.
10:11:11 <fog> or like, leveraging how the chip is optimised for random access..
10:11:38 <fog> right, the fear is complacency from too much purity
10:11:41 <dsal> You're just speculating on what performance might be, though.  We have pretty good tools for measuring things.
10:11:56 <fog> and for performance critical things, it would be good to understand whats actually happening
10:12:22 <fog> normally i do benchmarks only to discover everything is optimised by the compiler anyway
10:12:32 <fog> and half of the things just get fusioned away
10:12:43 <fog> which is awesome! but not very informative
10:13:03 <dsal> I spent like, two days trying to speed up something in protolens.  It was doing the most dumb, na√Øve thing bytestring had to offer.  I went as far as to replace the part I was looking at with super hand-optimized C code.  It was *slightly* better.  Nothing I did that I thought would be faster measured faster in any meaningful, consistent way.  It just made the code more complicated.
10:13:51 <fog> then i dont really understand the allure of mutability
10:14:25 <dsal> I've also switched things to mutable vectors and had performance improvement.  That was a different problem. heh
10:15:24 <dsal> Specifically, the protolens thing was pretty small.  Switching from the dumb thing to direct memory manipulation didn't change much.  Large numbers of operations moving stuff around can make a difference.
10:15:27 <fog> i guess even for devising informative benchmarks, some level of understanding might be helpful, so that the corner cases can be examined
10:15:44 * hackage persistent-template 2.8.3.0 - Type-safe, non-relational, multi-backend persistence.  https://hackage.haskell.org/package/persistent-template-2.8.3.0 (parsonsmatt)
10:16:07 <fog> so what kind of things were mutable vectors better for?
10:16:24 <dsal> e.g., in last year's AoC, I made had my VM execute code using mutable state in ST.  That was core to lots of problems and had a lot of arbitrary things it had to do involving random reads and writes.  I kept both versions, but it proved to be faster in most cases.
10:17:05 <dsal> There was another one that was similar.  "Here's a bunch of stuff to do all at once to a blob of memory.  Tell me what the final state is."
10:17:32 <dsal> And in both of the above cases, I implemented it without mutable state first and then compared my implementations.
10:20:33 <fog> i guess the Seq approach would become faster as the percentage of values updated in a pass becomes smaller
10:20:51 <fog> otherwise it would be accessing most of the partitions anyway
10:21:59 <fog> wait, are you saying the mutable version was faster in most cases?
10:22:23 <dsal> In my cases where I thought it would help, yeah.
10:22:55 <fog> i guess i was hoping that whenever some random subset was being updated, that the mutability would help
10:23:53 <fog> that by having direct access to the memory locations, it would avoid having to examine the unchanged values, like if they were part of a partition
10:24:09 <fog> im not sure if that would just be to do with the time taken to descend through the partition tree
10:25:05 <fog> i guess a benchmark would help there, but at least, does that hold up to a sanity check? or am i missing something...
10:26:11 <fog> what made you think that mutability would help for some case?
10:26:15 * hackage minilight 0.4.4 - A SDL2-based graphics library, batteries-included.  https://hackage.haskell.org/package/minilight-0.4.4 (myuon)
10:26:44 <dsal> I guess I don't understand why you haven't tried writing it yet.  You've got basically    `update :: [(Index,Int)] -> T Index Int -> T Index Int`  right?  You should be able to roll out five implementations of that quickly.
10:27:09 <fog> well, just because of the search for understanding! 
10:27:12 <dsal> Well, in the VM case, I'd never used ST before and wanted to try it out.  It was a little awkward, but a bit faster.
10:27:38 <dsal> In the other case, it was because I was doing thousands of updates per iteration and taking a while.
10:27:45 * hackage slack-progressbar 0.1.0.0 -   https://hackage.haskell.org/package/slack-progressbar-0.1.0.0 (thomasjm)
10:28:31 <dsal> I guess I understand stuff better by poking it and seeing what it does than talking about it.  You get to see what your code looks like as well as measure performance.  Thinking is helpful, but not as accurate at the edges.
10:28:53 <fog> hmm, that sounds about right, beyond about 256*256, randomly perturbing 1% of the values is way too slow
10:29:27 <dsal> Well, you're using a list.  That's definitely the worst way to do it.
10:29:46 <fog> ok, ill do some benchmarks and return with 
10:30:32 <dsal> The mutable code will be more complicated and ugly.  Sequencing kind of sucks.  It's not just a world before and after a mutation, but also a world before and after a read.  That *feels* heavy.
10:33:11 <jota191> hi guys, compiling a module of my project takes forever with a memory leak included. This happens if I use cabal build, but using cabal repl or loading all modules directly in a ghci session is fast. What could be happening?
10:33:53 <monochrom> cabal repl and ghci use the interpreter, not the compiler. That could be one difference.
10:33:54 <jota191> important, perhaps: my codebase has a lot of type level computations
10:36:25 <jota191> yes, I know, but it seems strange since at term level code is very simple, I mean, most work should be typechecking. Is there any difference between GHCi's typechecker and ghc's typechecker?
10:36:58 <monochrom> should be no difference. but bugs are possible.
10:37:00 <maerwald> weird, that cabal install doesn't disambiguate exe:foo vs lib:foo, given that you cannot install libs without passing --lib
10:39:50 <monochrom> maerwald, I don't know what the cabal devs were thinking (or not thinking, even) when they coded up this, but I would consider this:
10:40:22 <jota191> I'm using ghc-8.6.5, perhaps an update could help
10:41:49 <monochrom> Draw this table: On one axis, there are 4 kinds of users: those who assume that "cabal install foo" installs both lib and exe, those who assume it's just lib, those who assume it's just exe, and those who assume that it's ambiguous
10:42:55 <monochrom> One another axis, possible behaviours of "cabal install foo": it installs the exe, it complains of ambiguity.
10:43:21 <monochrom> In the cells, you mark whether the user is screwed over by the possible behaviour.
10:44:19 <monochrom> Now I don't always mean that you choose the behaviour that minimizes screwing over users.  But I do mean that you must know, for your preferred behaviour, whom you are screwing over.
10:44:55 <EvanR> how about one command to install lib, one command to install exe, another to install both 
10:44:58 <monochrom> And maybe what kind of screwing over, too.
10:46:05 <monochrom> Like, giving me an error message is less screwing over than silently "automatically" doing something other than what I assume.
10:48:48 <monochrom> And of course, another consideration is how important are the 3 kinds of users that are wrong.  (I don't know which 3 kinds, but exactly 1 kind can be correct.)  For example, you can advocate optimizing for well-informed, professional users; or you can take the opposite stance and advocate for optimizing for beginners and never-reads-docs "professionals".
10:50:13 <maerwald> monochrom: but it already refuses to install lib:foo, so why would you require a user to disambiguate and then tell them "oh, still can't"?
10:50:37 <merijn> "Because no one implemented it yet"?
10:50:53 <monochrom> Wait, it can't?
10:51:30 <monochrom> What is the command that disambiguates and still denied?
10:53:03 <maerwald> oh
10:53:07 <maerwald> I'm actually wrong
10:53:18 <maerwald> when doing lib:foo, it installs the library AND the binaries
10:53:20 <maerwald> ......
10:53:46 <monochrom> That screws over people too :)
10:53:48 <maerwald> I'm giving up on understanding this
10:54:20 <monochrom> Bugs are possible.
10:55:47 <dsal> I find that the more things I actively give up on understanding, the fewer things I have to be wrong about.
10:55:57 <dsal> The less you know.  ‚≠ê
10:56:11 <maerwald> The more you know, the more you know.
10:56:36 <dsal> Knwoledge is power.  Power corrupts.  I remain pure.
10:56:47 * dsal can't even *spell* knwlegie
10:58:45 * hackage list-filter 0.1.1.0 - Special takes and drops on lists  https://hackage.haskell.org/package/list-filter-0.1.1.0 (pgujjula)
11:00:12 <monochrom> I have a very simple model of beginners, and another very simple model of people with strong opinions.  I don't know whether going for very simple models counts as giving up on understanding or hitting upon a great insight.
11:01:28 <monochrom> Very simple model of beginners:  A backend that makes the right guess more than 70% of the time (actually impressive for a beginner), and a not-gate at the front end.
11:01:51 <dsal> haha
11:01:59 <monochrom> Very simple model of people with strong opinions: Random number generator with memoization.
11:03:42 <dsal> Both sound right to me.
11:11:14 * hackage ConcurrentUtils 0.5.0.0 - Concurrent utilities  https://hackage.haskell.org/package/ConcurrentUtils-0.5.0.0 (JamesCandy)
11:21:39 <k0ral> Hello, how does one export fields from a data family instance ? e.g. in `data instance Family = Instance { _field :: Field }`, it seems that using `Family(..)` in the export list does not export `_field`
11:24:06 <dmwit> uh
11:24:07 <dmwit> It does here?
11:25:44 <dmwit> Maybe cook up a minimal reproducible example for us?
11:26:16 <k0ral> okay, that's helpful enough to know that my issue is actually not the export list :)
11:29:15 <k0ral> thing that made me believe they were not exported is that they do not appear in the generated haddock
11:31:44 * hackage haskoin-store 0.23.23 - Storage and index for Bitcoin and Bitcoin Cash  https://hackage.haskell.org/package/haskoin-store-0.23.23 (jprupp)
11:32:57 <k0ral> and, obviously, the `Variable not in scope` thing
11:49:44 * hackage haskell-lsp 0.22.0.0 - Haskell library for the Microsoft Language Server Protocol  https://hackage.haskell.org/package/haskell-lsp-0.22.0.0 (luke_)
11:50:45 * hackage rss-conduit 0.5.1.0 - Streaming parser/renderer for the RSS standard.  https://hackage.haskell.org/package/rss-conduit-0.5.1.0 (koral)
11:53:15 * hackage haskell-lsp-types 0.22.0.0 - Haskell library for the Microsoft Language Server Protocol, data types  https://hackage.haskell.org/package/haskell-lsp-types-0.22.0.0 (luke_)
12:00:44 * hackage lsp-test 0.10.3.0 - Functional test framework for LSP servers.  https://hackage.haskell.org/package/lsp-test-0.10.3.0 (luke_)
12:27:44 * hackage headroom 0.2.2.0 - License Header Manager  https://hackage.haskell.org/package/headroom-0.2.2.0 (xwinus)
12:31:15 * hackage igrf 0.4.0.0 - International Geomagnetic Reference Field  https://hackage.haskell.org/package/igrf-0.4.0.0 (dmcclean)
12:46:17 <mlugg> Hi, I've been reading Matt Parsons' blog post about typed errors, which discusses problems with them and proposes the Ideal Error System (tm) using generic-lens (https://www.parsonsmatt.org/2018/11/03/trouble_with_typed_errors.html). This works, but (imo) looks kind of clunky and messy. He also recently created a library called plucky a bit later,
12:46:18 <mlugg> which allows the "plucking" of individual constraints for error handling, and seems relatively simple (although it cannot be used with mtl's MonadError typeclass due to fundeps). My question is: is this idea widely in use yet? If so, how is it usually done? Are there similar libraries for doing this nicely?
12:46:39 <solarliner> Hi, I'm trying to install and run the Haskell IDE Engine on VS Code, and it keeps trying to install Cabal even though the exact version is already installed, and cabal-helper fails with "function not exported" error https://gist.github.com/SolarLiner/2bdb3081eb3a2876db9c5bdadebebfbe
12:47:10 <solarliner> The thing is, the source code that it's trying to compile doesn't match the source code on GitHub (at the correct tag, here v1.0.0.0)
12:51:13 <merijn> mlugg: tbh, I find MonadError to be a terrible abstraction, personally I just use either IO exceptions which already support having a hierarchy and subclassing. Or I explicitly use ExceptT with a specific error type with an as limited scope as possible
12:52:59 <koz_> merijn: What's your take on MonadThrow?
12:53:07 <koz_> (versus MonadError)
12:53:15 <merijn> koz_: MonadThrow and MonadError are entirely unrelated
12:53:36 <merijn> MonadThrow is effectively a nicer way of writing "liftIO $ throwIO yourError"
12:53:52 <merijn> koz_: It's just a polymorphic API around IO exceptions
12:54:12 <koz_> Ah, I didn't read far enough, lol.
12:54:42 <koz_> You can only catch in a very limited number of settings.
12:54:56 <koz_> (IO, STM and Either SomeException, plus stacks on those)
12:55:23 <merijn> koz_: That's because it kinda mimics MonadUnliftIO in that it just wraps bracket/etc. and lifts the original IO versions into your stack
12:55:44 * hackage git-repair 1.20200504 - repairs a damaged git repository  https://hackage.haskell.org/package/git-repair-1.20200504 (JoeyHess)
12:56:09 <koz_> The advantage appears that _throwing_ code doesn't have to have IO-like stuff in it unless you wanna catch.
12:56:19 <koz_> (well, necessarily)
12:57:01 <koz_> Since like, [] and Maybe have MonadThrow instances.
12:57:58 <koz_> Although looking at the source, you probably don't want to be throwing in those monads.
13:01:26 <Uniaika> (throwing is particularly useful in a DB tranformer because you can abort the transaction)
13:20:44 * hackage password 2.0.0.1 - Hashing and checking of passwords  https://hackage.haskell.org/package/password-2.0.0.1 (nideco)
13:22:15 * hackage password-instances 2.0.0.1 - typeclass instances for password package  https://hackage.haskell.org/package/password-instances-2.0.0.1 (nideco)
13:42:15 * hackage ghc-boot-th 8.8.3 - Shared functionality between GHC and the `template-haskell`library  https://hackage.haskell.org/package/ghc-boot-th-8.8.3 (HerbertValerioRiedel)
13:45:05 <maralorn> If you calculate the hash of a password and compare the hash with a given one via the Eq instance of ByteString, doesn‚Äòt that open you up to timing attacks?
13:51:15 * hackage dobutokO2 0.38.1.0 - Helps to create experimental music from a file (or its part) and a Ukrainian text.  https://hackage.haskell.org/package/dobutokO2-0.38.1.0 (OleksandrZhabenko)
13:51:49 <dmwit> maralorn: Yes. Use a constant-time comparison instead.
13:52:15 * hackage ghci 8.8.1, ghci 8.8.3 (HerbertValerioRiedel): https://qbin.io/why-jj-evt3
13:56:25 <dmwit> The securemem package seems plausible; constTimeEq from crypto-api:Crypto.Util seems plausible.
13:57:40 <dmwit> Or you could make your hash 5 bytes longer and not worry about the timing attack. =P
13:58:45 * hackage ghc-boot 8.8.3 - Shared functionality between GHC and its boot libraries  https://hackage.haskell.org/package/ghc-boot-8.8.3 (HerbertValerioRiedel)
14:26:32 <maralorn> dmwit: Thanks
14:39:15 <koz_> I tried to use QuantifiedConstraints to write 'class (Eq1 f, forall a. KnownNat (Cardinality a) => KnownNat (LiftCardinality f (Cardinality a))) => Finitary1 (f :: Type -> Type) where...'. However, GHC yells at me saying that KnownNat doesn't support user-specified instances. Am I missing something here?
14:41:47 <koz_> Oh wait, this goes on _instances_ right?
14:55:44 * hackage ghc 8.8.3 - The GHC API  https://hackage.haskell.org/package/ghc-8.8.3 (HerbertValerioRiedel)
14:56:44 * hackage ghc 8.8.1 - The GHC API  https://hackage.haskell.org/package/ghc-8.8.1 (HerbertValerioRiedel)
14:58:16 <koz_> Nah, I get this on instances too, sigh.
15:32:57 <mniip> koz_, I think it's trying to say "user specified KnownNat axioms" in general
15:33:00 <mniip> whether local or global
15:34:05 <koz_> mniip: Yeah, I guess it's similar to Coercible in this regard.
15:34:15 <koz_> A bit disappoint, but I can work around this in my specific case.
15:34:16 <mniip> you can do that with coercible
15:34:36 <mniip> check this out https://hackage.haskell.org/package/coercion-extras
15:34:41 <koz_> So if you say something like (forall a b . Coercible a b => Coercible (f a) (f b)) and it works?
15:35:17 <mniip> https://hackage.haskell.org/package/coercion-extras-0.1.0.0/docs/Data-Type-Role-Representational.html#t:Representational
15:37:00 <koz_> mniip: OOh, cool.
15:37:02 <koz_> Noted.
15:55:29 <koz_> Or not, sadface.
15:55:40 <koz_> Is there a safety reason why this kind of QuantifiedConstraints shenanigan is forbidden?
15:55:45 <koz_> Or is it more 'nobody's ever wanted this'?
16:03:34 <koz_> Also mniip, since I have your attention: Data.Finite.Finite's Num instance says 'only the fromInteger function is supposed to be useful'. Does this mean I _shouldn't_ use (+), (*), etc?
16:06:27 <mniip> yup
16:06:43 <mniip> there's a bunch of more specific functions in the module
16:06:56 <mniip> 5/5/2020 [01:55:17] <koz_> Is there a safety reason why this kind of QuantifiedConstraints shenanigan is forbidden?
16:06:57 <mniip> I don't know
16:07:02 <mniip> open an issue perhaps
16:07:11 <koz_> mniip: On what? GHC, base, some other thing?
16:07:16 <mniip> GHC
16:07:24 <koz_> mniip: Sure, will do.
16:07:56 <koz_> mniip: I actually _want_ the modulo behaviour in my particular case.
16:08:15 <koz_> (I'm using an involution on modulo numbers as part of Finitary,w hich currenly munges in and out of Natural)
16:09:06 <mniip> you could use `modulo` just to be sure
16:10:09 <shachaf> Somehow I hadn't seen http://augustss.blogspot.com/2009/02/is-haskell-fast-lets-do-simple.html before.
16:10:26 <shachaf> Finally a clear answer to the question of whether Haskell is fast.
16:11:33 <mniip> press x to doubt
16:17:49 <Tuplanolla> What a delightful turn of events.
16:20:27 <koz_> Also, is there a name for this involution in a modulo-N system? inv(x) = (N - 1) * (x + 1)
16:21:06 <koz_> (my terminology could be off - I mean 'thing where f . f (x) = x holds')
16:25:03 <int-e> Hmm, I'd write that as -1 - x. I guess it's related to the bitwise complement but other than that I don't see why it should have a name.
16:25:31 <koz_> int-e: That's an interesting observation!
16:26:08 <Tuplanolla> Call it a canonical isomorphism out of spite for your audience.
16:26:30 <EvanR> -(x + 1) ? less negativity
16:26:53 <int-e> EvanR: But x |-> a - x is an involution for any fixed a.
16:27:07 <int-e> That's why I wrote it that way.
16:27:23 <koz_> EvanR: Peano Arithmetic - positivity in our maths.
16:27:23 <EvanR> yeah ok
16:29:56 <ski> hm, i suppose one could think of it as mirroring about `a/2'
16:30:44 * ski . o O ( dihedral group )
16:34:45 * hackage genvalidity-criterion 0.2.0.0 - Criterion benchmarks for generators  https://hackage.haskell.org/package/genvalidity-criterion-0.2.0.0 (Norfair)
16:36:41 <dmwit> ski: x |-> -(x + a) is also an involution for any fixed a.
16:37:31 <dmwit> It's just a different spelling of the same collection of maps, really.
16:37:45 <ski> yes
16:39:00 * ski sometimes doesn't know why people say "fixed"
16:39:39 <EvanR> a is a fixed variable, as opposed to a constant variable, or a variable constant
16:40:15 <ski> it's just a matter of how far out the variable is bound, no ?
16:40:54 <EvanR> da/dt = 0
16:41:00 <dmwit> Well, I said it because you said it.
16:41:08 <ski> that's a rebinding construct
16:41:45 <EvanR> fixed could also refer to lifetime or scope
16:42:25 <ski> yea, i was thinking scope, not extent
16:43:37 <ski> (although, i suppose extent could be useful, when thinking about dependent variables ?)
17:05:49 <koz_> I need (Gen a, Gen b) => Gen (Either a b). What's the easiest way to get that using Hedgehog.Gen stuff?
17:08:50 <koz_> s/(Gen a, Gen b) =>/Gen a -> Gen b ->/
17:09:18 <koz_> I went with \g1 g2 -> choice [Left <$> g1, Right <$> g2], but I was wondering if there was something less clunky.
17:09:45 <Rembane> koz_: What are the types of g1 and g2?
17:10:03 <koz_> Gen a and Gen b respectively.
17:11:14 * hackage hanabi-dealer 0.9.1.0 - Hanabi card game  https://hackage.haskell.org/package/hanabi-dealer-0.9.1.0 (SusumuKatayama)
17:11:15 <Rembane> Cool. bisequence looks promising, but I'm tired so I'm not really sure.
17:11:20 <Rembane> :t bisequence 
17:11:21 <koz_> :t bisequence
17:11:21 <lambdabot> error:
17:11:21 <lambdabot>     ‚Ä¢ Variable not in scope: bisequence
17:11:21 <lambdabot>     ‚Ä¢ Perhaps you meant one of these:
17:11:23 <lambdabot> error:
17:11:23 <lambdabot>     ‚Ä¢ Variable not in scope: bisequence
17:11:23 <lambdabot>     ‚Ä¢ Perhaps you meant one of these:
17:11:25 <koz_> % :t bisequence
17:11:26 <yahb> koz_: (Bitraversable t, Applicative f) => t (f a) (f b) -> f (t a b)
17:16:14 <Cale> bisequence would only get you Either (Gen a) (Gen b) -> Gen (Either a b)
17:16:57 <Rembane> Good point.
17:25:37 <dmwit> koz_: What you wrote looks reasonable to me.
17:26:20 <koz_> dmwit: I guess so, yeah.
17:33:00 <p0a> hello
17:33:06 <p0a> I have a bit of trouble understanding indentation in haskell
17:33:25 <p0a> in general if I have a function with long arguments, where is the break? I thought that it'd be f x<break>y where y is indented under x
17:33:39 <p0a> by break I mean newline and whitespaces
17:34:02 <p0a> but my editor indents y under f instead
17:34:08 <Axman6> wherever you want, as long as it compiles
17:34:58 <p0a> well what is usually the case?
17:39:51 <Axman6> I rarely see functions which have enough arguments nto need multiple lines
17:40:20 <p0a> okay
17:40:30 <p0a> its just long expressions, there's ony 2 arguments 
17:41:09 <Axman6> oh you mean as arguments to a function, not defining the arguments of a function. I usually stick them on new lines aligned with the beginning of the first argument
17:41:23 <Axman6> so what you said
17:42:15 <p0a> right, hmmm... I thought that's what my editor was doing _yesterday_. Today is a whole new world howeer
17:42:39 <p0a> so yeah, randomness seems inhertitedly embedded into computers whenever I operate them, no matter what others say
18:03:01 <p0a> I'm a bit confused about an IO operation I'm doing
18:03:13 <p0a> I'm loading a graph from a file in the repl like `let g = loadGraph myFile'
18:03:20 <p0a> then I do `isGoodGraph <$> g'
18:03:34 <p0a> but I notice the answer changes when I change the contents of the file without re-loading the graph using a let. How does that work?
18:03:57 <Axman6> gentauro:  isn't the contents of the file, it is the recipe for loading the file
18:04:05 <monochrom> Because g is the load command, not the graph.
18:04:14 <Axman6> uh, s/ggentauro://
18:04:36 <Axman6> if you had written g <- loadFile myFile then g would be the graph
18:04:58 <monochrom> and then it's "isGoodGraph g", without <$>
18:05:31 <monochrom> All these are why the difference between "IO X" and "X".
18:05:45 <Axman6> in ghci, run: let x = loadFile myfile; g <- loadFile myFile; :t x; :t g
18:06:02 <Axman6> (where ; = newline)
18:06:28 <p0a> so repl allows you to escape IO?
18:06:38 <Axman6> it allows you to execute IO
18:06:44 <p0a> I see
18:06:51 <Axman6> you are inside a do block, you cannot escape
18:07:02 <p0a> namely the repl block
18:07:03 <p0a> right?
18:07:11 <Axman6> I don't know what that means
18:07:15 <p0a> me neither, nevemrind
18:07:31 <oats> it's like being in an IO do-block
18:07:54 <p0a> well actually being able to let g be the load command 
18:07:59 <p0a> is quite convenient since I'm changing the file a lot
18:08:09 <p0a> hand't thought of that
18:08:44 * hackage string-interpolate 0.2.1.0 - Haskell string/text/bytestring interpolation that just works  https://hackage.haskell.org/package/string-interpolate-0.2.1.0 (williamyaoh)
18:09:02 <monochrom> The REPL is a strange thing that optimizes for "get things done" at the expense of a coherent theory.
18:09:36 <oats> hehe
18:10:07 <monochrom> Even the "it's an IO do-block" cannot do justice because other IO do-blocks do not evaluate or print anything just because you write "1+1"
18:10:30 <p0a> yeah I am not expecting homogeneity
18:10:33 <p0a> just features
18:11:03 <Cale> It used to be more comprehensible than it is now though
18:11:29 <p0a> features Cale, they must be implemented at no cost too great :P
18:12:05 <nshepperd> it is what it is
18:12:14 <Cale> It used to be pretty much just enter an expression, if its type is an IO action, it gets executed, if not, print is applied to it to make it one (and possibly a type error ensues if there's no Show instance)
18:12:32 <Cale> if its type indicates that it is an IO action*
18:27:11 <p0a> so I want to do `any odd [2,3,4]'
18:27:29 <p0a> but I want to report the first odd number I encounter
18:27:47 <p0a> what's the customary way to do that, debug messages I mean
18:28:12 <jgt> p0a: I think you want to use Debug.Trace
18:28:26 <p0a> jgt: thank you 
18:29:44 <Axman6> there's also find
18:29:46 <Axman6> :t find
18:29:48 <lambdabot> Foldable t => (a -> Bool) -> t a -> Maybe a
18:32:30 <oats> lol, that seems like a more reasonable solution than trace
18:41:01 <dsal> p0a: if your lines are too long, you could take parts apart and name them in a where clause.
18:42:31 <p0a> find requires that I modify the code however 
18:43:03 <p0a> I think Tracing is a good idea 
18:44:10 <Axman6> if Debug.Trace ever appears in a released app, Phillip Wadler personally travels to your home and hits you over the head with a baseball bat. you've been warned.
18:44:19 <dsal> p0a: the code has to be modifed if you want it to do something different.
18:44:46 <Axman6> p0a: modifying code is literally what we do, all the time, constantly, it is the only constant. it is the job. 
18:45:23 <p0a> well debugging is what I needed however
18:45:44 <p0a> I'm more interested in learning some debugging than modifying the code. I know how to modify code, I don't know how to debug 
18:46:00 <Axman6> debugging is making smaller functions
18:46:54 <p0a> no more wisdom! enough! When Wadler comes I'll distract him by asking for an autograph
18:51:59 <Axman6> some functions have debugging versions of themzelves too, like foldl has scanl
18:52:05 <Axman6> :t scanr
18:52:06 <lambdabot> (a -> b -> b) -> b -> [a] -> [b]
18:52:11 <Axman6> that's also a think
18:54:31 <oats> I would like to be whacked by the honorable philip wadler some day
18:54:35 <oats> it'd be a privilege
18:55:34 <dmwit> I only know one debugging trick. But it's a really, really good trick, and it has gotten me through about 20 years of debugging.
18:55:38 <MarcelineVQ> I'm not scared of Wadler getting in, my door has unbalanced hinges
18:55:55 <oats> dmwit: easy, don't make bugs
18:56:04 <oats> can't have bugs if you never made any
18:56:11 * oats taps temple
18:56:26 <MarcelineVQ> don't write code *taps nose-side*
18:56:27 <dmwit> I've never managed to make that one work, personally. For xactly this reason I hat the "it typechecks, it must work" mantra.
18:56:34 <dmwit> I'm really, really good at writing well-typed bugs.
18:56:54 <dmwit> Hm. My e key seems to be dying. I trust you to insert e's in all the right places for me.
18:58:19 <Axman6> good thing it's not a very useful key
18:58:55 <MarcelineVQ> after all, the most common letters are  r s t l n [
18:59:08 <dmwit> I think that I can do a lot without it. So many words don't contain it.
19:04:06 <slack1256> dmwit: What is your debugging trick?
19:04:15 <slack1256> (please, do not be a pun)
19:04:23 <sm[m]> Axman6: not at all, I ship with loads of Debug.Trace instrumentation, activated by --debug=N, very useful for troubleshooting real world cases 
19:06:29 <sm[m]> my debugging tip: combine Debug.Trace with pretty-show or similar, readable debug output is more effective
19:07:02 <gaze__> what's the right way to concatenate a bunch of files and shove them in a file using Conduit?
19:09:30 <Axman6> foldr ((>>) . sourceFile) (pure ()) [filenames] |.  .| sinkFile dest?
19:09:38 <Axman6> uh, s/|. //
19:13:10 <p0a> sm[m]: as I gather it the debug trick was to make sure the types match :P
19:17:04 <dsal> It is nice doing stuff with conduit where I can stick a debug line in the middle.
19:19:15 <gaze__> how would I display the name of each file as each one was processed?
19:19:26 <gaze__> is there a way to do it without an unsafeperformIO?
19:19:44 * hackage finitary 2.0.0.0 - A better, more type-safe Enum.  https://hackage.haskell.org/package/finitary-2.0.0.0 (koz_ross)
19:20:10 <Axman6> definitely
19:21:03 <Axman6> foldr (\fileName acc -> putStrLn fileName >> sourceFile fileName >> acc) (pure ()) [filenames] .| sinkFile dest
19:21:27 <Axman6> uh there needs to be a liftIO  in there for the putStrLn
19:21:40 <gaze__> ohh you're still in IO... I see.
19:23:32 <dmwit> slack1256: Cut the program in two pieces. Predict what each will do. Then test if it does that. At least one of the pieces will not match your prediction, and now you have only half-as-big a problem to debug.
19:24:22 <dsal> % let any' f = maybe False (const True) . traceShowId . find f  in   any' odd [2, 3, 4] -- p0a, you could squeeze in a debug any impl.
19:24:22 <dmwit> Okay, maybe it is three-quarters-as-big. As long as you keep slicing judiciously, the number of slices needed will still only scale as the log of the size of the codebase, and forall n. log n < 30
19:24:22 <yahb> dsal: Just 3; True
19:26:34 <p0a> dsal: my program logic
19:26:37 <p0a> dsal: has a lot of any's 
19:26:43 <p0a> any's of any's of or's of any's etc
19:27:08 <p0a> so you have to stick it at the right moment. Anyway, it's not the easiest is all I am saying
19:28:44 <p0a> so the fact that debug.trace is not referentially transparent is the feature I want to use I suppose
19:29:28 <dsal> p0a: I named mine any'  
19:29:39 <dsal> But you can do a variant that names stuff.
19:34:28 <p0a> for example changing any to find
19:34:40 <p0a> I encounter this issue, I have two finds; I want to terminate as soon as one has a Just
19:34:52 <p0a> i.e. (find odd list1) || (find odd list2) -- wrong code
19:35:08 <p0a> I could fix it with some if/then/else's, but is there a more elegant solution?
19:35:40 <dsal> <|>  ?
19:35:45 <p0a> nice
19:35:54 <p0a> where is <|> from?
19:35:57 <dsal> > find odd [2, 4, 6] <|> find odd [2, 4, 7]
19:35:59 <lambdabot>  Just 7
19:36:08 <dsal> Applicative / Alternative
19:36:14 <p0a> > find odd [2,3,4] <|> find odd [2,4,7]
19:36:16 <lambdabot>  Just 3
19:36:17 <dsal> @hoogle (<|>)
19:36:17 <lambdabot> Control.Applicative (<|>) :: Alternative f => f a -> f a -> f a
19:36:17 <lambdabot> Text.Parsec (<|>) :: (ParsecT s u m a) -> (ParsecT s u m a) -> (ParsecT s u m a)
19:36:17 <lambdabot> Text.Parsec.Prim (<|>) :: (ParsecT s u m a) -> (ParsecT s u m a) -> (ParsecT s u m a)
19:36:21 <p0a> nice, thank you 
19:36:51 <dsal> > find odd [2, 4, 6, 9] <|> undefined
19:36:53 <lambdabot>  Just 9
19:37:18 <p0a> how do I import <|> from it?
19:37:29 <p0a> import Control.Applicative (<|>) ? or ((<|>)) ?
19:37:42 <dsal> Yeah, more parens.
19:38:55 <dsal> Hmm...  what is asum . map f?
19:39:04 <dsal> > asum . map (find odd) $ [[2, 4, 6], [2, 4, 7]]
19:39:06 <lambdabot>  Just 7
19:47:20 <dmwit> I believe there is no shortcut name for asum . map f, unfortunately.
19:47:34 <dmwit> It is foldMap with an appropriate newtype.
19:47:37 <dsal> Huh.  OK, well, I couldn't find one.
19:47:40 <dmwit> I have wanted a name for it several times in the past.
19:47:41 <dsal> :t \f -> getAlt . foldMap (Alt . find f)
19:47:42 <lambdabot> (Foldable t1, Foldable t2) => (a -> Bool) -> t1 (t2 a) -> Maybe a
19:48:00 <dmwit> Right.
19:48:14 <dsal> Though I'm not sure I understand the difference between First and Alt
19:48:37 <dmwit> p0a: There's also `find odd (list1 ++ list2)`.
19:48:44 <MarcelineVQ> First has the behavior of Alt Maybe iirc
19:49:07 <dmwit> dsal: Alt is more general: it works for any alternative.
19:49:10 <dsal> Oh, hah.  Yeah.  Just concatenate the inputs.  That seems really obvious in retrospect.
19:49:21 <dsal> Yeah, I'm wondering if First just exists so Last can without sticking out.
19:49:30 <dmwit> :t \f -> getAlt . foldMap (Alt . f)
19:49:31 <lambdabot> forall k (t :: * -> *) (f :: k -> *) (a1 :: k) a2. (Foldable t, Monoid (Alt f a1)) => (a2 -> f a1) -> t a2 -> f a1
19:49:34 <dmwit> ugh
19:49:43 <dmwit> :t \f -> getAlt . foldMap (Alt @Type . f)
19:49:45 <lambdabot> error: parse error on input ‚Äò@‚Äô
19:52:48 <MarcelineVQ> dsal: possibly though you can use Dual to get Last
19:53:37 <dmwit> I think for Maybe, I prefer First to Alt in some cases, for the same reason I prefer map to fmap when the type it is operating on is a list.
19:54:01 <dmwit> More-monomorphic things require less thought from the reader.
20:02:58 <dsal> Sometimes a name can make a huge difference, yeah.
20:08:29 <p0a> dmwit: can't do ++ because it's two different tests
20:09:09 <p0a> sth like find odd list1 <|> find even list2
20:09:34 <p0a> but <|> is a nice lifesaver 
20:11:57 <p0a> but now suppose I wanted (find odd l1 <|> find even l2) to evaluate not only to the thing I'm looking for
20:12:20 <p0a> but also a string "odd" or "even", i.e. ===> Just ("odd", 3) or ===> Just ("even", 2)
20:12:55 <p0a> oh I know I'll use (,) <$> "odd" <$> find ...
20:13:24 <p0a> I mean, ((,) "odd") <$> find
20:18:27 <p0a> learning haskell with #haskell feels like im on 200% mode
20:21:56 <p0a> If <|> is like ||, what corresponds to `or' ?
20:22:11 <p0a> I don't want to fold <|> over a list
20:22:21 <p0a> I tried `some' in Control.Applicative but for some reason it just hangs the repl
20:24:13 <c_wraith> :t asum
20:24:15 <lambdabot> (Foldable t, Alternative f) => t (f a) -> f a
20:24:31 <p0a> Where is asum defined?
20:24:45 <c_wraith> Control.Applicative
20:25:00 <c_wraith> or not?
20:25:19 <p0a> there's one in Control.Applicative.Alternative and one in Data.Foldable
20:25:28 <p0a> you mean the former?
20:25:52 <c_wraith> Data.Foldable is the one I really mean, I guess
20:26:19 <p0a> okay, a bit all over the place eh
20:26:27 <p0a> perhaps it makes sense though
20:26:30 <p0a> thanky ou 
20:29:32 <p0a> and asum is like or
20:29:37 <p0a> is there something like `any'
20:29:45 <p0a> I can do asum $ map f list tho
20:34:23 <c_wraith> asum is kind of also like any
20:34:45 <dmwit> No, there is nothing for asum . map f.
20:35:06 <dmwit> ...as I mentioned 50 minutes ago. =P
20:35:06 <p0a> fuck so frustrating
20:35:22 <p0a> had I anticipated that I wanted diagnostic messages this would've been so much easier
20:35:24 <dmwit> bit extreme, but ok
20:35:37 <p0a> I'm frustrated with the fact that I can't fix the code lol
20:36:14 <p0a> there's asum but there isn't anything like `all' for Maybe
20:36:35 <p0a> i.e. maybeAll [Just 1, Just 2] ==> Just [1, 2] but a Nothing in there gives a Nothing as a result
20:36:43 <p0a> Oh I can do traverse with id?
20:37:12 <p0a> yeah
20:38:05 <p0a> what of this `if (all f xs) then Just xs else Nothing'
20:38:10 <p0a> What can you do about that? 
20:43:09 <dsal> > traverse (find even) [[1, 2, 3], [4, 5, 6]]
20:43:11 <lambdabot>  Just [2,4]
20:43:26 <dsal> > traverse (find even) [[1, 3], [4, 5, 6]]
20:43:28 <lambdabot>  Nothing
20:44:10 <dsal> Traverse with id is sequence
20:45:54 <p0a> got it
20:48:43 <dmwit> % ensure p x = x <$ guard (p x)
20:48:44 <yahb> dmwit: 
20:48:53 <dmwit> % traverse (ensure even) [2,4,6,8]
20:48:54 <yahb> dmwit: [2,4,6,8]
20:49:01 <dmwit> % traverse (ensure even) [1,2,4,6,8]
20:49:01 <yahb> dmwit: *** Exception: user error (mzero)
20:49:04 <dmwit> p0a: ?
20:49:08 <dmwit> oops
20:49:09 <dmwit> gross
20:49:20 <dmwit> Oh, haha, it's going IO
20:49:36 <dmwit> % traverse (ensure even) [1,2,4,6,8] :: Maybe [Int]
20:49:36 <yahb> dmwit: Nothing
20:49:46 <dmwit> % traverse (ensure even) [2,4,6,8] :: Maybe [Int]
20:49:46 <yahb> dmwit: Just [2,4,6,8]
20:49:58 <dmwit> Much better.
20:50:25 <dsal> mzero isn't my favorite error
20:51:02 <dmwit> Yes, IO was not the best choice of Alternative for that, and I didn't think ahead enough to predict that problem.
20:51:11 <dmwit> Nuts!
20:51:39 <Axman6> p0a: traverse id = sequence
20:52:04 <Axman6> :t sequence
20:52:05 <lambdabot> (Traversable t, Monad m) => t (m a) -> m (t a)
20:52:14 <dsal> @src sequence
20:52:14 <lambdabot> sequence []     = return []
20:52:14 <lambdabot> sequence (x:xs) = do v <- x; vs <- sequence xs; return (v:vs)
20:52:14 <lambdabot> --OR
20:52:14 <lambdabot> sequence xs = foldr (liftM2 (:)) (return []) xs
20:52:20 <dmwit> sequence is a red herring. traverse+ensure is better
20:52:29 <Axman6> sure
20:52:32 <dsal> That's... Not what I expected
20:53:54 <p0a> Thank you
20:59:25 <siraben> conal's work on functional images really works; https://github.com/sbond75/OpenGLTesting/tree/master/OpenGLTesting_C/macOS/gallery
21:02:58 <Axman6> the use of tuples makes me sad
21:03:31 <siraben> Axman6: how so?
21:04:36 <Axman6> having like 5 pointers per pixel feels wasteful when it could be data Colour = Colour {r,g,b :: {-#UNPACK-#} !Float}
21:05:15 <siraben> Ah, I'll consider that. What about passing that to C?
21:05:55 <Axman6> nothing really needs to change, if you define unColour :: Colour -> (Float, Float, Float)
21:07:05 <Axman6> I'm definitely prematurely optimising btw
21:07:51 <EvanR> fixed could also refer to lifetime or scope
21:07:54 <EvanR> oops
21:08:08 <siraben> Axman6:  I made the change, it was pretty minor.
21:08:35 <siraben> This is also software rendered, we'll have to figure out how to call Haskell to compute the values in parallel.
21:09:52 <siraben> Anyone know how to resolve the error "Illegal foreign declaration: requires unregisterised, llvm (-fllvm) or native code generation (-fasm)" that appears my repl? I have to keep commenting out the foreign exports to remove the error.
21:10:04 <siraben> I already have {-# OPTIONS_GHC -fasm #-} at the top of the file.
21:11:03 <Axman6> I think you may need to run ghci with -fobject-code
21:12:22 <siraben> Axman6:  Thanks, that worked. How do I declare that in the Haskell file?
21:12:36 <siraben> :set -fobject-code would only last the session, IIRC
21:12:42 <Axman6> I'm not sure it makes sense to, it's a GHCi flag
21:15:07 <dmj`> is there any reason why :kind! wouldn't evaluate a type family application
21:16:44 <Axman6> ig it gets stuck?
21:18:53 <dmj`> Axman6: I have a type family "Same" that tests structural equality, and "And" which is like type level &&
21:20:29 <dmj`> Axman6: I had the cases in incorrect order, my bad
21:20:36 <dmj`> it was just expanding the type
21:20:41 <dmj`> not evaluating the family
21:20:52 <dmj`> ya gotta keep it in the fam
21:29:45 * hackage network-uri-static 0.1.2.2 - A small utility to declare type-safe static URIs  https://hackage.haskell.org/package/network-uri-static-0.1.2.2 (snak)
22:25:33 <fragamus> hi im trying purescript and the irc channel is dead and I need to ask a simple question.
22:25:41 <fragamus> may I ask it here
22:26:25 <Axman6> sure
22:27:12 <fragamus> I was using this gist two days ago and now it is not working
22:27:13 <fragamus> https://try.purescript.org/?gist=1b0ef044e8b4af58c62ea42400d7ba7d&backend=thermite&session=8382d8a0-3b0c-55b4-c582-31bd3b605a2c
22:27:33 <EvanR> wait... this isn't a question at all!
22:28:42 <fragamus> ok Why did it stop working?
22:31:36 <shachaf> It's not working because the module Control.Monad.Aff is unknown.
22:32:11 <fragamus> yeah but it was known two days ago
22:32:49 <EvanR> make it known. Maybe later you can do the work to find out how it became unknown
22:33:21 <fragamus> I think it became unkown on the server
22:33:31 <fragamus> crap ok thanks
22:33:48 <shachaf> I'm pretty sure anyone who knows the details of the PureScript environment to that degree (and wants to answer) will be in the PureScript IRC channel.
22:34:02 <shachaf> This kind of question doesn't translate to another language.
22:34:35 <Feldmaus> Other modules are unknown too, btw
22:37:37 <MarcelineVQ> https://github.com/purescript/trypurescript/commit/cc1b4992f2770fccf2ee5e47a40642e94d285b17  
22:38:58 <MarcelineVQ> "The Try PureScript client no longer uses a bundle of preloaded modules" no clue at all how to specify what packages/modules you want though, idk what a 'require' is either
22:42:18 <monochrom> I don't understand the logic behind "the irrelevant channels have a better chance than the really relevant channel"
22:42:52 <EvanR> if you add up enough small chances the chance approaches 100%, or goes beyond even
22:43:12 <monochrom> Like if I can't find a medical channel, I feel that #haskell can help me with questions about coughing and fever. Yeah right greatest idea ever.
22:43:43 <monochrom> If I can't find a lawyer channel, I feel that ##c will help me with music copyright questions.
22:43:50 <p0a> They might
22:43:58 <p0a> they're good at lawyering
22:44:07 <p0a> :PP all that ISO C standard stuff
22:44:12 <monochrom> haha
22:44:30 <shachaf> It's true, ##c is full of language lawyers.
22:44:34 <shachaf> It might be able to help me with my physics questions.
22:44:39 <shachaf> @quote monochrom einstein
22:44:39 <lambdabot> monochrom says: einstein's theory implies that haskell cannot be faster than c
22:44:50 <monochrom> haha well played
22:47:21 <p0a> speaking of optimization
22:47:36 <MarcelineVQ> the language that can be said faster, is faster
22:47:43 <p0a> if I have map f $ map g xs, does that optimize to map (f `compose` g) xs ?
22:48:00 <p0a> I.e. single iteration as opposed to double iteration of xs?
22:48:03 <shachaf> Maybe in some cases.
22:48:11 <shachaf> You can try it and find out.
22:48:17 <p0a> I was hoping it did in basic cases
22:48:33 <p0a> I wasn't sure but I thought that's what is meant by 'write idiomatic haskell'
22:48:39 <monochrom> If you can read Core, you can quickly test it.
22:48:45 <p0a> read Core?
22:48:59 <monochrom> GHC's intermediate language. one of.
22:49:05 <p0a> ah I see 
22:49:25 <shachaf> Write your function into a file and run ghc -O2 -ddump-simpl -fforce-recomp file.hs
22:49:48 <shachaf> Then you can see what it compiles into. In this case it does this transformation.
22:49:52 <shachaf> Probably due to rewrite rules?
22:50:09 <monochrom> Yeah if anything it's a rewrite rule.
22:50:14 <p0a> I thought this optimization is a consequence of lazyness
22:50:26 <p0a> since otherwise how would you ever do all odd $ map (1+) [1..]
22:50:36 <EvanR> consequence of functor law?
22:51:19 <shachaf> If it was a consequence of non-strictness, it wouldn't be an optimization, since it would be required by the language specification.
22:51:50 <p0a> ok so I'm wrong?
22:51:58 <monochrom> laziness only gives you big-O equivalence between map (f . g) and map f . map g.  But you are investigating the constant multiplier.
22:52:11 <p0a> let me think about that statement 
22:52:25 <shachaf> You can evaluate either map f (map g xs) or map (f . g) xs and get the same results. The number of calls to map doesn't affect the actual value.
22:52:59 <p0a> monochrom: doesn't lazyness guarantee the constant is independent of f,g ?
22:53:01 <p0a> that's what I am asking
22:53:28 <monochrom> I don't understand that question.
22:54:12 <p0a> okay well I believe map f is of Order O(n m) where f is of order O(n) and the list of length m
22:54:40 <p0a> nevermind
22:54:44 <p0a> I can't phrase what I want to say
22:55:35 <p0a> and perhaps it doesn't matter. I need to stick to identifying bottlenecks and using the profiler *if* and *when* I encounter a performance issue
22:55:55 <iqubic> Yes. Despite both expressions getting the same result, one of them traverses the list twice, and therefore takes longer to compute.
22:56:20 <monochrom> "traverse the list twice" is wrong wording at the very least.
22:56:28 <iqubic> monochrom: How so?
22:57:27 <monochrom> Suppose I have "map f (map g xs)".  Which list is traversed twice, and who traverses it for the 2nd time?
22:58:26 <iqubic> Doesn't map traverse the list once each time it's called?
22:58:52 <EvanR> in other languages perhaps
22:58:52 <iqubic> Which means that expression that calls map twice traverses the list twice.
22:59:55 <iqubic> How is Haskell different?
23:00:02 <EvanR> lazy evaluation
23:00:08 <nshepperd> there are two list traversals, but the second one is of a different list than the first one
23:00:51 <iqubic> Oh. Right.
23:01:18 <monochrom> This is why you should answer my guiding questions directly rather than reiterate your belief.
23:01:52 <shachaf> But repeating my existing beliefs over and over is why I'm in this channel in the first place.
23:02:01 <iqubic> Yeah. I see that now. But then again, hindsight is 20/20
23:02:18 <monochrom> excuses execuse
23:02:20 <monochrom> This is why you should answer my guiding questions directly rather than reiterate your belief.
23:02:38 <iqubic> Again, hindsight is 20/20
23:03:18 <MarcelineVQ> people in glass houses sink with the ship
23:03:22 <shachaf> I think you should apply this advice to foresight, rather than hindsight.
23:03:56 <iqubic> foresight is rarely 20/20. People always make silly mistakes.
23:04:19 <nshepperd> people in glass houses find their vegetables grow quite well
23:04:20 <monochrom> All you need to do is to risk being wrong but show commitment and directness and say "xs is traversed twice, map f is who traverses xs the 2nd time".  Then maybe you immediately see what's wrong with it, or if you don't it's still OK, I can point it out, but either way you earn my respect.
23:07:04 <monochrom> People keep hiding behind the facade of "human makes mistakes, we all make mistakes".
23:08:09 <monochrom> Empirically I find it disingenious. People who harp on that never do anything about it.
23:08:47 <monochrom> The thing we need to do is to keep asking ourselves "how do I detect that I make a mistake?"
23:09:19 <monochrom> Mistakes are detected by going concrete and specific, i.e., testing.
23:10:29 <nshepperd> 'traverses the list twice' seems to reflect a stateful view of the piece of code
23:10:43 <dmj`> is it possible to use a closed type family to change the associativity of a type level expression
23:11:41 <shachaf> I think the actual answer of what happens when the code is evaluated is pretty different from what anyone said.
23:11:43 <nshepperd> you can see a pipeline like 'map f . filter g . map h . map i' as a series of operations modifying a list
23:11:59 <shachaf> If you follow the instruction pointer, what happens isn't that a list is traversed and then another list is traversed.
23:12:35 <nshepperd> this doesn't seem inherently wrong conceptually but probably inappropriate for the context of estimating computational cost
23:12:37 <dmj`> type Foo = (("a" ## "b") ## "c") ## "d" (becomes) type Foo = "a" ## ("b" ## ("c" ## "d"))
23:13:45 <p0a> shachaf: abstractly you can't even demand computation it sounds like there's no other statement you can make, except the type
23:18:31 <p0a> 'ensure' is not a function? I have to define it myself right?
23:18:54 <p0a> in the code `traverse (ensure even) [1,2,4,6,8]'
23:19:57 <dsal> p0a: he defined it way up there.
23:20:00 <monochrom> dmj`: I think no.
23:20:09 <dmj`> monochrom: was afraid of that
23:20:31 <p0a> dsal: thank you
23:20:38 <p0a> dsal: I didnt' realize that was the definition, that line confused me
23:21:57 <dmj`> monochrom: I have two types just like this and structural equality is failing due to precedence. The first type was constructured via a type family on a Generic Rep, the second defined manually
23:22:57 <monochrom> Generic doc has a warning somewhere against relying on associativity at all.
23:23:49 <siraben> How can I return (1,2,3) to C from Haskell?
23:24:14 <siraben> I have to use hsc2hs right?
23:24:27 <Axman6> have to? no. but you could.
23:24:46 <Axman6> what do you want (1,2,3) to look like on the C side?
23:25:17 <siraben> typedef struct Color { Uint8 r, g, b; } Color;
23:25:31 <dmj`> monochrom: what is the default fixity of a field
23:26:30 <monochrom> a field doesn't have fixity....  But a function that reads a field has the same default fixity as any function, i.e., 9 IIRC
23:29:03 <dasw> I'm shocked to learn that FP languages have no loops. For loops are so basic. What's bad about for loops that FP paradigm eliminates them?
23:29:27 <monochrom> Loops are completely useless without state.
23:29:53 <siraben> I'm guessing
23:29:54 <siraben>  -- skip padding bytes after "c"
23:29:56 <dmj`> monochrom: what about like when I use RecordWildCards to unpack a field as a function, that doesn't have fixity?
23:29:59 <siraben> oops
23:30:00 <siraben> http://ix.io/2kQn
23:30:07 <Axman6> we have loops, they're called [a]
23:30:22 <siraben> dasw: Recursion is more natural.
23:30:35 <siraben> Axman6: Does that storable instance look right?
23:30:41 <EvanR> for loops are an improvement over while loops. while loops are an improvement over goto. You probably aren't shocked that goto isn't that popular anymore
23:30:53 <Axman6> and I would argu that loops have lead to more security problems in software than most language features, after null
23:31:36 <dasw> siraben: wouldn't recursion lead to stack overflow if there are too many loops to replace?
23:31:37 <EvanR> "for each" is an improvement over for loops with explicit index math
23:31:50 <Axman6> siraben: ... probably, but maybe something like hsc2hs is a good idea, because it will give you the proper offsets to the fields (I would be surprised if that doesn't work but it might not)
23:32:05 <Axman6> dasw: we don't use stacks for function calls
23:32:17 <siraben> dasw: Recursion can be optimized into loops by the compiler. Also, the stack isn't used by the runtime.
23:32:23 <dasw> Axman6: i was under impression that recursion uses stack
23:32:30 <Axman6> it does in lame languages
23:32:31 <monochrom> Even gcc does tail-call optimization. Look it up.
23:32:46 <siraben> dasw:  Scheme, a Lisp dialect, also does tail-call optimizaiton.
23:32:50 <Axman6> there are many ways to compile programs, using a C style stack is just one of them
23:32:57 <dasw> siraben: how about python?
23:33:00 <monochrom> And has been doing that for more than 2 decades.  How old are you?  80?
23:33:22 <siraben> dasw:  Nope, Python explicitly does not implement TCO.
23:33:42 <siraben> Not sure what Guido's argument was, though.
23:33:46 <monochrom> Why do young people still perpetual "recursion must use stack" like they're 1970s old-geezers?
23:33:48 <dasw> siraben: using TCO means no risk of stack overflow?
23:33:54 <EvanR> recursion to simply do loops feels a lot like goto, whether the implementation has made it exhaust the stack or not
23:33:59 <siraben> dasw:  If the recursive calls are in tail position.
23:34:12 <monochrom> Is this some sort of imaginative nostagia the same way people love vinyl records?
23:34:13 <Axman6> no. We definitely use a stack in Haskell, but not for function calls
23:34:18 <siraben> Right.
23:34:56 <dasw> siraben: does that mean if one uses python to do FP, there's the risk of stack overflow due to too much recursion?
23:34:58 <siraben> dasw: Functional languages are very different to imperative ones, it may be worthwhile to forget what you know and relearn under a new paradigm.
23:35:02 <monochrom> Guido's argument is to avoid functional programming.
23:35:31 <monochrom> Recall that he was even against the pretty benign "x if mycond else y"
23:35:31 <EvanR> Guido may have progressed since those days
23:35:48 <siraben> monochrom:  Wow, I didn't know that.
23:36:03 <Axman6> yet he was happy with python's weird list comprehensions
23:36:04 <siraben> dasw:  Of course, that's why they use loops.
23:36:31 <EvanR> or generators
23:41:01 <monochrom> Recall that python's lambda is still castrated.
23:41:29 <siraben> dasw:  In particular, I would learn more about reasoning about recursive programs, then operations like foldr on lists that abstract this pattern of recursion away.
23:41:43 <siraben> Then folds on trees, other data types.
23:42:01 <siraben> Generalize to recursion schemes, even.
23:42:55 <siraben> > foldr (*) 1 [1..6]
23:42:57 <lambdabot>  720
23:43:08 <siraben> Look ma, no loops!
23:43:16 <iqubic> > product [1..6]
23:43:18 <lambdabot>  720
23:43:30 <iqubic> Look ma, standard library function!
23:45:40 * siraben hides
23:46:17 <siraben> Obligatory mention of https://willamette.edu/~fruehr/haskell/evolution.html
23:46:55 <dsal> > productOf folded [1..6] -- oblens
23:46:57 <lambdabot>  720
23:47:01 <MarcelineVQ> > let foldo = 720 = foldo
23:47:03 <lambdabot>  <hint>:1:17: error:
23:47:03 <lambdabot>      parse error on input ‚Äò=‚Äô
23:47:03 <lambdabot>      Perhaps you need a 'let' in a 'do' block?
23:47:08 <MarcelineVQ> oops, well :>
23:47:10 <iqubic> I understand everything up until List-Encoding.
23:48:03 <iqubic> I even have a proof of the combinatory factorial in my Haskell learning/tinkering folder.
23:52:20 <jgt> oats: I suggested trace because they said they wanted to write "debug messages"
23:59:07 <siraben> iqubic:  up until meaning including the section on list encoding?
23:59:50 <iqubic> siraben: the list-encoding version of Factorial is the first one that I don't understand
