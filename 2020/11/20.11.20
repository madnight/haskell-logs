00:17:59 <mikevdg> I've got a Monad question regarding https://github.com/opqdonut/haskell-exercises/blob/master/templ/W6B.hs
00:18:12 <mikevdg> Excuse the CCPP stuff in that file. Line 676
00:18:17 <mikevdg> *CPP
00:18:46 <mikevdg> you have `where g state = let (v, state1, log) = runSL op state0`
00:19:11 <mikevdg> How is that possible? How can you say `where f x = ...` if x doesn't have a value?
00:19:28 <mikevdg> I understand you can do it for pattern matching with tuples and lists. (a, b) = f x...
00:19:40 <mikevdg> But is that possible with functions too?
00:20:02 <mikevdg> (g is (Int -> (a, Int, [String])
00:23:20 <koz_> mikevdg: 'where g state = ...' is basically 'where g = \state -> ...'.
00:23:24 <koz_> Is that what threw you?
00:23:30 <merijn> mikevdg: That's just defining a function
00:23:45 <mikevdg> I've been staring at this exercise for an hour. 
00:24:13 <merijn> "where f x = ..." is just "defining a local function 'f' with a single argument 'x'"
00:24:30 <mikevdg> yea, that makes sense now. Now that I think about it, I do it all the time.
00:24:35 <mikevdg> double x = x * x
00:24:51 <koz_> mikevdg: Yeah - you can write that as 'double = \x -> x * x'
00:24:57 <mikevdg> f x = ... where f is some anonymous function... same pattern.
00:25:19 <mikevdg> well... it's not anonymous if you call it f.
00:25:44 <mikevdg> foo = MyMonad f where f arg = ...
00:27:17 <mikevdg> It threw me because I was defining a function that was being used as a variable.
00:28:51 <mikevdg> you guys are awesome.
00:29:04 <koz_> mikevdg: We try.
00:32:15 <opqdonut> mikevdg: I hope you like the exercises! Check out https://haskell.mooc.fi which is based on the same exercises, but also contains reading material
00:32:31 <merijn> mikevdg: Well, technically it *is* defining a variable. It's just a variable that happens to be a function :)
00:32:36 <mikevdg> hey, it's the author! Sure, will do.
00:32:37 <opqdonut> mikevdg: part 1 is out, but it doesn't cover monads, part 2 will come soon(tm)
00:33:33 <guest1120> what's diffrent with <|> and >> ?
00:33:41 <merijn> guest1120: Eh, lots :p
00:33:52 <guest1120> simply?
00:34:09 <[exa]> guest1120: if you asked for *> and >> the answer would be simple, but <|> and >> differ even in type signature
00:34:11 <koz_> :t (<|>)
00:34:13 <lambdabot> Alternative f => f a -> f a -> f a
00:34:16 <koz_> :t (>>)
00:34:17 <lambdabot> Monad m => m a -> m b -> m b
00:34:20 <koz_> Simply. :P
00:34:20 <merijn> guest1120: <|> is a choice combinator, exact meaning but in, for example, parsers it's usually "left *or* right"
00:34:48 <merijn> > Just 2 <|> Just 3
00:34:50 <lambdabot>  Just 2
00:34:56 <merijn> > Nothing <|> Just 3
00:34:58 <lambdabot>  Just 3
00:35:02 <merijn> > Just 2 >> Just 3
00:35:04 <lambdabot>  Just 3
00:35:08 <guest1120> ok
00:35:08 <merijn> > Nothing >> Just 3
00:35:11 <lambdabot>  Nothing
00:35:20 <dminuoso> guest1120: Note that the "or" should be taken with a grain of salt. 
00:37:17 <merijn> Yes
00:37:25 <merijn> > [1,2] >> [3,4]
00:37:27 <lambdabot>  [3,4,3,4]
00:37:32 <merijn> > [1,2] <|> [3,4]
00:37:34 <lambdabot>  [1,2,3,4]
00:37:52 <guest1120> what?
00:37:55 <koz_> merijn: I had never considered >> in a list context before.
00:38:33 <merijn> koz_: :)
00:38:37 <dminuoso> guest1120: Roughly, if you build a parser: `string "foobar" <|> string "fooquux"` and throw it against "fooquux" then depending on the parser library it might not parse.
00:39:03 <koz_> Like, upon thought it makes sense, but I guess I hardly ever use lists for their monad properties.
00:39:24 <merijn> guest1120: Alternative has <|> and empty, with the definition that "<|> is associative" and "empty is the left and right identity for <|>" so "x <|> empty = x = empty <|> x"
00:40:46 <merijn> guest1120: The exact meaning of the associative operation depends on the instance. For parsers it's usually "try left, then on failure try right", and empty is "a failing parser", but for lists it's "concatenate" with empty being the empty list
00:42:44 * mikevdg reads about Data.Semigroup
00:43:18 <merijn> Semigroups are great, we should to more of those :D
00:44:04 <guest1120> dminuoso: merijn to combinate parsers, >> or <|> I should use?
00:44:07 <guest1120> I'm confused now
00:44:21 <tdammers> merijn: I propose adding that to the Zen Of Python, just to mess with Python cultists
00:44:24 <guest1120> parse (spaces >> symbol) "lisp" input
00:44:30 <merijn> guest1120: Well, that depends on what you want :)
00:44:44 <guest1120> parseExpr = parseAtom  <|> parseString 
00:44:47 <merijn> guest1120: "a >> b" is "a, then b", "a <|> b" is "a or b"
00:45:00 <merijn> tdammers: :)
00:45:12 <merijn> tdammers: I have more important business
00:45:25 <guest1120> merijn: ok
00:45:34 <tdammers> merijn: yeah, but would be fun, wouldn't it
00:45:35 <merijn> tdammers: Like, figure out a good name for "Monoid m => Bool -> m -> m" so I can *finally* get it into base
00:45:38 <[exa]> the kids today don't even start their first python tutorial with `import this`.
00:46:03 <mikevdg> import turtle;
00:46:19 <tdammers> [exa]: just yesterday I've seen someone argue that "oneThing if condition else otherThing" is great syntax
00:46:24 <kritzefitz> merijn, That type doesn't seem familiar to me. What would that operator do?
00:46:26 <merijn> tdammers: I'm using mif in my code now, but I'm not happy with it
00:46:47 <merijn> kritzefitz: "\b x -> if b then x else mempty"
00:47:03 <merijn> kritzefitz: It's amazing how astonishingly useful that is
00:47:07 <tdammers> if anything, I'd go with mwhen rather than mif
00:47:11 <guest1120> lex, happy, parsec, they're same?
00:47:31 <tdammers> mif suggests Monoid m => Bool -> m -> m -> m, or maybe even Monoid m => Bool -> m -> m -> m -> m
00:47:38 <merijn> tdammers: Yeah, but that leads to confusion with monad's and whenM, etc.
00:47:59 <tdammers> it's very similar to when from Control.Monad though, isn't it
00:48:26 <merijn> yeah
00:48:38 <tdammers> in fact, if Monad could have the Monoid instance it deserves, it would be the exact same thing
00:49:54 <dminuoso> guest1120: Presumably by lex you meant alex?
00:50:08 <dminuoso> guest1120: At any rate, they are all parsing/lexing libraries but with different ergonomics and properties.
00:50:25 <guest1120> dminuoso: which one is easier?
00:50:45 <dminuoso> "easier" is hard to answer in general
00:50:46 <merijn> kritzefitz: Some example uses: https://github.com/haskell/cabal/blob/master/Cabal/src/Distribution/Simple/Program/GHC.hs#L203-L209 also note that from/to in the surrounding code https://github.com/haskell/cabal/blob/master/Cabal/src/Distribution/Simple/Program/GHC.hs#L174-L199 are just slightly specialised versions of that function
00:51:10 <guest1120> I'm learing parsec by write yourself a scheme with 48 hours
00:51:14 <merijn> kritzefitz: It's great for selectively including bits and pieces in complex compound structures
00:51:20 <guest1120> but some codes are old
00:51:21 <dminuoso> But `megaparsec` is a safe default for a parsing library
00:51:49 <dminuoso> That is, if you want a tl;dr for "which parsing library should I use", you generally cant go wrong with megaparsec.
00:52:06 <tdammers> also megaparsec > parsec in 99.9% of cases these days
00:52:30 <guest1120> but parsec comes with ghc?
00:52:42 <tdammers> does it? I don't think it does
00:52:50 <koz_> tdammers: Isn't parsec a boot library?
00:53:04 <tdammers> I thought it wasn't, but maybe it is by now
00:53:15 <guest1120> Text.Parsec?
00:53:26 <kritzefitz> merijn, It does seem useful. But I just realised thta I rarely use a lot of Monoids and now I'm wondering if I'm missing out.
00:53:40 <koz_> kritzefitz: You definitely are.
00:53:57 <koz_> You can get a remarkable range of behaviour out of foldMap by varying up your Monoids.
00:54:30 <dminuoso> guest1120: The boot libraries should not be a relevant factor for what you should use.
00:54:46 <merijn> kritzefitz: You are :p
00:54:51 <dminuoso> The fact that GHC comes with them is not for your convenience.
00:55:07 <merijn> kritzefitz: Note that the entire surrounding 150-200 lines of code I just linked is basically all composing a bunch of monoids together :p
00:55:29 <mikevdg> Haskell: A new way to break your brain every day. `mempty :: Sum Int` --> Sum {getSum = 0}
00:56:37 <merijn> mikevdg: Don't forget Dual
00:56:40 <guest1120> ok
00:56:50 <mikevdg> merijn: Dual?
00:56:50 <dminuoso> Mmm, why exactly does GHC ship with these boot packages at all? Is ghc linked dynamically?
00:57:02 <mikevdg> :t Dual
00:57:03 <lambdabot> a -> Dual a
00:57:09 <koz_> dminuoso: I think they're needed to build GHC itself?
00:57:24 <mikevdg> Is there a thing in, e.g. ghci, to also get a bit of documentation on a type?
00:57:26 <koz_> There's also Down.
00:57:31 <merijn> mikevdg: "newtype Dual a = Dual a" where "mempty = Dual mempty" and "mappend (Dual x) (Dual y) = Dual (mappend y x)"
00:57:31 <koz_> @hoogle Dual
00:57:32 <lambdabot> Data.Monoid newtype Dual a
00:57:32 <lambdabot> Data.Monoid Dual :: a -> Dual a
00:57:32 <lambdabot> Data.Semigroup newtype Dual a
00:57:32 <kritzefitz> The first thing that comes to minds, when I think about monoids, is that WriterT uses Monoids for the written things. So if I want to collect elements into a list it degrades (:) to (++) and that bother me. But I guess I'm just thinking about this the wrong way.
00:57:35 <mikevdg> Or do I have to go to Hoogle?
00:57:51 <merijn> > [1,2,3] <> [4,5,6]
00:57:52 <lambdabot>  [1,2,3,4,5,6]
00:57:57 <merijn> > Dual [1,2,3] <> Dual [4,5,6]
00:57:59 <lambdabot>  Dual {getDual = [4,5,6,1,2,3]}
00:58:03 <koz_> :D
00:58:18 <koz_> Another cool Monoid: Ordering.
00:58:22 <merijn> kritzefitz: There's more monoids than lists, though
00:58:33 <merijn> koz_: You're forgetting the *best* Monoid
00:58:37 <koz_> Lists are actually a very uninteresting monoid.
00:58:40 <dminuoso> koz_: Oh mmm. I was somehow under the impression that a GHC installation somehow came with these packages
00:58:44 <koz_> merijn: (Monoid b) => a -> b?
00:58:52 <merijn> koz_: Of course
00:59:18 <koz_> newtype MonoidInjection a b :P
00:59:35 <guest1120> why there're <> and ++ two ways to do the same thing?
00:59:46 <merijn> :t (++)
00:59:47 <dminuoso> guest1120: (<>) is more general than ++
00:59:48 <lambdabot> [a] -> [a] -> [a]
00:59:49 <kritzefitz> merijn, sure there are. But I guess I'm missing the familiarity with Monoids to recognize when their properties would actually be useful to me.
00:59:51 <merijn> :t (<>)
00:59:51 <koz_> guest1120: (++) is list-only, while <> will work for any Semigroup.
00:59:52 <lambdabot> Semigroup a => a -> a -> a
01:00:02 <Martinsos> dminuoso: So specifically for that example with shapes, which one do you think is a better approach? Data types or type classes? And why?
01:00:22 <merijn> kritzefitz: Note that the monoid that koz_ mentioned "instance Monoid m => Monoid (r -> m)" applies recursively :)
01:00:31 <merijn> :t comparing fst
01:00:32 <lambdabot> Ord a => (a, b) -> (a, b) -> Ordering
01:00:43 <merijn> :t comparing snd <> comparing fst
01:00:44 <lambdabot> (Ord a1, Ord a2) => (a2, a1) -> (a2, a1) -> Ordering
01:00:50 <dminuoso> Martinsos: It's hard to say with that little information.
01:00:56 <koz_> It's quite context-specific.
01:01:28 <merijn> > sortBy (comparing fst) [(1,2), (2,2), (1,1), (2,1)]
01:01:30 <lambdabot>  [(1,2),(1,1),(2,2),(2,1)]
01:01:41 <Martinsos> dminuoso: you mean to say it depends on which extension is important for us, what we might need in the future?
01:02:09 <merijn> > sortBy (comparing snd) [(1,2), (2,2), (1,1), (2,1)]
01:02:09 <merijn> > sortBy (comparing <> comparing snd) [(1,2), (2,2), (1,1), (2,1)]
01:02:09 <merijn> eh, whoops
01:02:09 <merijn> > sortBy (comparing fst <> comparing snd) [(1,2), (2,2), (1,1), (2,1)]
01:02:12 <lambdabot>  [(1,1),(1,2),(2,1),(2,2)]
01:02:12 <lambdabot>  [(1,1),(2,1),(1,2),(2,2)]
01:02:13 <lambdabot>  error:
01:02:13 <lambdabot>      • Couldn't match type ‘(a, a1) -> Ordering’ with ‘Ordering’
01:02:13 <lambdabot>        Expected type: (a, a1) -> (a, a1) -> Ordering
01:02:20 <dminuoso> Martinsos: In general, if you want something extensible covering the expression problem, then tagless final or datatypes a la carte are common approaches in Haskell
01:02:37 <merijn> > sortBy (comparing snd <> comparing fst) [(1,2), (2,2), (1,1), (2,1)]
01:02:39 <lambdabot>  [(1,1),(2,1),(1,2),(2,2)]
01:02:50 <koz_> Those are _very_ nuclear hammers.
01:03:41 <Martinsos> dminuoso: hmmm thanks, I will have to investigate those! I think I was reading datatypes a la carte but concluded I don't need that complexity right now. I guess what I am wondering, when somebody asks "should I do this with data types or type classes", and information is relatively simple (like that example with shapes), what is the correct answer? Probably: do it with data types since that is the simplest / least code, and let's see
01:03:41 <Martinsos> how it evolves?
01:04:03 <merijn> kritzefitz: Note how the monoid for functions lets you monoidally combine functions, regardless of number of arguments, which is especially neat with Ordering allowing for arbitrarily complex sorting criteria
01:04:47 <dminuoso> Martinsos: So Im in a similar spot, I've solved the expression problem in this particular project with "have a code generator" and if people want to extend my data type, they have to make a PR and Ill rerun the code generator. :p
01:05:13 <dminuoso> For this particular project.
01:05:15 <kritzefitz> merijn, indeed, that is a very nice Monoid. I wasn't aware it existed.
01:05:24 <dminuoso> There is no correct answer really
01:05:35 <dminuoso> But roughly I'd avoid typeclasses if you are not sure they will help you
01:05:54 <dminuoso> Often, the expression problem is not as bad as it seems
01:08:19 <Martinsos> dminuoso: thanks! I think that sounds like a good rule, avoid typeclass if not needed, and in most cases the problem will not actually be problematic. Cool, thank you
01:08:34 <koz_> Yeah - the reality is that All The Extensibility All The Time isn't helpful, and in some cases, it'll even slow you down.
01:08:41 <koz_> Over-generalizing early can be pretty bad.
01:08:54 <hololeap> is Show1 derivable in GHC 8.8.4?
01:09:14 <koz_> hololeap: I don't think *1 is derivable for any * in any version?
01:09:18 <[exa]> kritzefitz: btw, my favorite example with function semigroup is `rotList = drop <> take` (even with multiple parameters)
01:09:37 <dminuoso> [exa]: That example was pretty cool :)
01:09:56 <koz_> kritzefitz: For a recent Real Job For Real Money example from me - I used Monoid to implement 'find exactly one' with error handling.
01:10:04 <hololeap> koz_, ok i ask because it is listed here: https://github.com/haskell-compat/deriving-compat
01:10:19 <hololeap> "Provides Template Haskell functions that mimic deriving extensions that were introduced or modified in recent versions of GHC."
01:10:35 <koz_> The Monoid in question is something like 'data FindExactlyOne a = NoneFound | OneFound a | TooManyFound'.
01:11:00 <koz_> (the instances of Semigroup and Monoid are left as an exercise to the reader)
01:12:39 <merijn> the more you use Semigroup/Monoid/foldMap, the more use you see for them :)
01:13:02 <koz_> Realistically, it's {M,m}onoids all the way down?
01:13:03 <merijn> koz_: Did you see the glorious addition of foldMap' ? (*finally*)
01:13:08 <koz_> merijn: Where?
01:13:08 <kritzefitz> koz_, I think I remember having a similar problem. Now I realize that, semantically I also implemented a Monoid but didn't really use the typeclass for that. So I guess those are the kinds of opportunities I should look out for.
01:13:14 <merijn> koz_: In base :)
01:13:19 <koz_> kritzefitz: It gets easier with practice.
01:13:27 <koz_> merijn: No I did not. Indeed, *finally*.
01:13:28 <mikevdg> opqdonut: I've just read through some parts of your MOOC. If I earn 5 ECTS, what can I use them for? 
01:13:29 <merijn> koz_: GHC 8.8 or 8.10 added it?
01:13:50 <koz_> hololeap: I wasn't aware *1 could be derived automagically at all.
01:14:39 <hololeap> well, deriving Show1 with that GHC version not working for me
01:14:41 <mikevdg> opqdonut: and is there a fancy certificate at the end? :-)
01:15:01 <koz_> hololeap: I don't even know how it would work with any version. I'm pretty sure that's not stock derivable at least.
01:18:15 <idnar> :t foldMap'
01:18:16 <lambdabot> (Foldable t, Monoid m) => (a -> m) -> t a -> m
01:18:26 <idnar> :t foldMap
01:18:27 <lambdabot> (Foldable t, Monoid m) => (a -> m) -> t a -> m
01:18:34 <idnar> wut
01:18:39 <koz_> idnar: The type sig won't tell you the difference.
01:18:46 <merijn> idnar: foldMap' is strict
01:19:21 <idnar> ohhh right, I was thinking foldMap_
01:19:32 <dminuoso> Also, foldMap' by default associates differently internaly
01:19:37 <dminuoso> By default
01:19:52 <merijn> koz_: Now I just need a strict tuple to go with that :p
01:20:48 <kritzefitz> Strict tuples would be so great.
01:21:04 <koz_> merijn: So like, 'data StrictTuple a b = StrictTuple !a !b'?
01:22:31 <kritzefitz> I guess you could use it to break the internals of lots of types, but it seems like it could be nice, if you could provide strictness as part of type parameters.
01:23:23 <kritzefitz> e.g. `[!Int]` would be a list, where the elements would be evaluated with the spine.
01:30:20 <dminuoso> kritzefitz: Indeed it would be cool, we could then write strictness polymorphic code akin to levity polymorphism.
01:30:48 <merijn> dminuoso: I've already floated that idea a bunch of times :p
01:31:11 <dminuoso> Any plans after your M.Sc? :>
01:31:18 <dminuoso> Seems like the perfect PhD thesis.
01:31:30 <merijn> dminuoso: My M.Sc. was finished literal *years* ago
01:31:33 <dminuoso> Oh 
01:31:36 <dminuoso> Hold on
01:31:55 <merijn> It's my PhD thesis I'm procrastinating on :p
01:32:25 <dminuoso> Well, what about a second PhD then!
01:32:30 <merijn> hah
01:32:34 <merijn> never >.>
01:32:59 * dminuoso wonders whether a second doctor entitles you to being called "Doctor Doctor"
01:33:09 <koz_> dminuoso: Doctor Squared.
01:33:17 <dminuoso> heh
01:33:18 <merijn> I'm already drs. :p
01:33:36 <dminuoso> merijn: This is honestly so confusing. Why are you already dr?
01:33:43 <merijn> Which sadly, is not the plural of dr
01:33:47 <dminuoso> oh
01:33:49 <dminuoso> haha
01:33:51 <dminuoso> what is drs then?
01:34:13 <merijn> dminuoso: drs is doctorandus, it's roughly equivalent to MSc. from the time before EU standardized on bachelor/master
01:34:29 <dminuoso> Ah, presumably its similar to Dipl. in Germany
01:34:51 <merijn> But since I started university before that switch I'm entitled to use either drs or MSc. as I see fit
01:35:00 <pjb> dminuoso: IMO, doctor titles sum, not multiply.
01:35:11 <merijn> Anyway
01:35:30 <merijn> Only morons do a phd, I'm not sure what kinda idiot you gotta be to do *two* of the damn things
01:36:00 <dminuoso> pjb: addition/multiplication are relative. In fact, there are fields in math in which b^2 and 2*b denote the same thing
01:36:08 <merijn> otoh, if your dumb and stubborn enough to start a 2nd phd, I guess you'd be dumb and stubborn enough to make it
01:36:14 <pjb> merijn: when you want to advance two fields. For example, Elon Musk could be doctor in car batteries, and doctor in rocket engines.
01:36:15 <merijn> s/your/you're
01:37:01 <dminuoso> pjb: PhDs dont advance you.
01:37:18 <pjb> merijn: no, it advances science.
01:37:22 <merijn> hah
01:37:25 <dminuoso> "advances science"
01:37:29 <merijn> I like your optimism :)
01:39:26 <dminuoso> Much of academia follows "publication creep" where you're pushed to publish as often as possible in magazines with high impact factors.
01:39:38 <dminuoso> The primary motivation is not science, it's just money.
01:40:43 <dminuoso> It's getting increasingly difficult to filter out the useless, irrelevant or (semi-)plagiarized publications if you want to do some work in a field
01:40:53 * hackage Z-IO 0.1.8.0 - Simple and high performance IO toolkit for Haskell  https://hackage.haskell.org/package/Z-IO-0.1.8.0 (winterland)
01:41:02 <merijn> dminuoso: Oh, I got a simple hack for that
01:41:26 <merijn> dminuoso: I just assume it's irrelevant and don't read it ;)
01:43:17 <hololeap> the TH generated Show1 instance from deriving-compat seems to work fine
01:44:14 <hololeap> although i have to put the `$(deriveShow1 ''Foo)` line at the very bottom... template haskell is weird
01:53:01 <c_wraith> a top-level TH splice separates the file into what's sort of multiple compilation units.  It resolves names from before the splice, then does the splice with only those names in context, then looks at the rest of the file.
02:34:44 <merijn> c_wraith: Except a bug made it work for several years with classes >.>
03:30:06 <sszark> I'm extending an existing module by re-exporting it along with my own additions. Is there a way to export the whole scope in a module instead of declaring everything?
03:30:39 <tdammers> yes. you can do whole-module reexports
03:30:50 <tdammers> module Foo where (module Bar); import Bar
03:34:00 <ClaudiusMaximus> can you do    module Foo where (module Bar, module Foo) ; import Bar ; foo = 3 -- to export everything from Foo too?  i haven't tested...
03:36:00 <tdammers> IIRC yes
03:36:22 <ski> Martinsos : i think it's good to think of it as first considering sum/variant types (with pattern-matching) ("FP") vs. product/record types (with "message-dispatching") ("OO"), these two being more directly comparable
03:49:54 <sszark> thank you tdammers ClaudiusMaximus 
04:11:48 <merijn> sszark, ClaudiusMaximus: In fact you can even import only *part* of Foo and re-export that part
04:15:40 <Martinsos> ski: I didn't quite get where you are getting with this, would you mind giving me more details/context? How would that fit into deciding how to represent those Shapes / expressin problem?
04:16:08 <dexterfoo> Is "ViewPatterns" GHC extension good?
04:16:47 <ski> dexterfoo : it can be useful, sometimes
04:17:50 <ski> (avoiding an explicit `case'. being able to fall-through to a later defining equation, if the "secondary" match fails)
04:20:15 <ski> Martinsos : consider your type `Shape'. you want shapes to come in different alternative variants, such as circle,rectangle,&c. and you also want shapes to have various properties like perimeter,area,&c.
04:21:30 <Martinsos> ski: exactly, we want them to have some common interface, but also to be able to work with them individually. I think we can add that we also want to be able to put them all together in a list.
04:21:47 <ski> so, for each alternative kind of shape, you need to have an implementation of each property. imagine schematically displaying this in a table. the rows are your different alternatives, the columns are your different properties
04:22:37 <Martinsos> ski: sure, makes sense
04:24:57 <ski> the "OO" approach, the product/record types approach, is to slice your table into separate rows. each row becomes a function (a "constructor" in OO terms), that takes whatever parameters the alternative requires (like radius, or width & height), and returns a record with one implementation for each property (each "method"). in OO terminology, this is a class (with a class constructor taking tha parameters) 
04:25:03 <ski> that implements the `Shape' interface (or inherits from the (hopefully abstract) base class `Shape', if you must)
04:26:19 <Martinsos> ski: Sure, still following! Sorry I will stop writing, if I don't write anything it means I am following :D
04:26:21 <ski> the "FP" approach, the sum/variant types approach, is to slice your table into separate columns. each column (each property) becomes a pattern-matching function that checks which alternative kind of shape it's given, and computes the property for that case
04:27:03 <Martinsos> Aha got it
04:27:25 <ski> the alternatives becomes the data constructors of your sum / variant / algebraic data type
04:27:33 <Martinsos> And while product approach is more natural / nicer for OO, sum/variatns approach is more natural/nicer for FP?
04:28:05 <Martinsos> We could say that first one groups logic by variant, whiel second one groups logic by operation?
04:28:48 <ski> now, in the "OO" approach, if you want to add another kind of alternative, that's easy, just define a new record-producing operation / define a new OO class. but if you want to add another property, now you must edit all the existing classes
04:29:35 <ski> while, in the "FP" approach, if you want to add another property, that's easy, you just define a new pattern-matching function. but if you want to add another alternative, you now have to edit all the existing pattern-matching functions
04:31:10 <Martinsos> ski: I knew about the extendibility perspective, but what I didn't know is that one is considered OO approach while another is considered FP approach. I get it that that is also incorrect, to call them like that, but it does give an interesting perspective.
04:32:00 <Martinsos> ski: But to me implementing "OO" approach in Haskell doesn't seem much harder, we can do it with type classes right? Although it does require more code so I guess that makes it less elegant and therefore the variant approach is more of a "FP" approach?
04:32:01 <ski> Martinsos : exercise : define (FP) a sum type `ShapeS' with alternatives `CircleS',`RectangleS', and pattern-matching functions `perimeterS',`areaS'. then define (OO) a product type `ShapeP' with fields `perimeterP',`areaP' and record-consatructing operations `circleP',`rectangleP'
04:32:38 <ski> note that if you want a property/method that takes additional parameters, that's just a field that's a function
04:32:59 <Martinsos> ski: Ok here you are basically implementing type classes manually right?
04:34:06 <Martinsos> ski: Thanks, I get the examples! I have been playing with both examples previously so I did try them in practice, but I was still wondering is, which one is better, if one does not have a ton of information on how the codebase will evolve? I guess the "FP" is simpler and therefore maybe preferred as the first try.
04:34:14 <ski> also note that pattern-matching allows you to match on nested patterns, and on multiple parallel patterns, not just a single level of alternatives. also, if you throw recursive types into the mix, it can get more interesting/complicated. but the above should hopefully showoff the basic duality between sum types and product types
04:35:12 <ski> it's not about either one or the other, you want to have both available to you. sometimes one is more appropriate, sometimes the other. sometimes you can envisage alternatives being changed or added to (or removed from) more often than properties. sometimes the other way around
04:36:40 <Martinsos> ski: Thanks for detailed explanation, it is clearer now!
04:37:25 <Martinsos> ski: for the Shape problem, which one would you use? If you don't know in which direction you will be adding more (operations or variants)? Probably with sum type since it is simpler and we don't have info for either?
04:37:48 <ski> Martinsos : "And while product approach is more natural / nicer for OO, sum/variatns approach is more natural/nicer for FP?" -- FP languages have tended to emphasize sum types more. it seems it's only now that sum types with pattern-matching are starting to seep out into more mainstream languages. for a long time, there's been an OO dogma that using `switch' or `instanceof' is bad (and the latter is bad, i'd 
04:37:53 <ski> argue. but the motivation for wanting to use it is valid, the problem people want to address is real)
04:40:17 <ski> and so, you've gotten the VisitorPattern for (clumsily & verbosily) simulating/encoding sum types with pattern-matching, in OO. it's basically just a CPS / Church encoding of sum types. `A + B' being encoded as `forall o. (A -> o) * (B -> o) -> o'. `(A -> o) * (B -> o)' is the visitor, it's the branches of a `case'-`of', packaged up together
04:41:52 <ski> Martinsos : "We could say that first one groups logic by variant, whiel second one groups logic by operation?" -- yes (although the way i'm using the term "operation" is more like in the Abstract Data Type sense. so i'd probably say "property"/"method" instead of "operation", in your characterization)
04:44:22 <ski> "one is considered OO approach while another is considered FP approach. I get it that that is also incorrect, to call them like that, but it does give an interesting perspective." -- they're crude terms, there's more to FP and OO than this. but it does place the finger on some important differences between how both tends to be used, and it's a quick way to get the point across. if someone already understands 
04:44:28 <ski> what are product/record types, and what are sum/variant types, then i'd probably prefer using those terms instead of "OO" and "FP"
04:44:49 <joel135> . o O (interaction laws https://youtu.be/LHIKGC_oxFA?t=801)
04:46:15 <ski> Martinsos : "But to me implementing \"OO\" approach in Haskell doesn't seem much harder, we can do it with type classes right?" -- you may note that i haven't mentioned type classes at all. (and the main reason i started talking about this now was to steer you away from comparing with type classes right away). if you do the exercise i suggested, you should not be defining any type classes at all, nor making 
04:46:21 <ski> any instances of any such
04:47:14 <cads> hey would anyone be willing to pair program with me and show me how they set up and deploy a haskell based microservice?
04:47:31 <cads> say, something that I can send a string to, and it'll send me back the Sha3-512
04:47:39 <cads> asking a lot
04:47:45 <cads> whadda say?
04:48:28 <ski> Martinsos : and yes, doing the product/record types approach in Haskell is not that hard (but perhaps it must be pointed out to someone that it's possible, if it doesn't occur immediately as an alternative). which is another reason the "FP" term is crude. as is the "OO" term, since OO tends to also include things like implementation inheritance, and open recursion. you can do those in Haskell as well, but 
04:48:34 <ski> you have to do a little bit more work (especially for open recursion)
04:49:24 <ski> Martinsos : "Ok here you are basically implementing type classes manually right?" -- emphatically no
04:49:38 <cads> in return, I can offer to close a bug report in a python script of your choice (within 1 to 2 hours of labor)
04:50:11 <absence> why can i pass a (forall a. IsString a => a) to a function that expects String?
04:50:31 <merijn> absence: Because "instance IsString String"? :)
04:50:40 <merijn> :t fromString "foo"
04:50:41 <lambdabot> IsString a => a
04:50:47 <merijn> > fromString "foo" :: String
04:50:49 <lambdabot>  "foo"
04:50:53 <ski> absence : because it has type `a', for every type `a' that is an instance of `IsString'. and since `String' is such a type, your input (also) has type `String'
04:51:00 <mlugg76> (the other mlugg is a timed-out me, sorry!) Can you define a typeclass instance for a type family? E.g. if I have a type family like https://termbin.com/o4le, since every instance of the family has a `Show` instance, can I make `show :: Foo a -> String` valid?
04:51:20 <merijn> mlugg76: "no"
04:51:36 <merijn> mlugg76: Because a type family is basically a type level function
04:51:38 <ski> if
04:51:43 <ski>   foo :: forall a. IsString a => a
04:51:46 <ski> then also
04:51:54 <absence> merijn, ski: ah, so the fact that the function expects String forces the a to be String via the instance, that makes sence. thanks!
04:51:55 <ski>   foo :: IsString String => String
04:51:59 <mlugg> merijn: okay, ty
04:52:07 <ski> and since `IsString String' holds, we further have
04:52:12 <ski>   foo :: String
04:52:23 <merijn> mlugg: Basically "Foo a" is not a parameteric type, it's "the result of the Foo type family applied to 'a'"
04:52:42 <ski> absence : so, you're specializing your (overloaded) polymorphic value, to type `String', before passing it to the function. you're passing a `String'
04:52:56 <merijn> mlugg: So if you have "type instance Foo Bool = Int" (or whatever the right syntax is, then "Foo Bool" isn't 'Foo' (with it's instance) it's "Int"
04:53:15 <ski> absence, yep
04:55:17 <ski> Martinsos : btw, you might note that with `Circle = Double',`Rectangle = Double * Double', you have `ShapeS = Circle + Rectangle'. similarly, with `Perimeter = Double',`Area = Double', you have `ShapeP = Perimeter * Area'
04:56:28 * ski . o O ( <https://en.wikipedia.org/wiki/Additive_category#Matrix_representation_of_morphisms> )
04:58:58 <ski> MarcelineVQ : "which one is better, if one does not have a ton of information on how the codebase will evolve?" -- i'm not sure there's a good general answer to this. "it depends"
04:59:02 <ski> er, sorry
04:59:06 <ski> Martinsos ^
05:01:21 <Martinsos> ski: Thanks for all these explanations, this is great! Hm hm hm. I will try to play with the example more later, but I think I do get the concept. I don't quite understand the problem with recursion, but I can tackle that once I encounter it. And thanks for that link, I will check it in more depth.
05:02:12 <ski> Martinsos : you can also compare sum/variant/"FP" vs. product/record/"OO" with Deep vs. Shallow embedding, see <https://wiki.haskell.org/Embedded_domain_specific_language>. in this comparision, think of the record version as usually having just one field, but there's nothing stopping someone from doing a shallow embedding with "multiple alternative interpretations" (perhaps a "main/intended" one, and some 
05:02:18 <ski> auxilary ones)
05:02:49 <Martinsos> ski++
05:03:01 * ski twitches
05:06:32 <ski> e.g. for an idiomatic/applicative parser, you could both compute the main parsing action, that is fed parsing input, and does its work. but also compute, "on the side" ("statically", before parsing even starts), `NULLABLE' (can it succeed, consuming no input),`FIRST' (what are the possible tokens that could start an input that is accepted by the parser), for efficiency purposes
05:08:49 <ski> or, i once did a CGI monad thing, that contained a "play" and a "replay" part. when it emits a form, it generates a page. then when the form is filled in and replied, the computation starts over, but replaying the already done parts, until it gets to the point where it stopped last
05:08:53 <dminuoso> I finally it, a use for unsafeCatchPure :: Exception e => a -> (e -> a) -> a
05:08:56 <dminuoso> An evil one ontop!
05:09:18 <merijn> Is there a non-evil one?
05:09:47 <dminuoso> No idea, but it's either that or invest a lot of work in phadej's tree-diff
05:09:57 <Martinsos> ski: You slightly lost me there :D, I am afraid this is beyond my current Haskell knowledge, but I stored these more advanced things you said for later reading once I am more knowleagdable :D!
05:10:01 <dminuoso> Im gonna use pure exceptions to modify control flow!
05:10:03 <merijn> tree-diif is some fantastic stuff
05:10:16 <dminuoso> Indeed, it just lacks one thing we sorely miss
05:10:18 <merijn> dminuoso: I will haunt you until your death
05:10:30 <ski> (the category theory link is related, but will require some mbackground. and it's not too essential, it's just an "in case you know about CT, this might be interesting to compare to")
05:11:58 <dminuoso> merijn: No but seriously. The main issue with tree-diff is if you have Rec "foo" (M.fromList [("keyA", 1), ("keyB", 2)]) vs Rec "foo" (M.fromList [("keyA", 1)])
05:12:16 <dminuoso> There's sadly no way to represent deletion of keys
05:12:22 <dminuoso> Which causes a lot of ugliness
05:12:38 <dminuoso> or uh
05:12:45 <dminuoso> Rec "foo" (M.fromList [("keyA", ...), ("keyB", ...)])
05:12:55 <dminuoso> Rec "foo" (M.fromList [("keyA", ...)])
05:13:20 <dminuoso> Instead, you get a Swp operation on the record, and del/add operations on the value pointed by the key
05:13:41 <dminuoso> Im trying really hard to hack this into the Pretty dictionary
05:14:29 <dminuoso> (or actually, I think its even a Cpy on the record?)
05:22:00 <ski> Martinsos : anyway .. OO can also be related to lexical/static scoping (in particular non-local variable references. commonly implemented as "closures"), but alternatively also in terms of existential types. Abstract Data Types can also be related to existential types, but in a different way (leading to different ways of and issues with, structuring code)
05:22:07 <ski> @where on-understanding
05:22:07 <lambdabot> "On Understanding Types, Data Abstraction, and Polymorphism" by Luca Cardelli,Peter Wegner in 1985-12 at <http://lucacardelli.name/Papers/OnUnderstanding.A4.pdf>
05:22:10 <ski> @where on-understanding-revisited
05:22:10 <lambdabot> "On Understanding Data Abstraction, Revisited" by William R. Cook in 2009-10 at <http://www.cs.utexas.edu/~wcook/Drafts/2009/essay.pdf>
05:23:55 <ski> the latter talks about this. the former talks about different senses of "polymorphism", and also about ADTs as existentials
05:27:17 <ski> "Polymorphic Type Inference" by Michael I. Schwartzbach in 1995-03 at <https://cs.au.dk/~amoeller/mis/typeinf.p(sdf|df)>,<https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.57.1493> might be interesting to learn more basics of type systems, type inference, also mentioning universals and existentials
05:27:28 <ski> .. but all this is probably a discussion for another day
05:29:31 <dminuoso> [exa]: Btw, I decided to go with indents+parsec. The subtle edge cases of building up that tree kept building up.
05:30:26 <dminuoso> `I.withBlock mkTree node tree` is much simpler to write, I was already at 150 lines and started to write an evaluator to build up my tree.
05:43:14 <reactormonk> I've got a recursive datastructure with a Plated instance - How can I create a Lens where I have the already transformed children available in the transformation as well?
05:56:21 <[exa]> dminuoso: I always failed using the parsec/megaparsec indent blocks properly
05:56:25 <[exa]> perhaps I should retry.
05:56:38 <dminuoso> [exa]: Did you try by hands or using the indents library?
05:57:18 <[exa]> there's an extra library for that, aha.
05:57:28 <[exa]> is there a megaparsec variant?
05:58:13 <dminuoso> No, although one could write it. The implementation is fairly simple and straight forward.
05:59:02 <[exa]> I always failed to make it work with parse items that span multiple lines
05:59:20 <dminuoso> Well this is working just fine for me
05:59:22 <[exa]> hm gonna retry. :]
05:59:29 <[exa]> you don't have multiline tokens right?
05:59:57 <dminuoso> Right
06:00:06 <dminuoso> Im just parsing the cisco ios configuration language
06:00:14 <Uniaika> dminuoso: RIP
06:00:20 <[exa]> route in peace?
06:00:24 <dminuoso> haha
06:00:34 <dminuoso> That's not a bad one, I can see myself printing a shirt with that
06:01:06 <[exa]> with cisco it may also be
06:01:49 <[exa]> 'Reset without warning upon receiving an ASN >= 64k In Peace'
06:02:05 <[exa]> (nah they already fixed that)
06:02:32 <dminuoso> Wait. Received via BGP?
06:02:48 <[exa]> you didn't hear the czech routing vs. cisco story?
06:02:51 <dminuoso> Good thing we dont use Cisco for our external eBGP...
06:02:53 <dminuoso> No did not
06:03:01 <dminuoso> Peering is not my department
06:03:03 <[exa]> I'll try to dig that up
06:05:18 <[exa]> basically this https://www.ciscozine.com/cisco-ios-causes-internet-disruption/ but there's a mailinglist post somewhere with the initial reaction of the administrators
06:05:26 * [exa] ---> #offtopic
06:35:54 <ski> joel135 : hah, the state example on slide 8 made me think of Dialectica,PV,Chu, and now Uustalu, when talking about functor-functor interaction map (which looks suspiciously related to a dinatural transformation) mentions Chu :)
06:36:49 * ski . o O ( "Questions and Answers -- A Category Arising in Linear Logic, Complexity Theory, and Set Theory" by Andreas Blass in 1993-09-16 at <https://arxiv.org/pdf/math/9309208v1.pdf> )
07:00:35 <dminuoso> % Foo { foo = const 1, bar = foo 10 }
07:00:36 <yahb> dminuoso: ; <interactive>:61:28: error:; * Couldn't match expected type `Int' with actual type `String -> Int'; * Probable cause: `foo' is applied to too few arguments; In the `bar' field of a record; In the expression: Foo {foo = const 1, bar = foo 10}; In an equation for `it': it = Foo {foo = const 1, bar = foo 10}
07:01:00 <dminuoso> Mmm, what is the idiomatic way to define a recursive data value?
07:01:17 <dminuoso> Manually with fix?
07:02:29 <merijn> or just a recursive binding
07:03:31 <dminuoso> merijn: Oh you mean some `let f = Foo { foo = const 1, bar = foo f 10 } in f` ?
07:03:57 <merijn> yeah
07:04:05 <merijn> Like the classis
07:04:13 <merijn> > let ones = 1 : ones in ones
07:04:15 <lambdabot>  [1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1...
07:04:43 <merijn> Anyone happen to know how to set the default value of a UTCTime column to the epoch in persistent?
07:06:21 <dminuoso> merijn: Mmm, cant you set default=CURRENT_TIME?
07:06:37 <merijn> How is CURRENT_TIME the epoch?
07:06:41 <dminuoso> Oh
07:07:39 <dminuoso> Curious, you say UTCTime but epoch
07:07:57 <dminuoso> What kind of column does persistent map UTCTime to?
07:08:56 <merijn> TIMESTAMP (in sqlite)
07:10:41 <dminuoso> TIMESTAMP? Huh?
07:10:43 <dminuoso> What's that?
07:11:14 <dminuoso> Is that an alias for INTEGER?
07:11:51 <merijn> TEXT
07:12:03 <merijn> but as ISO8601 string
07:15:32 <dminuoso> I wonder, does persistent accept `foo UTCTime default=(cast(strftime('%s','now') as int)` as a column?
07:15:50 <dminuoso> Is that sort of what you meant?
07:15:54 <merijn> No clue what it accepts, hence why I ask :p
07:16:02 <dminuoso> heh
07:16:18 <dminuoso> 68:You can use a @default=${sql expression}@ clause to set a default for a field.
07:16:25 <dminuoso> So the above should work
07:16:53 * hackage leb128-cereal 1.2 - LEB128 and SLEB128 encoding  https://hackage.haskell.org/package/leb128-cereal-1.2 (JoachimBreitner)
07:16:58 <dminuoso> (possibly without the cast if this goes into a text column)
07:38:53 * hackage phonetic-languages-permutations 0.2.0.0 - Commonly used versions of the phonetic-languages-common package  https://hackage.haskell.org/package/phonetic-languages-permutations-0.2.0.0 (OleksandrZhabenko)
07:48:53 * hackage phonetic-languages-simplified-common 0.2.0.0 - A simplified version of the phonetic-languages-functionality  https://hackage.haskell.org/package/phonetic-languages-simplified-common-0.2.0.0 (OleksandrZhabenko)
08:04:17 <merijn> If I have a working ghcide, is there some way to dump out it's cradle config?
08:25:37 <dexterfoo> why does Map have both foldl and foldr? isn't it supposed to be unordered?
08:25:52 <merijn> dexterfoo: eh, no?
08:25:58 <merijn> Why would it be unordered
08:26:24 <merijn> In fact, containers guarantees Foldable and Traversable are "least to greatest" order of keys/elements
08:27:02 <dexterfoo> i see. but the performance of foldl and foldr are the same, right?
08:27:13 <merijn> "Yes, no, maybe, it depends"
08:33:23 <maerwald> The general answer is "no", unless you can be more explicit
09:19:00 <sszark> I'm working on a module extension that adds new logic and re-exports the old one. but i can't figure out how to only import the things i'm using for the extension, while at the same time exporting everything the rest. https://termbin.com/0y6u
09:19:16 <sszark> It seems like my local import for the extension part is overriding the full export in the module
09:28:22 <geekosaur> you basically can't; if yiu're going to re-export the whole module, you need to import the whole module
09:29:34 <sszark> Yeah, i figured as much. thanks
10:04:10 <tdammers> if the problem is that the full import clashes with names you define locally, you can do a qualified re-export
10:04:39 <tdammers> module Foo (module Bar, overriddenStuff) where import qualified Bar
10:14:51 <sondr3> I'm tryng to get cabal repl to load a project local .ghci file but I can't get it to work, all I want to do is set OverloadedStrings by default and load another module as well
10:14:56 <sondr3> No dice so far
10:16:18 <sondr3> I'm on Cabal 3.2.0.0, and my .ghci file is :set -XOverloadedStrings\n:load Other.Thing
10:20:46 <sondr3> Oh, it actually sets OverloadedStrings but don't load the other module
10:31:07 <electricityZZZZ> so in 1990, designing novel chips was "too expensive", and this was also true in 2000, 2010,... but people have built a lot of fancy fab equipment now, and surely that equipment must last. can one do dirt cheap fabrication of ASICs on "outdated" nodes?
10:53:23 <int-e> https://electronics.stackexchange.com/questions/7042/how-much-does-it-cost-to-have-a-custom-asic-made hmm. interesting niche: turn working FPGAs into ASICs that have an FPGA-like fabric in the bottom layers, thereby saving on mask costs
11:02:12 <electricityZZZZ> right,... i'm saying that today, this might become more valuable than historically,...
11:02:25 <electricityZZZZ> there also is https://www.intel.com/content/www/us/en/products/programmable/asic/easic-devices.html
11:03:02 <electricityZZZZ> cloud services mean that if you make a special chip you can make it available to everybody pretty easily,...
11:03:58 <geekosaur> but every time you turn around they come up with faster or more capable but more expensive fab, and people want to use that more capable
11:04:20 <davean> electricityZZZZ: You can do cheap fabrication on old nodes - its common to do that. Its just that for computation its not useful.
11:04:31 <electricityZZZZ> well it's a competition between what you can do in software on a CPU today and those gains
11:04:48 <electricityZZZZ> vs an FPGA or an ASIC (fabricated on an old node, at least to begin with)
11:04:54 <davean> electricityZZZZ: so they're used for non-computational things usually, or microcontrolers, etc.
11:06:16 <electricityZZZZ> furthermore if your software can be programmed so that the process from CPU all the way to ASIC was smooth, that would be pretty great
11:12:16 <davean> electricityZZZZ: I find your tautology compelling.
11:13:15 <electricityZZZZ> what are you referring to when you say my tautology
11:13:22 <electricityZZZZ> the "smooth process from CPU to ASIC"?
11:14:29 <davean> yes
11:14:38 <electricityZZZZ> yeah
11:15:05 <electricityZZZZ> furthermore, the ASIC aspect of things can help with revenue models
11:16:43 <int-e> ASIC for computation... I guess the famous examples where this has been done are ASIC bitcoin miners (sigh...) and Google's TPUs?
11:17:04 <electricityZZZZ> yeah but wouldn't redis-as-an-ASIC make sense?
11:17:21 <int-e> no
11:17:29 <davean> int-e: thats the famous example vs. like network controllers?
11:17:34 <int-e> too specific, and it's memory
11:17:47 <electricityZZZZ> there might also be security advantages to having a firewall etched into an ASIC
11:17:49 <int-e> davean: do those count as computation?
11:17:51 <davean> electricityZZZZ: also a lot of NVMe drives?
11:18:53 <davean> int-e: They run offload, application specific offload, packet routing, delivery, firewalls, etc? Its how we can manage multi 100Gb connections usefully to single systems and VMs. 
11:19:05 <davean> They run bytecode interpriters
11:19:08 <electricityZZZZ> davean: are you saying that NVMe drives already have hardware key-value stores onboard?
11:19:25 <davean> electricityZZZZ: No, but thats true. What I'm saying is higher end NVMe drives are run by FPGAs.
11:19:58 <electricityZZZZ> davean: yeah so that's "equivalent to an ASIC", mostly
11:20:09 <davean> what?
11:20:44 <int-e> FPGAs have a far lower one-time cost. That's basically their point.
11:21:19 <davean> int-e: no, also reprogrammability. Which is why you see them in any non-low-end NIC 
11:21:26 <electricityZZZZ> i'm saying perhaps your FPGA-on-NVMe device sufficienly covers more efficient CPU offloading to the point that it isn't worth moving to ASIC in many/most applications
11:21:32 <int-e> davean: Obviously there are tons of ASICs around. They are cheap *if* you buy them in the 100s of thousands or millions.
11:21:35 <davean> Its not FPGA-on-NVMe
11:21:52 <davean> Its FPGA-is-what-makes-it-NVMe
11:21:58 <electricityZZZZ> oh wow really?
11:22:14 <davean> int-e: Except they're not flexable.
11:22:26 <davean> int-e: so their cheapness is moot in a lot of applications - like NICs
11:22:27 <int-e> (And I do distinguish between FPGAs and ASICs)
11:22:56 <int-e> davean: I don't disagree.
11:23:36 <electricityZZZZ> davean: but you said the FPGA applies to high end NVMe (i.e. lower end NVMe drives don't have the FPGA), so what you are saying isn't self-consistent
11:23:52 <davean> No, it is, I think you're just lost.
11:24:05 <davean> There has to be a controller to turn flash into NVMe
11:24:25 <davean> that controller on lower end devices is a small CPU. On higher end devices its a medium size FPGA.
11:24:30 <electricityZZZZ> but my application doesn't distribute its program to that controller
11:24:42 <davean> Some do
11:25:47 <davean> And the POINT of smartnics, etc is specificly to bake stuff the OS would be doing in software into the data plane of the NIC.
11:27:07 <electricityZZZZ> yeah,...
11:27:14 <davean> And some NVMe is run as a KV store
11:27:17 <davean> also some HDs
11:27:26 <davean> there are NVMe modes
11:27:57 <electricityZZZZ> ...and i guess some intel CPUs are going to be shipping with integrated FPGAs. ... shouldn't we have a way of programming that in a productive manner? that would be sort of the first step in this logical chain
11:29:14 <davean> Oh are Intel CPUs still relivent?
11:30:03 <monochrom> Is it true that on Windows 64-bit, 64-bit GHC #defines mingw32_HOST_OS, and it's stlll "32" there?
11:30:06 <int-e> davean: apart from wishful thinking, why would they go away...
11:30:27 <davean> int-e: Because they've been pretty terrible for years and mostly uneconomical buys.
11:30:52 <davean> Its sad
11:31:27 <davean> They were still relivent in laptops, but recently that doesn't even seem true.
11:31:48 <int-e> "relevant"
11:32:18 <monochrom> Actually I should hop over to #ghc for that.
11:32:22 <electricityZZZZ> but they have so much fab infrastructure and so many people
11:32:35 <geekosaur> so did itanium
11:32:41 <electricityZZZZ> and they aren't that far behind
11:32:46 <davean> electricityZZZZ: Uh, they aren't?
11:32:55 <merijn> geekosaur: You misspelled the Itanic ;)
11:33:14 <int-e> geekosaur: Even Itanium somehow took off for a while... I never understood why.
11:33:25 <merijn> int-e: Because it's a super cool architecture
11:33:43 <merijn> int-e: Itanium wasn't bad, it was just severely lacking in software support (i.e. compilers)
11:33:44 <davean> int-e: I saw a big push to get it into places - I even ran some code on like I think it was a 32-way one? It was fun! But I certainly didn't have to pay for that ...
11:33:46 <int-e> Larrabee is the only project that failed completely
11:33:52 <merijn> int-e: Itanium taught is one thing
11:34:22 <electricityZZZZ> merijn: yeah lack of software support for itanium is partially why i am asking about programming cpus with integrated FPGAs
11:34:27 <int-e> merijn: at least two things... a) VLIW takes too much memory bandwidth b) VLIW and backward compatibility don't mix
11:34:32 <merijn> "No one will buy shit if it can't run x86" :p ARM might stand a chance because they've built up software support via mobile and the big guys are interested to low server power
11:34:51 * electricityZZZZ will be back later
11:34:56 <merijn> electricityZZZZ: Man, the software for dedicated FPGAs is already shite :p
11:35:02 <davean> merijn: Yah, I mean mobile is the main platform these days, so ...
11:35:05 <merijn> How would you even reasonably use said FPGA :)
11:35:08 <davean> Though x86 isn't going anywhere soon.
11:35:35 <merijn> Oracle had integrated FPGA and was working on some cool hardware before they killed SPARC
11:36:25 <int-e> (b)... AIUI, the second generation Itaniums had pretty much all the superscalar architecture complexity that EPIC was supposed to do away with.)
11:56:53 * hackage discord-haskell 1.8.1 - Write bots for Discord in Haskell  https://hackage.haskell.org/package/discord-haskell-1.8.1 (Aquarial)
12:00:13 <Sose> anyone had an issue with haskell LSP server where they can't use programs like "hlint" or a formatter? happens when I try from either Emacs or VSCode so it's probably not an editor integration issue...? here is a fresh log from VSCode doing clear log, restart server and format file... the weird bit is at the very end https://gist.github.com/Sose/dec29fa3781f2b3df1eab91fadf5f4df
12:00:54 <Sose> it's looking for a file called "/opt/ghc/8.8.4/lib/ghc-8.8.4/settings" but there's not even a /opt/ghc directory on my system and rest of the server and integration seems to work okay
12:07:28 <maerwald> does someone know what happens if the parser in Aesons fromJSONKey fails? It appears it still parses the json and defaults to something weird
12:19:04 <Sose> regarding my issue, apparently it's known and there's a workaround for now. you always end up finding the solution 15 minutes after asking... https://github.com/haskell/haskell-language-server/issues/412
12:19:43 <texasmynsted> Beyond this (https://github.com/joelburget/easytest) any recommendations for testing tutorials for Haskell?
12:31:17 <maerwald> https://github.com/bos/aeson/blob/8579faf30e0f977425fbf330038fb1d5c2c34727/Data/Aeson/Types/FromJSON.hs#L1981 hmm
12:46:11 <yummy> what does the +d mean in :t +d (+)?
12:46:52 <glguy> it looks like it defaults the type
12:47:10 <glguy> :t +d (+)
12:47:12 <glguy> :t (+)
12:47:12 <lambdabot> Integer -> Integer -> Integer
12:47:13 <lambdabot> Num a => a -> a -> a
12:47:44 <yummy> is there documentation on this somewhere?
12:47:56 <sm[m]> texasmynsted: check out tasty and hspec's docs, they are the big two
12:48:03 <glguy> I'd guess it's in the GHC User's Guide
12:48:14 <texasmynsted> okay. 
12:48:21 <texasmynsted> Switching to Tasty
12:48:30 <glguy> https://downloads.haskell.org/ghc/latest/docs/html/users_guide/ghci.html#ghci-cmd-:type
12:48:37 <texasmynsted> EasyTest was not compiling
12:59:45 <dsal> I like tasty.  I mostly aim towards quickcheck, with some plain assertions and occasionally golden tests.
12:59:56 <dsal> But tasty does all the things I want.
13:18:22 <jcd> howdy
13:25:57 <Zetagon> hello!
13:29:53 * hackage heap-console 0.1.0.0 - interactively inspect Haskell values at runtime  https://hackage.haskell.org/package/heap-console-0.1.0.0 (TheMatten)
13:30:28 <texasmynsted> https://yannesposito.com/Scratch/en/blog/Holy-Haskell-Starter/ 😂
13:30:28 <jcd> Hello! I need some help. I don't know what to do with monad that gets returned from a library function.
13:30:48 <Rembane> jcd: Which monad and which library function? 
13:31:30 <jcd> The monad comes from a function called 'parseRequest' from the 'Network.HTTP.Client' module.
13:31:39 <dminuoso> jcd: a "monad" is generally not something you get back, "monad" is just a bizarre name for an interface.
13:32:07 <dminuoso> jcd: So say you get something back of type T, and T has monad, it may be less helpful to think "you're getting a monad back". You're just getting a value back.
13:32:26 <dminuoso> (Or more correctly, you get something back of type `T S`, and T has monad)
13:32:51 <jcd> Then how do I extract the 'Request' out of 'm0 Request'?
13:32:56 <dminuoso> That depends!
13:33:01 <dminuoso> jcd: parseRequest :: MonadThrow m => String -> m Request 
13:33:01 <jcd> I thought fmap-like functions were the way
13:33:10 <dminuoso> Is a polymorphic value. First, you must understand that the choice of `m` is yours.
13:33:16 <dminuoso> You decide what `m` you want.
13:33:32 <jcd> then, 'Just'?
13:33:37 <dminuoso> You can pick any `m` you like, under the constraint that it has an instance MonadThrow.
13:33:49 <dminuoso> Well, let's see. Just is a data constructor, the matching type constructor is Maybe.
13:33:59 <dminuoso> Check whether an `instance MonadThrow Maybe` exists to see whether its a valid choice
13:34:15 <dminuoso> https://hackage.haskell.org/package/exceptions-0.10.4/docs/Control-Monad-Catch.html#t:MonadThrow
13:34:21 <jcd> On it.
13:41:25 <jcd> So, I see that it is a valid choice! I don't know if it's the one I _should_ choose though.
13:42:51 <dminuoso> That's up to you. In this case, the idea is "parseRequest" could fail. Rather than forcing the function to use say `Maybe` or `IO` to communicate failure in, it defers the choice to you.
13:43:13 <sm[m]> interactively inspect Haskell values at runtime ... that's interesting
13:43:14 <dminuoso> It says "Hey, you pick something of type `m`, as long as I can throw an exception with it", for some value of "exception"
13:43:45 <sm[m]> doc: https://hackage.haskell.org/package/heap-console-0.1.0.0/docs/Heap-Console.html
13:45:30 <dminuoso> Internally the package will use `throwM (InvalidUrlException s "Invalid URL")` if it failed to parse your input. What this ends up doing depends on your choice, if you pick Maybe, you get a Nothing back.
13:45:42 <dminuoso> If you pick IO, you get an IO exception
13:45:49 <dminuoso> And so forth
13:45:53 * hackage predicate-typed 0.7.4.5 - Predicates, Refinement types and Dsl  https://hackage.haskell.org/package/predicate-typed-0.7.4.5 (gbwey)
13:46:31 <jcd> Okay! Got it! How would I properly wrap it? Is there an approriate place to show a 3-line code example?
13:46:42 <dminuoso> What do you mean with "wrap it"?
13:46:49 <dminuoso> @where paste
13:46:49 <lambdabot> Help us help you: please paste full code, input and/or output at eg https://paste.tomsmeding.com
13:47:32 <jcd> https://paste.tomsmeding.com/5trqeFMa
13:47:57 <jcd> Would it be better to say 'assign'?
13:48:21 <dminuoso> jcd: I see. So since httpLbs puts you into IO already, you could chose IO for parseRequest too
13:49:01 <dminuoso> Also note that this use of `Maybe` is not valid
13:49:12 <dminuoso> Sadly, there's not a very explicit way to make the choice, it is inferred on usage
13:49:14 <dminuoso> Say, if you wrote:
13:49:44 <dminuoso> let f :: Maybe Request; f = parseRequest "https://google.de" in ...
13:50:01 <dminuoso> Then the choice is made in the type signature. Equivalently if you wrote:
13:50:11 <dminuoso> case parseRequest "https://google.de" of
13:50:14 <dminuoso>   Nothing -> ...
13:50:16 <dminuoso>   Just x -> ...
13:50:43 <dminuoso> Then the choice is implicit because GHC can infer Maybe from the data constructors in the pattern matching.
13:51:29 <dminuoso> For your case, IO is probably a fine choice. Then you can just write: https://paste.tomsmeding.com/USndzrdo
13:51:43 <dminuoso> (The choice of IO is inferred based on the context automatically)
13:52:53 * hackage phonetic-languages-simplified-common 0.2.1.0 - A simplified version of the phonetic-languages-functionality  https://hackage.haskell.org/package/phonetic-languages-simplified-common-0.2.1.0 (OleksandrZhabenko)
13:53:05 <jcd> Because of the 'do'!
13:53:12 <dminuoso> no, not the do
13:53:16 <jcd> Oh!
13:53:22 <jcd> the '<-'
13:53:27 <dminuoso> Also, no.
13:53:34 <dminuoso> If you keep guessing, you'll eventually figure it out.
13:53:36 <dminuoso> :)
13:53:48 <dminuoso> Well, the `do` notation is related, but not quite the point
13:53:55 <minimario> quick question: what's the logic behind making andM return Just False instead of Nothing when we call it on andM (Just False) Nothing?
13:55:12 <dminuoso> jcd: https://paste.tomsmeding.com/Xx49wa4s here there's two points of interest.
13:55:26 * ski . o O ( <https://hackage.haskell.org/package/hood>,<https://hackage.haskell.org/package/GHood>,<https://wiki.haskell.org/Debugging#Printf_and_friends> )
13:56:02 <dminuoso> jcd: First there's the type signature telling us the do-notation uses IO. You left it out, which gives us another point of inference
13:56:06 <dminuoso> The type of httpLbs
13:56:37 <jcd> Since the httpLbs is the expression that gets returned?
13:56:49 <dminuoso> Not because it's returned, just its presence.
13:56:55 <dminuoso> It could be on any line in the do block, in fact.
13:57:12 <dminuoso> Roughly, do-notation is just a syntax desugarer
13:57:37 <dminuoso> It desugars uses of line breaks and <- into (>>) and (>>=)
13:57:54 <dminuoso> % :t (>>)
13:57:55 <yahb> dminuoso: Monad m => m a -> m b -> m b
13:57:56 <dminuoso> % :t (>>+)
13:57:57 <yahb> dminuoso: ; <interactive>:1:1: error:; * Variable not in scope: >>+; * Perhaps you meant one of these: `>>=' (imported from Prelude), `>>' (imported from Prelude), `>>>' (imported from Control.Arrow)
13:57:59 <dminuoso> % :t (>>=)
13:58:00 <yahb> dminuoso: Monad m => m a -> (a -> m b) -> m b
13:58:03 <ski> minimario : it's short-circuiting. since the first action (successfully) executes to `False', we abort and don't try the second
13:58:18 <dminuoso> jcd: Do you see in each type, there's multiple occurences of the *same* m?
13:59:04 <dminuoso> jcd: Recall, same story as above! This is a polymorphic value, *you* decide what you want for each of those type variables. So if you make the choice `IO` for the first `m` in say (>>=), then the other `m` in (>>=) become IO too.
13:59:16 <ski> minimario : similarly, if you did `andM readLn readLn', and you typed in `False' for the first one, it would abort the `andM', and not try executing the second `readLn'
14:00:07 <dminuoso> jcd: It's roughly similar to if you had `a + b + c + d`, and if `a` was of type Int, then `b`, `c` and `d` have to be of type Int.
14:00:13 <dminuoso> % :t (+)
14:00:14 <yahb> dminuoso: Num a => a -> a -> a
14:00:14 <ski> minimario : if you don't want short-circuiting, you can use `liftM2 (&&)'/`liftA2 (&&)'. but the point here was exactly when you want short-circuiting
14:00:40 <jcd> dminuoso: Yeah, I do! That explains the errors my REPL was dumping out. 
14:01:30 <jcd> dminuoso: Thank you so much for the help! Recommend any literature so I don't have cry anymore.
14:01:49 <minimario> ski: ah ok that makes sense
14:02:02 <ski> @hoogle Monad m => (a -> m Bool) -> [a] -> m Bool
14:02:03 <lambdabot> Control.Monad.Extra anyM :: Monad m => (a -> m Bool) -> [a] -> m Bool
14:02:03 <lambdabot> Control.Monad.Extra allM :: Monad m => (a -> m Bool) -> [a] -> m Bool
14:02:03 <lambdabot> Extra anyM :: Monad m => (a -> m Bool) -> [a] -> m Bool
14:02:12 <ski> @hoogle Monad m => [m Bool] -> m Bool
14:02:13 <lambdabot> Control.Monad.Extra orM :: Monad m => [m Bool] -> m Bool
14:02:13 <lambdabot> Control.Monad.Extra andM :: Monad m => [m Bool] -> m Bool
14:02:13 <lambdabot> Extra orM :: Monad m => [m Bool] -> m Bool
14:02:18 <ski> similar thing, in those
14:02:46 <ski> minimario : ooc, which module were you looking in ?
14:02:49 <dminuoso> jcd: Im not good on book references. I keep suggesting CIS194 because it's the only reference material I looked at in the past 12 months and it looked decent.
14:03:16 <dminuoso> And it happens to be one of the few resources that's not as dated
14:03:44 <ski> @where CIS194
14:03:44 <lambdabot> https://www.seas.upenn.edu/~cis194/spring13/lectures.html
14:04:05 <minimario> oh lol i was just doing an intro haskell tutorial and one of the exercises was to implement andM haha
14:04:06 <dminuoso> There's also a more recent version of CIS194 from Joachim Breitner around, the style is slightly different.
14:04:14 <dminuoso> https://www.seas.upenn.edu/~cis194/fall16/
14:04:18 <ski> minimario : ah, okay
14:05:01 <moet> is there an equivalent to KnownNat(natVal) somewhere for types of kind Bool? .. or should i just define it?
14:05:02 <ski> dminuoso : any overheard opinion on the relative merits ?
14:07:38 <ski> minimario : btw, note that the `andM' you were using was a monadic version of the binary conjunction, while the one in e.g. <https://hackage.haskell.org/package/extra/docs/Control-Monad-Extra.html> is `Monad m => [m Bool] -> m Bool', a monadic version of `and :: [Bool] -> Bool', taking a list
14:08:31 <minimario> oh hm there are two versions?
14:08:32 <minimario> interesting
14:08:49 <minimario> second one is just a generalization i guess :)
14:09:51 <dminuoso> ski: No.
14:10:34 <dminuoso> I only skimmed Brent's version, so Im not equipped to compare them well enough. But I know Brent spoke well of Joachim's take on the course.
14:11:17 <dminuoso> And at least the entry seems a bit easier since the first weeks build up ontop of each other, giving coherent excercises leading to a sokoban clone.
14:11:20 <ski> minimario : one is a monadic generalization of `(&&)'. the other of `and'
14:12:39 <ski> dminuoso : ok. i vaguely recall someone (in here), suggesting the 2013 spring version (over others). but it's possible that was before fall of 2016
14:13:18 <dminuoso> ski: I picked up a few arguments for both, but I never noticed any criticism about either.
14:17:53 * hackage dense 0.1.0.1 - Mutable and immutable dense multidimensional arrays  https://hackage.haskell.org/package/dense-0.1.0.1 (cchalmers)
14:18:11 <johnw> I wonder why there is Data.ByteString.isInfixOf, but not Data.ByteString.Lazy.isInfixOf; the others are present
14:26:20 <dminuoso> johnw: Mmm, even the isSuffixOf implementation is a bit questionable.
14:26:31 <dminuoso> Guess the reason seems that bytestring has no streaming facilities
14:27:13 <dminuoso> Is https://hackage.haskell.org/package/text-1.2.4.0/docs/src/Data.Text.Internal.Lazy.Search.html#indices portable to bytestring perhaps?
15:01:25 <minimario> why does Data.List.init exist if take already exists?
15:02:53 <glguy> > init (repeat 'a')
15:02:55 <lambdabot>  "aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa...
15:02:58 <monochrom> How would you take to emulate init? You will run into an annoyance and an insurmountable problem.
15:03:04 <monochrom> err
15:03:07 <monochrom> How would you use take to emulate init? You will run into an annoyance and an insurmountable problem.
15:03:40 <glguy> monochrom: is the answer timeouts? ^_^
15:04:50 <minimario> oh i guess if the list is infinite
15:05:00 <minimario> but for finite lists they're equivalent, no?
15:05:16 <minimario> if i do something like init xs = take (length xs-1) xs or something
15:05:17 <glguy> it's undesirable to have to complete the whole length first before producing any list elements
15:05:26 <johnw> dminuoso: thanks for looking
15:05:35 <monochrom> Your "equivalence" probably ignores very important time and space complexities.
15:05:52 <minimario> can you explain that?
15:05:58 <minimario> they're both O(n) right?
15:06:02 <minimario> at least time
15:06:06 <minimario> i'm not sure how to reason about space
15:06:13 <monochrom> You are also ignoring lazy evaluation.
15:06:38 <monochrom> Suppose length xs = 10^10
15:06:59 <monochrom> "print (init xs)" can start printing the early elements right away.
15:07:10 <monochrom> It also stays in O(1) spacee.
15:07:46 <glguy> > head (init (1:2:undefined))
15:07:49 <lambdabot>  1
15:08:15 <monochrom> "print (take (length xs -1) xs)" will take its time to count the list length. Also when the counting is done, you're occupying 10^10 units of space memoizing the whole bloody list.
15:08:37 <minimario> oh i see
15:08:49 <monochrom> Factoring in thrashing, it is as good as taking forever.
15:09:22 <monochrom> Lt. Cmd. Data says, "believe me, for an android, 4.2 seconds feels like forever"
15:10:05 <monochrom> Now marvel at how I could pun on "take" so many times.
15:10:09 <minimario> haha makes sense, thanks guys:)
15:10:36 <minimario> just starting to learn these constructs, always feels very weird seeing what is inside Data.List and what is not
15:14:53 * hackage tart 0.3 - Terminal Art  https://hackage.haskell.org/package/tart-0.3 (JonathanDaugherty)
15:25:25 <texasmynsted> How does overloaded strings work?  Specifically right here --> https://gist.github.com/mmynsted/7a7d365b20159bfcbb02f5e8371f1c82#file-gistfile1-txt-L48
15:25:36 <texasmynsted> I do not think that I should need toText
15:25:45 <texasmynsted> But I do.
15:26:14 <monochrom> overloaded strings is for string literals only.  You have a variable.
15:27:12 <texasmynsted> well snap, that makes perfect sense then
15:34:44 <texasmynsted> I forget where that toText came from
15:36:18 <texasmynsted> oooh I bet from relude
15:38:15 <moet> glguy: picking up on our discussion from a few days ago: i'm embedding an external runtime in haskell via a c api and the Boolean package
15:38:46 <moet> eventually i settled on a type family to map haskell types to their representation in the external thing
15:39:02 <moet> and then i just implement all the numeric classes against the representation tiype
15:39:08 <moet> ish
15:43:14 <Axman6> monochrom: aww, you never showed ths zipWith const xs (drop 2 xs)
15:44:55 <texasmynsted> okay. why would relude's toText be found in the main app but not for the test?
15:49:09 <texasmynsted> yes. Okay that was it.
15:49:16 <texasmynsted> :-)
15:56:09 <texasmynsted> How does this ./test work for tasty? https://github.com/feuerbach/tasty#readme
15:56:29 <texasmynsted> I can run `cabal test`. 
15:56:39 <kokwok> how can i write sumNumbers x = sum (map read (wordsBy (not . isDigit) x)) with point free notation?
15:56:41 <texasmynsted> But I see no way to pass the parameters to the tests
15:58:23 <monochrom> sum . map read . wordsBy (not . isDigit)
15:59:10 <jcd> I humbly come back in search of assistance.
16:01:05 <texasmynsted> What is "When using the standard console runner," that is referenced in the Tasty readme?
16:01:22 <texasmynsted> I mean what is the "standard console runner"?
16:01:36 <texasmynsted> I would expect it to be "cabal test"
16:03:08 <dsal> texasmynsted: You can think of OverloadedStrings as replacing every occurrence of `"x"` with `fromString "x'`
16:03:11 <dsal> :t fromString
16:03:12 <lambdabot> IsString a => String -> a
16:03:30 <dsal> It doesn't mess up your quotes like I did, though.
16:03:37 <texasmynsted> gotcha
16:10:29 <texasmynsted> I can get the tasty options to be honored at compile-time but not at run time. I do not know where the runtime they describe exists. 
16:10:41 <texasmynsted> I can only execute the Tasty tests via "cabal test"
16:29:58 <jcd> How do you loop over a list of IO actions? 
16:30:58 <monochrom> the same way I loop over a list.
16:36:46 <sm[m]> texasmynsted: have a look at cabal test --help, there is --test-option[s]
16:37:17 <sm[m]> or you can probably run the test executable directly with something like cabal exec -- NAMEOFTESTEXE ...
16:40:02 <sm[m]> incidentally, you can also build your test suite into your main app: https://github.com/simonmichael/hledger/blob/master/hledger/Hledger/Cli/Commands.hs#L270
16:42:58 <minimario> tooling question: anyone come across the issue where "quick fix" on vscode loops and never fixes anything
16:56:16 <jcd>  
16:56:17 <jcd> I have an list of IO-contexted values, '[IO a]'. I have a function that removes a context 'B'. According to the documentation, I should be able to use the context-removing function on the list, but I can't. The IO context remains intact. Is there a special function/method/technique that I'm not using?
16:57:27 <glguy> jcd: You can't "remove" IO, you can only sequence IO actions together to make more interesting IO actions
16:57:45 <glguy> I don't know what context 'B' is though
16:59:02 <Ariakenom> " the documentation"  "the context-removing function"
16:59:07 <Ariakenom> what are these?
16:59:13 <jcd> https://paste.tomsmeding.com/k8d1Va08
16:59:56 <jcd> The library I'm using is Network.HTTP.Client.
17:00:10 <glguy> jcd:     let responses = map (get_response manager) search_urls   -- becomes --   responses <- mapM (get_response manager) search_urls
17:00:32 <monochrom> You should be looking into mapM_ or mapM, instead of map.
17:01:09 <Ariakenom> you can use ghci to do things interactively 
17:02:28 <Sose> is Data.List.intersect really slow on long lists? any alternatives? I have two lists that are both around 150k elements and I'm trying to find the intersections :D 
17:03:30 <monochrom> Or even better, write your own recursion, so you understand why map doesn't cut it.
17:03:52 <glguy> Sose: it's checking elements together pairwise, so yes it'll be quite slow for anything more than a handful of elements
17:04:04 <glguy> Sose: to do better you'll need something like Data.Set
17:04:20 <monochrom> intersect is quadratic time
17:04:50 <glguy> or to sort the lists and write a function that takes advantage of the fact that the lists are sorted to do it in one pass through the two lists
17:04:51 <Sose> alright, I'll look into Data.Set
17:04:55 <monochrom> My list is below two hundread, it's fast enough for me.
17:05:52 <jcd> I see! mapM is under 'Basic Monad functions'. Ouch haha. Okay thank you!
17:19:45 <Sose> okay, just doing a dumb fromList, intersection and toList with Set takes the time required from minutes to seconds.. thanks again
17:28:15 <fubu> Hello! 
17:28:35 <fubu> Why doesn't this function work?
17:28:37 <fubu> fibs2 = 1 : 1 : (head $ zipWith (+) fibs2 (tail fibs2))
17:32:24 <Ariakenom> fubu: head takes the first element out of a list. while : adds an element to a list. so in x : (y : z)  x is an element, y is an element, but z is a list.
17:32:49 <Ariakenom> so z shouldnt be an element, it should be a list
17:34:44 <Ariakenom> z in my exanple is your (head $ zipWith (+) fibs2 (tail fibs2))
17:39:47 <fubu> thank you. I fixed the function, but now i'm failing to see how it works
17:39:52 <fubu> fibs2 = 1 : 1 : (zipWith (+) fibs2 (tail fibs2))
17:40:26 <fubu> I kind of get it, but I don't see how the evaluation works in terms of what is on the stack at each point in time
17:41:26 <dsal> What's "the stack"?
17:41:29 <Ariakenom> you can do the step by step evaluation on paper
17:42:22 <fubu> hm, evaluation stack?
17:43:07 <dsal> That's a list, not a stack, though.  Thinking of a stack will be rather confusing.
17:43:23 <fubu> i'm also not sure how the two instances of fibs2 on the right hand side are connected
17:43:34 <dsal> zipWith does that.
17:43:35 <dsal> :t zipWith
17:43:36 <lambdabot> (a -> b -> c) -> [a] -> [b] -> [c]
17:43:37 <fubu> because i know if we do something like f x = g x x, then x only gets evaluated once
17:43:50 <fubu> oh yeah, i know how the terms are being added
17:44:00 <fubu> i'mi not too sure how it's getting evaluated though
17:44:14 <fubu> like you start with 1 : 1 : (zipWith (+) fibs2 (tail fibs2))
17:44:30 <fubu> and then it calls the first fib2 on the RHS to evaluate it
17:44:43 <Ariakenom> fubu, once or twice, like that, doesnt matter in this case
17:44:54 <dsal> > zipWith (+) [a, b, c, d] (tail [a, b, c, d]) :: [Expr]
17:44:56 <lambdabot>  [a + b,b + c,c + d]
17:45:10 <Ariakenom> you can start with a simpler version
17:46:02 <Ariakenom> nat = 0 : (map (+1) nat)
17:46:07 <dsal> So once you get past the [1,1] at the beginning.  It's just   an infinite stream of numbers added to the same infinite stream of numbers after dropping the first one.
17:46:15 <Ariakenom> or even simpler
17:46:24 <Ariakenom> zeroes = 0: zeroes
17:46:36 <fubu> yeah i understand the nat case
17:47:35 <dsal> > let fibs2 = a : b : zipWith (+) fibs2 (tail fibs2)  in   fibs2  :: [Expr]
17:47:37 <lambdabot>  [a,b,a + b,b + (a + b),a + b + (b + (a + b)),b + (a + b) + (a + b + (b + (a ...
17:48:10 <dsal> It's not super easy to think about that in terms of Peano numbers...
17:48:30 <dsal> > zipWith (+)   [1..5] [2..6]
17:48:32 <lambdabot>  [3,5,7,9,11]
17:48:54 <dsal> That's all it's doing.
17:49:03 <fubu> ok, i understand the zipWith now
17:49:14 <fubu> but what's the order in which things get evaluated?
17:49:19 <fubu> because at the first stage, you have something like
17:49:34 <fubu> 1 : 1 : (zipWith (+) [1,1,???] [1,???]) right
17:49:42 <fubu> where you only know the 1 : 1 exists
17:49:57 <fubu> so does it then lazily do the 1+1?
17:50:03 <fubu> and attach it to the list
17:50:07 <fubu> and then fill in the next step?
17:50:24 <fubu> and now it's 1 : 1 : (zipWith (+) [1, 1, 2, ...] [1, 2, ...] )
17:51:09 <Ariakenom> well you know the first value of the two zipWith lists. so you know the first value of that result
17:51:38 <fubu> but isn't + lazy?
17:51:45 <fubu> so the result will be stored as 1+1 right
17:52:07 <Ariakenom> zipWith (+) [1,1,..] [1,..] = 2: zipWith (+) [1,..] [..]
17:52:20 <Ariakenom> fubu, sure
17:52:51 <Ariakenom> if you print (show) 1+1 then you will get the string "2"
17:53:16 <dsal> "stored" is kind of a weird word.  I don't think it's "stored" anywhere.
17:53:54 <fubu> how does the evaluation process work?
17:54:03 <dsal> > let fibs2 = a : b : zipWith (+) fibs2 (tail fibs2)  in   fibs2  :: [Expr]
17:54:05 <lambdabot>  [a,b,a + b,b + (a + b),a + b + (b + (a + b)),b + (a + b) + (a + b + (b + (a ...
17:54:36 <fubu> like after compilation how does haskell know what order to do the lazy evaluation in?
17:55:08 <dsal> I'm not sure what you mean.  There's a clear dependency.
17:55:22 <Ariakenom> the things that are computed are forced by main::IO ()
17:55:41 <fubu> like
17:55:43 <dsal> Yeah, if you don't ever use the value, it's never computed, so it's not relevant.
17:56:04 <fubu> you have fibs2 = 1 : 1 : (zipWith (+) fibs2 (tail fibs2))
17:56:17 <fubu> are all 3 fibs2 variables linked to each other
17:56:35 <dsal> > let fibs2 = 1 : 1 : (zipWith (+) fibs2 (tail fibs2)) in head  fibs2
17:56:37 <lambdabot>  1
17:56:42 <fubu> so that when you compute a new value in fibs2, all 3 variables are automatically updated?
17:56:52 <Ariakenom> yeah, same variable
17:57:03 <Ariakenom> same memory object
17:57:39 <dsal> i.e., it's not a variable at all, and you're not so much computing values or updating things.  It's just a list.
17:58:20 <Ariakenom> note that things are always updated from less information to more information. they dont change previous information
17:58:33 <dsal> > let as = 1 : 2 : 3 : as  in    take 11 as  -- it just happens to include itself
17:58:35 <lambdabot>  [1,2,3,1,2,3,1,2,3,1,2]
17:59:03 <dsal> But if you were to evaluate this concept manually without thinking about how you'd do it in C or something, it'd be pretty clear.
17:59:15 <fubu> ah yes, i come from C :P
17:59:35 <dsal> Dropping some of that baggage can be hard.  :)
17:59:54 <dsal> You don't tell Haskell how to do things (mostly).  You tell Haskell what you want done.
18:00:23 <dsal> You want a list that's made up of itself?  You express it in the most simple way, and the runtime does the thing it could do to make that happen.
18:00:39 <Ariakenom> so (zeroes = 0: zeroes) the tail of the list actual is the same pointer as the beginning
18:00:39 <dsal> > let a = a in a  -- of course, it has to at least make sense
18:00:43 <lambdabot>  *Exception: <<loop>>
18:01:48 <dsal> But lists aren't necessarily slots of discrete values.  They can just be an infinite recipe list for producing values.
18:01:58 <dsal> > [ x^2 | x <- [1..] ]
18:02:00 <lambdabot>  [1,4,9,16,25,36,49,64,81,100,121,144,169,196,225,256,289,324,361,400,441,484...
18:02:13 <dsal> That's all the squares.  If you want the sum of all squares...
18:02:16 <dsal> > sum [ x^2 | x <- [1..] ]
18:02:22 <lambdabot>  mueval-core: Time limit exceeded
18:02:33 <dsal> lambdabot times out, but if you did that yourself and waited long enough, you'd know the sum of all squares.
18:03:20 <Ariakenom> (!) this claim is unverified
18:04:19 <dsal> That's true, I personally lack the patience to wait for it to finish.
18:06:04 <Ariakenom> (I just used a twitter.com meme in IRC)
18:06:12 <dsal> > sum . take 1000 $ [ x^2 | x <- [1..] ] -- but you can get a taste by just grabbing the first 1000
18:06:14 <lambdabot>  333833500
18:06:42 <dsal> Oh wow, I didn't even get the reference out of context.  heh
18:06:50 <Ariakenom> :D
18:08:28 <dsal> While it is true that that's what I asked the computer to do, it's also an example of a poorly selected algorithm.
18:21:17 <ski> dminuoso : ok
18:24:45 <kupi> > 1
18:24:47 <lambdabot>  1
18:25:23 <kupi> >foldl (+) 0 [0..]
18:26:12 <kupi> oh no
18:26:32 <dsal> You should've used foldl'
18:26:58 <kupi> i intentionally used the lazy version
18:27:12 <kupi> wanted to see how does it respond
18:28:16 <kupi> >"still alive"
18:29:17 <texasmynsted> sm[m]: Okay I will try your suggestions. Thank you.
18:29:33 <dsal> kupi: lambdabot supports private intimate chats as well.
18:29:51 <Ariakenom> kupi, space after > 
18:31:09 <dsal> Oh, my client doesn't render it that way.  It automatically puts it into some kind of markdown quote format.
19:06:12 <Axman6> lambdabot also doesn't respond if you use the wrong syntax...
19:14:07 <siraben> Anyone working with GHC 8.12's linear types with a Nix overlay?
19:18:23 * hackage Z-IO 0.1.8.1 - Simple and high performance IO toolkit for Haskell  https://hackage.haskell.org/package/Z-IO-0.1.8.1 (winterland)
20:06:41 <hoker> hi, so I was going through CIS194 and got stuck on this last part, "supply monad": https://www.seas.upenn.edu/~cis194/fall16/hw/07-laziness.html
20:07:01 <hoker> i was able to write the monad functions, but didn't understand how to label the leaves of a tree with natural numbers
20:31:52 <c_wraith> hoker: the idea is to use the supply to generate a list of natural numbers.  Then recurse through the tree, replacing the node value with the next value from the supply when you hit a leaf.
20:32:23 <hoker> yeah, so I didn't actually understand the point of mapSupply, mapSupply2, and bindSupply
20:32:54 <c_wraith> those are the implementation for the Functor/Applicative/Monad instances
20:33:06 <c_wraith> Since you have those, you can just use do notation.
20:34:58 <c_wraith> The next section shows using do notation in a similar way.  With a different type, but it shows the same ideas.
20:35:22 <hoker> yeah, can you explain intuitively what the functor/applicative/monad instances mean?
20:36:01 <hoker> like i guess mapSupply is like taking some stream labeler and returning a new stream labeler
20:36:08 <hoker> where the transformation comes from the function
20:36:13 <c_wraith> not quickly. They're... more abstract than most people are ready to grasp the first time they run into them. 
20:37:30 <c_wraith> at an extremely vague level - Functor lets you modify inside a type. Applicative lets you combine values in a fixed way. Monad lets you combine them in a context-sensitive way.
20:37:38 <c_wraith> But that's really too vague to mean much.
20:37:54 <hoker> oh i've seen them a few times, i just meant in the context of supply specifically
20:38:01 <hoker> i'm not too comfortable
20:38:04 <hoker> but i'm familiar
20:40:03 <c_wraith> the only interesting part from the perspective of using it is the value named get.  (It's not a function, at least from the outside)
20:40:17 <c_wraith> You can bind the result of get to fetch the next value from the supply
20:41:37 <c_wraith> The various instances you have take care of chaining the new value forward afterwards.  The nice thing there is that you don't have to handle parameters carefully to make sure you don't mess up the handling of the stream.  They're just taken care of for you.
20:43:23 <c_wraith> Given the starting point in the last exercise, you should be able to do something like   runSupply nats (do x <- get ; y <- get ; return (x, y))   and get a result of (0,1)
20:44:52 <c_wraith> Does that make it any clearer what's going on there?
21:42:53 * hackage optparse-applicative 0.16.1.0 - Utilities and combinators for parsing command line options  https://hackage.haskell.org/package/optparse-applicative-0.16.1.0 (huw)
21:49:29 <perry69420> I found a minor error here - https://wiki.haskell.org/Dynamic_programming_example . How can I edit the page? (I do not have an account, making the account is a long process)
21:54:15 <perry69420> This line "Optional: If you know Applicatives and that Maybe is an Applicative, you can write it in a more regular way: " should be using Alternative, not Applicative
22:19:06 <ski> perry69420 : what wording do you think would be more appropriate ?
22:20:56 <ski> replacing both occurances of "Applicative" with "Alternative" ?
22:25:11 <perry69420> ski yeah, I think both should be swapped. I guess it should be mentioned that Alternative required Applicative for clarity but that's a secondary issue
22:26:12 <ski> perhaps saying "If you know Applicatives & Alternatives and ..." for the first one ?
22:28:18 <perry69420> That seems good. But should it mention Functor as well then?
22:28:59 <ski> could say "Applicative and Alternative functors", i suppose
22:29:11 <perry69420> That sounds better!
22:34:22 <ski> ("is an Alternative" or "is Alternative" ?)
22:35:24 <perry69420> I'm not sure, sorry! English isn't my first language
22:36:58 <perry69420> "If you know Applicative and Alternative Functor classes and that Maybe is an Alternative". Is this fine?
22:37:00 <ski> i think people tend to say "Applicative functor", and not just "Applicative", as a noun. but "Applicative"/"Alternative" as an adjective could work
22:39:17 <ski> i think i'd lean towards the adjectival form here. if someone objects, they can change it
22:39:20 <perry69420> then "is Alternative" makes more sense
22:39:23 <perry69420> Sounds good
22:40:31 <ski> (for "Monad", i don't think it works using it as an adjective. "monadic" is used, but not for quite the corresponding thing)
22:41:52 <ski> (now i'm wondering whether to say "know about" or just "know")
22:42:14 <perry69420>   haha I think you're overthinking. 
22:42:56 <ski> perry69420 : refresh
22:43:34 <perry69420> Thanks ski. I'll mail them to make me an account as well
22:45:19 * ski nods
23:31:52 * hackage pandora 0.3.2 - A box of patterns and paradigms  https://hackage.haskell.org/package/pandora-0.3.2 (iokasimovmt)
23:38:50 <Sose> is there a way to make "brittany" always format "where" the same way? funnily enough in my current file it seems to alternate every time and it looks a bit funny imo...
23:41:08 <Sose> wait a sec.. it just did that suddenly, or atleast left them alone after I made them all look similar
