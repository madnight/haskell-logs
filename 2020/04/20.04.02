00:03:43 * hackage expresso 0.1.2.3 - A simple expressions language based on row types  https://hackage.haskell.org/package/expresso-0.1.2.3 (willtim)
00:46:08 <gentauro> is there an example on how to use the `.cabal` file to create several binaries? I mean, my project will re-use a lot of code between those binaries :)
00:46:13 * hackage math-grads 0.1.6.7 - Library containing graph data structures and graph algorithms  https://hackage.haskell.org/package/math-grads-0.1.6.7 (AlexKane)
00:46:58 <gentauro> something like `build.bash` (CTRL+F) http://blog.stermon.com/articles/2018/08/13/haskell-safe-base64-only-depending-on-prelude.html
00:47:32 <gentauro> (I want to use .cabal + stack to do the build, instead of my writting `ghc …`)
00:47:35 <gentauro> :)
00:47:37 <opqdonut> gentauro: IIRC you define a library in your cabal file with the shared code, and then multiple executables
00:48:38 <gentauro> opqdonut: I tried that
00:49:44 <gentauro> but when I renmae my `Main.hs` (containing the `main` funciton) to `Foo.hs` and then copy it to `Bar.hs`) but then it complains with: "output was redirected with -o, but no output will be generated because there is no Main module."
00:49:48 <gentauro> :(
00:50:20 <gentauro> I mean, if the module containing `main` needs to be called `Main.hs`, then this will not be possible amirite?
00:50:48 <opqdonut> you can have a Foo.hs that still contains "module Main"
00:50:55 <opqdonut> or perhaps you need to have Foo/Main.hs and Bar/Main.hs
00:51:22 <opqdonut> do you have main-is: Foo.hs in your cabal file?
00:51:55 <gentauro> opqdonut: oh
00:52:12 <gentauro> so Foo.hs should still have the module Main
00:52:16 <gentauro> that actually works :o
00:52:31 <gentauro> I thought that GHC would complain if the filename didn't match the module name
00:52:41 <gentauro> you always learn something new :)
00:52:42 <gentauro> (thx)
00:53:05 <opqdonut> I was just guessing around, good that you got it working
00:53:22 <gentauro> :)
00:56:53 <gentauro> just another question. Should files be named in Pascal-case? I see many examples where they are just lowercase all the way (example: `main.hs`)
00:57:05 <gentauro> what are the `best-practices / conventions` for this?
00:57:36 <gentauro> (it's annoying having to `SHIFT + m` everytime I want to edit the main file) :)
01:00:19 <jackdk> I have just installed haskell-platform for windows, and it is failing to build packages with build-type:Configure. I confirm that the mingw/msys paths are in %appdata%/cabal/config and contain sh.exe etc, but cabal does not seem to think that a posix-style sh is available. what am I missing?
01:04:54 <sclv> jackdk: you can run with verbose to see more traces
01:12:43 * hackage rio 0.1.15.0 - A standard library for Haskell  https://hackage.haskell.org/package/rio-0.1.15.0 (MichaelSnoyman)
01:15:03 <jackdk> sclv: okay, wtf. My cabal config had `extra-prog-path: C:\program files\haskell platform\8.6.5\msys\usr\bin, c:\users\me\appdata\cabal\bin`
01:15:19 <jackdk> without the comma, I get errors about not being able to run `configure`
01:15:28 <jackdk> correction, with the comma
01:15:48 <jackdk> if I create two distinct extra-prog-path lines, build-type:Configure packages successfully build
01:17:09 <sclv> yeah the syntax on that field is jank
01:18:35 <jackdk> this is a serious stumbling block for people starting out. known bug?
01:43:13 * hackage hsinspect 0.0.12 - Inspect Haskell source files.  https://hackage.haskell.org/package/hsinspect-0.0.12 (tseenshe)
01:44:50 <jophish> chessai: Hi, you mention here that support for backpack in haskell.nix is stopping you from switching from the nixpkgs infrastructure: https://github.com/input-output-hk/haskell.nix/issues/244#issuecomment-532149792
01:45:02 <jophish> are you using backpack along with the nixpkgs infrastructure?
01:45:14 <jophish> because I though that was not really in a usable state
01:46:13 * hackage dobutokO2 0.22.0.0 - A program and a library to create experimental music from a mono audio and a Ukrainian text  https://hackage.haskell.org/package/dobutokO2-0.22.0.0 (OleksandrZhabenko)
01:50:43 <chessai> jophish: yes, i used to do so very frequently, and recently not as much but still a bit
01:51:58 <chessai> jophish: it is in a usable state, some things are still broken
01:54:13 * hackage dobutokO2 0.22.1.0 - A program and a library to create experimental music from a mono audio and a Ukrainian text  https://hackage.haskell.org/package/dobutokO2-0.22.1.0 (OleksandrZhabenko)
02:06:42 <jakalx> gentauro: case insensitive tab completion to the rescue :)
02:17:12 <gentauro> jakalx: xD
02:38:39 <zincy_> I have a list of tuples where the second element in each tuple is a list. [(a, [b])]
02:39:03 <zincy_> How do I run a monadic action over  my `b`s
02:39:29 <zincy_> and then use traverse to get    m [(a, [b])]
02:40:18 <[exa]> zincy_: so you have say `f :: Monad m => b -> m ()`  ?
02:40:25 <zincy_>       traverse (second ((<$>) monadicAction)) initList
02:40:29 <zincy_> That was my failed attempt
02:41:13 <zincy_> [exa]: Sorry it would return a `m b`
02:43:25 <zincy_> so f is `f :: Monad m => b -> m b`
02:43:32 <Clint> so you want to [(a, [m b])] -> m [(a, [b]) ?
02:44:10 <merijn> :t \f -> traverse (second (traverse f)) -- am I getting close?
02:44:11 <lambdabot> (Monoid d, Traversable t1, Traversable t2, Applicative f) => (a -> f b) -> t1 (d, t2 a) -> (d, t1 (f (t2 b)))
02:44:28 <merijn> ah, not quite
02:45:16 <merijn> :t \f -> traverse (traverse (second (traverse f))) -- just add more traverse?
02:45:18 <lambdabot> (Monoid d, Traversable t1, Traversable t2, Traversable t3, Applicative f) => (a -> f b) -> t1 (t2 (d, t3 a)) -> (d, t1 (t2 (f (t3 b))))
02:45:43 <merijn> oh, then the second is redundant, I guess
02:45:55 <merijn> :t \f -> traverse (traverse (traverse f))
02:45:58 <lambdabot> (Applicative f, Traversable t1, Traversable t2, Traversable t3) => (a -> f b) -> t1 (t2 (t3 a)) -> f (t1 (t2 (t3 b)))
02:46:54 <merijn> :t \f -> traverse (traverse (traverse f)) :: (Int -> IO Bool) -> [(Char, [Int])] -> IO [(Char, [Bool])]
02:46:56 <lambdabot> error:
02:46:57 <lambdabot>     • Couldn't match type ‘Int -> IO Bool’ with ‘IO [(Char, a)]’
02:46:57 <lambdabot>       Expected type: (Int -> IO Bool)
02:47:37 <zincy_> clint: Yes
02:47:54 <Clint> zincy_: why do you want to do that?
02:48:19 <[exa]> :t (traverse.traverse.traverse) :: Monad m => (b -> m b) -> [(a,[b])] -> m [(a,[b])]
02:48:22 <lambdabot> Monad m => (b -> m b) -> [(a, [b])] -> m [(a, [b])]
02:48:36 <[exa]> (am I using the wrong type?)
02:48:54 <zincy_> Clint: That is a good question, I am finding a local maxima as opposed to a global right now :D
02:49:03 <[exa]> anyway this deserves some shortcut for `traverse^3`
02:52:52 <zincy_> Nice I never would have thought traverse is so powerful
02:53:08 <zincy_> I am trying to wrap my head around how the `a` in the fst element is untouched
02:54:22 <zincy_> It works anyway thanks!
02:54:23 <Clint> because of Functor (,)
02:54:45 <zincy_> Clint: Ah yes, thanks
02:55:20 <zincy_> Why does the functor instance pick the second element to map over
02:55:26 <zincy_> Seems like an arbritrary choice
02:58:05 <merijn> zincy_: It's the only possible one
02:58:32 <merijn> The correct Functor for tuple is "instance Functor ((,) a)"
02:58:51 <merijn> Functor's only ever take one argument, therefore it *has* to be the last one
02:59:42 <merijn> (Unless you had a type level flip which can be partially applied (which can't exist in Haskell)
03:10:48 <jpcooper> Hello. Could anyone try running a `stack update`? I get: Failed populating package index cache, BadChecksum 0
03:21:34 <zincy_> merijn: Ah thanks
03:23:06 <dminuoso> zincy_: Remember that (,) :: * -> * -> *
03:23:57 <dminuoso> And our type system does not allow for something like `instance Functor (, a)` (assuming a type level tuple-section kind of notation)
03:25:59 <dminuoso> newtypes let you get around that restriction though.
03:26:36 <dminuoso> So you could have a `newtype Fst s t = (t, s)` with some `instance Functor Fst a`
03:26:48 <dminuoso> `instance Functor (Fst a)` I mean.
03:54:33 <zincy_> dminuoso: That makes sense
04:00:31 <merijn> Ugh, why did network drop support for iNADDR_ANY
04:07:17 <merijn> Does anyone know how to achieve iNADDR_ANY with the new network API?
04:08:32 <cdunklau> merijn: "{-# DEPRECATED iNADDR_ANY "Use getAddrInfo instead" #-}"
04:09:24 <merijn> cdunklau: It doesn't exist in 3.1 at all
04:10:56 <cdunklau> merijn: that was from 2.7.0 i think
04:13:43 * hackage prim 0.1.0.0 - An ergonomic but conservative interface to ghc-prim  https://hackage.haskell.org/package/prim-0.1.0.0 (dailectic)
04:17:20 <cdunklau> merijn: the changelog entry is not all that enlightening either https://github.com/haskell/network/blob/2.7/CHANGELOG.md#version-2700
04:17:25 <cdunklau> ¯\_(ツ)_/¯
04:17:36 <ph88> I would like to re-use code from this package https://gitlab.com/joneshf/purty i have two problems: it's not published on hackage/stackage and also i don't know how i can build it from source. Any advice on how to proceed ?
04:17:52 <cdunklau> i'd like to be of more help, but i've never touched this package
04:20:40 <merijn> ph88: That appears to be purescript code, so why would it be on hackage or stackage?
04:21:05 <merijn> oh, it's haskell pretty printer *for* purescript
04:21:31 <merijn> ph88: Looks like it needs to be build with bazel
04:26:43 * hackage prim 0.1.0.1 - An ergonomic but conservative interface to ghc-prim  https://hackage.haskell.org/package/prim-0.1.0.1 (dailectic)
04:39:43 <ph88> merijn, i was considering to just copy the source files and make a new project so not having to deal with bazel ... but maybe it's a bad idea, what do you think ?
04:40:35 <merijn> No clue, I also don't know what license they use
04:44:40 <ph88> looks like some self created license https://github.com/joneshf/purty/blob/master/LICENSE 
04:45:00 <merijn> That looks like BSD3
04:45:38 <merijn> Like, I'm 98% certain that's BSD3
04:57:16 <ph88> ah ok
04:57:23 <ph88> is it possible to rename a function on import ?
04:58:17 <sm[m]> try getting it to build with cabal & stack and then contribute that ?
04:58:18 <sm[m]> (ph88)
04:59:04 <sm[m]> that's a quite unusual setup for a haskell project, in the wild at least
04:59:57 <tdammers> ph88: no
05:00:10 <tdammers> ph88: you can create an alias for it though, and possibly also re-export that
05:00:12 <tdammers> e.g.:
05:00:19 <tdammers> import qualified Data.Text as Text
05:00:29 <tdammers> packText = Text.pack
05:00:29 <sm[m]> I guess step 1 would be ask the maintainer's advice
05:01:25 <tdammers> but yes, contact maintainer
05:02:23 <ph88> ok thank you :)
05:03:03 <tdammers> also, you don't need to ditch the bazel stuff just to make it a cabal project; you can probably make a .cabal file that can sit next to the bazel stuff in the same repo
05:06:00 <tdammers> from a quick glance, it looks like a fairly vanilla .cabal file should do the trick
05:11:46 <gentauro> merijn: probably the best piece written on Haskell build tools -> https://gist.github.com/merijn/8152d561fb8b011f9313c48d876ceb07
05:12:02 <gentauro> I <3 it !!! (I'm really liking the .cabal files now)
05:13:20 <merijn> Yeah, I noticed it's somehow doing the rounds on reddit/twitter/etc. now, I would've probably written something better if I expected it spreading wider than just me linking it here because I'm lazy :p
05:13:52 <gentauro> merijn: can we just get it `pinned` on /r/haskell/ xD
06:20:43 * hackage composition-prelude 3.0.0.0 - Higher-order function combinators  https://hackage.haskell.org/package/composition-prelude-3.0.0.0 (vmchale)
06:33:17 <jollygood2> I have data data MonthName = January, Feb... ; and old data type data Month = { monthYear :: Int, monthMonth :: Int }. should I make monthMonth MonthName, rather than Int? suggestion for better names would be appreciated too
06:38:13 * hackage hpqtypes 1.9.0.0 - Haskell bindings to libpqtypes  https://hackage.haskell.org/package/hpqtypes-1.9.0.0 (arybczak)
06:53:13 * hackage persistent-generic 0.1.0.0 - Derive Persistent classes generically  https://hackage.haskell.org/package/persistent-generic-0.1.0.0 (DavidJohnson)
06:55:06 <dmj`> jollygood2: data Month = Month { _year, _month :: Int }
06:57:51 <jollygood2> you prefer month being Int, over MonthName?
06:59:10 <gentauro> jollygood2: I would prefere at sumtype implementing the Enum instance :)
07:03:54 <jollygood2> you mean data MonthName = .. deriving (Enum, Bounded), or something else? I already have that [plus some more things], but it wasn't relevant for my question
07:04:12 <dminuoso> jollygood2: It's hard to say. There's a lot of things where an explicit sum type gives more robust programs, but it comes at the cost of ease of use.
07:04:59 <merijn> tbh, I would just use the types from the time package :p
07:06:17 <dminuoso> One important factor is how you use the sumtype in your program. If you never discriminate on values of it in code, or have literals flying around, the value of a sum type greatly diminishes.
07:06:55 <dminuoso> But indeed, if a well designed library has explicit support, and date/time is certainly an area where it's not the best idea to roll it yourself, go with the library instead.
07:13:07 <jollygood2> there isn't a type in time package that store just a year and month. using fromGregorian with a dummy day rubs me the wrong way
07:13:32 <jollygood2> stores*
07:32:43 * hackage file-path-th 0.1.0.0 - Template Haskell utilities for filepaths.  https://hackage.haskell.org/package/file-path-th-0.1.0.0 (dfithian)
07:35:23 <dmj`> jollygood2: sometimes ya gotta roll your own
07:53:07 <merijn> Why does getAddrInfo only ever return 127.0.0.1, that's...useless
07:56:15 <[exa]> merijn: it shouldn't? why?
07:57:01 <merijn> [exa]: Well, how else do I figure out which IP to bind to
07:57:31 <merijn> They got rid of iNADDR_ANY, with the note to use getAddrInfo, I try and use getAddrInfo and it just always returns 127.0.0.1...
08:00:23 <[exa]> merijn: so you are trying to guess a specific IP address on an interface for binding?
08:00:34 <[exa]> ( instead of 0.0.0.0 )
08:01:32 <merijn> [exa]: I *expect* getAddrInfo to return a list of *all* IPs so I can pick which one(s) to bind
08:06:39 <glguy> merijn: use the passive flag
08:08:59 <[exa]> merijn: also, picking IPs automatically is hard, you don't know which ones are bound to say a private VPN tunnel, some protected subnet, or plain unusuable used in a bluetooth PAN
08:09:11 <[exa]> highly suggest you just leave that for the administrator 
08:09:23 <merijn> glguy: That's what I also tried, but then I didn't get the messages I expected to receive
08:10:22 <merijn> [exa]: Yeah, but none of that is relevant for me >.>
08:10:58 <glguy> You can get multiple results if you don't specify that you wanted ipv6 or ipv4 wildcards, and it matters if your OS maps ipv4 connections into ipv6 addresses or not. When multiple answers come back you need to bind all of them
08:11:14 <merijn> glguy: I'm not getting multiple results, though
08:11:23 <merijn> glguy: I get a list with a single element, which refers to 127.0.0.1
08:11:27 <[exa]> merijn: anyway I'm not sure if there's a portable way for listing IP addresses. You would need DNS names for local addresses (i.e. properly filled in localhost in hosts) for that to work
08:11:44 <glguy> No, you won't get localhost with the passive flag
08:11:45 <[exa]> merijn: if you're linux only, you better use the iproute API which is DNS-independent
08:12:12 <merijn> glguy: With AI_PASSIVE I get 0.0.0.0, but then I don't see any traffic on the port, despite my C program getting packets there
08:12:23 <glguy> Then you have a different problem
08:12:42 <glguy> The zero address is the old any constant you used to use
08:16:43 * hackage cobot-io 0.1.3.1 - Biological data file formats and IO  https://hackage.haskell.org/package/cobot-io-0.1.3.1 (ozzzzz)
08:28:06 <amf> in hedgehog, is there a "simpler" (e.g. based on deriving) for sum types. this example seems like it could be reduced for sum types without any recursion/products https://github.com/hedgehogqa/haskell-hedgehog/blob/master/hedgehog-example/src/Test/Example/STLC.hs#L157
08:45:37 <amf> there is Hedgehog.Gen.enumBounded which looks like what i want
08:50:01 <frdg> genericLength :: Num a => full -> a   I was exploring Hoogle and I came across this type signature. What does "full" refer to?
08:51:52 <tabaqui1> I'm playing with the singletons library now
08:52:13 <tabaqui1> But I'm stuck with chars (and strings)
08:52:38 <lyxia> frdg: it's a type variable
08:52:49 <opqdonut> frdg: it's just a type variable like a
08:52:49 <tabaqui1> I guess, that I can defined my own "PrintableChar = CapitalA | CapitalB" and "instance SignKind where type Demote PrintableChar = Char..."
08:53:09 <tabaqui1> but it looks overextensive
08:53:10 <opqdonut> the full type seems to be
08:53:13 <opqdonut> genericLength :: (ListLike full item, Num a) => full -> a
08:53:24 <tabaqui1> can I optimise it somehow.
08:53:26 <tabaqui1> ?
08:53:39 <tabaqui1> What I really want is to make singletons for filepaths
08:53:56 <tabaqui1> like [[Char]]
08:54:31 <frdg> Lyxia, opqdonut: Right. Do they call it full to say that you shouldn't pass it an infinite list?
08:55:44 <opqdonut> frdg: no, I think it's just a naming convention for the ListLike constraints, they're always "ListLike full item"
08:55:44 <frdg> cause of course you can't take the length of an infinite list
08:55:55 <opqdonut> so fill returns to the whole list, item refers to the type of the element
08:56:08 <frdg> opqdonut: ok thanks!
08:56:23 <opqdonut> something like `head :: ListLike full item => full -> item` demonstrates this better perhaps
09:05:12 <maralorn> Do I have to do anything, and what can I do to make sure a tcp-server lauchend with forkIO get‘s cleaned up nicely at the end of my app/ or when the calling thread get‘s killed?
09:06:29 <dsal> maralorn: Use async instead?
09:11:09 <ph88> how can i put one of the project dependencies into the project itself ? i'm building with stack
09:12:29 <maralorn> dsal: If I use "void . async" instead of "void . forkIO" what I was doing up to now. Would that already change anything or would I need to write more logic by myself?
09:13:57 <maralorn> Or do I need to call "withAsync" (which looks like it will get me into a kind of js-like callback cascade) …
09:14:06 <dsal> The async package has a lot of higher abstractions for doing things with multiple threads, making cleanup easier.
09:14:27 <dsal> If you're talking about just the tcp connection itself, bracketing is a good way to handle that.
09:14:36 <dsal> withAsync is good for chaining a bunch of threads together.
09:14:45 <dsal> (or just Conc and similar)
09:20:13 * hackage hsexif 0.6.1.8 - EXIF handling library in pure Haskell  https://hackage.haskell.org/package/hsexif-0.6.1.8 (EmmanuelTouzery)
09:21:24 <maralorn> dsal: Thanks I‘ll give it a shot. I suspect withAsync will be sufficient.
09:22:39 <dsal> Do you have a fixed number of threads?  concurrently is pretty good.  mapConcurrently is handy.  If you need to cleanup other resources, bracket is the way.
09:34:13 <dimsuz> Hi! I'm learning Yesod and writing a simple program to seed the DB using Persistent. It turns out I'm writing a lot of identical functions which differ only in type specification inside, but I don't know how to generalize this. They all look like this: https://gist.github.com/dimsuz/f8c1008538252c108b383e84f6f6a3bf. The devil lies witin
09:34:14 <dimsuz> 'deleteWhere' line. Can these two functions become one? :)
09:36:13 <shafox> I am building a REST-ful API. Which library should I be looking for building a server and api as well ? 
09:36:46 <sm[m]> could you clarify that question :)
09:36:58 <dsal> I built my GoPro stuff with scotty and it's been pretty good.
09:38:09 <shafox> sm[m], As in web frameworks however only the api part, template rendering not required as I am going to provide json as my response. 
09:40:17 <frdg> Im trying to write my own unfoldr. What kind of function would have this signature? (b -> Maybe (a, b)) 
09:40:50 <zincy_> If a parent thread dies after calling forkIO do all the child threads die too?
09:41:27 <sm[m]> shafox: pretty much any of them is fine for serving json. scotty mentioned by dsal is the simplest. servant and yesod are the two popular industrial strength ones. postgrest is the best if you are api-ing a big postgres db 
09:45:06 <oats> :t (\n -> if n `mod` 2 == 0 then Nothing else Maybe (n*2, n-1))
09:45:07 <lambdabot> error:
09:45:07 <lambdabot>     • Data constructor not in scope: Maybe :: (b, b) -> Maybe a
09:45:07 <lambdabot>     • Perhaps you meant variable ‘maybe’ (imported from Data.Maybe)
09:45:08 <dsal> I kind of want to try out servant, but the web part is very much secondary to my project.
09:45:14 <oats> :t (\n -> if n `mod` 2 == 0 then Nothing else Just (n*2, n-1))
09:45:16 <lambdabot> Integral b => b -> Maybe (b, b)
09:45:22 <oats> frdg: there's one for you ^ :)
09:46:41 <frdg> oats: thanks!
09:47:32 <frdg> ...ohhh I see now !
09:48:41 <oats> :t (\s -> if length s == 0 then Nothing else Just (fromEnum (head s), tail s))
09:48:43 <lambdabot> Enum a => [a] -> Maybe (Int, [a])
09:48:47 <oats> there's another :)
09:50:43 * hackage hanabi-dealer 0.7.2.0 - Hanabi card game  https://hackage.haskell.org/package/hanabi-dealer-0.7.2.0 (SusumuKatayama)
09:53:43 <maralorn> dsal: Yeah, I settled for concurrently_ works fine. But now I am a bit puzzled about a function which should fork but return a non blocking IO. I can‘t use withAsync in there.
09:55:30 <dsal> Mixing threads and nonblocking IO seems like it could be a bit confusing.
09:58:05 <tabaqui1> Back to my singletons question. Well, I am able to use Church nats to encode my literals from "[-.0-9A-Z_a-z]", but I still need to rewrite all instances manually. It is OK
10:10:09 <maralorn> dsal: I feel like it‘s just not supposed to happen. The code calling my non blocking IO does not now that there is something to be possibly killed there.
10:11:25 <dsal> Yeah, I don't understand all of what you're doing, but just using blocking IO seems more straightforward.  You mean have measured threads as being too expensive, though.
10:14:02 <maralorn> dsal: I don‘t understand that last part. What could be to expensive?
10:20:45 <dsal> maralorn: The only reason to avoid blocking IO is to multiplex the reads/writes yourself to avoid having threads monitoring them.  If having a thread service each connection is cheap enough, you'd just do that and only having blocking IO.
10:22:24 <monochrom> non-blocking I/O always entails reinventing your own OS.
10:24:09 <monochrom> likely the #1 reason why programs that use their own select-loops are perpetually buggy and insecure.
10:28:11 <maralorn> In my case I have the reflex-frp runtime running which is a blocking IO action. I want to start a service that continuously listens on a socket and then triggers an frp event when it receives something. This is very easy to do since the monad I execute in the runtime has MonadIO and I can just forkIO my listener there. But I can‘t use withAsync because it would block my runtime, wouldn‘t it?
10:28:44 <maralorn> Uh, I feel like I am having some kind of unliftIO problem here …
10:41:58 <dsal> I did a gross unliftIO of a StateT in my recent project.  
11:24:52 <ph88> how can i put another package as part of my project ?
11:28:43 <hseg> hi. i'm stuck with a design issue: am representing Laurent polynomials over a ring r as Map (Vector n Integer) r. Would want to write sub :: Laurent n r -> Vector n (Invertible r) -> r, where Invertible r is the group of units of r 
11:28:54 <gentauro> if I define: `newtype Foo a = Foo [ a ] deriving (Eq, Ord, Show)` would this imply that `a` is also deriving: `(Eq, Ord, Show)`?
11:29:02 <hseg> but it is unclear how/whether it's possible to do so
11:29:36 <hseg> (laurent polynomials = polynomials in x, x^-1)
11:30:10 <gentauro> I mean, I can do: `Foo [ Bar ]` unless `data Bar = … deriving (Eq, Ord, Show)` right?
11:30:19 <gentauro> s/can/can't/
11:30:39 <ph88> gentauro, you can write your own instances to avoid this i think
11:31:21 <ph88> but you probably don't want that in this case. I'm confused why you are asking about it
11:31:57 <gentauro> newtype Foo a = Foo [ a ] deriving (Eq, Ord, Show)
11:32:38 <hseg> _could_ make things partial, and in fact they already are due to needing to call `recip`
11:32:55 <gentauro> and `data Bar = Bar`. I can actually do this: `fs = Foo [ Bar ]` and `:t fs` results in `fs :: Foo Bar`. So my constraints aren't really uphold
11:33:12 <hseg> but that would require requiring a partial Fractional instance on r
11:33:16 <gentauro> my Bar data type is just a simple constructor
11:33:22 <hseg> which is suboptimal
11:33:38 <gentauro> but should't GHC enforce the constraints of Eq, Ord, and Show?
11:34:20 <hseg> gentauro: Do you mean to ask whether newtype Foo a = Foo [a] deriving Eq creates an instance of the form instance Eq a => Eq Foo ?
11:34:39 <gentauro> hseg: yes
11:34:54 <hseg> because it doesn't, but the latter is derived from the instance it does write, viz. Eq [a] => Eq Foo
11:35:06 <hseg> .. that should be Eq (Foo a)
11:35:35 <ahri_> I'm trying to use megaparsec to parse an integer (>0) followed by a newline character, from a byte stream with 'type Parser = ParsecT Void B.ByteString IO' - I seem to be missing something because I was trying to define 'integer :: Parser Integer; integer = L.lexeme (pure ()) L.decimal' - this doesn't work because "Couldn't match type ‘Word8’ with ‘Char’ arising from a use of ‘L.decimal’" but 
11:35:36 <gentauro> hseg: sow lets forget I'm using a list
11:35:41 <ahri_> I've not used MegaParsec before and I'm a bit confused about what I ought to b doing - do you have any hints?
11:36:12 <hseg> gentauro: newtype Foo a = F (Bar a) deriving Eq would generate Eq (Bar a) => Eq (Foo a)
11:36:25 <gentauro> lets assume I'm doing: `newtype Foo a = Foo a deriving (Eq, Ord, Show)`. Would `a` be contextuallized to `Eq, Ord, Show` at compile time?
11:36:33 <hseg> yes
11:37:08 <[exa]> ahri_: L.decimal is expecting to parse Strings and Chars, not ByteStrings that contain Word8's
11:37:34 <[exa]> ahri_: I guess there should be a variant of 'decimal' that works on ByteStrings somewhere in the library
11:37:51 <ahri_> [exa]: ok, that makes sense, I did wonder but plowed on regardless! I'll have a look
11:38:05 <hseg> gentauro: see https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/glasgow_exts.html#inferred-context-for-deriving-clauses
11:38:44 <gentauro> hseg: hmmmm: `newtype Foo a = Foo a deriving (Eq, Ord, Show)` + `data Bar = Bar` + `f = Foo Bar` + `:t f … f :: Foo Bar` + `f … No instance for (Show Bar) arising from a use of ‘print’`
11:39:10 <gentauro> is because context is delayed to implemented later (instance ... where)?
11:39:12 <[exa]> ahri_: I guess it should be in Text.Megaparsec.Byte.Lexer
11:39:27 <gentauro> otherwise I don't see the point in compiling this which clearly doesn't hold
11:39:29 <hseg> gentauro: becuase Bar doesn't have a show instance
11:39:49 <gentauro> hseg: That is my point ...
11:40:26 <hseg> yeah, but the fact that Show a => Show (Foo a) doesn't mean Foo _ is always showable
11:40:40 <hseg> consider e.g Foo (a -> b) -- should this be showable?
11:40:55 <gentauro> hseg: nope
11:40:57 <gentauro> it's a function
11:41:02 <hseg> exactly
11:41:16 <hseg> from GHC's perspective, Bar is no better in this regard than (a -> b)
11:41:36 <hseg> that's the entire point behind the Show a => part of the instance declaration
11:42:19 <ahri_> [exa]: https://hackage.haskell.org/package/megaparsec-4.4.0 doesn't seem to have a package like that, and its ByteString modules are quite empty!
11:42:23 <gentauro> hseg: so you can leave the `declaration` open to the end-users so they can decide how `Bar` should look like
11:42:35 <gentauro> interesting
11:42:47 <ahri_> perhaps there's another lib altogether for byte stuff
11:42:54 <hseg> ... not exactly
11:43:13 <gentauro> but I think I got the answer to what i was asking for (GHC doesn't enforce Show on the provided polymorphic types)
11:43:33 <hseg> ? it does, unless i'm misunderstanding you
11:43:47 <[exa]> ahri_: megaparsec version 4 is like a few years old
11:43:49 <ahri_> [exa]: no wait, I'm dumb and I'm looking at really old docs
11:43:52 <ahri_> hehe
11:43:54 <hseg> cf the fact that show (Foo Bar) is illegal without Show Bar
11:43:55 <[exa]> :]
11:44:16 <hseg> unless you want Foo :: Show a => a -> Foo a ?
11:44:17 <gentauro> hseg: how would define Show for `(a -> b)`? You can't right?
11:44:34 <[exa]> happens to me every now and then, especially from google links. Hackage could have a warning sign that the package has newer version
11:44:46 <hseg> you can, by brute force -- all you need is a function (a -> b) -> String
11:45:04 <hseg> e.g. instance Show (a -> b) where show _ = "function"
11:45:25 * [exa] votes for "undefined" which is valid syntactically and on type-level
11:46:15 <gentauro> hseg: sure, you need to do that (on your own, GHC will not enforce it)
11:46:19 <gentauro> my point still stands
11:46:46 <ahri_> [exa]: sorry about that oversight, and for your help in pointing me in the right direction!
11:47:00 <gentauro> just by deriving on a type, will enforce that it's polymorphic types should be also deriving the instances as well (at compile-time)
11:47:06 <gentauro> errors will be noticed at runtime
11:47:15 <gentauro> a bit like JavaScript'ish ...
11:47:23 <hseg> no, compile time
11:47:45 <hseg> try compiling your "showable foo, unshowable bar" example
11:47:50 <hseg> anyway, gtg
11:47:57 <gentauro> hseg: I just did at GHCi
11:48:03 <gentauro> no complain at all
11:48:06 <hseg> ghci's repl isn't a compiler
11:48:19 <gentauro> oh
11:48:24 <gentauro> I will try to compile it
11:48:39 <hseg> show id in ghci complains of a lack of instance
11:49:03 <hseg> so you're probably picking up an older instance from the repl
11:51:19 <gentauro> hseg: good, good, GHC complies :)
11:51:23 <gentauro> niceness :)
11:51:44 <gentauro> I was getting a bit worried here xD
11:58:13 * hackage lens-th-rewrite 0.1.0.0 - Rewrites Template Haskell splices using the API  https://hackage.haskell.org/package/lens-th-rewrite-0.1.0.0 (DavidJohnson)
12:07:36 <infinity0> could someone take a look at this, involving type inference in GHCi https://gist.github.com/infinity0/0209edac0d4f77ea62c06947d9720246
12:07:54 <infinity0> i know there is some other thing to do with MonoLocalBinds or some polymorphism thing but i'm pretty sure this isn't that
12:11:15 <f-a_> If I have ghc 8.8.3, can I/should I install cabal-install3.2.0.0 ?
12:11:44 <phadej> yes
12:11:48 <infinity0> i installed 3.0.0.0 but i had to cherry-pick 6c8922541 and f5006c9b4 on top of it
12:11:59 <f-a_> excellent, I will bootstrap.sh now
12:12:07 <phadej> f-a_: or you can wait slightly until bindists will be done (hopefully soon)
12:12:44 <phadej> f-a_: don't use bootstrap.sh
12:12:52 <phadej> most likely, it won't work.
12:12:57 <phadej> `cabal install cabal-install` instead
12:13:15 <f-a_> I don't have a working cabal on this machine
12:13:24 <phadej> download the 3.0 bindists
12:13:25 <f-a_> I mean I could get the one from the debian repos
12:13:44 <f-a_> roger
12:16:16 <infinity0> debian only has cabal 2.4 atm, that's why i built my own from git
12:16:51 <sclv> ghcup pulls recent bindists for many linuxen
12:37:23 <ysangkok> infinity0: maybe hvr's ubuntu ppa will work for debian too? i know it has cabal-install
12:38:43 * hackage cpphs 1.20.9.1 - A liberalised re-implementation of cpp, the C pre-processor.  https://hackage.haskell.org/package/cpphs-1.20.9.1 (phadej)
12:57:34 <bruteForce> hello I am new to haskell and right now I am struggling with a tree structure which has a special type with Constant and Variable 
12:57:48 <bruteForce> sorry I am also new to here
12:58:12 <bruteForce> I wanna controll if the parameter is a variable how I can do this
12:58:36 <ChaiTRex> bruteForce: Control it how?
12:59:15 <bruteForce> data UnaryOperator = Minus deriving (Eq, Read, Show)data BinaryOperator = Plus | Times deriving (Eq, Read, Show)data Expression a = Leaf a                  | UnaryOperation UnaryOperator (Expression a)                  | BinaryOperation BinaryOperator (Expression a) (Expression a)                  deriving (Eq, Read, Show)data Value = Variable
12:59:15 <bruteForce> String           | Constant Int           deriving (Eq, Read, Show)type ExprV = Expression Value
12:59:25 <bruteForce> this is my data 
12:59:50 <bruteForce> I wanna check if it is a variable or a Constant
13:00:10 <bruteForce> sorry I can't describe myself very well
13:00:34 <ChaiTRex> OK, with a function, you can do on one line `f (Variable xs) = ...` and on the next line `f (Constant x)` = ...`.
13:00:44 <ChaiTRex> Inside a function, you can use a `case` statement.
13:00:56 <jumper149> You can pattern match on the data contructors 'Constant' and 'Variable'.
13:01:33 <ChaiTRex> `case varOrConst of` on one line, `     Variable xs -> ...` on the next, `     Constant x -> ...` on the next.
13:02:01 <bruteForce> what do you mean by pattern match
13:02:09 <ChaiTRex> Also, for pasting more than one line, please use a pastebin.
13:02:24 <ChaiTRex> bruteForce: The examples I've given use pattern matching.
13:03:05 <bruteForce> thank you ChaiTRex I will try
13:03:06 <ChaiTRex> bruteForce: `f (Variable xs) = ...` takes a variable or constant and, if it's a variable, gets the variable's name into `xs`.
13:03:28 <ChaiTRex> bruteForce: The pattern there is `(Variable xs)` and it matches in the variable case.
13:04:23 <bruteForce> okey thanks again ChaiTRex
13:16:22 <amf> i have a gadt `data (X a) where X1 :: OtherGADT a -> X a`, i now realize i want to pass in a list of OtherGADT's, however every type tetris attempt i make doesn't work, line 186 is what i want along with working code https://pastebin.com/zpmG1RFr
13:17:05 <amf> i suspect i need to do some hetrogenous list? i'm a terrible typer, any pointers to hackage or advice is appreciated
13:18:08 <amf> im trying to build up a command line to the linux `tc` utility at the type level, but those dang list types got me
13:23:36 <maralorn> Any reason I should prefer MVar over TMVar? 
13:34:44 <dsal> maralorn: TVar?
13:34:59 <dsal> I never use MVars myself and reach for STM early.
13:35:13 <dsal> STM is wonderful magic.
13:36:24 <maralorn> dsal: STM has TMVar 
13:36:58 <dsal> Oh.  It's like mvar, but stm.  I've not used that one.
13:37:16 <maralorn> But yeah. TVar is also on the table... 
13:37:19 <ysangkok> dsal: how did you learn to use STM? marlows book?
13:37:28 <dsal> Maybe?  Mostly just using it a lot.
13:37:56 <dsal> Though know TMVar for some reason -- that hasn't come up in any of my programs. heh
13:38:27 <dsal> Mostly, I just use TVar and a little TChan.
13:38:27 <ysangkok> i must be too many toy programs if i never feel like i need concurrency :O
13:38:45 <dsal> I have a lot of haskell running in anger.  :)
13:39:15 <dsal> Though my most recent project doesn't have me managing any concurrency, other than synchronizing through sqlite.
14:22:59 <maralorn> I am very confused. Now I have two threads startet with concurrently_ in my main. If one of them dies the other stays alive. I then can only finish the program by SIGTERM ctrl+c doesn‘t even work …
14:23:47 <maralorn> That is exactly the opposite of the intendend behavior. What evil things are my threads doing?
14:25:26 <frdg> is unfoldr used often?
14:26:32 <dsal> frdg: It's used sometimes... It's been an obvious solution to problems a few times for me.
14:26:51 <dsal> There are many of these abstractions that just seem silly or weird until you're doing something and suddenly realize what it's for.
14:27:15 <dsal> maralorn: Can you show some code?
14:27:56 <frdg> dsal: Ok thanks
14:27:59 <dsal> concurrently (and race) should cancel a thread.  Are you spawning more threads?
14:28:29 <maralorn> dsal: I am using concurrently nested which shouldn‘t be a problem I guess?
14:28:57 <dsal> You mean you're spawning a tree of threads?
14:29:16 <dsal> Stopping the execution of a thread doesn't stop the execution of threads that are spawned below.
14:29:38 <maralorn> dsal: And I simply don‘t know and can‘t show you what the reflex-frp stuff is doing.
14:29:42 <dsal> There's an erlang-esque link operation, or you can bracket.
14:30:40 <maralorn> Well concurrently says " If either action throws an exception at any time, then the other action is cancelled, and the exception is re-thrown by concurrently."
14:31:06 <dsal> Yeah, but are you sure the thread that's still running is "the other" in this case?
14:31:10 <dsal> Here's an example abstraction I made in my MQTT client:  https://github.com/dustin/mqtt-hs/blob/master/src/Network/MQTT/Client.hs#L331-L338
14:31:28 <maralorn> dsal: No, I am not.^^
14:31:30 <dsal> In particular, I wanted to be able to see what threads are running after I expected cleanup to have taken out most of them.
14:31:46 <dsal> This let me run them with tracing and see all the events.
14:32:44 <dsal> I built a little tool to follow thread creation/destruction events and tell me the names of threads that were running after cleanup.
14:34:11 <dsal> From there, I made this sexy graph:  https://usercontent.irccloud-cdn.com/file/hhlIoOT6/logs
14:34:13 <dsal> And found my bug.
14:34:29 <maralorn> Amazing
14:34:49 <koz_> dsal: Those sexy teeth.
14:35:28 <maralorn> Yeah, I guess my problem is that "mainWidgetWithCss" from reflex does somehow not dye when it receives the exception.
14:35:44 <maralorn> die
14:35:46 <dsal> I had to run an eventlog over the course of a few days to figure out what went wrong, but it was a thing.
14:36:14 <dsal> How is that launched?
14:47:58 <maralorn> dsal: Well it‘s really just "race_ (mainWidgetWithCss ...) (myOtherThreadDoingThings)"
14:48:14 <dsal> And you're sure the other exited?
14:48:17 <dsal> And that one didn't?
14:49:17 <maralorn> Yes, myOtherThearDoingThings threw an exception because a port it wanted to bind to was in use.
14:50:04 <maralorn> But sadly also the errormessage from that only got printed once I ctrl+c’d the program.
14:50:38 <maralorn> I am a bit fuzzy about stdin/stdout and concurrency.
14:51:06 <dsal> Maybe it didn't?  There's no locks around stdio
14:51:53 <maralorn> But if (mainWidgetWithCss ...) uses forkIO somewhere that thread will live on, right?
14:52:04 <MarcelineVQ> there's is buffering though
14:52:24 <maralorn> I think it‘s because of the buffering.
15:03:14 <tabaqui1> % f' [x, y] | True <- x, True <- y = True
15:03:14 <yahb> tabaqui1: 
15:03:26 <tabaqui1> what does it mean? Is it some new guard syntax?
15:05:37 <MarcelineVQ> not new really, it's called pattern guards https://wiki.haskell.org/Pattern_guard
15:06:43 <tabaqui1> oh, it has been looking like "f | (predicate :: Bool) = <expr>
15:06:58 <tabaqui1> without "<-" syntax
15:08:02 <tabaqui1> looks like case which is able to evaluate
15:08:54 <monochrom> It was 10-year new because it was entered into Haskell 2010.
15:09:22 <monochrom> Although, using True for the pattern is really boring.
15:09:34 <tabaqui1> right, but I've never met such syntax before
15:10:02 <tabaqui1> this one is cool, and it is really strange that I met it first time in last 3 years
15:10:29 <monochrom> A more interesting example is: f x | [] <- [0..x] = "hello"
15:10:38 <tabaqui1> nah, it is simple :)
15:10:52 <tabaqui1> let 2 = 3 in 2 + 2 :)
15:10:53 <monochrom> It says: if [0..x] matches the pattern [], then the answer is "hello"
15:11:20 <tabaqui1> yeah, I know this trick :)
15:13:04 <tabaqui1> can I somehow evaluate expressions in pattern synonims? Like "pattern Foo :: Int -> (Int, Int); Foo (x + y) <- (x, y)"
15:13:22 <tabaqui1> and later "case (x, y) of Foo 5 -> ..."?
15:13:45 <tabaqui1> *synonyms
15:14:54 <hhefesto> Can one mix lenses and recursion-schemes?
15:14:55 <monochrom> I think no.
15:15:08 <ysangkok> > (\x -> | [] <- [0..x] = "hello") (0)
15:15:10 <lambdabot>  <hint>:1:8: error: parse error on input ‘|’
15:15:12 <tabaqui1> shame
15:15:32 <tabaqui1> ysangkok: % (\x | [] <- [0..x] -> "hello") 0
15:15:37 <tabaqui1> % (\x | [] <- [0..x] -> "hello") 0
15:15:37 <yahb> tabaqui1: ; <interactive>:91:5: error: parse error on input `|'
15:15:41 <tabaqui1> oh, sorry :)
15:16:04 <tabaqui1> % (\x -> { _ | [] <- [0..x] -> "hello"} ) 0
15:16:04 <yahb> tabaqui1: ; <interactive>:92:8: error: parse error on input `{'
15:16:13 <tabaqui1> % (\x -> _ | [] <- [0..x] -> "hello" ) 0
15:16:13 <yahb> tabaqui1: ; <interactive>:93:12: error: parse error on input `['
15:16:16 <ysangkok> ooh yahb can see commands that are not starting at character 0 :D nice
15:16:23 <tabaqui1> % (\x -> _ | [] <- [0..x] = "hello" ) 0
15:16:23 <yahb> tabaqui1: ; <interactive>:94:12: error: parse error on input `['
15:16:30 <tabaqui1> I give uo
15:16:32 <tabaqui1> *up
15:16:57 <{abby}> % (\case x | [] <- [0..x] -> "hello") 0
15:16:57 <yahb> {abby}: "*** Exception: <interactive>:95:2-34: Non-exhaustive patterns in case
15:17:28 <solonarv> > [0..0]
15:17:30 <lambdabot>  [0]
15:17:55 <tabaqui1> % take 10 $ [0..(-5)]
15:17:55 <yahb> tabaqui1: []
15:18:17 <{abby}> % [0, negate 1 .. negate 5]
15:18:17 <yahb> {abby}: [0,-1,-2,-3,-4,-5]
15:18:55 <tabaqui1> I thought it calls "succ" until last element
15:19:04 <tabaqui1> so [0..negate 1] should be infinite
15:19:37 <ChaiTRex> tabaqui1: No, it goes until it's greater than the end element.
15:19:54 <tabaqui1> define "greater"
15:19:59 <{abby}> (>)
15:20:01 <tabaqui1> :i [a..b]
15:20:09 <{abby}> enumFromTo
15:20:09 <tabaqui1> % :i (\a b -> [a..b])
15:20:09 <yahb> tabaqui1: ; <interactive>:1:2: error: parse error on input `\'
15:20:15 <tabaqui1> % :t (\a b -> [a..b])
15:20:15 <yahb> tabaqui1: Enum a => a -> a -> [a]
15:20:27 <tabaqui1> {abby}: there is no Ord instance
15:20:29 <{abby}> the method that implements [a..b] is called enumFromTo
15:20:40 <tabaqui1> % :i enumFromTo
15:20:40 <yahb> tabaqui1: class Enum a where; ...; enumFromTo :: a -> a -> [a]; ...; -- Defined in `GHC.Enum'
15:21:05 <{abby}> tabaqui1: great, so it's (>) `on` fromEnum, then
15:21:14 <tabaqui1> it seems so
15:21:54 <tabaqui1> btw, has anyone seen jle' recently?
15:22:00 <{abby}> eftInt x0 y | isTrue# (x0 ># y) = []
15:22:20 <ysangkok> tabaqui1: they are in the channel
15:22:26 <ysangkok> but with a backtick
15:23:05 <MarcelineVQ> tabaqui1: in many clients you can start a name and hit tab a few times to find them, e.g.  jl<tab><tab>
15:24:10 <tabaqui1> ok, thanks :)
15:24:37 <ysangkok> but it is a bit confusing because it will do windows-style completion in e.g. irssi
15:25:01 <ysangkok> so it will complete until first match and then cycle from the first possiblity
15:25:04 <tabaqui1> weechat does it in same way
15:25:16 <tabaqui1> it confused me, so I'm asking
15:25:53 <ysangkok> yeah i prefer to have the unix way with bells and alarm :D
15:26:10 <tdammers> between vim, zsh, weechat and firefox, I have given up on predicting how completion is going to play out
15:26:54 <tabaqui1> My laptop has no beeper :(
15:27:29 <ysangkok> tabaqui1: but your terminal emulator can translate to a gnome beep that goes over pulseaudio and all that stuff
15:28:00 <ysangkok> tabaqui1: and if you have no sound at all, some terminal emulators can be configured to flash the screen
15:28:00 <tabaqui1> dunno, never tried it before
15:28:13 <tabaqui1> mate (gnome 2)
15:28:26 <ysangkok> tabaqui1: what happens if you do: echo -e '\a'
15:28:33 <quantumplation> Hi all!  I'm trying to figure out the semantics of mixing two different monads in a do block, something like this: https://gist.github.com/Quantumplation/dabfa0afe13458595b76700209a9d5c0 (bit of a contrived example, but bear with me)
15:28:35 <tdammers> I went through great lengths to get rid of the beeps
15:28:50 <tabaqui1> ysangkok: nothing
15:28:52 <quantumplation> Anyone have tips for how to cleanly achieve something like the commented pseudocode in the middle
15:28:58 <tabaqui1> kitty, zsh
15:29:25 <monochrom> You simply can't mix two monads in the same do-block.
15:30:13 <koz_> monochrom: One does not simply... mix two monads.
15:30:14 <monochrom> You will have do-blocks, perhaps one inside one outside. But then simply the inside counts as its own expression, just like any other expression.
15:30:15 <tabaqui1> % :i maybe
15:30:17 <yahb> tabaqui1: maybe :: b -> (a -> b) -> Maybe a -> b -- Defined in `Data.Maybe'
15:30:19 <tabaqui1> quantumplation: ^
15:30:39 <ysangkok> tabaqui1: i think i had to do some of this: https://askubuntu.com/a/228278/19466
15:30:40 <monochrom> s/You will have do-blocks/You will have two do-blocks/
15:30:42 <tabaqui1> like "maybe (return ()) announceNewSomething (id newsId)"
15:30:43 <{abby}> or you could write a good ol' case expression
15:31:00 <koz_> monochrom: Actually, why the heck not: https://i.imgflip.com/3v5j4p.jpg
15:31:16 <quantumplation> mm, the maybe approach is interesting.  I tried the nested do blocks, but I couldn't get it to work.  How would I do that?
15:31:17 <monochrom> Yeah heh
15:31:29 <quantumplation> I could do the case match, just trying to get used the haskell semantics of how things thread together :)
15:31:52 <quantumplation> s/used the/used to the/
15:32:12 <{abby}> case id newS of { Just xs -> announceNewSomething xs; Nothing -> pure () }
15:32:37 <{abby}> feel free replace { ; } by increasing (resp. decreasing) indentation and newlines
15:33:53 <quantumplation> sure, like I said, the case match is what occured to me first, I just thought there might be a way to do it by utilizing the structure of the monad
15:34:00 <quantumplation> Alright, I'll just use the case statement, thanks everyone :)
15:34:05 <tabaqui1> ysangkok: yep, now it works. Thanks!
15:35:06 <hhefesto> does anyone know if recursion-schemes and lenses mix well?
15:35:49 <ysangkok> tabaqui1: since you didn't have it before, to prevent future heart attacks: note that e.g. zsh will beep when it cannot disambiguate
15:36:35 <tabaqui1> ysangkok: ok, noted
15:37:57 <monochrom> Your case doesn't use Maybe's monad structure at all.
15:38:22 <monochrom> It's very simple.
15:38:49 <monochrom> Suppose you have "Just x -> Just (f x); Nothing -> Nothing", then you're using Maybe's functor structure.
15:39:26 <monochrom> Suppose you have "Just x -> (f x :: Maybe b); Nothing -> Nothing", then you're using Maybe's monad structure.
15:39:37 <monochrom> But clearly you're doing none of those.
15:40:55 <quiet-laika> does brittany work with ghc 8.3.3?
15:41:09 <quiet-laika> `parse error:"when parsing ghc flags: encountered warnings: [\"-XMonadFailDesugaring is deprecated: MonadFailDesugaring is now the default behavior\",\"-XMonadFailDesugaring is deprecated: MonadFailDesugaring is now the default behavior\"]"`
15:41:21 <quiet-laika> err, 8.8.3
15:41:54 <quantumplation> :hmm: How would I write the case statement if announceNewSomething returned a value that I didn't care about? the Nothing branch can't just return pure (), because now the type of the two branches is different, even if I don't care about the value
15:42:33 <{abby}> () <$ announceNewSomething _
15:42:44 <{abby}> or void $ announceNewSomething _, from Control.Monad
15:43:01 <{abby}> (where _ is your argument(s), not a literal _)
15:43:01 <monochrom> or add "pure ()" and use >> or *>
15:43:46 <monochrom> or even use ">>= \_ -> pure ()"
15:43:55 <monochrom> to show you really understand monads.
15:44:07 <quantumplation> lol
15:44:10 <quantumplation> Thanks everyone!
15:44:24 <{abby}> surely >>= const (pure ()) to show you understand higher-order functions as well
15:45:29 <dsal> :t surely
15:45:31 <lambdabot> error: Variable not in scope: surely
15:45:49 <monochrom> "don't call me shirley"
16:10:13 * hackage influxdb 1.7.1.3 - Haskell client library for InfluxDB  https://hackage.haskell.org/package/influxdb-1.7.1.3 (MitsutoshiAoe)
16:16:43 * hackage tree-sitter-ql 0.1.0.3 - Tree-sitter grammar/parser for QL  https://hackage.haskell.org/package/tree-sitter-ql-0.1.0.3 (rewinfrey)
16:19:27 <reactormonk> ... is there a tiny bit of trifecta documentation somewhere?
16:27:53 <sm[m]> it's in http://hackage.haskell.org/package/trifecta-2.1/docs/Text-Trifecta.html
16:28:09 <Axman6> reactormonk: parseres is the library you should be using it, all the documentation is in there, and Trifecta has instances for its classes
16:28:21 <reactormonk> Aye, gotcha
16:31:43 * hackage numhask-space 0.3.1 - numerical spaces  https://hackage.haskell.org/package/numhask-space-0.3.1 (tonyday567)
16:41:37 <zincy_> Maybe its just because its too late and I am tired
16:42:39 <zincy_> But why wont this work with tuple sections                  (((,) . _name . _age) <$> [persons]
16:42:47 <zincy_> One too many parenthesis
16:43:18 <zincy_> But just assume Data Person = { _name :: String, _age :: Int }
16:45:45 <wrunt> because you're calling _name on the result of _age, which is an Int
16:46:21 <wrunt> try (\person -> (_name person, _age person)) <$> [persons]
16:47:10 <zincy_> wrunt: Thanks that works but I was wondering about tuple sections
16:48:55 <wrunt> :t (,)
16:48:56 <lambdabot> a -> b -> (a, b)
16:49:36 <wrunt> :t (,) . id
16:49:38 <lambdabot> a -> b -> (a, b)
16:50:07 <wrunt> (,) . _name still wants two arguments
16:51:35 <wrunt> you can't compose a function to take a single person and then do two different things with it, unless you give it a name
16:51:45 <wrunt> unless there's a combinator to do that which I don't know about
16:52:10 <Axman6> you can also use (\Person{..} -> (_name, _age)) to be slightly more succinct
16:53:01 <Axman6> or ((,) <$> _name <*> _age)
16:56:11 <MarcelineVQ> "<zincy_> wrunt: Thanks that works but I was wondering about tuple sections" What were you wondering about tuple sections?
16:58:11 <jackdk> zincy_: you can do it but it looks silly: `fmap (bimap _name _age . join (,))`. I would not do this, for the sake of whoever has to read my code later
16:58:45 <jackdk> wrunt: `join` using the `(->) r` instance of `Monad`
16:59:04 <jackdk> `join :: (r -> r -> a) -> r -> a`
16:59:40 <Axman6> % :t liftA2 @((->) x) (,)
16:59:40 <yahb> Axman6: ; <interactive>:1:15: error: Not in scope: type variable `x'
16:59:51 <wrunt> :t join (,)
16:59:53 <lambdabot> b -> (b, b)
17:00:00 <Axman6> % :t liftA2 @((->) String) (,)
17:00:00 <yahb> Axman6: (String -> a) -> (String -> b) -> String -> (a, b)
17:01:04 <dottie> irc.grepnet.org #chats - world's TOP HACKER IRC, even Edward Snowden is there, connect NOW or you're a loser.
17:01:50 * jackdk is happy to be a loser
17:02:13 <wrunt> I feel like there should be a way to do this with Control.Arrow too...
17:03:35 <wrunt> _name &&& _age
17:04:12 <jackdk> wrunt: nice
17:04:39 <wrunt> but that doesn't tell zincy_ anything about tuple sections, sorry
17:04:59 <jackdk> I don't think there's anything relevant to say
17:07:19 <monochrom> With &&& perhaps you don't need the tuple constructor, since &&& already provides it.
17:07:33 <monochrom> > (sin &&& cos) (pi/6)
17:07:37 <lambdabot>  (0.49999999999999994,0.8660254037844387)
17:07:41 <jackdk> indeed; it becomes `fmap (_name &&& _age)`
17:09:28 <Axman6> > (sin &&& cos) (pi/6) :: (CReal, CReal)
17:09:30 <lambdabot>  (0.5,0.8660254037844386467637231707529361834714)
17:26:57 <dnattie> irc.grepnet.org #chats - world's TOP hacker IRC! Even Snowden and Stallman are there... Join NOW and connect with 1337!
17:28:43 * hackage box 0.3.0 - boxes  https://hackage.haskell.org/package/box-0.3.0 (tonyday567)
17:38:29 <koz_> @pl \f x y -> f (pure x) (pure y)
17:38:29 <lambdabot> flip flip pure . ((.) .) . (. pure)
17:38:33 <koz_> Lawls.
17:39:00 <Axman6> :t \f -> f `on` pure
17:39:02 <lambdabot> Applicative f => (f a -> f a -> c) -> a -> a -> c
17:43:33 <koz_> Axman6: Woo, sweet.
17:47:05 <koz_> Where's on from?
17:47:19 <solonarv> base Data.Function
17:47:45 <koz_> solonarv: Thanks!
17:50:59 <koz_> Also, how would I (properly) spell this with microlens? 'canProvideTable <- foldMap HS.toList . HM.elements . HM.filterWithKey go <$> view satisfiers'?
17:51:15 <koz_> HS is HashSet, HM is HashMap
17:52:31 <koz_> (actually that HM.elems is kinda redundant since HashMaps are themselves Foldable)
17:54:53 <koz_> > Just "foo" <|> empty
17:54:56 <lambdabot>  Just "foo"
18:01:13 * hackage web-rep 0.3.1 - representations of a web page  https://hackage.haskell.org/package/web-rep-0.3.1 (tonyday567)
18:04:41 <solonarv> what's the return type of 'view satisfiers''?
18:04:53 <solonarv> something like HashMap Key (HashSet Foo) ?
18:04:59 <koz_> solonarv: Indeed.
18:05:59 <solonarv> hm, I don't see any indexed optics in microlens
18:06:15 <solonarv> so I think that filterWithKey makes this impossible
18:06:36 <koz_> Oh well, one can dream.
18:07:35 <solonarv> you could do it with lens, though
18:08:04 <koz_> I doubt my colleagues would appreciate me dragging lens into our codebase. :P
18:08:25 <solonarv> aww :(
18:08:30 <koz_> It is what it is.
18:09:42 <solonarv> it would look something like: canProvideTable <- asks $ toListOf $ ifolded . withIndex . filtered (uncurry go) . folded
18:13:35 <koz_> Hmm, I see.
18:14:00 <koz_> But 'view satisfiers' is needed to 'project out' the HashMap - asks wouldn't get me that by itself.
18:17:24 <pie_[bnc]> #justIOthings
18:20:19 <Axman6> % :t toListOf
18:20:19 <yahb> Axman6: Getting (Endo [a]) s a -> s -> [a]
18:20:38 <Axman6> hmm, a shame that's not defined over MonadReader like view
18:21:51 <koz_> Axman6: Yeah, something like viewAll.
18:22:08 <koz_> Or 'takeInTheSights' or something.
18:34:13 * hackage gopro-plus 0.3.0.0 - GoPro Plus Client API.  https://hackage.haskell.org/package/gopro-plus-0.3.0.0 (dustin)
18:35:13 * hackage chart-svg 0.0.1 - See readme.md  https://hackage.haskell.org/package/chart-svg-0.0.1 (tonyday567)
18:38:02 <solonarv> koz_: oh oops, there should be a 'to satisfiers' at the front
18:38:14 <koz_> solonarv: Ah, OK. Was wondering what I was missing.
18:49:16 <farmfromjakestat> Alright I got a potentially interesting problem for anyone interested. Given a numbers prime factorization, calculate all of its factors
18:50:11 <farmfromjakestat> I found this page useful: https://math.stackexchange.com/questions/2782625/how-to-get-all-the-factors-of-a-number-using-its-prime-factorization/2782729#2782729
18:51:54 <farmfromjakestat> I got it in the form of [[1,2,4,8],[1,3,9]] for the number 72 (2^3 * 3^2), and can combine that, but some factors have more than two primes and I am not sure how to take that into account
18:52:20 <Axman6> @hoogle subsequences
18:52:21 <lambdabot> Data.List subsequences :: [a] -> [[a]]
18:52:21 <lambdabot> GHC.OldList subsequences :: [a] -> [[a]]
18:52:21 <lambdabot> Protolude subsequences :: () => [a] -> [[a]]
18:52:32 <Axman6> > subsequences [1,2,3,4,5]
18:52:34 <lambdabot>  [[],[1],[2],[1,2],[3],[1,3],[2,3],[1,2,3],[4],[1,4],[2,4],[1,2,4],[3,4],[1,3...
18:54:17 <Axman6> is it true that if you have a given list of factors, that any combination of those factors multiplied together is also a factor of n?
18:54:49 <Axman6> > map (product &&& id) $ subsequences [1,2,4,8]
18:54:52 <lambdabot>  [(1,[]),(1,[1]),(2,[2]),(2,[1,2]),(4,[4]),(4,[1,4]),(8,[2,4]),(8,[1,2,4]),(8...
18:55:08 <Axman6> > map (product) $ subsequences [1,2,4,8]
18:55:11 <lambdabot>  [1,1,2,2,4,4,8,8,8,8,16,16,32,32,64,64]
18:55:32 <Axman6> > map (product) $ subsequences [1,3,9]
18:55:35 <lambdabot>  [1,1,3,3,9,9,27,27]
18:58:23 <farmfromjakestat> looks promising, but subsequences takes in a list, but input is a list of lists
18:58:49 <farmfromjakestat> my input looks like [[1,2],[1,3],[1,7]]. You can one element from the first list, one from the second, and one from the third
18:58:59 <farmfromjakestat> And I need a combination of all of those posibilities
18:59:30 <Axman6> > sequence [[1,2],[1,3],[1,7]]
18:59:32 <lambdabot>  [[1,1,1],[1,1,7],[1,3,1],[1,3,7],[2,1,1],[2,1,7],[2,3,1],[2,3,7]]
18:59:43 <farmfromjakestat> Ah I see
19:00:04 <Axman6> (I would remove ones from all the lists though
19:00:36 <farmfromjakestat> The ones is kinda important because 1 is a factor, and so is 3, and the only way to get that is 1 * 3 * 1
19:03:32 <farmfromjakestat> Seems to be working, thank you so much!
19:05:27 <lisbeths> Is there something like haskell with all currying where the type declaration can be done with currying and the function definitions can do any operation haskell can do but with only currying?
19:18:36 <ByteEater> the changelog for base seems to be stuck at 4.12.0.0
19:18:39 <ByteEater> http://hackage.haskell.org/package/base/changelog
19:19:06 <ByteEater> can I a newer one anywhere?
19:19:16 <ByteEater> find
19:19:53 <solonarv> ByteEater: hmm, you can probebly find it if you dig around in the ghc manual
19:20:03 <solonarv> the issue is that base-4.13 is just not on hackage
19:20:12 <Axman6> lisbeths: I have no idea what that question is supposed to mean. can you give an example if what you want?
19:20:37 <ByteEater> and now even 4.14
19:20:50 <ByteEater> thx, solonarv
19:33:45 <lisbeths> Theoretically haskell can be rewritten to use polish notation such that it's notation is similar to lisp.
19:34:10 <lisbeths> Similarly the notation of haskell can formulate a turing complete series of functions which can be used single file with currying.
19:34:29 <lisbeths> Ideally you wouldn't just want these curried functions to be turing complete but efficient in the machine and as capable as any other haskell code.
19:40:45 <wrunt> lisbeths: I'm not quite following you, but some of what you say reminds me of Conal Elliott's Compiling to categories (http://conal.net/papers/compiling-to-categories/) in which part of what he does is rewrite all code to be pointfree rather than using lambdas.
19:41:55 <wrunt> I'm not sure in what sense a series of functions can be turing complete. A series is just a series (i.e. a sequence), is it not? It doesn't do computation.
20:06:07 <dmwit> farmfromjakestat: hm, are you sure you wouldn't prefer something like
20:06:14 <dmwit> > subsequences [2,3,7]
20:06:16 <lambdabot>  [[],[2],[3],[2,3],[7],[2,7],[3,7],[2,3,7]]
20:06:27 <dmwit> > map product (subsequences [2,3,7])
20:06:30 <lambdabot>  [1,2,3,6,7,14,21,42]
20:07:20 <dmwit> Dunno. Maybe for repeated factors your idea is better. Okay, maybe ignore me.
20:11:35 <remexre> is ShowS a time improvement, or just space?
20:11:53 <Cale> Mostly time
20:12:08 <Cale> remexre: xs ++ ys takes O(length xs) steps to reduce
20:12:35 <Cale> remexre: So if you build up a string by repeatedly appending to the end of it, it's a disaster, you get quadratic time performance
20:12:43 <Cale> If you can build it the other way, it's fine
20:12:56 <remexre> hm
20:13:03 <Cale> but sometimes you don't have the option, so composing functions (which is always O(1)) is much more efficient
20:13:15 <dmwit> That's... a bit misleading.
20:13:23 <remexre> if (xs ++ ys)'s thunk is only consumed by a linear scan, is this still the case
20:13:33 <dmwit> The magic isn't that you're composing functions, the magic is that you're re-associating so that you're always prepending.
20:13:56 <Cale> The analysis can go a lot of different ways
20:14:24 <Cale> But yeah, you're composing functions which add things to the beginning of whatever their argument is
20:15:01 <Cale> A nice simple example (which doesn't use ShowS, but whatever), is something like reverse
20:15:06 <Cale> reverse [] = []
20:15:13 <Cale> reverse (x:xs) = reverse xs ++ [x]
20:16:19 <Cale> This'll take roughly n + (n-1) + ... + 1 steps
20:16:33 <Cale> which is n(n+1)/2 which is O(n^2)
20:17:10 <Cale> So, instead of building lists directly, we can build a functions which add things to the beginning of other lists
20:17:15 <Cale> [] becomes the identity function id
20:17:21 <Cale> [x] becomes (x:)
20:17:30 <Cale> and (++) becomes (.)
20:17:36 <Cale> reverse' [] = id
20:17:44 <Cale> reverse' (x:xs) = reverse' xs . (x:)
20:18:00 <Cale> and now the total cost is linear in the length of the list
20:18:14 <Cale> since (.) is constant time, as is (x:)
20:19:02 <remexre> hm
20:19:23 <dmwit> We *must* have a wiki page on this by now.
20:19:25 <remexre> the correctness of this makes sense, but I feel like there's something laziness-like that should make the naive approach better
20:19:42 <remexre> I think I read some wiki page for the prolog version a while ago, heh
20:20:11 <Cale> At the very least, there's nothing you can do to make (++) more efficient unless you change the structure of lists
20:20:16 <dmwit> Well, we do, but it's trash. =P
20:20:36 <monochrom> Laziness makes "take 1 (map f infinitelist)" better, yes.  Laziness is still defeated by a loop over "foo ++ [x]"
20:20:49 <remexre> like I get the general approach, but I can't shake the feeling that a linear scan through (x ++ y) shouldn't take asymptotically more time than ((\tl -> x0:x1:x2:...:tl) . (\tl -> y0:...:tl) . [])
20:20:53 <Cale> xs ++ ys doesn't share any of its tails with xs, so those are new lists that aren't already in memory, up to the point where you get to the tail which is just ys
20:20:59 <Cale> and ys gets shared with the existing list
20:21:24 <Cale> it's not quite that
20:21:36 <remexre> er, $ []
20:21:41 <Cale> Note that there are O(n) occurrences of (++) in my naive reverse
20:22:11 <Cale> and each one has to retrace the start of the list (getting shorter as you recurse more deeply, but it still adds up to quadratic time)
20:22:28 <monochrom> A super simple test of whether laziness saves you is to use undefined.  For example (take 1 (undefined ++ [])) is illuminating.
20:22:35 <dmwit> remexre: It doesn't. The situation we're talking about is when the number of calls to (++) scales up.
20:22:48 <Cale> *Each* (++) is only linear time
20:22:59 <Cale> but with the original reverse, there are a lot of (++)'s to do
20:23:10 <dmwit> remexre: (((((a ++ b) ++ c) ++ d) ++ e) ++ ...) is much more expensive than (a ++ (b ++ (c ++ (d ++ (e ++ ...)))))
20:23:13 * hackage readme-lhs 0.5.0 - See readme.md  https://hackage.haskell.org/package/readme-lhs-0.5.0 (tonyday567)
20:23:44 <monochrom> Another example: "False && undefined" vs "undefined && False" shows you laziness saves which one.
20:23:50 <dmwit> remexre: https://stackoverflow.com/a/60499715/791604
20:23:54 <remexre> so I guess I'm kinda still mentally modelling the code generated by a scan through a lazy list as something like a list over a generator (in python)
20:24:23 <Cale> [] ++ ys = ys
20:24:28 <remexre> I guess you pay for the stack frames in a pythonish transcription of that reverse? It doesn't seem O(n^2) though
20:24:29 <Cale> (x:xs) ++ ys = x : (xs ++ ys)
20:24:32 <monochrom> Likewise (undefined ++ [x]) vs ([x] ++ undefined) under take 1.
20:24:39 <Cale> right?
20:24:49 <remexre> Cale: yeah
20:24:55 <Cale> Do you see how that (++) takes linear time in the length of xs?
20:25:10 <monochrom> I wouldn't use Python cost models for Haskell.
20:25:18 <remexre> It still feels like what time it takes depends on the actual thunk being evaluated
20:25:26 <Cale> (supposing that we fully evaluate the result of course)
20:25:27 <dmwit> remexre: The thing you are not taking account of: the cost of evaluating xs ++ ys includes the cost of evaluating xs, the cost of evaluating ys, *and* the cost of traversing xs once while evaluating the ++. This last one is the only one which is asymmetric: you do not traverse ys while evaluating ++.
20:25:42 <Cale> If we only want the first element, of course, it's constant time
20:25:49 <Cale> But even that's not good for reverse
20:25:58 <dmwit> remexre: This means that if the first argument itself involves calls to ++ you pay more, but if the second argument involves calls to ++ you don't.
20:26:19 <dmwit> remexre: (...and definitely do take a look at the SO answer I linked when you get a chance.)
20:26:33 <remexre> dmwit: yeah, it's on monitor 2 rn
20:27:08 <Cale> Well, I suppose it's not that bad until you do the analysis for the second element ;)
20:27:18 <remexre> Cale: the precise case I'm thinking of is a bunch of (=<<)s on lists, then a linear scan
20:27:33 <Cale> Of course it'll take O(n) time to get the first element, but it shouldn't take linear time to get the next :)
20:27:36 <remexre> the linear scan could be e.g. length, could be e.g. printing it
20:27:51 <remexre> but something where I only need one element at a time
20:28:10 <Cale> hm?
20:28:43 <dmwit> I don't understand why you believe "linear scan" helps.
20:28:55 <Cale> (=<<) tends to cause a combinatorial explosion in the lengths of your lists, so often if you're doing a lot of that, you'll just be exponential time by necessity
20:29:08 <Cale> (again, assuming we evaluate the entire resulting list)
20:29:20 <remexre> dmwit: since (forM (x ++ y) foo) is equivalent to (forM x foo >> forM y foo), I guess
20:29:45 <remexre> equivalent in effect*, so I'd really wish they were equivalent in perf
20:29:59 <Cale> Oh, that should be fine
20:30:24 <Cale> Those are asymptotically equivalent
20:30:26 <remexre> oh
20:30:30 <monochrom> No.
20:30:32 <remexre> okay, that's what I've been trying to get at
20:30:33 <MarcelineVQ> ?
20:30:38 <monochrom> This is forM not forM_
20:30:41 <remexre> er
20:30:43 <remexre> I meant forM_
20:30:45 <remexre> sorry
20:31:03 <remexre> I always use mapM if I care about the result lol
20:31:39 <Cale> I use forM a lot when I care about the result, but I assumed we were only talking about cost and not result
20:31:57 <monochrom> forM costs more than forM_ too.
20:32:10 <dmwit> I do not buy "those are equivalent".
20:32:27 <Cale> Of course, forM_ (xs ++ ys) means you'll end up computing a bunch more
20:32:36 <dmwit> For many monads, associate (>>) however you like and the performance will be the same.
20:32:40 <Cale> But it only affects the constant factor
20:32:44 <dmwit> But associate (++) differently and the performance differs.
20:32:45 <Cale> because it's linear time either way
20:32:48 <dmwit> So the claim can't be literally true.
20:32:56 <Cale> What?
20:33:01 <Cale> There's only one occurrence of (++)
20:33:15 <monochrom> What if x hides n occurrences of ++
20:33:18 <dmwit> `forM (x++y) foo` does one extra traversal of `x` compared to `forM x foo >> forM y foo`.
20:33:31 <dmwit> So I do not buy that we can completely ignore the difference.
20:33:47 <dmwit> If `x` has no occurrences of `++`, then fine. But that's sort of the whole question we're talking about.
20:33:54 <dmwit> So making that assumption throws away the entire question.
20:34:01 <Cale> Well, yeah, that's fair.
20:34:09 <monochrom> More concretely, what if forM_ (slowReverse xs) foo
20:34:31 <Cale> I was considering the cost of the expression as given modulo the costs of evaluating the variables in it
20:35:22 <Cale> It's going to take O(length xs) steps to reduce xs ++ ys, and then O(length xs + length ys) steps to reduce the forM_ (xs ++ ys) foo into an action
20:35:26 <monochrom> Note that ((x1 >> x2) >> x3) >> x4 isn't always nearly as slow as ((([x1]++[x2])++[x3])++[x4]
20:35:35 <dmwit> I am considering the cost of the expression modulo doing that same rewrite, but deeply: changing *all* occurrences of `forM_ (x++y) foo` to `forM_ x foo >> forM_ y foo`, even if doing this rewrite reveals more opportunities to do that rewrite.
20:35:39 <dmwit> Then the question becomes interesting again.
20:35:53 <Cale> and then in the second case, it takes O(length xs) steps for the forM_ xs foo and O(length ys) steps for the forM_ ys foo
20:36:35 <Cale> So it's like the difference between O(n+m) and O(2n+m)
20:36:59 <Cale> which, normally the big O notation would let you regard as equivalent
20:37:40 <Cale> But if this is part of some bigger recursive thing, then you might not be within your rights to be that sloppy :)
20:38:43 <dmwit> remexre: Clear as mud? =D
20:39:00 <remexre> dmwit: approximately :P
20:39:32 <remexre> I'm considering leaving a comment, {- consider rewriting in C if too slow -}
20:39:39 <Cale> It might help to just do the entire reduction of reverse [1,2,3] by hand
20:39:58 <remexre> or alternately, making another thread do this part of the work
20:40:29 <dmwit> remexre: If you literally are appending two lists, then `forM_ (x++y) foo` and `forM_ x foo >> forM_ y foo` are approximately equally fast. But if `x` is a complicated expression -- especially one that might end up recursing back to this same chunk of code -- then the situation is different.
20:40:35 <remexre> since ig I care more about the time to run all the effects after loading the data, than about the actual compute time
20:43:01 <Cale> Oh, is there an actual program?
20:43:03 <remexre> dmwit: the real code is basically, I load the data from a file, parse a tree from it, recursively flatmap the tree into a vector (each non-leaf represents a transformation, vector is of transformed leaves), then store them in a vector
20:44:23 <remexre> the store in the vector is the only thing that needs to happen on the main thread (or a mutex needs to be taken out on the vector, I guess)
20:45:04 <dmwit> Flattening a tree sounds like exactly the situation where you should take care to associate (++) calls correctly.
20:46:35 <remexre> at this point, I'm basically choosing between
20:46:53 <remexre> 1) just flatmap in a pure function to produce a list, write the list into the vector
20:47:27 <remexre> 2) flatmap in a background thread, write in the main one (main thread latency limited, not throughput, so this is fine, I think)
20:47:30 <monochrom> You can get DoSed by a left-leaning tree.
20:48:07 <remexre> 3) effectful tree traversal, writing as I encounter an element
20:48:15 <remexre> 4) something else?
20:48:31 <dmwit> 4) flatmap in a pure function to produce a difflist
20:48:32 <Cale> Hang on, xs ++ ys for Vector is O(length xs + length ys)
20:48:44 <remexre> monochrom: trees are relatively trusted; at least, this isn't a network service, and the user provides the tree
20:48:59 <remexre> dmwit: oh, yeah; that for 1) and 2)
20:49:01 <Cale> So, you're not going to save anything by reassociating the appends
20:49:18 <dmwit> He doesn't claim to be appending vectors anywhere.
20:49:20 <dmwit> Always list.
20:49:32 <monochrom> This is why I hate people.
20:49:42 <remexre> oh, though I could append immutable vectors if that'd be faster, before writing them to the mutable one?
20:49:54 <remexre> monochrom: this is why I hate laziness :P
20:49:56 <Cale> It's not
20:50:06 <Cale> remexre: Almost none of this has anything to do with laziness
20:50:27 <Cale> remexre: We're assuming that everything is fully evaluated in these costs
20:50:41 <Cale> So eager vs. lazy is pretty irrelevant
20:50:49 <dmwit> (And your "linear scan" means this assumption is almost certainly correct.)
20:51:01 <remexre> hm
20:51:06 <remexre> oh, wait
20:51:12 <monochrom> When I am teaching a math equation like "return x >>= k  =  k x", and emphatically saying I'm teaching the math, people go berserk about "but they have different performance.  performance is oh-so-important in production code in the real world".
20:51:20 <Cale> This is simply analysis of costs of operations on immutable data structures
20:52:05 <Cale> If you go and rewrite this in OCaml, you'll face the exact same performance puzzles.
20:52:22 <dmwit> (Or C, for that matter.)
20:52:35 <Cale> Or even C, though you're unlikely to choose these data structures in the first place in C.
20:52:36 <monochrom> And they when they actually talk about their oh-so-important oh-so-real-world oh-so-production code, you see they completely botch their performance, and furthermore they know it and they dismiss it with "nah it's fine I have a 10GHz CPU" or "I know the input is small".
20:52:39 <remexre> yeah, but there I'd just write the generator version I'm imagining in the first place
20:52:46 <remexre> :P
20:52:51 <monochrom> HYPROCRITES
20:52:52 <Cale> remexre: hm?
20:53:39 <remexre> Cale: "concatenating" two generators/coroutines doesn't have the same increase in costs, correct?
20:54:10 <Cale> Well, sure, but that's basically the same as our function composition trick.
20:54:18 <remexre> oh.
20:54:26 <remexre> why don't lists just, well, do this
20:54:31 <remexre> like by default
20:54:38 <Cale> Because they're data structures in memory
20:54:44 <solonarv> because that representation is worse in other ways
20:55:24 <Cale> If you go to a bunch of trouble computing the start of a list, you often don't want to recompute it on the second traversal
20:55:32 <Cale> That's the main difference
20:55:37 <dmwit> remexre: It isn't free. "Running" a generator twice computes its contents twice. "Running" a list twice doesn't.
20:55:40 <dmwit> It's a tradeoff.
20:56:03 <remexre> then, List' a = G (Generator a) | C (a, List' a) ?
20:56:13 <remexre> eh, I guess that's impure to force
20:56:21 <remexre> but at the level of the generated code
20:56:29 <remexre> oh, | Nil
20:56:50 <dmwit> And how should the compiler know the right time to convert a G to a C|Nil ?
20:56:54 <Cale> Are you sure you don't just want  newtype List' a = List' ([a] -> [a]) ?
20:56:59 <dmwit> This is in general a deep performance question.
20:57:27 <dmwit> That can only properly be guided by something that has a solid knowledge of all the consumers of the data structure.
20:57:35 <remexre> dmwit: okay now that I'm thinking about it, it needs some sort of escape analysis, but...
20:57:36 <solonarv> oh, here's a problem with that representation: it can't have Functor or Traversable instances
20:57:43 <remexre> yeah what you just said
20:58:18 <solonarv> (Foldable is possible because you can do: foldMap f (List' gen) = foldMap f (gen []) )
20:58:20 <remexre> solonarv: wait, really? shouldn't Generator be a functor?
20:58:24 <Cale> solonarv: Okay, we can Codensity it :P
20:58:37 <solonarv> I was talking about Cale 's representation, not yours remexre 
20:58:41 <remexre> oh, lol
20:58:56 <solonarv> anyway gotta go sleep
20:59:09 <remexre> gn!
20:59:11 <dmwit> solonarv: Why can't we pull the same trick for Functor (of supplying [])?
20:59:30 <dmwit> solonarv: Under the assumption that it was a law-abiding DiffList to begin with it will be after.
20:59:53 <dmwit> fmap f (List' gen) = List' (fmap f (gen[]) ++)
21:00:04 <Cale> yeah, that'd work
21:00:30 <Amsterdam> hi everybody
21:00:31 <dmwit> (However, it is indeed an assumption. It is not literally true that all inhabitants behave properly.)
21:00:31 <Cale> and it also illustrates the downside to storing generators instead of lists
21:00:46 <Cale> Hello
21:01:06 <remexre> Cale: how so?
21:02:00 <Amsterdam> what are you talking about?
21:02:37 <Cale> remexre: Well, you're introducing an extra traversal of the list that you get from applying the generator to []. You need to map over the list which is one walk, and then (++) is implicitly going to cause another.
21:03:03 <remexre> Amsterdam: different list representations; the real List, vs List' a = G (Generator a) | C (a, List' a) | Nil, vs List'' = MkList'' (List a -> List a)
21:03:16 <Cale> Amsterdam: Can you cut it out with the CTCP stuff?
21:03:51 <Amsterdam> Cale: sorry
21:04:51 <Cale> (VERSION especially is not usually taken well, because it's typically a sign of someone probing for vulnerable IRC clients)
21:05:01 <remexre> Cale: er, I'm sorta modelling a generator as an impure computation... I guess I'm not sure how to formulate it in a pure way without linear types, but I don't think it'd be able to take an argument
21:05:54 <Cale> remexre: Well, it's not much different from a function which produces a list
21:06:43 <remexre> sure, I'm just not sure what (gen []) would become
21:06:47 <Cale> You could just use () -> [a], which would have similar space behaviour, but [a] -> [a] is nicer because it has that concatenation
21:07:14 <Cale> gen [] is the conversion from a generator to a list
21:07:46 <Cale> Of course, laziness makes it weird, but for this discussion, you can pretend everything is strict and everything we've been telling you holds the same
21:08:44 <remexre> I guess I don't see where the second walk is coming from; wouldn't a map always result in a new generator which does the walk "elementwise"?
21:08:53 <Cale> You can convert a list of type [a] to a "generator" of type [a] -> [a], i.e. a function which adds a bunch of elements to the start of any list you give it by using (++)
21:09:11 <Cale> and you go in the opposite direction by applying the generator to an empty list
21:09:24 <Cale> Note that xs ++ [] is not cost-free
21:09:40 <Cale> It costs as much as xs is long
21:09:51 <remexre> yeah, but it should be free on generators, right?
21:10:08 <Cale> well, yeah, f . id is constant time
21:10:27 <Cale> (and might even be free if you write that expression in your code so the compiler gets to optimise it)
21:10:47 <Cale> But what I mean is, converting to a generator, and then back to a list is not free
21:10:57 <Cale> because you effectively have to rebuild the list
21:11:03 <remexre> yeah
21:11:14 <Cale> So, the implementation of fmap given there
21:11:31 <Cale> which converted from a generator to a list, then did list map, then converted back to a generator
21:11:32 <remexre> oh, yeah yeah yeah yeah
21:11:57 <remexre> but in my linear scan, I only really need the (Generator a), so I don't pay the cost of converting back to a list
21:13:04 <dmwit> ???
21:13:14 <Cale> Yeah, I'm not sure what to make of that sentence
21:13:34 <Cale> There's a fundamental time/space tradeoff which takes place in the choice of data structures here
21:13:57 <remexre> since presumably {-# RULES generatorToList . G == id #-}, or w/e the syntax is
21:14:05 <Cale> The key thing is that lists are essentially cached -- if you evaluate a list, it stays evaluated as a data structure in memory and you can walk over it again cheaply
21:14:59 <remexre> yeah, I guess I just want the trade-off to be different for this case -- in my real code I'll probably just do the DiffList a = [a] -> [a], but I guess I still wanna see where my idea falls down vs normal lists
21:15:19 * dmwit feels lost
21:15:27 <Cale> If Generator a is just () -> [a]
21:15:48 <Cale> then basically the only difference is that you're never going to memoise the result
21:16:20 <Cale> (and with that representation, you also don't make any operations any faster...)
21:17:10 <c_wraith> I can see wanting to not memoize it, actually
21:17:16 <Cale> Sure
21:17:16 <c_wraith> lists can be memory hungry
21:17:20 <Cale> absolutely
21:18:02 <Cale> Also, possibly we want something more like  newtype Generator a = G (() -> (a, Generator a))
21:18:20 <remexre> yeah, that matches somewhat more
21:18:21 <Cale> Just to make it even harder to start caching things accidentally
21:18:30 <remexre> I think it has to be a linear arrow, though
21:18:34 <Cale> nah
21:18:48 <Cale> Just don't keep the old function :P
21:18:55 <Cale> Or do, it shouldn't matter all that much
21:20:02 <Cale> Of course, space performance will vary depending on what you choose to retain
21:20:34 <Cale> But making it a linear arrow just ties your hands in a not-necessarily-all-that-useful way
21:20:55 <Cale> It doesn't actually help performance once you're done with the hand-tying
21:21:48 <remexre> hm, I guess in the reverse I'm conceptualizing, it wouldn't, yeah
21:23:33 <Cale> (btw, the Linear Haskell proposal seems pretty useless to me on the whole, and I hope it never sees the light of day in GHC in its current form)
21:24:15 <remexre> heh, never actually read it; I know linearity at a "water-cooler convo" level
21:24:23 <remexre> and whatever amount of intuition one gets from Rust, I guess
21:24:56 <Cale> I've only seen one example use case that I actually thought was of any real value, and it involved making a certain use of unsafePerformIO safe
21:25:27 <remexre> runST?
21:25:42 <Cale> No, runST doesn't require Linear Haskell
21:25:54 <remexre> doesn't it need unsafePerformIO presently?
21:25:56 <c_wraith> Yeah, it seems like you could use use it to write an ST-like library that worked without a sealed scope.
21:25:56 <Cale> It was something kind of like runST, but without a monad.
21:26:10 <Cale> remexre: I mean the Linear Haskell thing still used unsafePerformIO
21:26:16 <remexre> oh, unfortunate
21:26:21 <Cale> and the linearity was making it safe
21:26:49 <Cale> It was a kind of cool trick for performing operations on binary encoded data as if it were unencoded
21:27:02 <Cale> and doing an in-place rewriting traversal
21:27:10 <remexre> oh, neat
21:27:19 <Cale> I'm not convinced you couldn't do the same trick with a combinator library though
21:28:14 <Cale> It was a neat trick though, and possibly the fact that you could linear lambda bind things that you wouldn't have names for in the combinator-style approach would make the resulting code more readable, I'm not certain
21:28:39 <Cale> I should get around to actually writing the combinatory version.
21:29:00 <Cale> Literally every other example use-case I've seen for Linear Haskell is entirely impractical imo though.
21:29:26 <Cale> All the resource management stuff means you give up the ability to handle exceptions in the middle of the linear part.
21:29:32 <JesseL> what function can work on [Just a]? this a
21:29:46 <Cale> uhhh
21:29:52 <JesseL> fmap f [Just a], let f work on a
21:30:07 <Cale> fmap f [Just a] = Just (f a), yeah
21:30:10 <JesseL> fmap [Just (f a)]
21:30:11 <dmwit> > fmap (fmap f) [Just a]
21:30:13 <lambdabot>  error:
21:30:13 <lambdabot>      • Ambiguous type variable ‘b0’ arising from a use of ‘show_M805001915279...
21:30:14 <lambdabot>        prevents the constraint ‘(Show b0)’ from being solved.
21:30:15 <remexre> JesseL: hm, something w/ the Compose functor?
21:30:18 <dmwit> > fmap (fmap f) [Just x]
21:30:21 <lambdabot>  error:
21:30:21 <lambdabot>      • Ambiguous type variable ‘b0’ arising from a use of ‘show_M824497465316...
21:30:21 <lambdabot>        prevents the constraint ‘(Show b0)’ from being solved.
21:30:22 <Cale> oh, fmap (fmap f) yeah
21:30:25 <Cale> I see what you're asking
21:30:25 <dmwit> too bad
21:30:43 <Cale> > fmap (fmap f) [Just (x :: Expr)]
21:30:44 <dmwit> > fmap (fmap (1+)) [Just 0]
21:30:45 <lambdabot>  error:
21:30:46 <lambdabot>      • Ambiguous type variable ‘b0’ arising from a use of ‘show_M678955512901...
21:30:46 <lambdabot>        prevents the constraint ‘(Show b0)’ from being solved.
21:30:47 <lambdabot>  [Just 1]
21:30:50 <Cale> tsk, sorry :)
21:30:50 <MarcelineVQ> fmap twice
21:31:03 <dmwit> oh
21:31:09 <Cale> with enough explicit type signatures, it would eventually work
21:31:10 <dmwit> > fmap (fmap (f :: Expr -> Expr)) [Just x]
21:31:13 <lambdabot>  [Just (f x)]
21:31:16 <Cale> yeah, it was f
21:31:39 <Axman6> :t f
21:31:40 <MarcelineVQ> oops, the world paused, wonder when my message came through, surely not at the right time
21:31:41 <lambdabot> FromExpr a => a
21:32:20 <JesseL> this can use MonadTrans?
21:32:28 <dmwit> With great effort.
21:32:31 <JesseL> Maybe and []
21:32:31 <dmwit> It is not worth it.
21:32:43 * hackage geojson 4.0.2 - A thin GeoJSON Layer above the aeson library  https://hackage.haskell.org/package/geojson-4.0.2 (newmana)
21:32:56 <JesseL> just run it once, not big deal
21:33:04 <dmwit> fmap (fmap f) is easy.
21:33:09 <dmwit> Why do you dislike it?
21:33:38 <dmwit> (I am not judging you. I just want to understand. It is easier to give good advice when I understand.)
21:33:39 <JesseL> I don't now, 'cause of fmap twice?
21:34:19 <dmwit> :t getCompose
21:34:21 <lambdabot> error: Variable not in scope: getCompose
21:34:29 <dmwit> % import Data.Functor.Compose
21:34:29 <yahb> dmwit: 
21:34:42 <dmwit> % (getCompose . fmap (1+) . Compose) [Just 0]
21:34:42 <yahb> dmwit: [Just 1]
21:35:00 <dmwit> Again, I think this effort is not worth it.
21:35:22 <JesseL> ok
21:35:30 <dmwit> > (length "import Data.Functor.Compose getCompose . fmap f . Compose", length "fmap (fmap f)")
21:35:33 <lambdabot>  (57,13)
21:36:17 <dmwit> You pay 300% overhead to switch from two fmap's to one. =P
21:36:19 <Cale> JesseL: If you're working with lists of Maybes so much that it bothers you, it might be valuable to define a newtype, and write a Functor instance for it.
21:37:13 <remexre> Cale: okay, now I think I fully get it; if I write out the type of the stack/continuation/thing held by the generator, it ends up being a (SnocList a -> SnocList a)
21:37:24 <remexre> er, for the generator created for reverse
21:37:46 <Cale> makes sense
21:56:18 <JesseL> T.isInfixOf "=" "ab=" == True, why?
21:56:24 <JesseL> import Data.Text as T
21:58:14 <MarcelineVQ> "The 'isInfixOf' function takes two 'Text's and returns 'True' iff the first is contained, wholly and intact, anywhere within the second."    "="  is within  "abs="
21:58:54 <JesseL> then what's the isSuffixOf use for?
21:59:16 <Cale> For checking if the given Text occurs at the end of the other
21:59:20 <JesseL> isPrefixOf should be check if it starts with
21:59:30 <Cale> it does
21:59:38 <JesseL> isSuffixOf should be check if it ends with
21:59:41 <ysangkok> why would you assume they are mutually exclusive?
22:00:05 <JesseL> then isInfixOf should be check if it is in and not the head not the last
22:00:14 <Cale> That's less useful
22:00:43 * hackage wkt-geom 0.0.11 - A parser of WKT, WKB and eWKB.  https://hackage.haskell.org/package/wkt-geom-0.0.11 (newmana)
22:00:43 <ysangkok> the head is an element, not necessarily a list
22:01:07 <JesseL> it's useful when parse like "a=b"
22:01:08 <Cale> Note that the other two aren't mutually exclusive either
22:01:24 <JesseL> knowing if '=' is in "a=b"
22:01:26 <Cale> isPrefixOf "foo" "foo" is True
22:01:33 <JesseL> "=ab" "ab=" "a=b"
22:01:53 <JesseL> isInfixOf "=" "ab=" is True
22:01:55 <Cale> It does check if = is in "a=b"
22:02:20 <Cale> isPrefixOf also doesn't check that it's not a suffix
22:02:21 <JesseL> but isInfixOf "=" "ab=" should be False
22:02:37 <JesseL> fine
22:02:45 <MarcelineVQ> because you can add that check yourself, isInfix of + not isSuffixOf, there's not a good reason to restrict isInfixOf
22:04:49 <Cale> You could also possibly use splitOn
22:05:23 <Cale> case T.splitOn "=" eqn of [x,y] -> ... success ... ; _ -> ... failure ...
22:05:58 <Cale> Well, then you probably want to guard the success case with checking that x and y aren't entirely whitespace or empty
22:06:14 <Cale> But also, this is not really how to do robust parsing
22:06:21 <JesseL> yes, I have already filter spaces and new lines
22:07:15 <JesseL> https://paste.ubuntu.com/p/M79yW9spgt/
22:07:30 <JesseL> read and parse a config file
22:08:11 <JesseL> filter comments, spaces, newlines, empty strings
22:08:11 <Axman6> there is no good reason why isInfixOf "=" "ab=" should be false
22:08:16 <JesseL> and check syntax
22:08:55 <JesseL> because infix is meaning?
22:09:17 <monochrom> I want to play another angle.
22:09:31 <monochrom> 1. There is no good reason to name that function "isInfixOf".
22:09:41 <monochrom> 2. There is no good reason to trust "meaningful" names.
22:10:02 <Axman6> isPrefix means is the string at the beginning, isSuffix means is it the end of the string, and isInfix means is it contained
22:10:27 <JesseL> then why not name it as isContained?
22:10:32 <Cale> list2Tuple is an uncatchable exception waiting to happen btw
22:11:01 <JesseL> yeah, when it's [a,b,c]
22:11:04 <Axman6> monochrom: agreed, the only meaningful thing would be a type level proof of the functionality of the function itself
22:11:24 <Cale> Probably you want the function which checks the syntax to give you Maybe the tuple that you wanted.
22:11:45 <JesseL> but that's not gonna happen, checkSyntax will check it
22:12:45 <JesseL> when there're two more '=' will product [a,b,c], but checkSyntax will check it
22:12:49 <Cale> It might not happen in the code as written, but you're missing out on a chance for the type system to remove the potential for this failure altogether
22:13:02 <JesseL> so list2Tuple won't work on [a,b,c] or [a]
22:13:37 <JesseL> only when isInfixOf do what I thought it should do
22:13:50 <JesseL> but apparently it doesn't
22:13:58 <Cale> If the thing which is checking that the expression is syntactically correct has to produce the pair directly, then you don't have to worry about the list being the wrong length
22:36:23 <JesseL> splitWith x xs = L.filter (/= "") . T.splitOn x $ xs
22:37:08 <JesseL> checkSyntax op txt = if | T.isInfixOf op txt -> if | (L.length . splitWith op $ txt) == 2 -> Just txt | otherwise -> Nothing | otherwise -> Nothing
22:41:36 <Axman6> looks like you need parser combinators
22:43:31 <JesseL> which one? happy, parserc or autoparse?
23:23:13 * hackage http2 2.0.4 - HTTP/2 library  https://hackage.haskell.org/package/http2-2.0.4 (KazuYamamoto)
23:56:49 <Poscat[m]> Is it possible to log the details of HTTP requests when using servant and servant-client?
23:57:36 <Amir51> Hello! I have a question regarding persistent data structures, and I thought Haskell-ers would be knowledgeable on the topic
23:59:00 <jackdk> don't wait for permission to ask, just ask. Hopefully someone is familiar enough to help
