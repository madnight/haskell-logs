00:05:47 <jchia> jackdk: I just sent the question to haskell-cafe.
00:06:14 <c_wraith> wow, I had an old enough version of Streaming that the Applicative instance was entirely different
00:06:32 <c_wraith> err, Alternative
00:08:55 <jchia> c_wraith: Which version?
00:09:21 <c_wraith> No longer know (I removed it), but my hackage index cache was 441 days old. :P
00:32:45 * hackage barbies-th 0.1.4 - Create strippable HKD via TH  https://hackage.haskell.org/package/barbies-th-0.1.4 (FumiakiKinoshita)
00:42:04 <c_wraith> jchia: did you see copy? https://hackage.haskell.org/package/streaming-0.2.3.0/docs/Streaming-Prelude.html#v:copy
00:43:27 <jchia> c_wraith: Thanks. I'll try it.
00:48:47 <liiae> what is dependency injection?
00:49:13 <carbolymer> an implementation of inversion of control principle
00:49:36 <liiae> carbolymer: is there a stuff in hasklll?
00:51:44 <Axman6> there are lots of things that can be considered dependency injection.  the Reader monad is probably the most common example
00:52:57 <carbolymer> liiae, I did like 15 min research a year ago and I found https://stackoverflow.com/questions/14327327/dependency-injection-in-haskell-solving-the-task-idiomatically and https://www.schoolofhaskell.com/user/meiersi/the-service-pattern
00:52:59 <liiae> Reader monad is about environment
00:53:41 <c_wraith> jchia: this works for me, but notice the order matters, as S.length_ doesn't produce r:   S.length_ $ S.print $ S.copy $ foldr S.cons (return ()) [10..20]
00:54:25 <Axman6> liiae: when you ask a question of people who have much more experience with that topic than you do, it's usually considered pretty rude to them tell them they are wrong. what is an environment? A set ao dependencies passed into a program at runtime, usually
00:55:32 <liiae> Axman6: sorry
00:56:21 <liiae> there're terms do different behavior btween haskell and the others
00:56:49 <c_wraith> ... speaking of streaming, why did I have to write foldr S.cons (return ())?  Shouldn't there be some sort of fromFoldable already?
00:57:13 <liiae> interface can do communication between class in other OO langauges, 
00:57:30 <liiae> interface can do ad-hoc polymorphism in haskell
00:57:35 <liiae> typeclass
00:58:03 <c_wraith> that's usually a pretty counterproductive way to look at typeclasses
00:58:45 <c_wraith> The natural conclusion of doing so is that you create a bunch of data types that carry no data just to use their instances
00:58:56 <c_wraith> Which is super-awkward when you could just be passing around a function.
01:00:36 <liiae> I can't implement that Fix f in Kotlin, and I can't implement that interface communication between class in haskell...
01:01:34 <c_wraith> but you don't need it.  You've got functions.
01:01:49 <Axman6> replace "I can't" with "I don't know how to"
01:01:57 <c_wraith> Or in more complex cases, records of functions
01:02:01 <liiae> that's right
01:02:13 <liiae> I don't know how to
01:02:19 <liiae> how to do x in y
01:07:16 <tdammers> "dependency injection" is a very simple concept, really. it just means that when you have a section of code that depends on some other code elsewhere, rather than hard-coding a reference to it into the use site, we pass it it from outside
01:08:48 <tdammers> for example, when I write this: do { config <- readFile "./config.xml" >>= parseConfig; ... }, then the filename "./config.xml" is a dependency that I've hard-coded into this `do` block
01:09:10 <dminuoso> Dependency injection is a very "patterny" term for passing arguments.. :)
01:09:30 <tdammers> it's better to have the caller provide the dependency for me, "inject" it
01:09:50 <tdammers> so I have them pass it as an argument
01:10:13 <tdammers> in a language like Haskell, it almost goes without saying, you just avoid hard-coding things
01:10:35 <tdammers> but in OOP, it's less obvious, and people like to use more elaborate machinery to make it happen
01:11:24 <tdammers> and then there's also the fact that when you have a lot of these dependencies, you need to pass a lot of arguments, and that makes for clumsy code, so you need a way to wrap that up and reduce the boilerplate
01:12:07 <tdammers> in Haskell, we can use things like Reader (or ReaderT) for that, while OOP languages typically require more elaborate machinery
01:12:24 <tdammers> this is also why you won't see a "dependency injection framework" in Haskell anytime soon
01:12:47 <maerwald> that's why I always say transformers are actually OOP :P
01:13:06 <merijn> maerwald: You mean mtl?
01:13:09 <tdammers> MonadReader and transformers are somewhat orthogonal
01:13:29 <tdammers> MonadReader is just a typeclass that generalizes the semantics of Reader / ReaderT
01:14:01 <maerwald> merijn: no
01:14:08 <tdammers> it's useful in a monad stack design, but you can just as easily implement MonadReader for a monadic context that isn't constructed as a monad stack
01:17:38 <lortabac> tdammers: there is a package called registry, which is a DI framework, but TBH it doesn't look particularly useful
01:19:16 <tdammers> lortabac: sure, of course these things *exist*, I've just never seen people use them for practical real-world projects
01:23:51 <zincy_> Is creating a typeclass for the sole purpose of marshalling between types useful or just plain weird?
01:24:56 <merijn> zincy_: 95% chance of it being bad :)
01:25:35 <tdammers> is there a specific reason why a set of monomorphic conversion functions won't do the trick?
01:25:43 <maerwald> between two unknown types?
01:32:01 <zincy_> merijn: :D
01:32:28 <zincy_> There is absolutely no reason why that isn't a better approach.
01:32:42 <zincy_> maerwald: Known types
01:33:24 <maerwald> zincy_: no, I mean: class To a b | a -> b where to :: a -> b
01:33:26 <maerwald> sth like that
01:34:10 <merijn> maerwald: That still ends up sucking every single time I've tried that/seen it tried
01:34:20 <tdammers> there's 
01:34:24 <tdammers> Convert / Convertible
01:34:27 <tdammers> common pattern
01:34:38 <tdammers> but the problem is that sometimes you want the fundep, and sometimes you don't
01:35:11 <merijn> tdammers: And then you try and use it with a literal and it all goes to shit due ambiguous types, etc.
01:35:16 <tdammers> without it, you have to use type applications or type annotations everywhere to disambiguate, but with it, you can't have all the instances you want
01:35:19 <merijn> Or half the instances overlap
01:35:26 <tdammers> that too 
01:35:46 <tdammers> as in, writing instances for higher-kinded types gets tricky fast
01:36:22 <tdammers> instance To (Maybe a) a, and also instance To a b => To (Maybe a) (Maybe b) -- hmm
01:43:15 * hackage tdlib 0.1.5 - complete binding to the Telegram Database Library  https://hackage.haskell.org/package/tdlib-0.1.5 (Poscat)
01:59:21 <untseac> hello. do you guys know a site like this but for haskell? https://www.scala-exercises.org/
02:00:08 <merijn> @where exercises
02:00:08 <lambdabot> http://www.haskell.org/haskellwiki/H-99:_Ninety-Nine_Haskell_Problems https://github.com/bitemyapp/learnhaskell http://www.reddit.com/r/dailyprogrammer/ http://www.reddit.com/r/programmingchallenges/
02:00:50 <untseac> lol great. thanks merijn 
02:05:12 <merijn> Of course there's Advent of Code too, I think they keep their past puzzles online
02:09:19 <untseac> well these are not as pretty as scala exercises but it's a good start
02:09:20 <liiae> newtype ReaderT r m a = ReaderT { runReaderT :: r -> m a }
02:09:37 <liiae> when m is IO, which value has ReaderT r IO a?
02:10:18 <merijn> liiae: "r -> IO a"
02:10:57 <liiae> merijn: this is not a value
02:11:07 <liiae> :t (+1)
02:11:08 <lambdabot> Num a => a -> a
02:11:10 <liiae> :t id
02:11:11 <lambdabot> a -> a
02:12:57 <liiae> f :: a -> IO a; f x = return x
02:13:24 <liiae> ReaderT f :: ReaderT a IO a?
02:13:47 <merijn> Yes
02:15:05 <liiae> what's the point? the effect for ReaderT, the meaning of ReaderT, what it's used to
02:15:38 <merijn> not explicitly having to pass the 'r' argument everywere
02:16:56 <liiae> for example?
02:17:16 <liiae> f r; id r; const r _; ...
02:17:20 <absence> is it possible to generalise functions like intersperse to something like Foldable or Traversable, or do they mandate that all "contained" elements are treated equally?
02:17:51 <merijn> absence: Not really, because that'd require some way to extend those structures
02:18:17 <merijn> absence: Or rather, you could come up with a structure that allowed that, but there'd be very few instances and probably not worth the effort
02:18:18 <liiae> g r = const r $ f r?
02:18:59 <Jon> I've just drafted a blog post on Template Haskell, based on something I worked on recently. If anyone feels like reading a draft, I'd be interested in feedback on clarity before I make it public. TIA! https://jmtd.net/log/template_haskell/boilerplate/
02:19:29 <absence> merijn: yeah, for Traversable i guess that would be a problem, but doesn't Foldable functions do away with the structure?
02:20:01 <merijn> absence: But then what would intersperse even do?
02:20:15 <merijn> absence: Also, the Foldable case is already doable
02:20:21 <absence> merijn: the interspersed values would be part of the folded result
02:20:32 <merijn> "intersperse x . Foldable.toList"
02:20:57 <absence> ahh right
02:21:11 <merijn> absence: I guess that could work, but otoh it seems kinda niche
02:21:50 <absence> merijn: it's nice to abstract over [] and NonEmpty at least
02:23:45 * hackage hadolint 1.17.7 - Dockerfile Linter JavaScript API  https://hackage.haskell.org/package/hadolint-1.17.7 (lorenzo)
02:28:44 <liiae> ReaderT f :: a -> IO a;  
02:28:57 <liiae> ReaderT f "a" :: IO String, why this is wrong?
02:31:57 <[exa]> I'm somehow missing the IO param for the transformer there
02:32:17 <liiae> oh
02:32:36 <liiae> ReaderT f :: ReaderT a IO a
02:33:32 <[exa]> ok good, now what about extracting the IO with 'runReaderT'?
02:34:26 <liiae> runReaderT :: r -> m a
02:34:54 <[exa]> :t runReaderT
02:34:55 <lambdabot> ReaderT r m a -> r -> m a
02:35:02 <[exa]> 'tis a field
02:35:58 <liiae> newtype ReaderT r m a = ReaderT { runReaderT :: r -> m a }
02:35:59 <liiae>  
02:36:25 <liiae> runReaderT ReaderT r :: m a
02:36:45 <merijn> bleh, why is split-sections not the default yet? >.>
02:37:02 <[exa]> anyway, the point is that `ReaderT f "a"` for you will generate the IO action (provided `f` is some parametrized IO) wrapped in the ReaderT, you need to unwrap it to get plain IO
02:37:27 <[exa]> :t runReaderT (ReaderT (\x -> print x >> return x))   --liiae
02:37:29 <lambdabot> Show a => a -> IO a
02:39:12 <jchia> I have a recursive function go :: Vector a -> b -> b. It recursively calls itself with a modified first argument (Vector a). To avoid making new Vector copies in each recursion, is it a good idea to use STVector instead and run in an ST? The Vector is basically keeping records using things read from the b.
02:39:30 <jchia> The size of the Vector is fixed
02:39:45 <jchia> or is that too impure?
02:39:46 <liiae> [exa]: runReaderT ReaderT f == f?
02:39:52 <merijn> jchia: "maybe"
02:40:06 <merijn> jchia: Are you reading and updating the entire Vector each step?
02:41:07 <siers> are functions, lenses and json codecs profunctors? (because the two latter are essentially holding a function inside)
02:41:14 <jchia> merijn: Updating one element in each step
02:41:21 <merijn> jchia: Then "maybe"
02:42:06 <merijn> jchia: but vector is pretty good at doing immutable updates fast, tbh
02:42:28 <jchia> The haddock for Data.Vector.modify superficially suggests that it tries to avoid making a copy but I've heard that it always almost makes a copy. https://hackage.haskell.org/package/vector-0.12.1.2/docs/Data-Vector.html#v:modify
02:42:33 <merijn> I spend a day replacing my immutable updates with ST and it had 0 speed impact >.>
02:44:27 <lortabac> jchia: if O(1) update/indexing is not a requirement, you may consider Data.Sequence
02:45:07 <merijn> lortabac: I highly doubt that'll be faster
02:45:30 <merijn> jchia: Also, if you want speed you'll probably gain a lot more by switching to Unboxed vectors
02:45:30 <lortabac> merijn: it won't be faster, but it will not copy the entire sequence
02:45:49 <lortabac> it depends on the requirements
02:45:51 <merijn> What's the point of avoiding copies if not to be faster?
02:46:45 * hackage tdlib-types 0.2.0 - Types and Functions generated from tdlib api spec  https://hackage.haskell.org/package/tdlib-types-0.2.0 (Poscat)
02:46:53 <lortabac> I don't know, faster updates, at the cost of slower reads
02:47:35 <merijn> No, I meant that I'm not even sure that'll be faster for updates
02:48:28 <merijn> In fact, if his vectors are fairly small (lets say sub-100 elements) then I'm almost certain that no way Data.Sequence's updates will be faster than unboxed vector and just copying
02:49:07 <merijn> Rule #1 of fast code: Arrays are the universal data structure of high performance :p
02:49:09 <lortabac> I had a similar problem recently, but my vectors were > 1 milion elements
02:49:37 <lortabac> it was super-slow, because of copying
02:51:45 * hackage mason 0.2.1 - Fast and extensible bytestring builder  https://hackage.haskell.org/package/mason-0.2.1 (FumiakiKinoshita)
02:52:03 <liiae> [exa]: main =  runReaderT (ReaderT f) "3" >>= print
02:54:32 <liiae> f :: (Show a) => a -> IO ()
02:54:37 <liiae> f x = print x
02:54:41 <liiae> main =  runReaderT (ReaderT f) "3"
02:54:56 <liiae> runReaderT (ReaderT f) == f
02:58:13 <liiae> main = runReaderT (ReaderT print) "3"
03:02:44 <liiae> main = flip runReaderT "3" $ ReaderT (\x -> print x)
03:04:36 <dminuoso> If you have a `T` and a filtered `T` that you drag around, would you prefer a type level phantom type to keep them apart (In the sense of `T Filtered` and `T Unfiltered`) in type signatures, or newtypes?
03:05:45 * hackage incremental 0.3 - incremental update library  https://hackage.haskell.org/package/incremental-0.3 (FumiakiKinoshita)
03:06:27 <merijn> dminuoso: Why not both? ;)
03:07:00 <merijn> dminuoso: DataKinds phantom in the newtype! :p
03:08:44 <liiae> f g x = g x; this is dependency injection?
03:09:11 <merijn> Yes, no, maybe?
03:09:45 <dminuoso> merijn: T already is a newtype :)
03:09:46 <liiae> main = flip runReaderT "3" $ ReaderT (\x -> do print x; print (x <> "2"))
03:09:49 <dminuoso> Well, a data to be exact.
03:10:06 <liiae> that x is "3", 
03:10:31 <liiae> but what this runReaderT is related to DI?
03:15:14 * hackage tehepero 0 - Prettier error  https://hackage.haskell.org/package/tehepero-0 (FumiakiKinoshita)
03:15:34 <dminuoso> liiae: Imagine having some common environment containing a few logger functions, that is available everywhere.
03:16:02 <dminuoso> Instead of just hardcoding uses of `hPutStrLn` in all your code, you use that common environment that is "injected" somewhere outside.
03:16:42 <dminuoso> In Haskell terms that boils down to having some `data Env = Env { logWarn :: String -> IO a, logErr :: String -> a }`, and adding a `Env` argument to every function/binding.
03:17:32 <dminuoso> So some `IO Int` becomes `Env -> IO Inv`, some `String -> IO Char` becomes `Env -> String -> IO Char`. All Reader/ReaderT helps with, is that you dont have to explicitly pass that Env around in your entire program.
03:18:17 <dminuoso> That is essentially "dependency injection". It's just argument passing to us because adding a function argument to anything is not hard for us.
03:21:46 <liiae> dminuoso: wait where is that a come from in `data Env = ...`
03:22:57 <dminuoso> oh sorry
03:23:09 <dminuoso> `data Env = Env { logWarn :: String -> IO (), logErr :: String -> () }`
03:23:11 <dminuoso> gah
03:23:13 <dminuoso> `data Env = Env { logWarn :: String -> IO (), logErr :: String -> IO () }`
03:23:14 <dminuoso> Of course :)
03:25:49 <liiae> then there wouldn't be "So some `IO Int` becomes `Env -> IO Inv`"
03:26:32 <liiae> logWarn Env String :: IO ()
03:26:40 <arahael> Sounds practical, i get the impression that we're supposed to do some sort of fancy free monad. So rather than `IO Int`, we have `Env Int`
03:27:00 <merijn> arahael: Sure, if you hate speed ;)
03:27:32 <liiae> logErr Env String :: IO ()
03:27:36 <arahael> merijn: I've never done it. ;)
03:28:37 <liiae> wrong, logWarn Env :: IO ()
03:28:50 <liiae> logWarn (Env "3) :: IO ()
03:29:38 <liiae> I mixed up...
03:30:24 <liiae> data Env = Env (String -> IO ())
03:30:43 <liiae> logWarn Env :: String -> IO ()
03:31:22 <arahael> merijn: Seriously though, is there really a significant impact on speed?
03:31:52 <merijn> arahael: All the free monads/extensible effects stuff performs much worse than transformers atm
03:32:16 <arahael> merijn: Blegh.  They seem so, so sexy when it comes to testability, though. :(
03:32:48 <liiae> logWarn (Env print) :: a -> IO ()
03:33:37 <utdemir[m]> Depends what you're doing, I guess. Yeah, they'd be much slower than, say, ST. But if you're doing I/O on your monad stack that will be the bottleneck rather than the overhead of dictionary lookups.
03:35:01 <merijn> utdemir[m]: I was referring to his comment about free monads instead of ReaderT
03:37:29 <merijn> Does vector have a convenient way to do a parallel map (using sparks, maybe?)
03:56:45 * hackage cachix 0.3.8 - Command line client for Nix binary cache hosting https://cachix.org  https://hackage.haskell.org/package/cachix-0.3.8 (domenkozar)
04:05:38 <untseac> what's a free monad/extensible effect?
04:06:16 <dminuoso> untseac: Are you familiar with what a free monoid is?
04:06:21 <untseac> nop
04:06:26 <untseac> is it this? http://okmij.org/ftp/Haskell/extensible/index.html
04:07:14 * hackage nix-narinfo 0.1.0.1 - Parse and render .narinfo files  https://hackage.haskell.org/package/nix-narinfo-0.1.0.1 (srk)
04:09:31 <untseac> watching a video on it. I'm betting it's better that I know free monoid first.
04:09:41 <dminuoso> Right, it might help you put into the right mindset.
04:09:55 <dminuoso> It's related in the sense of freeness, but unrelated otherwise.
04:14:16 <timCF> Hello! Any Aeson users there? I'm having a problem with weird deriving bug. For example I have a type `data CurrencyCode = BTC deriving (Generic)` and then I'm using GHC.Generic implementation of FromJSON class like this `instance FromJSON CurrencyCode`. But for some reason when I'm trying to decode some JSON value, it expects empty list as valid
04:14:17 <timCF> CurrencyCode value: `eitherDecode "[\"BTC\"]"` causing the error `Left "Error in $: parsing () failed, expected an empty array"`
04:15:48 <dminuoso> timCF: What happens if you encode `[BTC]`?
04:16:15 <dminuoso> Oh wait.
04:16:24 <dminuoso> timCF: `eitherDecode "[\"BTC\"]` did you try this without a type signature?
04:16:24 <timCF> Hm, I don't need `ToJSON` but I'll try it
04:16:40 <timCF> I'm trying it with type signature
04:16:46 <dminuoso> What type signature did you use?
04:16:54 <timCF> Just omitted it there in chat
04:17:17 <timCF> type is `Either String [CurrencyCode]`
04:19:37 <timCF> dminuoso `encode [BTC]` returned `"[[]]"`
04:20:34 <dminuoso> Interesting, so BTC is encoded into an empty array
04:20:50 <timCF> The interesting thing is that type is very simple, and I'm not writing implementation by hand. There is no possibility to make mistake, but for some reason it's working this way
04:22:10 <dminuoso> Indeed, singleton datatypes appear to be encoded into an empty array.
04:22:37 <dminuoso> The obvious solution is to write out the instance by hand.
04:22:47 <dminuoso> (I just tried it locally)
04:23:09 <dminuoso> https://hackage.haskell.org/package/aeson-1.5.1.0/docs/src/Data.Aeson.Types.FromJSON.html#line-1523
04:23:32 <timCF> Yes, probably the easiest fix for that simple type
04:23:33 <dminuoso> It appears to be intentionally - presumably the generics will do the same thing if its isomorphic to unit.
04:24:00 <dminuoso> Ah, I found out why
04:24:15 <dminuoso> Apparently tuples in general are encoded as arrays, and () is considered an empty tuple for this.
04:25:33 <dminuoso> I guess its yet another reason why generic based instances shouldnt be used. You're safer off writing them out by hand. :)
04:27:33 <timCF> Idk, I'm fan of TH, but GHC.Generics seems cool as well. Though not so powerful as TH.
04:30:04 <dminuoso> One big issue is that it couples your code to representation. 
04:31:21 <dminuoso> That is, you're forced to make semantic ADTs anyway, so you have `data FooReal` and `data Foo` and write the real "serialization" in `Foo -> FooReal`.
04:31:38 <arahael> That's why I never really like a lot of the fancy ORM wrappers that a lot of other languages have.  Database schema becomes tightly coupled to however it is you've defined it in the application.
04:32:39 <dminuoso> Worse, if the API is not even under your control, you end up having quirky data structures that you need to dismantle somewhere else.
04:33:42 <arahael> I'm an old-fashioned believer in code genreration. :( Have a schema, use code-gen to spit out the proper output. :(  (Ie, what it sounds like TH is frequently used for, here)
04:33:47 <dminuoso> Indeed! :)
04:34:16 <dminuoso> The effort to write the boilerplate aeson is very minimal though, in most cases it takes not so much time. And you gain so much down the road from increased maintainability and separation of concerns. :)
04:34:20 <arahael> It's clunky, suboptimal, etc - but it solves all the problems very elegantly.
04:34:48 <dminuoso> arahael: I actually like to use TH, generate the splices, and then copy/paste the splices into the code.
04:34:53 <arahael> And this cost is all compile-time.  Any problem you have with this interface, is at compile time, not in the field.
04:34:57 <dminuoso> So the TH is executed "ahead of compile-time" so to speak.
04:35:26 <arahael> dminuoso: Ah, I haven't gotten to that point yet.  Infact, honestly, I barely do TH at all - still at the "Haskell is my hobby" stage.
04:35:41 <arahael> But that sounds pretty cool, in either case.
04:36:10 <dminuoso> well Im writing a protocol library that needs to deal with thousands of fields - there's good machine readable format specifications, but a real pain to write all that by hand
04:36:39 <arahael> That's indeed screaming for code gen. :)
04:39:30 <tdammers> sounds one of those cases where TH is an acceptable tradeoff
04:40:43 <tdammers> it's still more elegant than outright source-level code genb
04:44:35 <dminuoso> It's easier to engineer for non-trivial code. Until now I used a prettyprint based code generator, but any modification likely causes incorrect Haskell code generation.
04:44:52 <dminuoso> And I found, the more I wrote the more I started mimicing the TH API.
04:45:28 <dminuoso> TH was made for generating Haskell code.. :)
04:45:58 <dminuoso> It's just a bit dirty to do sneaky IO during TH to load format specification files, but oh well.
04:46:24 <arahael> My one concern about TH is that it is apparently difficult to use it if you're cross-compiling, though I fear that my desire to cross-compile is at odds with modern software dev.
04:47:15 * hackage uri-bytestring-aeson 0.1.0.8 - Aeson instances for URI Bytestring  https://hackage.haskell.org/package/uri-bytestring-aeson-0.1.0.8 (reactormonk)
04:47:42 <dminuoso> If you splice it in by hand, there's no issue.
04:47:50 <dminuoso> Then there's just a separate compilation step.
04:48:40 <arahael> Nice.
04:49:13 <tdammers> the cross-compiling thing is a real concern, but it's more a result of how TH was implemented in practice than of the concept of TH itself
04:49:59 <tdammers> the only thing that's truly prohibitive about the current design of TH here is that both "meta" and "concrete" code share module scope, that is, whatever is imported gets imported both into the TH context and into the plain Haskell context
04:50:29 <dminuoso> Are there any plans to add TH scoped imports?
04:51:02 <dminuoso> I mean pragmas exist, it should be trivial to find something non-intrusive, no?
04:51:32 <dminuoso> {-# ThImport qualified Dub.Dab as D #-}
04:52:09 <arahael> tdammers: That does sound somewhat curious - I guess you'd work around that by doing the TH stuff in a separate module?
04:52:19 <dminuoso> arahael: You frequently cant.
04:52:32 <arahael> :(
04:52:41 <tdammers> yeah, I consider it a wart
04:53:07 <tdammers> if I were to design a meta-language for Haskell, I'd make the distinctions between the meta and concrete levels more explicit and clearer, including imports
04:53:51 <arahael> I'm suprised someone hasn't, I mean, GHC has a billion extensions already.
04:54:35 <dminuoso> tdammers: Actually, with the CPP processor and separate TH execution, you can achieve separation!
04:56:14 <dminuoso> Another big security issue with TH is that it can do IO. So adding a line into your build-depends can easily introduce create a security incident. If you split this off and force package vendors to pre-splice their TH, that problem would disappear already.
04:56:42 <arahael> I think Go splits it off nicely in theirs.
04:56:55 <dminuoso> (It wouldn't for users who use a packages TH facilities, but if you willingly execute someone elses code generation without audit, you deserve to have your computer owned)
04:57:28 <arahael> Now, *that* is something that is truely not in modern software dev. :( 
04:57:39 * arahael mutters "npm", "pip", etc.
04:57:44 <tdammers> dminuoso: compiling untrusted Haskell code is a big security no-no anyway
04:57:44 <arahael> "brew", even.
04:58:17 <dminuoso> tdammers: otoh, how often do you audit all transitive libraries in your projects? Repeatedly?
04:58:25 <tdammers> honestly: never
04:58:49 <arahael> Somehow I feel better using hackage, than, say, npm.
04:58:51 <tdammers> and there are more issues with that than just "TH can do IO"
04:59:12 <arahael> npm and pip have *serious* issues with security.
04:59:24 <tdammers> tbh, so does hackage
04:59:29 <dminuoso> arahael: They don't. The users do.
05:00:12 <arahael> dminuoso: The vast majority of the users barely even seem to care about it. :/
05:00:32 <arahael> tdammers: Hmm.
05:01:08 <dminuoso> arahael: Anyone can upload stuff to hackage.
05:01:16 <dminuoso> There's no formal audit process.
05:01:32 <tdammers> part of the reason why hackage isn't seeing any scandals is because it's a much smaller target. by several orders of magnitude.
05:01:42 <dminuoso> And even if you establish trust to a package because you know a packages author, in some point in the future maintainership is given/taken away.
05:02:23 <phadej> which reminds me, that I have to do that...
05:03:16 <tdammers> an intrinsic problem of cabal's "dependency resolution" based approach is that reproducible builds are a difficult problem
05:04:01 <dminuoso> Well you can get around that with nix at the very least.
05:04:09 <merijn> tdammers: Well, freeze file + index-state?
05:04:12 <tdammers> I was about to say that
05:04:28 <tdammers> merijn: plus somehow checking the integrity of it all
05:04:51 <merijn> tdammers: The tarballs are signed and checked, though
05:05:05 <tdammers> on the client side?
05:05:39 <merijn> When you download the index you get checksums of the tarballs
05:06:08 <tdammers> ah, yeah
05:06:30 <tdammers> so as long as you commit the index to your repo, you can verify
05:06:41 <arahael> I'm perfectly happy to do something like that.
05:06:47 <merijn> Also
05:06:56 <merijn> If you care about this, run your own hackage
05:07:01 <merijn> Then you control all these things
05:07:04 <arahael> I always commit my .lock files. 
05:07:11 * arahael quickly goes and commits his lock files.
05:08:00 <dminuoso> merijn: Running your own hackage. Hah. That's funny.
05:08:03 <merijn> tdammers: The index downloads are signed to, afaik
05:08:25 <arahael> Sometimes annoyingly difficult to justify the server to run the hackage.
05:08:26 <merijn> dminuoso: cabal supports multiple hackage servers, so running an internal verified one seems reasonable enough
05:08:43 <dminuoso> merijn: The issue is not cabal.. it's the hackage-operation part.
05:08:48 <jchia_> Is there a tutorial on how to use hoist in different situations? Staring at the type signature I don't get any intuition.
05:08:50 <merijn> tdammers: Now, if the hackage server itself is compromised you lose the verification, but then you're pretty hosed anyway
05:09:46 <dminuoso> I thought running our own hackage server was a great idea, until I tried to do it.
05:10:09 <dminuoso> Then I got in touch with some people that keep hackage.haskell.org running - now I don't think it's a good idea to do this yourself.
05:10:11 <merijn> tdammers: Anyway, there was discussion of package signing support for hackage and cabal-install, but like all features it won't exist until someone cares enough to implement it
05:11:14 <arahael> dminuoso: That's unfortunate. :(  What was the issue with it?
05:11:35 <arahael> dminuoso: In practice, I try to use specific git repos "directly" instead of hackage - hosting your own git server is trivial.
05:11:43 <tdammers> dminuoso: which is actually kind of bad, because conceptually, running your own artifact repo is *the* way to go
05:12:42 <tdammers> merijn: yeah, that's why I asked. though signing addresses a slightly different issue, namely that you want to infer trust in a package from trust in a package author, the package author's identity, and the integrity of the package code as asserted by the package author
05:13:15 <tdammers> whereas the scenario I was hinting at is rather just asserting that when you build your project in another environment, you get the same thing that you got locally.
05:13:39 <merijn> tdammers: Right, but that's solved by the index being signed and containing checksums of tarballs
05:13:47 <dminuoso> arahael: The best thing to look at is skete currently.
05:13:57 <dminuoso> hackage was never designed for people to run it themselves.
05:14:13 <merijn> tdammers: So if you assume the hackage-server index/keys aren't compromised, then you get a trusted index and checksums for all the tarballs with it
05:14:22 <tdammers> merijn: it is, if you manually vet all your dependencies and then lock them down via checksums
05:14:22 <Uniaika> :28
05:14:26 <Uniaika> (woops)
05:14:33 <merijn> tdammers: In which case index-state + freeze file is sufficient
05:14:41 <tdammers> merijn: yes, exactly
05:14:42 <dminuoso> arahael: The main problem with git is that it doesnt scale well. Bumping versions becomes a real pain in the butt for instance.
05:14:48 <dminuoso> Especially once you have a rich ecosystem of packages.
05:15:04 <merijn> dminuoso: The main problem with git is that it actively hates its users and has a shit UX >.<
05:15:20 <arahael> dminuoso: It scales well enough for individual projects, though.
05:15:47 <arahael> dminuoso: And you can often pin against branches, too.
05:16:26 <dminuoso> arahael: If it works for you, then that's really great. You'll notice when it stops working for you. ;)
05:16:32 <dminuoso> Until then it's perfectly fine
05:17:02 <arahael> dminuoso: When it stops working fine, I suppose we'd have a problem with the SOUP register. ;)
05:17:29 <arahael> (I don't actually use haskell at work, sadly, but the point remains)
05:31:41 <liiae2> in that Reader, add = do a <- (+1); b <- (*2); return a+b
05:31:55 <liiae2> what about ReaderT?
05:32:36 <liiae2> main = flip runReaderT "3" $ ReaderT (\x -> do print x; print x)
05:33:08 <liiae2> why this x is not explicit
05:33:35 <liiae2> is
05:41:50 <ezzieyguywuf> between State and Lens, it feels a lot like taking oop concept and "jamming" them into "pure" functional
05:41:55 <ezzieyguywuf> is that an unfair assessment?
05:42:23 <liiae2> err. that code couldn't run, http://learnyouahaskell.com/for-a-few-monads-more
05:42:27 <liiae2> about Reader
05:43:03 <merijn> ezzieyguywuf: Define "OOP"
05:43:27 <merijn> ezzieyguywuf: Like, what's OOP about State? There's no objects or inheritance anywhere
05:43:56 <merijn> And I guess next up would be "define jamming" and "define pure" :p
05:44:02 <ezzieyguywuf> lol
05:44:38 <kenran> I'm having trouble getting a nix derivation to work inside a container and I don't know why. The derivation uses leiningen to build a minimal clojure project and works just fine on my local machine: https://pastebin.com/x5AjKP6Z
05:44:44 <ezzieyguywuf> "OOP" = encapsulating stuff in classes with methods for modifying the guts, "jamming" = making stuff work that the language probably wasn't meant for, "pure" = not OOP
05:44:57 <ezzieyguywuf> and it's ok for you to say "you're just wrong ezziey"
05:45:17 <merijn> ezzieyguywuf: I mean, "pure" has a very specific meaning and there are, in fact, purely functional languages that support OOP
05:45:26 <ezzieyguywuf> interesting
05:45:29 <merijn> There was even an OO'Haskell project at some point
05:45:40 <cdunklau> ezzieyguywuf: mutation is not a core requirement for OOP
05:45:44 <ezzieyguywuf> yea maybe I'm just missing the point
05:45:46 <kenran> I found out that I have to export HOME in some way, or else leiningen won't build even on my machine. But after that it works fine locally.
05:45:58 <ezzieyguywuf> perhaps State and Classes are just two different angles to approach the same problem
05:46:06 <ezzieyguywuf> using different general programming, er, paradigms
05:46:34 <kenran> But after that, I get the following errors: https://pastebin.com/RtwSrvEm
05:46:53 <kenran> oh god, sorry guys, I misclicked the channel ><
05:47:10 <merijn> ezzieyguywuf: I mean the State type is just "I want a value to evolve over multiple operations", it's implementations is literally just "s -> (a, s)" which is about as functional as you get
05:51:40 <ezzieyguywuf> yea, the more I read about it the more I realize it is a convenience for not having to manually pass around tuples
05:51:56 <ezzieyguywuf> don't get me wrong, I'm enjoying haskell and the functional style
05:52:08 <ezzieyguywuf> these are just my ponderings
05:53:04 <merijn> Not to mention the fact that many people here would consider Haskell their favourite imperative language ;)
05:53:27 <ezzieyguywuf> I still don't know what 'imperitive' means
05:53:36 <ezzieyguywuf> but mostly because I jsut gloss over it anny time I read about it
05:54:07 <hc> "object orientation" has evolved, too. in some sense, you could consider an algebraic data structure in haskell an "object", just without any methods too closely attached to work with it
05:54:10 <merijn> ezzieyguywuf: Basically all the mainstream languages (well...I guess SQL is mainstream, and that's the exception) are imperative languages
05:54:49 <merijn> ezzieyguywuf: "Imperative language" basically meaning to a lot of direct concern of "how things should be done"
05:54:49 <Maxdamantus> I'd contrast imperative with declarative.
05:55:08 <merijn> Think of poster boys like C, where you spent a lot of time fiddling with how you expect variables to be mutated, etc.
05:55:14 <Maxdamantus> Where declarative means you declare things not in a particular order.
05:55:16 <merijn> ezzieyguywuf: For contrast, think of SQL
05:55:29 <ezzieyguywuf> merijn: ah, so C where you fiddle is imperitive?
05:55:47 <Maxdamantus> SQL is still fairly imperative.
05:55:48 <merijn> ezzieyguywuf: SQL is a declarative language, you state what result you expect to get and let the database sort out how to actually get it for you
05:56:14 <merijn> Maxdamantus: SQL (as in standard SQL) isn't
05:56:25 <merijn> Maxdamantus: vendor specific versions tend to be more so
05:57:11 <Maxdamantus> So you're thinking of within a single query, right? What sort of things are declarative there?
05:57:21 <merijn> ezzieyguywuf: If you got off the beaten path of programming languages you can find things like Prolog
05:57:29 <Maxdamantus> (I'm actually not too familiar with SQL, but my understanding of it doesn't involve much declaration)
05:58:02 <merijn> Maxdamantus: You specify what tables to get which data from and with which constraints. How that result set gets executed is completely opaque and irrelevant
05:58:57 <merijn> Maxdamantus: Maybe you're just thinking of overly simple queries, which are less obviously declarative
05:59:53 <merijn> Maxdamantus: Once you start joining 8+ tables there's *a lot* of different ways you could do that
05:59:54 <hc> ...irrelevant, unless you use a query planner to optimize performance ;) but yeah
06:00:01 <merijn> hc: Sssh!
06:00:32 <merijn> ezzieyguywuf: So the imperative vs declarative distinction is "how much of your code is specifying implementation details vs actual meaning"
06:01:32 <merijn> ezzieyguywuf: Where stuff like "every loop has a variable i that we increment and have to write out each time" is very much at the "imperative" side of things vs "map myFun" which is far more declarative (I don't care how my function gets applied to everything, just figure it out)
06:03:39 <tdammers> SQL is declarative in that you specify the constraints of the result set and how the result set should relate to persisted data, but you don't specify the steps that are necessary to fetch said data
06:03:47 <ezzieyguywuf> merijn: I think I'm starting to get it
06:03:53 <ezzieyguywuf> it seems like haskell tends to be more declarative
06:04:00 <merijn> ezzieyguywuf: Anyway, to address your way earlier comments. I find that newcomers are often overly dogmatic about "no mutable state ever!" I think you will find that most of the experienced Haskell programmers don't think like that at all
06:04:07 <tdammers> take for example this query: SELECT username, password FROM users WHERE username = 'tdammers' ORDER BY id LIMIT 1
06:04:08 <ezzieyguywuf> and yet you quipped that for many it is their favourite imperitive languauge.
06:04:46 <ezzieyguywuf> merijn: I think you hit the nail on the head. I'm trying to avoid such pitfalls, and so knowing that most experienced haskellers don't think that way is helpful to me
06:04:49 <ezzieyguywuf> thanks
06:04:51 <ezzieyguywuf> :)
06:04:54 <tdammers> you could do this by first fetching all the rows from the users table into RAM, sorting them by id, reducing them to the username and password columns, then filtering them by username, and then limiting the result to one row
06:05:36 <merijn> ezzieyguywuf: It's just that mutable state has an incredible large impact in your ability to reason about code. Since there's an upperbound to how much complexity your brain can handle. So where possible it should be avoided. But some problems are just much easier to solve efficiently with mutation
06:05:57 <tdammers> but you could also use an index to find the row that has username = 'tdammers' and the lowest id, and then reduce that one row to the username and password fields
06:06:11 <tdammers> the query doesn't say, and when you write the query, you don't usually care
06:06:27 <merijn> ezzieyguywuf: So Haskell defaults to no mutation (which is a much simpler default to think about), but we have like 8 different (maybe more?) ways of doing mutation
06:07:19 <merijn> ezzieyguywuf: What I meant by that is that I've written quite a lot of IO heavy code with lots of mutation in Haskell. And it's honestly quite nice compared to the same thing in, e.g. python :)
06:08:21 <merijn> "doing everything in IO with mutable state in Haskell" is a suboptimal way to use Haskell, but it honestly works perfectly fine and you can be incredibly productive that way :)
06:08:48 <tdammers> I like to think that Haskell's biggest practical advantage isn't purity, but rather the expressiveness and rigidity of its type system
06:09:05 <merijn> tdammers: I think the FFI is undersold
06:10:54 <tdammers> maybe
06:10:58 <tdammers> but Python has an FFI too
06:11:08 <merijn> tdammers: Have you tried using it?
06:11:19 <merijn> It's a lot more work and pain for simple things in python
06:11:28 <tdammers> merijn: I try to stay away from Python as much as possible
06:11:32 <merijn> I've never once though "I will quickly FFI out for this one simple thing"
06:11:39 <merijn> It's a heavy commitment
06:11:44 <tdammers> hmhm
06:12:11 <merijn> In Haskell FFI'ing to a single C function is so easy it has almost zero additional effort
06:12:45 <maerwald> how is that hard in python?
06:13:11 <maerwald> there are quite a lot of big-sized projects that do that
06:13:50 <merijn> maerwald: I didn't say it's hard
06:13:55 <maerwald> https://github.com/pkgcore/pkgcore
06:13:57 <merijn> maerwald: I said it's considerably more work
06:14:02 <tdammers> until you C code does C things, like maintain global state, demand that you call a global non-reentrant init_library() procedure which you must call exactly once, etc.
06:14:10 <tdammers> non-reentrancy in general
06:14:54 <dminuoso> Hah. getaddrinfo is even *documented* as reentrant in macOS. I can tell you, it's not.
06:15:06 <dminuoso> Apple hasn't even responded to my bug report in 3 years.
06:15:15 <tdammers> so much headdesking
06:15:20 <merijn> maerwald: All the FFI in python is project that wholesale wrap existing C APIs so the boilerplate pays off. How many python packages call out to a single C function to be faster
06:15:35 <tdammers> dishonest documentation. unresponsive proprietary software vendor. ah, the good stuff.
06:15:41 <merijn> tdammers: Until your python interpreter hard locks due to ignoring all signals while the GIL is released, yay!
06:15:46 <ezzieyguywuf> merijn: Yes, I thinking about it as "no mutation by default for my brains' sake" is a nice way to think about it
06:15:49 <ezzieyguywuf> and you are right
06:15:55 <ezzieyguywuf> and arguably you can do that in C++ too
06:15:57 <liiae2> why this t2 is ok, but t3 isn't? 
06:15:58 <liiae2> https://paste.ubuntu.com/p/kTjWqBYYbm/
06:16:03 <ezzieyguywuf> and then write a class when you need to mutate.
06:16:11 <liiae2> t2 an t3 both are unary function
06:16:30 <merijn> ezzieyguywuf: Right, but it's super painful in C++ because you need to write const *everywhere* and then you need to rewrite half your methods, etc.
06:16:47 <liiae2> but with do notation, they have different behavior?
06:16:48 <merijn> ezzieyguywuf: There's a reason why Rust went "const" as default and explicit mut, rather than reverse
06:16:53 <ezzieyguywuf> merijn: should be writing const everywhere anyway if you're properly c++ing
06:17:03 <ezzieyguywuf> merijn: ah, yea that makes a lot more sese
06:17:40 <dminuoso> tdammers: Best of it is the exihibited behavior. Given some preconditions about what arguments you pass, getaddrinfo *randomly* gives you EAI_NONAME. It doesn't flat out crash, it just makes your software randomly wrong. 
06:17:49 <merijn> ezzieyguywuf: And then in Haskell not only is everything const by default, but you need IO to do mutation too, so you can track everywhere it can affect you
06:18:33 <merijn> ezzieyguywuf: (actually, that's not entirely true, you can do limited mutation witout IO)
06:18:41 <ezzieyguywuf> merijn: yes I"m starting to see more and more that the beauty of haskell is the convention of "being honest about what you are doing", as someone in here put it the other day
06:19:30 <merijn> ezzieyguywuf: Specifically, the ST monad allows mutation, but restricts you to mutating memory only (no IO) and to a single thread
06:20:22 <tdammers> dminuoso: those are the best bugs. "It works 99.9% of the time, and when it doesn't, the failure is subtle and silent"
06:20:25 <merijn> ezzieyguywuf: Which makes sense. Consider a function like "sort :: Ord a => Array a -> Array a" whether it uses mutation internally doesn't affect the purity, as long as no one can observe it
06:21:02 <tdammers> see also "morally pure"
06:21:33 <gentauro> anybody tried to use a `local package` with a `stack script`? I really can't get it to work. Here is a snippet on how to do it with `packages` on `Hackage` https://stackoverflow.com/a/52713909
06:21:45 <merijn> tdammers: I'd say it's just pure, without any handwaving
06:22:52 <dminuoso> pure is always relative to observation
06:23:12 <tdammers> relative to what you choose to observe, even
06:23:47 <tdammers> e.g., in Haskell, we don't consider execution time an effect, so something that manages to slow a computation down without changing its outcome is still considered "pure"
06:24:22 <dminuoso> And when we pretend that purity has no side-effects, it does. When your computation causes CPU cycles to be used, heat to be generated, maybe the fan gets flipped on by it... a lot of side-effects.
06:24:24 <tdammers> then again, exactly because we don't consider execution time an effect, the compiler is also free to optimize the useless slowdown away
06:24:32 <tdammers> exactly
06:25:18 <ezzieyguywuf> merijn: you're saying sort could use IO internally and it shouldn't matter since it just returns an array?
06:25:52 <dminuoso> Some limited form of IO, yes.
06:26:04 <tdammers> and sometimes we have things that are, for all practical intents and purposes, pure, but GHC insists on them being impure, so we have to override those checks via unsafePerformIO
06:26:09 <dminuoso> That limited form of IO is called ST.
06:26:26 <merijn> ezzieyguywuf: It can use ST, which unlike IO, does have a function (slightly simplified) "ST a -> a"
06:26:30 <dminuoso> In fact, an efficient implementation of sort *should* use ST.
06:26:41 <dminuoso> Because most sorts only achieve their asymptotics if done with mutation..
06:28:54 <dminuoso> Other libraries like bytestring make very widespread usage of actual IO internally, using dangerously unsafe primitives to hide the IO. This is done for your convenience, so bytestring is properly fast. And the authors make the promise that the effects are not visible to you.
06:28:57 <ezzieyguywuf> interesting
06:29:06 <ezzieyguywuf> sounds like an implementation detail I don't need to worry about :-P
06:29:53 <merijn> ezzieyguywuf: It can be useful because some libraries (like vector) expose an ST based API, so you can have a regular mutable array like you're used to in any language with mutation, but inside pure functions
06:30:02 <dminuoso> Well, maybe it becomes relevant to you. Say you hold some large vector in your hand (with a couple million of elements).
06:30:26 <dminuoso> Now, perhaps you have an algorithm that is easily expressible and efficient if done with mutation, rather than copying things around endlessly
06:30:32 <merijn> ezzieyguywuf: You need to manually copy the array before and after mutation, but if you plan to do a lot of expensive mutation the cost of copying once up front and afterwards is negligible
06:30:52 <ezzieyguywuf> i see
06:31:23 <dminuoso> ezzieyguywuf: If you want, take a simple mutating quicksort and implement it yourself with ST. It's a neat excercise.
06:31:37 <merijn> dminuoso: quicksort = worst sort >:(
06:31:45 <dminuoso> It doesn't matter what sort, really.
06:31:53 <dminuoso> The choice of sort is besides the point.
06:31:59 <merijn> Man, I hate whoever came up with the name quicksort
06:32:22 <dminuoso> quicksort is just a good example because its implementation is simple
06:32:26 <merijn> I swear to god. The only reason people know and keep bringing up quicksort is because the name implies it's fast and that's all they memorise
06:32:29 <merijn> dminuoso: It's not
06:32:57 <merijn> dminuoso: In fact, it's so hard that a CS paper got published auditing 10 implementations of quicksorts listed in CS papers and textbooks and found bug in all of them
06:33:32 <merijn> Quicksort isn't quick, it isn't simple, it's not fast. It just sucks and I wish we'd all forget it existed >.<
06:34:24 <kjak> merijn: do you have the name of that paper?
06:34:30 <merijn> Merge sort = best sort. And insertion sort gets an honorable pass for "best sort that's not merge sort"
06:35:11 <dminuoso> Honestly most discussions about "best sort" miss the point.
06:35:15 * hackage dobutokO-poetry 0.4.0.0 - Helps to order the 7 or less Ukrainian words to obtain somewhat suitable for poetry or music text  https://hackage.haskell.org/package/dobutokO-poetry-0.4.0.0 (OleksandrZhabenko)
06:35:20 <merijn> kjak: Not of the top of my head
06:35:38 <kjak> merijn: ok, thanks anyway. i'll look for it.
06:35:40 <merijn> dminuoso: They do, because the answer is obviously merge sort, unless you can use like radix sort :p
06:35:40 <dminuoso> For a generic sort, merge sort is probably great. But quicksort does perform admirably when you have some reason to know you wont trap into worst case.
06:35:56 <dminuoso> a careful quicksort implementation has fantastic locality of reference.
06:36:03 <dminuoso> never underestimate the power of cache.
06:36:12 <merijn> dminuoso: Worse than merge sort though
06:36:25 <merijn> dminuoso: Merge sort is literally "3 linear scans repeatedly" :)
06:36:43 <merijn> It has perfect memory locality!
06:36:48 <merijn> It's also trivial to parallelise!
06:36:51 <merijn> And it's easy!
06:37:04 <merijn> (Granted, not when parallelising, but still!)
06:37:32 <dminuoso> You just cant do it inplace.
06:37:37 <merijn> dminuoso: You can!
06:37:37 <dminuoso> You pay for it with meomry
06:37:43 <merijn> dminuoso: There's a paper from 1984 explaining how
06:37:55 <dminuoso> Mm
06:37:57 <merijn> (Well, there's a bit of overhead, but it's constant size)
06:38:33 <dminuoso> Why do we bother with all these bad sorts. For most data, we can sort in *constant* time! :)
06:38:35 <merijn> oh, my bad 1994, not 1984
06:39:15 <gentauro> dminuoso: forgot asymptotic time-complexity?
06:39:31 <merijn> Ah, no, 1994 is the "Practical in place mergesort" paper, the actual merge strategy is from 1988
06:39:53 <dminuoso> gentauro: http://hjemmesider.diku.dk/~henglein/papers/henglein2011a.pdf
06:40:03 <dminuoso> There's even an implementation of that on hackage...
06:40:09 <merijn> dminuoso: https://www.researchgate.net/profile/Michael_Langston/publication/237542225_Practical_In-Place_Merging/links/02bfe5107fc3b01eb0000000.pdf
06:40:18 <dminuoso> merijn: Interesting
06:40:37 <carbolymer> why Foldable class doesn't have filter? what am I missing here?
06:40:38 <dminuoso> Ill have to give it a thorough read.
06:41:07 <dminuoso> carbolymer: Well it's trivial to do `filter f . toList` ..
06:41:24 <gentauro> dminuoso: from `constant` to `linear` there actually two-magnitudes of asymptotic time-complexity (`log` and `linear`)
06:41:27 <gentauro> :|
06:42:37 <merijn> Big O is all lies anyway >.<
06:42:39 <carbolymer> dminuoso, I guess, but [] is lazy and I'd like to use filter on strict foldables, so converting back and forth to list doesn't seem optimal to me....
06:42:42 <merijn> Wake up, sheeple!
06:43:00 <carbolymer> computers were mistake
06:43:09 <merijn> carbolymer: The answer is "because there's no way to reconstruct a Foldable"
06:43:15 <merijn> carbolymer: Foldable only collapses structures
06:43:26 <carbolymer> ooh
06:43:33 <merijn> carbolymer: Traversable does structure preserving map, but even that's not powerful enough for filte
06:43:39 <carbolymer> that's why we have Traversable and it's a different classes hierarchy
06:44:06 <merijn> carbolymer: Because filtering requires "preserve the structure, except these few components" which isn't really definable for all Traversables either
06:44:25 <carbolymer> I get ith
06:44:29 <carbolymer> it
06:44:35 <merijn> carbolymer: There is the wither package which has a Witherable class which tries to generalise "filter"/"mapMaybe"
06:45:22 <merijn> oh, it got refactored, apparently it's now "witherable-class"
06:45:40 <gentauro> 15:42 < merijn> Big O is all lies anyway >.<
06:45:41 <carbolymer> thanks
06:45:42 <gentauro> eih?
06:46:35 <merijn> gentauro: All of big O complexity analysis is built on top of a single assumption which we know is false and have known to be false for several decades
06:46:41 <gentauro> tro to submit code-kattas with `Safe` Haskell without using `O(lg n)` operations
06:46:54 <gentauro> s/tro/try/
06:47:15 <dminuoso> gentauro: Oh Im sorry. I should have said linear time.
06:47:42 <gentauro> merijn: you might elaborate on that statement …
06:47:48 <merijn> The problem is that "log n" is rather meaningless. Because, what does O(log n) mean? That for "sufficiently big inputs" it's faster than O(n), right?
06:48:10 <dminuoso> merijn: Most asymptotics fail to properly include the fact that random memory access is not constant time.
06:48:15 <merijn> Except, that's not true in reality.
06:48:26 <merijn> dminuoso: I think you meant to tell gentauro ;)
06:48:40 <dminuoso> Oh no, I was just adding to your statement.
06:48:46 <gentauro> merijn: and dminuoso are you mixing theory and implementation details?
06:48:54 <merijn> gentauro: Anyway, yes, dminuoso is right. Big O complexity assumes *uniform* memory access time
06:48:57 <dminuoso> gentauro: complexity analysis is not about implementation details.
06:49:00 <dminuoso> It's about asymptotics.
06:49:27 <gentauro> Pawel Winter, from DIKU, would absolutely destroy you at the oral exam xD
06:49:32 <merijn> gentauro: IF you assume uniform memory access time then "less memory access" is always better than "more memory access"
06:49:38 <dminuoso> I mean sure, it depends on what your perspective is. If you want to measure the complexity in a hypothetical universe in which thermodynamics don't apply, that's fine!
06:49:38 <merijn> gentauro: He would not
06:49:54 <dminuoso> But dont use that knowledge to extrapolate information about asymptotics of that algorithm implemented in our universe.
06:50:00 <gentauro> merijn: if you mixed implemenation details while answering his questions, he would
06:50:13 <merijn> gentauro: It's not an implementation detail
06:50:20 <dminuoso> It's fundamental laws of our universe.
06:50:38 <gentauro> well, if you go into hardware details
06:50:40 <dminuoso> Memory access is *not* constant time. The blackhole thermodynamics demonstrate this easily.
06:50:40 <merijn> Like, this stuff is explicitly discussed as the start of CLRS Intro to Algorithms
06:50:42 <dminuoso> No not hardware details
06:50:43 <dminuoso> PHYSICS.
06:50:45 <gentauro> it gets a bit trickier than that
06:50:47 <dminuoso> The laws of physics. The universe.
06:50:56 <merijn> gentauro: Big O is correct if you don't care about running on real machines
06:51:14 <merijn> If you care about real machines it's too flawed to consider
06:51:18 <gentauro> merijn: which is what you should learn at a CS course tackling algorithms
06:51:29 <gentauro> merijn: when you state "real machines"
06:51:33 <dminuoso> gentauro: Regardless of your ingenuity, you can't put infinite memory into an infinite small space. There is a thing called the Bekenstein bound.
06:51:34 <merijn> There's only one reason we still use Big O
06:51:36 <gentauro> which kind of architecture do you refere to?
06:51:46 <merijn> gentauro: "literally anything with a cache"
06:52:00 * gentauro and this is the reason you shouldn't mix "implemenation details/hardware" while talking asymptotic time complexities
06:52:07 <ezzieyguywuf> dminuoso: I might do that when it's time to learn ST
06:52:13 <ezzieyguywuf> trying to figure out State right now
06:52:13 <dminuoso> gentauro: The amount of information storable in a given sphere of spacetime is *limited* by the *surface* area of that sphere.
06:52:18 <gentauro> merijn: so we exclude the PS3?
06:52:34 <dminuoso> gentauro: That's not an implementation detail. That's fundamental about our universe. 
06:52:37 <merijn> gentauro: Hold on, Big O complexity is defined in relation to a model
06:52:51 <merijn> gentauro: This model is known to deviate from reality with an unbounded error size
06:53:09 <merijn> gentauro: So yes, Big O is "correct" with regards to the model it describes
06:53:21 <merijn> gentauro: But that model is useless in reality, because it has an unbounded error
06:53:36 <dminuoso> merijn: I guess an issue is that people neglect to define the model when talking about complexity analysis.
06:53:46 <merijn> gentauro: Which means that "conclusions drawn about your model using asymptotic analysis" is irrelevant to "what you need to do in reality"
06:54:01 <dminuoso> Just like in religious discussions, people seem to blindly assume that everyone is sharing your definitions and then start the discussion.
06:54:07 <dminuoso> Leading to needless confusion and misunderstanding.
06:55:27 <dminuoso> Besides, what's the usefulness of discussing the complexity of an algorithm in a fictional universe? What useful knowledge do you obtain from such a discussion?
06:56:06 <cdunklau> dminuoso: i mean, it's useful at least as a heuristic for comparing algorithms
06:56:14 <gentauro> just to finish my point. The best known asymptotic time complexity for a sorting algorithm is `bitonic` sort. So why don't we use it everywhere? Cos we really can't "implement" it to it's fully potential
06:56:21 <merijn> cdunklau: Only as rough heuristic
06:56:31 <gentauro> my point is that asymptotic time complexity is a theoretical field
06:56:33 <cdunklau> merijn: it depends
06:56:47 <gentauro> and you shouldn't mix "implemenation" / "hardware" details
06:56:49 <dminuoso> cdunklau: Comparing in what sense? You still don't get useful information from it that is applicable to the real world.
06:57:00 <merijn> gentauro: We're not talking about hardware details
06:57:10 <merijn> gentauro: We're talking about "model assumptions"
06:57:30 <gentauro> I rest my case
06:57:39 <merijn> gentauro: Do you agree that a model's usefulness is directly correlated with how well it corresponds to reality, yes or no?
06:57:42 * gentauro time to write some more code.
06:57:53 <cdunklau> dminuoso: sure you do. quicksort's average case vs insertion sort's
06:58:07 <cdunklau> it's obviously better (heuristically)
06:58:11 <merijn> cdunklau: Is it, though?
06:58:25 <dminuoso> cdunklau: Here comes the catch, they might compare differently in different models.
06:58:25 <merijn> (I mean, probably, but you can't conclude that)
06:58:28 <gentauro> oh, merijn, I think you misunderstand it with "algorithmic engineering". Try to look up Paolo Ferragina
06:58:44 <gentauro> nevertheless, code is calling
06:58:58 <cdunklau> dminuoso: sure but that's an implementation detail :3
06:59:17 <dminuoso> cdunklau: No, that's a model difference (to continue the terminology merijn has been using)
06:59:35 <gentauro> http://didawiki.di.unipi.it/doku.php/magistraleinformaticanetworking/ae/start
06:59:40 <dminuoso> If A has better asymptotics than B in model X, but worse in model Y - what does that tell you about A and B?
07:01:25 <dminuoso> Besides, it turns out that the model of non-constant memory access is surprisingly well matched in real world measurements.
07:01:34 <merijn> gentauro: If you wanna argue from authority I'd like to recommend you study section 2.3 (page 24) from "Introduction to Algorithms" by CLRS. It's the de facto standard university book for Big O complexity and discusses literally the issue I was just explaining
07:02:39 <dminuoso> Surprisingly, the performance characteristics induced by common memory hierarchies is well modelled by the assumptions of blackhole thermodynamis.
07:04:00 <cdunklau> dminuoso: is memory access non-constant in a way that's dependent on the size of the storage or whatever?
07:04:54 <merijn> cdunklau: It's non-constnt based on access pattern, if we consider "modern" caches (can we still call them modern 2+ decades later?)
07:05:09 <dminuoso> cdunklau: Yes. Essentially there's 2 limiting factors about our universe. 1) the speed of light and b) the maximum amount of information you can store in a given sphere is bounded by its surface area.
07:05:30 <dminuoso> cdunklau: That gives you O(sqrt(n)) for random memory access in a model following our universes laws.
07:05:44 <dminuoso> (The mathematics I can go further into if you are really interested)
07:06:12 <cdunklau> okay, so now from a practical standpoint, does that change the utility of big-O? 
07:06:20 <cdunklau> i mean, it obviously does to some extent
07:06:20 <merijn> cdunklau: Yes
07:07:06 <merijn> cdunklau: Does it happen often? No. But I've personally had some HPC code that went 5x faster by switching to worse asymptotics (by just using an array and looping redundantly over it, rather than being smart)
07:07:19 <merijn> cdunklau: It's actually a bit worse
07:07:37 <merijn> cdunklau: Because memory bandwidth is actually quite high, but so is latency
07:07:55 <merijn> cdunklau: So you *can* read quite a lot of memory fast, if you make sure it's all adjacent
07:08:02 <dminuoso> The useful of big-O itself is maybe the real question. Perhaps too frequently programmers use it to predict how good an algorithm will perform.
07:08:27 <cdunklau> but given that big-O isn't a complete metric, and other things like space complexity can also become important, i don't think non-constant memory access damages big-O's utility much
07:08:47 <dminuoso> cdunklau: Its not about big-O's utility. Its usefulness is unchanged.
07:09:02 <merijn> cdunklau: So accessing data in cache line sized chunks reduces the number of fetches needed (latency) and more cache hits is even faster
07:09:05 <dminuoso> It's rather that the models people use don't match our universe well .
07:09:18 <cdunklau> dminuoso: abolish the universe!
07:09:20 <merijn> cdunklau: The problem is that many people consider Big O "correct, with constant factor of wrongness"
07:09:27 <dminuoso> That would certainly be an option.
07:09:43 <merijn> cdunklau: While in reality it's *usually* close enough is, but when it goes wrong it can go off by orders of magnitude
07:09:58 <cdunklau> merijn: how often does that happen though?
07:10:10 <dminuoso> When the question ever becomes "how well does it scale" - then that's asking for a prediction. Why not use a model that is accurate?
07:10:14 <cdunklau> and in any event, that still seems like it's a useful heuristic
07:10:23 <ezzieyguywuf> where do I import State from?
07:10:27 <dminuoso> Why intentionally use an inaccurate model?
07:10:30 <dminuoso> that's the real question
07:10:40 <dminuoso> big-O still works, and is calculated the same way
07:10:42 <merijn> cdunklau: It's a somewhat useful heuristic, but then why not go with less formal equally useful ones?
07:10:43 <cdunklau> dminuoso: uh, simplicity is the big reason that pops into my head
07:11:00 <dminuoso> cdunklau: honestly, most people dont actually calculate complexity. they just look at wiki
07:11:03 <cdunklau> say, QM vs classical mechanics
07:11:03 <merijn> cdunklau: 2n is meaningless in big O, but still useful and arguably more so than Big O, since I can now distinguish 2n and 3n ;)
07:11:28 <merijn> cdunklau: So there's actually simpler, more accurate heuristics for many things ;)
07:11:40 <dminuoso> ezzieyguywuf: https://hoogle.haskell.org/ this is going to be your favourite page now :)
07:11:58 <dminuoso> ezzieyguywuf: mtl is going to be the right package, btw.
07:12:08 <ezzieyguywuf> is state strictly mtl?
07:12:12 <ezzieyguywuf> i.e. net prelude?
07:12:12 <cdunklau> dminuoso, merijn i like y'all :)
07:12:23 <dminuoso> ezzieyguywuf: Right. There's other packages as well, but mtl is a good start.
07:12:29 <ezzieyguywuf> ok great thank you.
07:13:14 <merijn> cdunklau: Look, I'm not saying "big o is useless" and "never use big O", but there's so many people treating it as unassailable gospel withou thinking or understanding that it annoys me :p
07:13:35 <cdunklau> merijn: entirely understandable
07:13:43 <merijn> cdunklau: And clearly the best way to solve this educational issue in programming is to go on long ranting lectures about why everybody's wrong on the internet!!
07:14:12 <cdunklau> merijn: it's the solution we deserve, not the one we want
07:14:23 <dminuoso> merijn's favourite hobby. ranting lectures about why everyone is wrong on freenode.
07:14:27 * dminuoso chuckles
07:17:53 <merijn> I wish everyone's wrongness was limited to freenode!
07:36:45 * hackage influxdb 1.7.1.6 - Haskell client library for InfluxDB  https://hackage.haskell.org/package/influxdb-1.7.1.6 (MitsutoshiAoe)
07:53:15 <frdg> Will concepts from a data structures and algorithms class that is taught using Java be applicable in Haskell?
07:53:41 <ap> Hi
07:54:11 <dminuoso> frdg: Somewhat, yes and no.
07:54:38 <dminuoso> frdg: Algorithms is certainly useful, though there's a whole world of functional algorithms that are just different. But you can port any imperative algorithm just fine.
07:54:55 <dminuoso> The data structures doesn't translate well because we dont have type hierarchies in the same sense.
07:55:02 <dminuoso> Albeit they can be modelled somewhat, but it's more limited.
07:55:02 <Guest27414> Can't seem to get STArray working. Can someone help?
07:55:03 <Guest27414> https://hastebin.com/ugulahiket.rb
07:55:06 <frdg> dminuoso: that's kinda what I figured. Thanks
07:57:01 <hyperfekt> Is there any way to disable all extensions in GHCi? Something like -i without any directories.
07:58:36 <dminuoso> hyperfekt: Well a fresh GHCi starts with just... extended defaulting rules I think?
07:59:12 <hyperfekt> dminuoso: Starting a fresh GHCi was the one thing I was trying to avoid. ^^
08:00:49 <dminuoso> hyperfekt: Id quickly process compiler/main/DynFlags.hs into a command that I can copy+paste then.
08:00:57 <dminuoso> Easiest way I can think of
08:01:53 <dminuoso> https://gitlab.haskell.org/ghc/ghc/blob/31704adc82c3a1e48ac05c51f02933fd996b642a/compiler/main/DynFlags.hs#L3093-3211
08:01:58 <dminuoso> That's the full list of extensions.
08:02:05 <dminuoso> A quick bit of sed work
08:02:44 <merijn> frdg: I disagree with dminuoso, tbh. A lot of the data structures stuff is still relevant
08:02:54 <merijn> frdg: We still have maps, sets, queues, lists, etc.
08:03:14 <merijn> frdg: With mostly the same characteristics as their Java/C++ equivalents
08:03:20 <dminuoso> Ah well in that sense. I understood it differently in the sense of how Java promotes say building classes to model things.
08:03:35 <hyperfekt> dminuoso: I'll probably end up going for parsing the output of ghc --supported-extensions then, thanks
08:03:51 <frdg> merijn: That's great to hear
08:04:04 <dminuoso> hyperfekt: Ah indeed even simpler, didn't know that flag. Neat.
08:04:10 <merijn> frdg: They're all immutable, but that basically just means there's no "modify" functions
08:04:27 <dminuoso> Disclaimer: That's not true.
08:04:31 <dminuoso> We have mutable containers...
08:04:50 <dminuoso> They are just not that popular, and their ergonomics is... sometimes lacking
08:04:56 <merijn> frdg: If you've worked with maps in other languages the docs of containers should be pretty self explanatory for example: https://hackage.haskell.org/package/containers-0.6.2.1/docs/Data-Map-Lazy.html
08:05:49 <frdg> merijn: I'll look into it...but I programmed in Java for like a week before I switched to Haskell.
08:07:00 <dminuoso> Then.. I recommend forget everything you learned. :P
08:07:02 <dolio> The mutable containers that most people actually use are not very complex. Almost all the stuff you'd learn about in a data structures class has the immutable version implemented.
08:08:26 <dolio> For example, I'm not sure I've ever seen a mutable search tree with actual reference tweaking to rebalance in Haskell.
08:09:00 <merijn> dolio: To be fair, I don't recall seeing those in not-haskell before either :p
08:09:04 <dolio> And it probably just isn't worth it to implement/use that version.
08:13:49 <dolio> Well, it's likely you've used one, if you've used a search tree at all outside of Haskell. :)
08:15:33 <merijn> Only ones I built myself :p
08:16:02 * merijn puts on l33t hacker glasses
08:16:44 <dolio> I think I've probably used one that I didn't implement myself. There's, like, TreeMap in Java or something, I think.
08:18:17 <dolio> I don't know if I've even seen something packaging up 'growable mutable array' in Haskell.
08:20:52 <dolio> Anyhow, the point is, most of the mutable stuff people are apparently worried about isn't fancy data structures (unless you're edwardk).
08:26:37 <Boarders> does anyone know how you can have cabal test only print the failures?
08:27:13 <dolio> Like, just the failing test cases? Isn't that on your test suite?
08:27:24 <dolio> It already only prints if there *are* errors.
08:29:02 <dolio> If it's just an exitcode test, I'm not sure cabal has enough information to only print failing test cases.
08:44:56 <exarkun> I'm trying out Control.Retry for the first time and trying to make sense of this behavior, https://gist.github.com/exarkun/329aa9e2432604770eda9d7407429709
08:45:45 <exarkun> Why does it try twice instead of ... 3600 times?
08:45:57 <exarkun> am I reading the output right?  is it actually trying twice?
08:48:45 * hackage reflex-process 0.3.0.0 - reflex-frp interface for running shell commands  https://hackage.haskell.org/package/reflex-process-0.3.0.0 (3noch)
08:49:24 <ja> Boarders: there is an environment variable for Tasty
08:49:55 <ja> Boarders: or the flag --hide-successes
08:53:37 <exarkun> oops that `div` is dumb
08:55:15 * hackage hapistrano 0.4.1.0 - A deployment library for Haskell applications  https://hackage.haskell.org/package/hapistrano-0.4.1.0 (juanpaucar)
09:07:15 * hackage vulkan 3.4 - Bindings to the Vulkan graphics API.  https://hackage.haskell.org/package/vulkan-3.4 (jophish)
09:08:15 * hackage VulkanMemoryAllocator 0.3.2 - Bindings to the VulkanMemoryAllocator library  https://hackage.haskell.org/package/VulkanMemoryAllocator-0.3.2 (jophish)
09:18:08 <dminuoso> Mmm, I feel like Data.Vector should have an instance IsList. Opinions?
09:18:49 <dminuoso> (I've started to want this when using writing aeson encoding routines with this particular protocol) 
09:19:04 <dsal> It doesn't?  That's surprising.
09:19:22 <Boarders> It looks like it does?
09:19:24 <dminuoso> Huh.
09:19:34 <Boarders> https://hackage.haskell.org/package/vector-0.12.1.2/docs/Data-Vector.html#t:Vector
09:19:50 <dminuoso> Argh. Shame on my hackage skills
09:19:53 <dminuoso> https://hackage.haskell.org/package/vector-0.12.1.2/docs/Data-Vector-Generic.html#t:Vector
09:20:04 <dminuoso> I was looking through that didn't pay enough attention.
09:20:19 <dminuoso> Thanks Boarders!
09:20:23 <dminuoso> That makes things simpler :)
09:20:29 <Boarders> you're welcome!
09:23:23 <joeyh> process's cleanupProcess has a comment about a "handle lock" that could cause deadlock if some thread is holding it. I'm trying to find what that lock is and what operations use it
09:27:55 <joeyh> hmm, I guess it must be withHandle, if something uses it and blocks
09:57:12 <ezzieyguywuf> I could use some help understanding State http://dpaste.com/1VEKFS4
09:57:49 <ezzieyguywuf> In this example I've pasted, I think I understand the general mechanics of State, however I don't understand why in the end, when I want to inspect the final actual state, I need to still pass in an initial state
09:59:34 <tomsmeding> ezzieyguywuf: what would you expect the current state to be before you `put` the emptyTopology in there via getEmpty?
10:00:03 <tomsmeding> so to answer your question "How do I start with an initial state?" -- well, by passing the initial state to execState ;)
10:00:23 <ezzieyguywuf> tomsmeding: It seems like I don't even need 'getEmpty' then, do I?
10:00:35 <tomsmeding> seems so indeed
10:00:54 <ezzieyguywuf> I could `execState addVertex T.emptyTopology`
10:01:26 <tomsmeding> yup
10:01:34 <ezzieyguywuf> hrm, maybe this will make more sense once I update my unit tests to expect this State stuff
10:01:37 <ezzieyguywuf> i'll give it a whirl :-P
10:01:41 <Tuplanolla> It looks like you're manually plumbing functions through the state transformer, ezzieyguywuf, so the result is not very pleasant to look at.
10:02:30 <Tuplanolla> You might want to try using the associated functions with `do` notation instead.
10:02:43 <ezzieyguywuf> Tuplanolla: you mean get rid of the ">>="?
10:02:48 <tomsmeding> also as an alternative way to write addVertex, would the following work?
10:02:52 <tomsmeding> addVertex = modify T.addVertex >> get (last . T.getVertices)
10:03:00 <Tuplanolla> More like get rid of the `state $ f`.
10:03:06 <tomsmeding> sorry 'get' must be 'gets'
10:03:29 <ezzieyguywuf> Tuplanolla: ah yes, absolutely. I wrote it out that way because I'm trying to understanding what's going on
10:03:47 <ezzieyguywuf> addVertex could definitely be written better, I just haven't wrapped my brain around how that looks yet
10:03:57 <Tuplanolla> Well, you're almost there.
10:04:04 <ezzieyguywuf> tomsmeding: perhaps!
10:04:45 <ezzieyguywuf> Tuplanolla: in fact, I did write a version with 'do' and explicit 'put' and 'get'
10:05:12 <ezzieyguywuf> and that made sense to me
10:05:48 <tomsmeding> side note: the 'last' is going to be slow if you have lots of vertices :)
10:06:16 <ezzieyguywuf> this is what that looked like http://dpaste.com/2F5M9S4
10:06:26 <ezzieyguywuf> I have a feeling tomsmeding 's approach may be better, at least it looks cleaner
10:06:40 <Tuplanolla> If you wanted to start from some generic empty state, you could specialize the execution function to work over a `Monoid`.
10:06:45 <ezzieyguywuf> tomsmeding: yea I know, I plan to get rid of the whole "last" thing
10:06:55 <tomsmeding> in that last one, you can simplify lines 9 and 10 to "t' <- gets T.addVertex" :)
10:07:43 <tomsmeding> though I think the 'modify' version looks cleaner, especially since addVertex is a "modifying" operation, as far as that's a thing in Haskell
10:07:59 <tomsmeding> is Topology your own library?
10:08:07 <ezzieyguywuf> tomsmeding: yes, I can link it
10:08:20 <tomsmeding> with that I mean you can edit the source code :p
10:08:32 <ezzieyguywuf> ah yes, absolutely
10:08:38 <tomsmeding> if so, and you need that vertex out of addVertex, you might want to change T.addVertex to return (T.Topology, T.Vertex) or something
10:08:52 <tomsmeding> assuming that's implementable in O(1)
10:08:57 <ezzieyguywuf> I'm thinking of changing T.addVertex to `T.addVertex :: T.Topology -> (T.Vertex, T.Topology)`
10:09:06 <tomsmeding> great minds think alike
10:09:09 <ezzieyguywuf> hah yea
10:09:15 <ezzieyguywuf> well that's what has be going down this whole State path
10:09:29 <ezzieyguywuf> but I started with baby steps in a scratch.hs file, which is what I pastebinned earlier
10:09:41 <tomsmeding> looking good
10:09:53 <tomsmeding> starting with baby steps is usually a good strategy
10:11:25 <gentauro> merijn: my comment was no way to sound like `authority`, I just wanted to enlighten you with a specific branch of algorithmic research that dealt with what you were saying. One of the easier ways to understand why theoretical asymtotic time complexities are so important, is actually `C++ introsort`. Mostly cos is 100% based on the time complexity warranties. You start by `quick sorting`, if the deepth 
10:11:31 <gentauro> goes deeper den `O(log n)`, you switch to `heap sorting` (in-place). But you never fully sort all the way, you stop whenever the number of elements can be placed in the cache of your CPU, and then on ALL the numbers, in chunks of that size, you do `insertion sorting`. Who would have thought that the fastest sequential algorithm, actually uses `insertion sort` on all it's numbers? Well, people who know 
10:11:37 <gentauro> how to use the theoretical asympototic time-complexities to their advantage from an engiennering point-of-view :)
10:11:40 * gentauro well that was a long message :)
10:11:43 <ezzieyguywuf> tomsmeding: your original example with modify results in "No instance for (MonadState....."
10:11:48 <ezzieyguywuf> I think I have to add an instance
10:13:05 <gentauro> dminuoso: you were also participating in this awesome discussion, please see my comment above :)
10:13:16 <gentauro> sry I couldn't make it before cos I was kind of `busy` xD
10:13:22 <tomsmeding> ezzieyguywuf: no instance for MonadState _what_, exactly? :p
10:14:23 <ezzieyguywuf> No instance fore (MonadState (StateT T.Topology Data.Functor.Identity.Identity T.Vertex) ((->) (T.Topology -> T.Vertex)))
10:15:16 <ezzieyguywuf> so I guess that's where the choice comes to writing an instance of something (I think either the wiki or learn you a haskell or the documentation goes into this) or just rewriting addVertex
10:15:20 <ezzieyguywuf> i think
10:16:02 <tomsmeding> no that error seems very weird
10:16:18 <tomsmeding> are you sure your type signature on addVertex is correct?
10:16:41 <tomsmeding> because for me it seems to work
10:17:31 <ezzieyguywuf> yea, 100% sure.
10:17:36 <ezzieyguywuf> maybe I mistyped the example
10:17:46 <tomsmeding> did you perhaps not see my note that you should replace 'get' with 'gets' in my code?
10:17:51 <tomsmeding> I typo'd there :p
10:18:04 <ezzieyguywuf> ah hah
10:18:11 <tomsmeding> yes then you get that error
10:18:14 <ezzieyguywuf> that was it
10:18:31 <tomsmeding> 'gets f' is the same as 'fmap f get', if that helps
10:18:50 <tomsmeding> (which is the same as 'f <$> get', because (<$>) = fmap :p)
10:19:05 <ezzieyguywuf> yea it kinda helps
10:19:14 <ezzieyguywuf> I'm getting confused between StateT's get and State's get
10:19:16 <tomsmeding> it's just a convenience function :)
10:19:29 <tomsmeding> State is just a type synonym for StateT on Identity
10:19:34 <ezzieyguywuf> no wait
10:19:37 <tomsmeding> so it's the same function
10:19:47 <ezzieyguywuf> ah hah
10:19:48 <ezzieyguywuf> you're right
10:20:03 <tomsmeding> everything you read about StateT should also apply to State, since it's just a special case
10:21:00 <ezzieyguywuf> yea in fact I've been misreading the documentation I didn't realize I was reading about StateT all this time
10:21:32 <tomsmeding> haddock documentation is kind of confusing like that sometimes, in my experience
10:21:48 <tomsmeding> all the information is _there_, but it's not the most ideal format somehow :p
10:21:52 <monochrom> Old books and old blogs had a distinct State independent of StateT. This is because old library versions did that. So beware.
10:21:55 <tomsmeding> and also, it's a reference, not a manual
10:22:17 <tomsmeding> thanks, TIL monochrom 
10:22:45 <monochrom> Yes confusions are doomed to happen if reference material is used for introductory material.
10:23:00 <monochrom> Generally all confusions are caused by wrong assumptions.
10:23:14 <tomsmeding> assumptions that a reference is not even trying to dispel :)
10:23:15 <monochrom> logically equivalent
10:24:14 <tomsmeding> sometimes people do write manual-like text in haddock, like e.g. here https://hackage.haskell.org/package/accelerate-1.2.0.1/docs/Data-Array-Accelerate.html#t:Acc
10:24:33 <tomsmeding> but it's scarce, and even if it's there it's usually not extensive :)
10:25:09 <tomsmeding> s/scarce/rare/
10:28:15 * hackage calamity 0.1.11.2 - A library for writing discord bots  https://hackage.haskell.org/package/calamity-0.1.11.2 (nitros12)
10:44:58 <exarkun> How do I write tests for code that uses https://hackage.haskell.org/package/retry-0.8.1.2/docs/Control-Retry.html#t:RetryPolicyM without having those tests take non-trivial wallclock time to complete?
10:46:00 <merijn> exarkun: You mean the tests for that package that check if it work?
10:46:13 <exarkun> tests for my application code which uses that package
10:46:21 <exarkun> I just added retry logic to my application using that library
10:46:36 <exarkun> I don't want coal in my stocking so I want to add some automated test coverage for the change in functionality
10:47:00 <merijn> Use very small timeouts, run them in parallel (100 tests in parallel is 1/100th the time of in sequence)
10:47:19 <merijn> And finally "assume it works fine and don't" ;)
10:47:40 <merijn> exarkun: You could have a separate fast and slow config for testing and only run the long one on CI or something
10:48:20 <exarkun> am I crazy for wanting a Control.Retry feature where the sleep function is accepted as a parameter by some API so I can supply an extra-fast version of it?
10:49:01 <exarkun> or is there a better retry library than Control.Retry (or is this so simple I'm crazy for using a library instead of rolling my own)
10:49:43 <exarkun> The idea about running the tests in parallel is interesting, hadn't really thought about that.  Not sure what my test runner can do as far as parallel execution though.
10:51:46 <merijn> exarkun: It's weird that they don't provide a "withRetryPolicy" to temporarily overwrite the entire underlying policy
10:52:04 <merijn> exarkun: I know tasty supports parallel test running, not sure about hspec
10:52:53 <exarkun> ahh that rings some bells I guess
10:53:28 <exarkun> hm, tasty even runs in parallel by default?
10:53:59 <exarkun> (so say the docs)
10:54:45 <merijn> Yeah, you need to manually control how many threads it uses, though
10:55:18 <exarkun> ah, maybe that's why I never noticed it
10:55:24 <merijn> It uses the "TASTY_NUM_THREADS" environment variable I think
10:55:52 <merijn> At least, my test suite has: setEnv "TASTY_NUM_THREADS" "100"
10:56:00 <merijn> So that seems like a solid guess ;
10:58:35 <exarkun> :)
10:59:39 <exarkun> seems like there's also a --num-threads argument
10:59:56 <merijn> exarkun: Yeah, but I wanted to not have to give arguments :)
11:14:56 <dsal> exarkun: I'm using Control.Retry, but it seems kind of big and weird.
11:19:28 <exarkun> dsal: ohno
11:24:20 <dsal> I don't test actions very much.  Testing functions is easy.  This is an area I need to get better at.
11:29:32 <exarkun> turns out the code I modified is in my web stuff and I still have a blog post bookmarked and unread about how to test web stuff mixed with IO monad
11:29:40 <exarkun> so quite possibly I won't actually write any tests for this change ... :/
11:37:29 <ezzieyguywuf> so, is the State in mtl and the State in transformers different?
11:37:33 <ezzieyguywuf> should I prefer one over the other?
11:37:39 <ezzieyguywuf> (they seem somewhat different)
11:38:29 <monochrom> they are the same
11:38:48 <monochrom> (they were different a decade or more ago)
11:40:18 <monochrom> they were rivals. then they signed a unification treaty. in corporate speak, they signed a stock-swapping merger.
11:40:21 <ezzieyguywuf> ah, but mtl also includes MonadState
11:40:24 <ezzieyguywuf> and seemingly better documentation
11:41:14 <monochrom> under the treaty, they did division of labour. transformers focuses on defining StateT. mtl focuses on defining MonadState, mtl re-exports transformers's StateT.
11:42:32 <monochrom> they also expect that more people use mtl even when they just use StateT.
11:42:47 <monochrom> so probably they put more docs in mtl
11:43:10 <ezzieyguywuf> ah i see
11:43:16 <ezzieyguywuf> quite a peaceful treatie i can see
11:45:07 <dolio> Before that there were two packages on top of transformers that were similar to mtl, so you had various people using both, and it was annoying.
11:50:01 <monochrom> The Concert of Monads.
11:50:15 * hackage linux-framebuffer 0 - Linux fbdev (framebuffer device, /dev/fbX) utility functions  https://hackage.haskell.org/package/linux-framebuffer-0 (SergeyAlirzaev)
11:52:38 <tomsmeding> ezzieyguywuf: for additional background (not particularly useful at the moment, probably) -- the reason why transformers still exists and everything hasn't just become mtl, is that transformers requires less haskell language extensions, and is considered "cleaner" by some
11:52:56 <tomsmeding> but for the working programmer, you just use mtl :)
11:53:37 <merijn> tomsmeding: because many people, like me, avoid using mtl in most circumstances
11:53:59 <tomsmeding> for certain definitions of "working programmer", apparently
11:55:52 <merijn> I would, in fact, take the even more extremist position that having any of the MTL classes in a public API is a code smell and almost certainly something you will end up regretting :)
11:56:21 <slack1256> merijn: Any war story?
11:57:16 <merijn> slack1256: The problem is that a monad stack can only ever have, for example, one MonadReader instance. That means that different APIs relying on MonadReader will conflict and be mutually exclusive which is a pain
11:57:30 <monochrom> Does dead code has code smell? :)
11:57:45 <monochrom> damn English. s/has/have/
11:57:50 <merijn> slack1256: I'd say that DSL-like classes in the style of MTL can be very useful for composability
11:58:04 <merijn> slack1256: It's just that the ones in mtl are too general and unspecific
11:58:23 <monochrom> OK yeah I'm a fan of DSL classes.
11:58:25 <merijn> slack1256: Consider MonadLogger in, well, monad-logger
11:58:42 <monochrom> MonadReader is definitely an academic curiosity.
11:58:45 * hackage packcheck 0.5.1 - Universal build and CI testing for Haskell packages  https://hackage.haskell.org/package/packcheck-0.5.1 (harendra)
11:59:05 <merijn> slack1256: That's sensible. It's highly likely you want only a single logger in your codebase, it nice to be able to write code that's agnostic of the actual logging being used, etc.
11:59:48 <merijn> slack1256: In my code I have a MonadSql which exposes a number of primitives for transactions/dealing with the database upon which all my query using code is build. Which only queries to be used in different contexts
12:00:42 <merijn> slack1256: The MTL classes being unspecific and generic invite people to use them for very different things, but that is their downfall. The requirement of being unique for a specific monad/monad stack means that you can never use two different libraries using them.
12:01:15 <monochrom> the same curse with (,) and Either.
12:01:43 <merijn> Yeah
12:01:47 <slack1256> I see
12:02:03 <monochrom> when there is a "built-in" product type and a "built-in" sum type, you can bet people will just use them and not take advantage of nominal typing.
12:02:17 <merijn> I've just recently spent a whole bunch of time and effort replacing (,) and either with custom ADTs
12:02:23 <monochrom> when number types, string types.
12:02:23 <merijn> Honestly, even Bool is bad :p
12:02:55 <monochrom> s/when/and/
12:03:24 <tomsmeding> re:only a single instance for a monad stack etc.: am I completely misunderstanding the point, or is that solved with a newtype wrapper?
12:03:55 <merijn> tomsmeding: It's not
12:04:15 <monochrom> you solve by writing your own classes
12:04:26 <merijn> tomsmeding: Consider this: one of you dependencies exposes functions with a  "MonadReader Foo m" constraint, the other wants "MonadReader Bar m"
12:04:30 <monochrom> write classes relevant to your application domain
12:04:37 <merijn> tomsmeding: You can never call those functions from a single monad
12:04:57 <tomsmeding> merijn: oh right, that makes sense, and is indeed crap
12:05:14 <tomsmeding> thanks :)
12:05:45 * hackage hedgehog-classes 0.2.5 - Hedgehog will eat your typeclass bugs  https://hackage.haskell.org/package/hedgehog-classes-0.2.5 (chessai)
12:05:55 <slack1256> I guess you can hack something with `hoist` but it really a hack to mashup the two ReaderT layers.
12:06:06 * enikar . o O ( why Bool is bad? )
12:06:30 <slack1256> Boolean blindness is the keywords you want to search enikar.
12:06:41 <slack1256> s/is/are/
12:06:45 <monochrom> "Bool is bad" is an abbreviation for "abuse of Bool is bad".  Likewise knives guns etc.
12:06:47 <enikar> ok, I understand.
12:08:05 <monochrom> Bool is not bad but when you could "data Dir = Down | Up" but you are too lazy so you "reuse" Bool and say "False is up, True is down" that's bad.
12:08:45 <enikar> Yes, I agree.
12:09:56 <EvanR> oh nice can i abrieviate that to call other stuff bad
12:10:04 <EvanR> monads are bad
12:10:12 <monochrom> who's bad?
12:10:19 <merijn> EvanR: Calling things bad is bad
12:10:34 <EvanR> (abuse of anything is kind of by definition bad)
12:11:00 <merijn> EvanR: Naah, just look at substance abuse, that's perfectly fine!
12:11:01 <EvanR> oh we don't even need the word. "bad use"
12:11:06 <ezzieyguywuf> merijn: do you avoid using modules that depend on mtl then?
12:11:08 <tomsmeding> I once saw someone hacking on a C++ program, where he needed to pass some additional kind of flag to a function
12:11:15 <tomsmeding> it already had four boolean parameters, and now got a fifth
12:11:26 <tomsmeding> luckily it was in a competitive programming context and the program was <200 lines :p
12:11:27 <merijn> ezzieyguywuf: That's like a significant part of Hackage, so no ;)
12:11:32 <ezzieyguywuf> tomsmeding: sounds like a bad api
12:11:33 <EvanR> merijn: your dutch is showing!
12:11:49 <ezzieyguywuf> merijn: but you avoid 'using' it in code you write yourself?
12:12:02 <ezzieyguywuf> i.e. even if you're using a module that uses it, the code you write will tend not to rely directly on mtl?
12:12:04 <monochrom> But, when too many people abuse something, and you are too afraid to call it out "that's what's wrong with humanity", you shift the blame to the thing because it's safer.
12:12:18 <merijn> ezzieyguywuf: In fact, I even have mtl somewhere in my code, it's just good to be aware of the downsides of "overly polymorphic" mtl classes :)
12:12:21 <monochrom> Humanity is bad.
12:12:27 <merijn> EvanR: No, that's my academic side
12:12:33 <ezzieyguywuf> ah
12:12:39 <ezzieyguywuf> well, I'm only using mtl for State right now
12:12:52 <maerwald> monochrom: Human is. There's no humanity.
12:12:54 <ezzieyguywuf> if I even need it for anything else I'll ponder or polymorphicitity
12:12:59 <merijn> EvanR: Since like >80% of academica is functional alcholism :p
12:13:43 <monochrom> maerwald: Hrm, I'll have to think about that.
12:13:45 <tdammers> and the remaining 20% is just plain alcoholism?
12:14:26 <monochrom> haha I bet "social construct" and all
12:14:45 <merijn> tdammers: ;)
12:14:49 <monochrom> ObTopic: mtl is a social construct. :)
12:18:14 <tdammers> inb4 "object-oriented alcoholism"
12:18:54 <dminuoso> ezzieyguywuf: I'm a big fan of local monad (transformer) effects, those are perfectly fine and great. 
12:20:23 <dminuoso> But the moment when you start dragging monads into all parts of your program, or more, make them all polymorphic over some monad (via mtl) - then it's debatable. There's certainly engineering arguments why all code should be monad polymorphic (i.e. tagless final), but it comes at certain costs.
12:21:06 <maerwald> Monads are boring. It's much more exciting to avoid them.
12:25:19 <EvanR> IO or IO-like at the top level. descend into pure code or more restricted IO code as necessary 
12:25:25 <Cale> dminuoso: haha, I kind of have the opposite opinion -- that monad transformers are only really well-suited to designing larger-scale combinator libraries which probably don't even expose the mtl operations.
12:26:05 <dminuoso> Cale: I'd be very interested in your reasoning here.
12:26:14 <maerwald> sounds like the framework approach
12:26:17 <Cale> Well, use cases where you introduce a transformer for a few lines of code can be okay as long as it doesn't grow out of hand
12:26:46 <merijn> Cale: I think he mostly means like MaybeT/ExceptT which are pretty nice at small scale
12:26:47 <Cale> But the mtl's operations are insufficiently meaningful a lot of the time.
12:27:05 <Cale> I'm thinking of things like ask/get/put
12:27:23 <infinity0> what's everyone's favourite package for doing text uis?
12:27:36 <slack1256> brick
12:27:45 <Cale> I have to say reflex-vty
12:28:08 <Cale> I haven't *really* used brick, but I wrote reflex-vty, and like it
12:28:12 <dminuoso> I've basically come to realize that ReaderT-based stacks (my usual stack is `newtype M a = M { runM :: ReaderT Env (LoggingT IO) a }` are the only sane thing to drag around, because otherwise you're using MonadBaseControl everywhere the moment you start interfacing with libraries.
12:28:21 <Cale> (together with Ali Abrar)
12:28:27 <maerwald> Cale: now you have to learn FRP though :)
12:28:39 <infinity0> cool i'll take a look at both, thanks!
12:28:43 <dminuoso> unliftio I can reason about, and it keeps my code simple - everything else creates hard to read global effects that is disgusting to work with once you have exceptions.
12:28:52 <infinity0> i am more lookign for high-level widgets, i want to create a program where you can select (up/down) lines of a log in the left panel, then in the right panel it pretty-prints that line, and you can maybe fold/unfold parts of it
12:29:36 <Cale> Yeah, we don't yet have all that stuff, but we do have *some* things along those lines, and a sufficiently generic framework that it should be possible to build them
12:29:58 <infinity0> cool ok
12:30:22 <slack1256> I avoid monad stacks if the base is IO already.
12:30:24 <Cale> I would recommend starting with the TODO list example that's in the repo (which is like an editable list)
12:30:38 <dminuoso> Cale: So what's your opinion regarding that? Are you fine with MBC in your codebase? Once you accept non-trivial transformer stacks globally, it's virtually impossible to avoid - no?
12:30:56 <Cale> dminuoso: unliftio is better than MonadBaseControl, but it's still not guaranteed that it's okay to just lift any ReaderT through an arbitrary higher-order function
12:30:58 <maerwald> slack1256: how do you express "expected" failure that should be handled?
12:31:22 <dminuoso> What do you mean?
12:31:23 <Cale> dminuoso: Maybe you're lifting a reference to something which isn't thread-safe through a forkIO for example
12:31:45 <Cale> unliftio obscures that kind of thing, by not caring what exactly you're reading
12:31:48 <merijn> The forkIO story of MonadUnliftIO is currently kinda bad and needs work
12:32:04 <merijn> It interacts rather badly with ResourceT, for one
12:32:07 <dminuoso> Hold on, can you both elaborate on what the problem is exactly
12:32:08 <Cale> yep
12:32:17 <dminuoso> I'm not aware of any strange interaction with forkIO
12:32:28 <Cale> Well, the interaction is exactly what you'd expect
12:32:29 <merijn> dminuoso: ResourceT :p
12:32:45 <Cale> You get access to the very same value from the other thread
12:32:57 <infinity0> i generally avoid large stacks, if you need to layer different state types IMO using lens is much easier
12:33:02 <Cale> But maybe that isn't semantically the right thing to be doing, depending on what that value represents
12:33:14 <dminuoso> Ahh.
12:33:17 <merijn> dminuoso: https://github.com/snoyberg/conduit/issues/425
12:33:58 <Cale> So, I much prefer that each higher order operation we're going to be lifting things through gets its own type class, if we're going to abstract over them at all
12:34:06 <dsal> merijn: what's your MonadSQL look like?
12:34:14 <merijn> dsal: Hacky :p
12:34:34 <Cale> and then the uses of ReaderT are wrapped in a way which expresses more than "you have access to an arbitrarily typed value"
12:34:36 <dsal> I've been doing HasDBConnection, which I'm told is worse.  :)
12:34:42 <merijn> dsal: By which I mean it's an ad hoc bad abstraction over Persistent :p
12:35:13 <dsal> I've not tried any higher level DB abstractions, though.  I had a chance to do that, but went the "easy" (i.e., known) route.
12:35:45 * hackage gitit 0.13.0.0 - Wiki using happstack, git or darcs, and pandoc.  https://hackage.haskell.org/package/gitit-0.13.0.0 (JohnMacFarlane)
12:35:51 <Cale> If we're going to be writing code that is polymorphic over many different monads, I hate mtl's type classes showing up, and much prefer application-specific ones.
12:36:14 <Cale> Each constraint should *mean* something.
12:36:36 <Rembane> Cale: Something more than "we can get values from this one"?
12:36:44 <Cale> I don't want to see  MonadReader r m  constraints, even if alongside that are constraints on r
12:37:04 <tdammers> MonadReader AppContext m seems fairly meaningful to me
12:37:10 <Cale> I'd much rather have MonadDatabase m
12:37:36 <Cale> Or  MonadApplicationState m -- if you want an all-encompassing application reader sort of thing
12:38:12 <Cale> (you can probably come up with a less gross name than that :P)
12:38:44 <monochrom> Pokemon m
12:38:49 <Cale> Usually it logically breaks down into finer grained things
12:38:54 <Cale> at least slightly
12:39:59 <maerwald> Why prefix everything with Monad anyway?
12:40:27 <slack1256> maerwald: If I am in pure IO, usually exceptions
12:40:28 <Cale> Well, yeah, I did that just by analogy with the MTL, but personally, I usually like to leave that out
12:40:36 <maerwald> slack1256: so you never use Either?
12:40:41 <Cale> (often I still will make it a subclass of Monad for convenience anyway though)
12:41:15 <slack1256> maerwald: I use it as a return value via `try`. But I avoid it as an extra layer on top of IO.
12:41:18 <Cale> Like, in a web application frontend, you often have some unauthenticated bits of configuration, and stuff which depends on a user token, and different parts of the frontend might rely on one level or the other, so having more than one class is likely
12:41:26 <slack1256> But if my code is pure, I will a monad stack without problems.
12:41:31 <slack1256> *will use
12:42:15 <maerwald> slack1256: if your IO code is Either-heavy, the only thing that makes sense is using ExceptT imo. Unless you like lots of large case of clauses
12:42:32 <Cale> I *hate* the term "monad stack" because it emphasises the wrong way to think about things -- if you have to think about the implementation of your monad, and the layers of monad transformers while writing most of your application, you've set things up wrong.
12:42:45 <merijn> Cale++
12:42:47 <maerwald> There's the other school that says "never use ExceptT with IO, only use MonadThrow"
12:42:51 <maerwald> But I disagree on that
12:43:48 <monochrom> I advocate "try" over "catch" in IO because you seldom need "catch" semantics.
12:43:56 <Cale> Note that we never refer to  f (g (h x))  at the value level as a "function stack"
12:44:19 <monochrom> But this also implies a ton of Eithers. When that gets hairy, I recommend going ExceptT or equivalent.
12:44:21 <slack1256> The functions only care about the values they are passed.
12:44:34 <slack1256> The effects interact differently depending on the order of the stacks
12:45:16 <slack1256> I like to think about that explicitly, so having the notion that the stacks influences the effects interactions is something I like about that term.
12:45:51 <slack1256> But this is probably out of topic.
12:47:56 <ezzieyguywuf> is this use of Arbitrary weird or wrong? https://gitlab.com/ezzieyguywuf/haskellcad/-/blob/master/src/Topology.hs#L351
12:47:58 <monochrom> Cale: Because (((f x) y) z) t is the real function stack :)
12:48:23 <EvanR> f (g (h x)) can involve many types. So that's exciting and distracts from the stackiness. Monad transformers always operate on monads
12:48:27 <ezzieyguywuf> essentially, I concoct some different states of Topology (my data type) and then let QuickCheck randomly pick one
12:48:38 <ezzieyguywuf> I was going to do the same for some of my other data types
12:49:10 <maerwald> random question: who invented monad transformers initially?
12:49:31 <EvanR> yes. Who to blame
12:51:12 <monochrom> perhaps Mark Jones? "Functional Programming with Overloading and Higher-Order Polymorphism"
12:55:54 <dsal> TIL I learned about function stacks.  I'm going to use them in my application.
12:56:01 <monochrom> It's a summer school lecture, not the first occurrence. But it has "previously been suggested by Moggi".
12:57:20 <monochrom> Sheng Liang, Paul Hudak, and Mark Jones. Monad transformers and modular interpreters.
12:57:37 <monochrom> that one is probably the first occurrence for specifically haskell
12:59:33 <nshepperd2> arrange your monad transformers in heaps, not stacks!
13:00:26 <monochrom> Bird's textbook also sties Liang-Hudak-Jones for monad transformers.
13:01:22 <maerwald> Was there ever any reasonable competitor to monad transformers except all the extensible effects stuff?
13:02:14 <dsal> I keep wanting to join some free monad church somewhere, but I'm bad at religions.
13:02:41 <monochrom> Yes, there were "newtype FComp m n a = FComp (m (n a)); newtype BComp m n a = BComp (n (m a))", discussed in the Jones summer school lecture too.
13:04:07 <chloekek> Data.Functor.Compose from base. :) Applicative composition is incredibly nice if you don’t need Monad.
13:04:54 <monochrom> Jones then commented that all his example uses of FComp and BComp were only ever using a fixed monad to transform an arbitrary monad, e.g., BComp Maybe n for arbitrary n.
13:05:03 <monochrom> so may as well define MaybeT.
13:05:15 <monochrom> I think that's how monad transformers won.
13:05:40 <dolio> Well, composition of monads is not necessarily a monad.
13:06:38 <dolio> There are conditions where it is, though.
13:06:49 <dolio> Like distributive laws.
13:08:51 <monochrom> darn I need to read this modular interpreter paper. it includes how to add laziness.
13:10:54 <monochrom> In the context of defining a toy language and writing its toy interpreter, a stack of monad transformers make sense. It acknowledges that when you mix two features, you need to specify how you want them to interfere, and for most features you just have to say their order in a stack of transformers.
13:11:09 <monochrom> e.g., StateT ExceptT vs ExceptT StateT
13:11:55 <dolio> That was basically what Moggi was doing. Defining semantics of programming languages.
13:12:10 <dolio> Wadler was the one who originated using it within a program, I think.
13:12:11 <dsal> I rarely see people from Free Church here.  Is there something people find particularly distasteful here?
13:12:46 <monochrom> They are liberated so they no longer speak here :)
13:14:38 <chloekek> Free is amazing if you have polymorphic variants implemented with row polymorphism. Without that it’s either horrifying type families or lots of lifting through Coproducts, which is the reason I don’t like it so much. But monad transformer classes also require tons of boilerplate. :(
13:15:58 <chloekek> I also try to use free arrows or free applicatives instead of free monads when possible, because they allow for introspection of the computation.
13:16:30 <tdammers> so Lisp is the answer after all! I knew it!
13:17:11 <dsal> Hmm...  I've not managed to piece things together well enough to replace a monad stack with freedom, so I don't exactly know the pain.  transformers haven't been a huge burden for me, but the idea of having even less burden is appealing.
13:21:33 <monochrom> Hahaha StateT Store (ReaderT Env (ContT Answer (StateT String (ErrorT (List))))))
13:21:35 <johntalent> If it wasn't for technical people to replace Wozniak, we'd still be using all uppercase...effectively screaming at each other! XD
13:21:56 <monochrom> (the "StateT String" part is probably better off as WriterT String)
13:22:07 <tdammers> johntalent: if you miss that, I highly recommend /r/totallynotrobots
13:22:38 <tdammers> monochrom: at that point, might as well go `RWST Dynamic Dynamic Dynamic IO`, a.k.a. the "I give up" stack
13:22:46 <monochrom> haha
13:23:16 * tdammers is tempted to publish acme-igiveup on hackage
13:23:35 <chloekek> Tri-functor ST would be amazing.
13:24:37 <chloekek> main :: ST s ({- syscalls: -} Syscalls) ({- possible exceptions: -} Void) ({- result type: -} ()), then remove the FFI, and you can always mock all I/O when calling anything.
13:25:59 <chloekek> Instead of putStrLn "Hello", you would take write from the Syscalls record passed through the reader component of tri-functor ST, and then call that.
13:26:23 <tdammers> sounds very pragmatic
13:26:26 <chloekek> Then, if a library by mistake calls putStrLn (which a library should probably not do), you can pass it a different write “syscall” instead.
13:26:32 <siers> are functions, lenses and json codecs profunctors? are the latter two profunctors essentially because they're holding a function inside?
13:29:00 <chloekek> siers: instance Profunctor (->) and instance Profunctor (Kleisli Aeson.Parser) both exist. I don’t know about lenses.
13:29:20 <dsal> What's a JSON codec profunctor?
13:29:27 <dsal> I'm a little confused by this whole hting.
13:34:04 <koz_> dsal: You know profunctors?
13:34:09 <koz_> Like, is that the issue?
13:35:32 <dsal> I don't understand what it means for there to be a json parser profunctor.
13:35:40 <koz_> It's not a JSON parser profunctor.
13:35:52 <koz_> It's the Kleisli profunctor, such that the effect is Parser from Aeson.
13:35:56 <dsal> I thin of profunctors as  kind of like bifunctor, but contravariant functor on the left and covariant functor on the right.  I don't know what that means.
13:35:56 <koz_> (which can fail with some errors)
13:36:12 <dsal> Er, sorry, in a meeting and not making sense.  heh
13:36:25 <koz_> dsal: You can think of any profunctor 'p a b' as a 'pipeline' that 'takes in' a and 'gives out' b.
13:36:35 <dsal> Yes, that's how I think of it.
13:36:42 <koz_> So Kleisli Aeson.Parser is basically 'a -> Aeson.Parser b'
13:36:48 <koz_> That's literally it.
13:37:05 <siers> I would like to see some weird profunctor examples
13:37:07 <dsal> Oh.  I guess that makes sense.
13:37:14 <koz_> siers: Sure, one second.
13:37:15 <siers> something that isn't so obvious
13:38:04 <koz_> Consider 'data Sum f g a = Sum (f a) (g a)'
13:38:20 <koz_> Sorry, whoops.
13:38:38 <koz_> My bad.
13:38:40 <koz_> Ignore that.
13:39:52 <koz_> Consider 'newtype Tagged s b = Tagged b'.
13:39:58 <koz_> (the classic exampel of a 'phantom type')
13:40:00 <koz_> That's a profunctor!
13:40:55 <siers> so it holds b, so you can rmap that, but since s is lost (or a "phantom type") you can also lmap that
13:41:04 <siers> technically valid
13:41:09 <koz_> Yep.
13:41:10 <siers> interesting
13:42:06 <koz_> Realistically, a profunctor is just a combination of a contravariant functor (the 'input' part) and a covariant functor (the 'output' part).
13:42:14 <koz_> (->) is the easiest example
13:42:28 <koz_> Since any a -> b can be thought of as 'taking in' an 'a' and 'giving out' a 'b'.
13:42:52 <siers> right. it seems that profunctors don't seem mind-blowing
13:42:57 <Nistur> so, trying to build haskell from source on Archlinux ARM, using a repackaged debian version as a host (which warns because of ncurses libraries not reporting version numbers) fails with `opt' failed in phase `LLVM Optimiser'. What fun
13:42:58 <koz_> They're really not.
13:43:07 <siers> koz_, anything that has a function inside of them is a profunctor
13:43:11 <phadej> really, really, when you think hard, a) profunctors are not difficult abstractions b) very very rare one.
13:43:21 <koz_> phadej is right on this one.
13:43:22 <phadej> so disappointing
13:43:31 <koz_> Also, phadej - I want to thank you for writing the Glassery.
13:43:34 <siers> (and is exposing that funtion's parameters)
13:43:45 <koz_> I just used it at work.
13:43:57 <phadej> glassery? how
13:44:06 <koz_> siers: Sorta. That's a bit of an oversimplification.
13:44:19 <koz_> phadej: Someone needed to be instructed on the difference between lenses and prisms.
13:44:31 <phadej> it would really need an update, stuff happened in three years
13:44:41 <koz_> phadej: It's still incredibly useful.
13:44:57 <phadej> yes, I don't think there's anything wrong as such
13:45:12 <koz_> I used it to write some property tests recently (for testing that my Prisms followed the laws).
13:45:32 <phadej> you should be able to find laws in both `lens` and `optics` docs too :)
13:45:40 <siers> do you speak haskell or just something strong enough to talk encode optics?
13:45:57 <siers> koz_, how come?
13:45:59 <koz_> phadej: I found the Glassery easier to read.
13:46:09 <koz_> siers: Having 'a function inside something' can mean lots of different things.
13:46:35 <koz_> For example, 'data Foo a = Foo (forall b . a -> b)' certainly 'has a function inside it'.
13:46:39 <koz_> However, that's not a profunctor.
13:47:02 <koz_> siers: What do you mean by 'speak Haskell'?
13:47:18 <siers> sorry, I mean "write haskell at work"
13:47:35 <koz_> siers: Yep, although these days, I mostly write Cabal files and watch things fail to build for silly reasons.
13:47:42 <koz_> (that's been my last... week at least?)
13:48:16 <siers> you're right, that's a nice counterexample
13:48:31 <koz_> siers: Like, what you said isn't exactly _wrong_, it's just missing some context.
13:48:58 <siers> so I guess I could restrict it to saying "anything that has a function with concrete types inside of it"
13:49:06 <koz_> The opposite in fact.
13:49:22 <koz_> newtype Foo = Foo (Int -> String) is has a function with concrete types inside of it.
13:49:29 <koz_> But that's definitely not a profunctor.
13:50:02 <siers> I know what I can resort to...
13:50:14 <siers> anything that can implement a profunctor is a profunctor
13:50:21 <siers> and I have a rough idea what those are
13:50:25 <siers> in other words – I GIVE UP!
13:50:28 <koz_> Lol, anything that can _lawfully_ implement Profunctor. :P
13:50:32 <siers> right
13:50:36 <koz_> Basically, you gotta be able to write a lawful dimap.
13:50:43 <koz_> If you can do that - profunctor.
13:50:46 <siers> yeah and l/rmap is derivable
13:50:56 <koz_> Yeah, they're just 'halves' of dimap really.
13:51:02 <koz_> (filling the 'gap' in with id)
13:51:04 <siers> applied "id"
13:51:06 <siers> yeah
13:52:09 <siers> someone at work was asking about profunctors, so I just opened up the docs and 'got' them in the end
13:52:13 <siers> not haskell, however :P
13:52:20 <siers> I was using haskell to find the definitions though
13:55:58 <nshepperd2> you can make a profunctor by taking any functor f and contravariant functor g and defining P a b = P (g a) (f b)
13:57:23 <koz_> nshepperd2: I believe there's something liek this in profunctors-the-library, right?
13:57:40 <nshepperd2> probably
13:58:40 <Athas> There's a lot of conflict around the 'random' library now.  But is there a good reason why it's considered important?  I also noticed random's atrocious performance recently, and I switched to another library for RNGs with no problem.
13:59:01 <Athas> I can see the issue with libraries like 'vector', which have no real alternative.
14:00:40 <koz_> Athas: Where's said conflict played out? Not saying you're wrong, just curious.
14:01:03 <Athas> koz_: the 'libraries' mailing list.
14:01:09 <koz_> Athas: Ah.
14:01:10 <Athas> And on the GitHub issue tracker, to an extent.
14:01:23 <koz_> Good to know I guess.
14:44:45 * hackage language-docker 9.1.0 - Dockerfile parser, pretty-printer and embedded DSL  https://hackage.haskell.org/package/language-docker-9.1.0 (lorenzo)
14:49:14 * hackage lightstep-haskell 0.10.2 - LightStep OpenTracing client library  https://hackage.haskell.org/package/lightstep-haskell-0.10.2 (DmitryIvanov)
14:55:14 * hackage hvega 0.9.1.0 - Create Vega-Lite visualizations (version 4) in Haskell.  https://hackage.haskell.org/package/hvega-0.9.1.0 (DouglasBurke)
14:59:12 <N3RGY> has anyone tried using polysemy in GHC 8.10? Is it actually zero-overhead now?
15:01:30 <N3RGY> It seems pretty cool but sounds like its performance pretty critically depends on 8.10's dictionary optimization fixes
15:03:53 <koz_> N3RGY: I don't think anyone's checked this. If isovector were around you could ask him?
15:03:59 <koz_> (he sometimes materializes briefly)
15:33:02 <dsal> who is isovector?
15:33:21 <koz_> dsal: Sandy Maguire.
15:33:31 <dsal> Oh cool.  I bought a book from him once.
15:33:38 <koz_> > fromIntegral (128 :: Word8) :: Int8
15:33:39 <lambdabot>  -128
15:33:52 <koz_> > fromIntegral (129 :: Word8) :: Int8
15:33:55 <lambdabot>  -127
15:34:11 <koz_> dsal: I think I know the one. I bought on release.
15:34:19 <koz_> > fromIntegral (255 :: Word8) :: Int8
15:34:21 <lambdabot>  -1
15:35:32 <dsal> @check \x -> abs x >= (0 :: Int8)
15:35:34 <lambdabot>  +++ OK, passed 100 tests.
15:35:35 <dsal> @check \x -> abs x >= (0 :: Int8)
15:35:37 <lambdabot>  +++ OK, passed 100 tests.
15:35:39 <dsal> @check \x -> abs x >= (0 :: Int8)
15:35:40 <lambdabot>  +++ OK, passed 100 tests.
15:36:00 <dsal> Needs more tests.
15:36:02 <koz_> > minBound :: Int8
15:36:04 <lambdabot>  -128
15:36:42 <dsal> I like that particular quickcheck exmaple.
15:37:03 * dsal exmaple is a term for people who immigrated from Canada.
15:37:58 <N3RGY> > abs ((-128) :: Int8) >= 0
15:38:00 <lambdabot>  False
15:39:01 <dsal> It's an interesting example for me because I've had a failure in real world code, but it's related to the sort of thing you'd actually write in a quickcheck test because it's obviously correct, but would fail on occasion.
15:39:48 <dsal> In my case, it was a hash function bounding itself to a range.    So it'd be more like    \x -> let h = hash(x, 11) in  h >= 0 && h < 11
15:43:42 <ClaudiusMaximus> > minBound `div` negate 1
15:43:44 <lambdabot>  error:
15:43:44 <lambdabot>      • Ambiguous type variable ‘a0’ arising from a use of ‘show_M619023542720...
15:43:44 <lambdabot>        prevents the constraint ‘(Show a0)’ from being solved.
15:45:23 <ClaudiusMaximus> > minBound `div` negate 1 :: Int
15:45:27 <lambdabot>  *Exception: arithmetic overflow
15:47:15 * hackage aeson-schemas 1.2.0 - Easily consume JSON data on-demand with type-safety  https://hackage.haskell.org/package/aeson-schemas-1.2.0 (leapyear)
15:52:27 <koz_> dsal: I had something like this blow up on me recently with property-based tests too.
15:52:36 <koz_> Basically, even the _Word_ instance of Enum is partial.
15:52:40 <koz_> Which is aggravating af.
15:59:15 * hackage th-nowq 0.1.0.5 - Template Haskell splice that expands to current time  https://hackage.haskell.org/package/th-nowq-0.1.0.5 (DmitryDzhus)
16:15:23 <dsal> koz_: Which parts?  I know succ/pred on bounded enum is partial.  I know that again periodically.
16:19:03 <koz_> > fromEnum (128 :: Word8) :: Int8
16:19:05 <lambdabot>  error:
16:19:05 <lambdabot>      • Couldn't match expected type ‘Int8’ with actual type ‘Int’
16:19:05 <lambdabot>      • In the expression: fromEnum (128 :: Word8) :: Int8
16:19:17 <koz_> Never mind, I have to find the big-af number.
16:20:07 <koz_> > fromEnum (9223372036854775808 :: Word)
16:20:09 <lambdabot>  *Exception: Enum.fromEnum{Word}: value (9223372036854775808) is outside of I...
16:20:25 <koz_> Basically, the mapping is _super_ stupid, in that it basically maps 0 -> 0, 1 -> 1 etc.
16:20:30 <dsal> Oh, right that too.
16:20:38 <koz_> So anything above the upper bound of Int is broken, even though there is 100% no reason why it has to be that way.
16:21:00 <koz_> What's hilarious is that the _non_-stupid mapping isn't that much more difficult (it can be implemented by means of shifts and masking)
16:21:08 <koz_> (or divmod if you're feeling lazy)
16:21:47 <koz_> This is literally partiality for the sake of partiality.
16:21:53 <dsal> I don't think it's unreasonable that  65536 :: Word8 doesn't work.
16:22:02 <dsal> > 65536 :: Word8
16:22:04 <lambdabot>  0
16:22:10 <koz_> Except that isn't what I'm proposing at all.
16:22:15 <dsal> Well.  Now I'm confused.
16:22:29 <koz_> I'm saying that a more sensible mapping is minBound -> minBound, minBound + 1 -> minBound + 1 etc.
16:22:37 <koz_> The cardinality of Int and Word is the same on every platform we care about.
16:22:47 <koz_> And this mapping violates zero laws and requires zero real inefficiencies.
16:23:15 <koz_> Like, I could _almost_ understand issues with this in cases like, say, Word64 on 32-bit architectures.
16:23:20 <koz_> Even though that's still IMHO inexcusable.
16:23:28 <koz_> But this is 100% not necessary for any reason whatsoever.
16:23:45 * hackage th-env 0.1.0.2 - Template Haskell splice that expands to an environment variable  https://hackage.haskell.org/package/th-env-0.1.0.2 (DmitryDzhus)
16:25:05 <dolio> Does enum even say that the mapping respects ordering?
16:25:13 <koz_> dolio: No.
16:25:33 <koz_> Since Ord isn't a requirement, you can't even state this sensibly.
16:26:18 <koz_> (and in fact, the current mapping doesn't either, because 'blowing up' isn't less than or greater than any value)
16:26:34 <dolio> Actually, I think enumFromThenTo probably relies on that.
16:26:41 <dolio> @type enumFromThenTo
16:26:42 <lambdabot> Enum a => a -> a -> a -> [a]
16:26:54 <koz_> dolio: It does, by default.
16:27:03 <koz_> However, this is not required, at least insofar as the Prelude docs say.
16:27:06 <dolio> Oh, is that a method?
16:27:17 <koz_> https://hackage.haskell.org/package/base-4.14.0.0/docs/Prelude.html#v:enumFromThenTo
16:28:00 <koz_> Essentially, I'm gonna add _that_ particular thing to my 'rant against Enum' section of finitary's docs.
16:28:03 <dsal> data Numbers = Five | Six | Four | Two | Zero | One | Three   deriving (Bounded, Enum)
16:28:52 <dolio> Yeah, I just wanted Word64's enum instance to be the exact bit mapping the other day.
16:29:12 <dolio> Because why not?
16:29:13 <koz_> dolio: That's not possible in general, because Word64 could have a larger cardinality than Int on some platforms.
16:29:17 * koz_ looks at ARMv7.
16:29:55 <ja> Int always has the size of a C int?
16:29:58 <dolio> Ah, well, on supported platforms anyway.
16:30:02 <koz_> ja: Yes, I believe so.
16:30:30 <dolio> No, isn't C int 32 bits?
16:30:39 <koz_> dolio: Nope.
16:30:45 <koz_> That's not what the standard ever said.
16:30:48 <dolio> It's not necessarily the system size, I thought.
16:30:58 <koz_> The standard _actually_ says that int must be at least 16 bits, and not larger than long.
16:31:02 <koz_> Literally nothing beyond that.
16:31:07 <koz_> it doesn't even have to be a power of 2.
16:31:09 <koz_> Or a power of _anything_.
16:31:26 <koz_> Because the C standard defines 'compatibility' as 'make the compiler writers' job as easy as possible because fuck users lol'.
16:31:38 <ja> yeah it would be cool to have compcert run your architecture on some develish imaginary architecture
16:31:50 <ja> and you can claim that basically any program contains undefined behaviour muahahaha
16:31:57 <koz_> ja: You laugh, but you can _still_ buy machines with 9-bit chars.
16:32:01 <dolio> Okay, well, in that case it's probably not safe to assume that C's int is the same size as GHC's Int.
16:32:15 <ja> koz_: i should buy one to keep them in business
16:32:20 <ja> and keep humanity struggling
16:32:29 <koz_> dolio: I believe when GHC is built, it uses a check at compile time to define Int# to match C's int.
16:32:32 <koz_> I could be wrong though.
16:33:00 <koz_> Regardless, even if it _wasn't_ C's int, even the 'fixed size' types aren't guaranteed to exist by the standard.
16:33:06 <dolio> I don't think that's right, because either primitive or vector had a problem where GHC's integers were bigger than the C that was being wrapped, I think.
16:33:21 <koz_> dolio: Again, I dunno. I'm very willing to be corrected on this fact.
16:33:31 <dolio> And the C had to be changed to size_t or one of those sorts of types.
16:33:42 <koz_> dolio: size_t is even worse.
16:33:54 <koz_> The standard basically says 'size_t must be unsigned and at least 16 bits wide'.
16:33:54 <dsal> koz_: int can be powerless?
16:33:59 <koz_> No further provisions _at all_.
16:34:15 <koz_> dsal: ??
16:34:55 <koz_> Which, amusingly enough, means that arrays of more than size_t elements are technically speaking not portable C.
16:35:00 <koz_> Enjoy living with that knowledge.
16:35:15 <dsal> <koz_> it doesn't even have to be a power of 2.  16:30 <koz_> Or a power of _anything_.    *wizard meme*
16:35:16 <koz_> s/size_t/2^16 - 1/
16:35:21 <koz_> dsal: Ah.
16:35:40 <koz_> Seriously, the more you examine the C standard, the more insane it reads.
16:35:42 <dolio> Neither do Haskell's.
16:35:52 <koz_> And for the latest update, they want to add _more_ UB.
16:35:57 <dolio> They're required to be larger though.
16:36:00 <koz_> dolio: The standard guarantees int is at least 22 bits wide.
16:36:06 <koz_> s/int/Int/
16:36:14 <koz_> s/standard/Haskell Report/
16:36:29 <dsal> 22 is an odd number.  Well, half of 22 is anyway.
16:36:40 <koz_> dsal: I dunno lol.
16:36:48 <koz_> I'm just repeating what the big document says.
16:36:56 <ja> i am reminded of that cryptocurrency that used ternary just to be different hahaha
16:37:02 <dolio> It doesn't require them to be bits, though, right? Just to fit -2^22 to 2^22-1 or something.
16:37:10 <koz_> dolio: Yeah, that's a good point.
16:37:18 <koz_> But I'm kind of assuming we're not on a ternary arch or something.
16:37:26 <koz_> Because once we go _there_, things get _really_ wacky.
16:37:40 <koz_> Fun fact though - C doesn't mandate twos-comp representation for signed integer types.
16:37:47 <koz_> (C++ as of latest standard _does_, however)
16:38:00 <ja> so that means signed integer overflow is ok in c++?
16:38:03 <koz_> Nor does C mandate IEEE floats for its 'float', 'double' and 'long double' types.
16:38:15 <koz_> ja: In C++-whatever-number-they're-up-to, I guess?
16:38:20 <koz_> Unless they decided to keep it UB.
16:38:24 <koz_> Which may or may not have occurred.
16:38:24 <dolio> Oh, it's 2^29, by the way.
16:38:35 <koz_> There are not enough drugs in the world to convince me to read _that_ document.
16:38:41 <koz_> dolio: Yeah, thanks for the correction, my bad.
16:38:43 <dolio> Because you can use 2 tag bits.
16:39:09 <dsal> ocaml had something like 30 bit ints.  I ran into a production bug there.  Then I fixed it for 31 bits.  Then I fixed it again for 32 bits.  I learn slowly sometimes.
16:39:34 <dsal> Maybe it was 29.  I don't remember.  I just remember fixing the same bug more than once.
16:39:46 <koz_> It's not a fun story in any language honestly.
16:40:01 <dsal> How many bits is Integer?
16:40:03 <koz_> In fact, the whole 'Int/Word can be of variable size' is the _only_ reason finitary relies on TH.
16:40:05 <koz_> dsal: Unbounded.
16:40:15 <koz_> Well, up to 'how much RAM you have'.
16:40:20 <dsal> Integerfinity
16:41:16 <dsal> > let ∞ = maxBound :: Integer in ()  -- I need a language extension to make this work.
16:41:18 <lambdabot>  <hint>:1:5: error: parse error on input ‘∞’
16:41:33 <koz_> dsal: Yeah, that's gonna be an interesting one.
16:42:01 * dsal rabbitholes on why ∞ isn't a valid variable name.
16:42:35 <dsal> Block:	Mathematical Operators, U+2200 - U+22FF
16:42:39 <dsal> That's weird.
16:44:00 <Tuplanolla> > (∞)
16:44:03 <lambdabot>  65535
16:44:34 <dsal> Heh.  That's confusing.
16:44:36 <dsal> :t (∞)
16:44:37 <lambdabot> Integer
16:44:39 <koz_> Tuplanolla: Who was the clever person who defined that. :P
16:45:12 <dsal> I don't quite understand what's happening there.
16:45:26 <koz_> :t signum
16:45:28 <lambdabot> Num a => a -> a
16:45:58 <dsal> Oh.  I just never even though to do that.
16:46:39 <dolio> ∞ is an operator character?
16:48:26 <dsal> Yeah.  I guess it's not a letter, but operator is a weird classification.
16:48:35 <koz_> > maxBound :: Int8
16:48:37 <lambdabot>  127
16:49:49 <dsal> > let (∞) = maxBound in    (∞) :: Word64
16:49:51 <lambdabot>  18446744073709551615
16:50:22 <dsal> > let (∞) = maxBound in    (∞) :: Bool
16:50:25 <lambdabot>  True
16:50:36 <dsal> The truth is infinite.
17:03:39 <koz_> How long does it take stack/Stackage/whatever the heck is responsible to figure out a more recent thing is available on Hackage?
17:15:41 <maralorn> exec "/nix/store/vm1rm3ssgrfz7czrqjxri53j0vrkiwca-ghc-8.8.3/bin/ghc"  "-B$NIX_GHC_LIBDIR" "$@"
17:15:52 <koz_> maralorn: ??
17:15:55 <maralorn> What does the -B do? I can‘t find dokumentation on it.
17:17:06 <maralorn> I mean. Of course there is this, which is hilarious: https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/runtime_control.html?highlight=b#rts-flag--B
17:27:15 * hackage franz 0.3.0.1 - Append-only database  https://hackage.haskell.org/package/franz-0.3.0.1 (FumiakiKinoshita)
17:38:00 <koz_> :t compare
17:38:02 <lambdabot> Ord a => a -> a -> Ordering
18:27:16 <dmwit> maralorn: Crazy. -B doesn't even appear in ghc --show-options here, but ghc definitely accepts it. I'd file a bug.
18:38:10 <sm[m]> koz_: stack will know if you run stack update. Often it will do that automatically, but in some cases I think not. Stackage: it just depends if someone is paying attention to that package, or needs it as a dep
18:38:17 <koz_> sm[m]: Yeah, I just realized.
18:38:20 <koz_> Thanks though!
18:38:59 <sm[m]> Often easy to send a pr to build-constraints.yaml yourself
19:16:59 <jackdk> Is anyone here familiar with "access denied" errors when using cabal v2- commands on windows? I am trying to diagnose a student's problem: https://www.irccloud.com/pastebin/l6g5QPxo/error.txt
19:18:52 <sclv> jackdk: are they on a recent cabal-install?
19:19:05 <jackdk> sclv: sorry, should've said that. 3.0.0.0
19:19:30 <jackdk> I've found old-looking GH issues about this sort of thing, but nothing that looked recent and promising
19:20:52 <sclv> yeah, it looks a bit mysterious
19:21:07 <sclv> the "who knows" approach i'd take if i was just solving my own problem is move to the latest cabal binary
19:21:22 <sclv> and cross my fingers that it was one of a million weird bugs fixed with haskell windows stuff
19:21:29 <jackdk> is that still `cabal v2-update` then `cabal v2-install cabal-install`?
19:21:37 <jackdk> windows is cursed, mate
19:21:44 <sclv> well i'd download the binary from haskell.org/cabal
19:21:52 <sclv> i never build cabal if i can help it
19:22:07 <sclv> also on cabal 3 you don't need the v2- prefix
19:22:36 <sclv> also if its a really long project root i might try to move it higher up in the path
19:22:44 <sclv> just in case it was like some long file name bullshit
19:23:00 <sclv> and also check if their homedir or something had any non ascii characters anywhere in the path
19:23:25 <sclv> none is necessarily the problem but those sorts of things historically have been problems, so, eh
19:24:43 <sclv> jackdk: oh hey also if they have antivirus running maybe its f'n with stuff
19:25:05 <jackdk> sclv: sigh yes good point, I will ask.
19:25:23 <sclv> easy to imagine you create a file, then try to delete it but in the  meantime av opened it to scan it
19:47:29 <monochrom> I wonder what's with "\\\\?\\C:\\path\\to\\project\\root"
19:47:45 <monochrom> this cannot literally exist.
19:51:14 <Hoppelhase> What is that supposed to be?
19:52:31 <Hoppelhase> Ah, from the error
19:58:29 <jackdk> probably something getting printed through mutliple layers of `show`?
20:00:09 <monochrom> I am not impeaching the double escaping backslashes. That's normal, we know each \\ means one \
20:00:24 <monochrom> I am looking at path, to, project, roo.
20:00:28 <monochrom> err root.
20:00:40 <monochrom> And the question mark.
20:01:41 <monochrom> Why the hell is a student's path name literally \\?\C:\path\to\project\root ?
20:03:09 <monochrom> I can't help but imagine it's PEBKAC, the student misunderstood a manual or a blog that says "\path\to\project\root" figuratively.
20:03:52 <monochrom> (Among other things. There's going to be an even more hilarious explanation for the \?\ before we even get to C:)
20:04:24 <jackdk> the \\\\?\\C:\\ appears copy-pasted from the error output of cabal v2-build. I have replaced the literal path to the student's checkout with a placeholder for privacy's sake.
20:10:18 <ja> monochrom: the question mark is normal: https://stackoverflow.com/a/21194605/309483
20:13:14 * hackage qtah-generator 0.7.1 - Generator for Qtah Qt bindings  https://hackage.haskell.org/package/qtah-generator-0.7.1 (khumba)
20:13:22 <monochrom> Then sorry.
20:18:01 <Hoppelhase> Wow, I've been working on Windows systems for 20 years now, and I've never seen this syntax before :o
20:18:16 <dsal> Why does anyone user backslash for paths?
20:18:48 <c_wraith> so that it works with that syntax?
20:18:53 <dsal> Has that been required since CP/M?
20:19:08 <monochrom> CP/M probably didn't have subdirectories.
20:19:43 <monochrom> But DOS people could be asking the opposite, "why does any use slash?" :)
20:20:05 <dsal> It's super inconvenient in programs.
20:20:22 <Hoppelhase> How so?
20:20:31 <jchia> "c:\\abc"
20:20:40 <jchia> you need to escape the backslash
20:20:55 <monochrom> Perhaps design your language to escape the slash instead?
20:21:04 <dsal> With a backslash.  And sometimes you have to escape the backslash you're escaping.
20:21:23 <jchia> a lot of languages use backslash to escape
20:21:28 <dsal> I've had code with like, six or eight backslashes in a row.
20:22:57 <monochrom> My real question transcends both. Why use one single string for paths so that you must lose one character.
20:23:23 <monochrom> You can't have "CP/M.exe" as a filename now.
20:23:50 <dsal> That's why nobody runs CP/M anymore.
20:24:13 <monochrom> or "TCP/IP.exe"
20:24:55 <monochrom> "A/B test report.doc"
20:25:00 <dsal> : is also kind of special in weird ways in weird places.
20:28:15 <jackdk> every now and then you still find things called "con" that break the world
20:28:47 <dsal> haha
20:29:06 <dolio> Yeah, we had a problem at work a while back where sometimes you would get a ton of stuff dumped while compiling.
20:29:49 <dolio> This was because we had a type called 'Con' in our code, and scala was generating a file with 'Con' in the name, which meant Windows would print it to the terminal.
20:29:54 <dsal> I know a little about DOS, but I've mostly lived in various UNIX systems.
20:30:28 <monochrom> Don't forget prn
20:31:10 <dsal> That awkward moment when you try to save a nude on the dl and it turns into a print job.
20:32:47 <crestfallen> hi basic question re: immutability: each definition of x in the second section is bound to its following 'in' statement. that's what bound means here right? http://ix.io/2oeo
20:35:03 <crestfallen> (the uncommented section)
20:36:22 <dsal> crestfallen: I don't understand the question.  what does immutability have to do with bindings?
20:36:50 <dsal> The names you define in the let are available in the `in` part.
20:38:33 <crestfallen> what does immutable variable in haskell mean? is the name of the SO post.    https://stackoverflow.com/questions/38040812/what-does-immutable-variable-in-haskell-mean
20:38:53 <crestfallen> it's the second answer not the upvoted one
20:39:00 <dsal> immutable means "not variable"
20:39:11 <dsal> If you say `x = 2` then x and 2 mean the same thing and you can't change x.
20:40:00 <dsal> That post is people confusing themselves in GHCI.
20:40:49 <dsal> % let x = 2 in (let x = 3 in print x) >> print x
20:40:50 <yahb> dsal: 3; 2
20:41:17 <dsal> There are two different bindings in two different scopes both named `x` there.  You can't change either `x` but you can have more than one thing name `x`
20:41:45 * hackage netcode-io 0.0.1 - Bindings to the low-level netcode.io library.  https://hackage.haskell.org/package/netcode-io-0.0.1 (Mokosha)
20:42:04 <crestfallen> ok yeah that is a narrower scope, x = 3, so it's printed first right?
20:42:22 <dsal> It's printed first because I printed it first.  :)
20:42:31 <dsal> % let x = 2 in print x >> (let x = 3 in print x) >> print x
20:42:31 <yahb> dsal: 2; 3; 2
20:43:32 <crestfallen> weird, if you look at the upvoted entry, they show the desugared version and remark about how there is a more narrow scope
20:44:09 <dsal> Yes?
20:44:13 <MarcelineVQ> dsal did not say "no it's not a narrower scope"
20:44:15 <jackdk> sclv, monochrom: 50 points to whoever guessed "antivirus"
20:44:37 <sclv> psychic debugging, my secret poower
20:45:03 <crestfallen> so without (>>) it would default to printing the more narrow scope first I guess?
20:45:25 <monochrom> sclv guesed antivirus
20:45:36 <dsal> It prints whatever is in scope, not whatever is the most narrow or broad scope.
20:46:38 <liiae> what Control.Arrow is?
20:46:44 <dsal> You can basically read `>>` as `;` there.
20:46:50 <crestfallen> it says narrow == more local, which have a priority.. is what it says anyway
20:46:52 <dsal> liiae: Looks like a module name.
20:47:14 <dsal> crestfallen: That's a super weird way to think about it, but if it helps, you, I guess.
20:47:46 <dsal> The `3` `x` in my example has a narrower scope than the `2` `x`, but I'm able to print both of them.
20:48:47 <crestfallen> thanks so dsal more generally:
20:48:52 <liiae> dsal: what it's used to
20:49:02 <crestfallen> I was watching a classic video called the value of values...
20:49:23 <dsal> liiae: https://hackage.haskell.org/package/base-4.14.0.0/docs/Control-Arrow.html
20:49:45 <crestfallen> by rich hickey
20:50:37 <crestfallen> essentially he's saying that immutability is important because records can be kept, so you're not stuck with the last state only, but can see how the state changes over time.
20:50:38 <dsal> Rich Hickey has some neat ideas, but a lot of them are a bit different from haskell.  Though immutability and basic scoping should both apply similarly.
20:50:56 <dsal> Immutability makes a lot of things easier, yeah.
20:51:44 <crestfallen> so in my paste and query above, you can see how the 'record' of the program is in tact, right?
20:51:57 <dsal> You get neat things like undo of map operations in a recursive function by like, just returning from that function.  Super neat when doing recursive descent problem solving.  (I used this technique to solve a puzzle I saw at the SF zoo excessively many years ago).
20:52:33 <dsal> crestfallen: In your example, you didn't change `x`, but you also can't access its original value, so it's not really relevant.
20:52:35 <monochrom> You'll have to get to the point of writing elaborate backtracking search algorithms before you can reap the benefit of past states.
20:53:08 <MarcelineVQ> the real state was the journal we made along the way
20:53:11 <monochrom> For beginners the more visible benefit is "it's just math".
20:53:33 <MarcelineVQ> that joke doesn't work at all, nevermind me
20:53:58 <dsal> What is the value of `x` in `x = x + 1`  ?
20:54:22 <dsal> ghci likes answering questions like that.
20:54:31 <crestfallen> but theoretically you can see an aspect of immutability in those simple let statements, because nothing is overwritten. I'm just getting to the basics of immutability if possible
20:54:46 <monochrom> I can help build on that joke. Stalin: The state owns the journal. McLuhan: The journal is the state. :)
20:55:20 <dsal> crestfallen: There's only one basic of immutability:  You can't change things.
20:56:32 <dsal> You can make new things based on things that you've had in the past.  You can even give things the same name in some cases.  But anything referencing your original `x` will always see the value that was there when it first met `x`.
20:56:36 <crestfallen> but in the monad example at the top, it's not a question of a being mutated right?
20:57:22 <dsal> I don't know what monad example you mean.  But try thinking of it this way.  Anything that sees `x` will never see it with a value other than the one it saw the first time.
20:57:42 <crestfallen> the do notation at the top of my paste, sorry
20:58:30 <dsal> You just made a second value with the same name.  You can do that.  I think the compiler will at least warn you when you've done that since you can't access the original one anymore.
21:01:53 <crestfallen> dsal, one moment pls
21:03:46 <crestfallen> idk i'm not getting warned. there is a distinction with the do notation from the let..in statements
21:05:34 <crestfallen> what I'm asking is whether the first action 'let a' is not evaluated at all? or something else..
21:05:52 <crestfallen> on line 5 if I had lines
21:06:23 <crestfallen> sorry line 4
21:06:32 <MarcelineVQ> the warning  isnot on by default, called -fwarn-name-shadowing
21:07:18 <MarcelineVQ> shadowing is the commonly accepted term for what is happening to c in   let c = 1 in let c = 2 in c
21:07:25 <crestfallen> but why would I be warned? I mean, what does the (seemingly accepted) behavior tell me?
21:07:49 <crestfallen> in regards to immutability
21:09:00 <MarcelineVQ> nothing in regards to immutability, shadowing is about scopes and binding names
21:12:38 <dsal> crestfallen: shadowing means you named two different things with the same name in such a way that will probably confuse you.
21:12:41 <MarcelineVQ> with something like    let c = 1 in let c = 2 in c   we have created a scope with let where there is a binding (name) for 1 called c. Within that scope we create a nother scope with let where there is a binding (name) called c bound. Within this latter scope anytime we use the name c directly, it refers to 2. This is called shadowing because this later c, the one that refers to 2, is shadowing/hiding the first.
21:15:27 <MarcelineVQ> It's not about immutability, if someone has mentioned immutability it's probably just to draw attention to the fact that we're not changing what c refers to, we have another c, both exist, but we can only refer (directly) to the most recently bound c in a given scope.
21:16:52 <crestfallen> thanks , well the name of the SO post is what does immutable variable mean in haskell so I'm a wee bit confused
21:17:15 <dsal> "immutable variable" means "not variable"
21:17:33 <MarcelineVQ> the fact that we're not changing what c refers to, we have another c, both exist, but we can only refer (directly) to the most recently bound c in a given scope
21:18:21 <dolio> No, it means that people in 1970 misunderstood what "variables" in mathematics were.
21:18:54 <dolio> And named something else with the same word.
21:18:56 <crestfallen> ok yeah so in fact no mutation takes place.. so in the do notation at the top of my paste, also its shadowing that is happening there
21:19:10 <dolio> Maybe 1960.
21:23:12 <crestfallen> am I correct directly above? ^
21:25:29 <crestfallen> it must be shadowing then
21:25:32 <MarcelineVQ> yes, a is shadowed
21:25:32 <crestfallen> thanks
21:26:31 <crestfallen> not variable == immutable variable yes that's interesting
21:28:21 <dsal> "immutable variable" just doesn't make sense.  Is it mutable or does it vary?
21:28:36 <dsal> er, immutable... something.
21:30:41 <crestfallen> so it leads me to think about the seed value in a fold..
21:31:30 <crestfallen> it's not updated, mutated, what have you. it's more like recursive steps
21:34:09 <dsal> > foldr f 0 [1..4] :: [Expr]
21:34:11 <lambdabot>  error:
21:34:11 <lambdabot>      • No instance for (FromExpr [Expr]) arising from a use of ‘f’
21:34:11 <lambdabot>      • In the first argument of ‘foldr’, namely ‘f’
21:34:38 <dsal> > foldr f 0 [1..4] :: Expr
21:34:40 <lambdabot>  f 1 (f 2 (f 3 (f 4 0)))
21:34:48 <dsal> Always getting that wrong...
21:35:59 <crestfallen> > scanr (+2) 0 [1..4]
21:36:01 <lambdabot>  error:
21:36:01 <lambdabot>      • No instance for (Num (Integer -> Integer))
21:36:01 <lambdabot>          arising from a use of ‘e_12014’
21:36:27 <dsal> :t scanr
21:36:28 <lambdabot> (a -> b -> b) -> b -> [a] -> [b]
21:36:56 <dsal> Not sure what you're trying to do there.
21:37:19 <crestfallen> need to review folds :)
21:39:05 <crestfallen> > scanr (*) 10 [4,5]
21:39:07 <lambdabot>  [200,50,10]
21:42:28 <crestfallen> when you say update dsal , is that the same as mutate?
21:43:21 <dsal> I don't think I said update, but they sound the same.
21:43:40 <crestfallen> no I mean when "one says.."
21:44:09 <crestfallen> but is that actually a seed value above: 10 ?
21:44:15 <dsal> I don't know why you need to think about it with so many different words.  If something sees `x` it will always see the same value.
21:45:09 <dsal> No matter what words you use, you can't change how the observer perceives `x`.
21:45:18 <monochrom> People mistake understanding for wording.
21:45:32 <monochrom> This is why I am a formalist.
21:46:05 <monochrom> If we are to play games, at least the axiom game is much less moving-goal-post than word games.
21:46:28 <crestfallen> I'm trying to figure out what one calls the results of scanr above: [200,50,10]
21:46:42 <dsal> I call it `:: [Int]`
21:46:47 <monochrom> result
21:47:04 <dsal> > scanr f 10 [4,5] :: [Expr]
21:47:06 <lambdabot>  [f 4 (f 5 10),f 5 10,10]
21:48:17 <crestfallen> I'm not trying to be clever or play word games
21:48:35 <dsal> It's hard to understand what's confusing.
21:49:36 <MarcelineVQ> that is one kromulan tatuological aphorism
21:49:50 <monochrom> :)
21:49:50 <dsal> hahaha
21:49:59 <dsal> It's hard to be clear in English.
21:50:33 <MarcelineVQ> to be real though, the result of a function is the function's result
21:50:37 <monochrom> [f 4 (f 5 10),f 5 10,10] is a much better explanation than any wording.
21:50:52 <crestfallen> well dsal one thing is , your last lambdabot query lists f 5 10 twice, why?
21:51:12 <monochrom> Perhaps you need a longer example.
21:51:23 <monochrom> > scanr f 10 [4,5,6,7] :: [Expr]
21:51:25 <lambdabot>  [f 4 (f 5 (f 6 (f 7 10))),f 5 (f 6 (f 7 10)),f 6 (f 7 10),f 7 10,10]
21:51:52 <monochrom> Try to guess what it does.
21:52:02 <monochrom> If necessary, use an even longer list.
21:53:07 <monochrom> Both examples and wordy explanations require imaginative extrapolations. However, examples are clearly a much better starting point.
21:55:16 <monochrom> Looking at the code of scanr may help or may not. It depends on whether you figure out what that code says. It is possible that you need the examples to help figure out what the code says.
21:55:27 <dsal> @src scanr
21:55:28 <lambdabot> scanr _ q0 []     = [q0]
21:55:28 <lambdabot> scanr f q0 (x:xs) = f x q : qs
21:55:28 <lambdabot>     where qs@(q:_) = scanr f q0 xs
21:55:35 <dsal> Use a pastebin, lambdabot!
21:58:17 <crestfallen> thanks no I essentially understand what the type [Expr] does. I was just wondering, why inhabitant 'f 7 10' is without parens there.
21:58:43 <crestfallen> in monochrom 's example
21:58:45 <monochrom> Then you relearn syntax and operator precedence.
22:00:05 <dsal> why would it need parens?
22:03:30 <crestfallen> operator precedence ? I know what it means but don't follow in the case of scanr sorry
22:06:25 <monochrom> If you can't apply a piece of knowledge, you still don't know what it means.
22:13:02 <dmwit> There are neither operators nor scanr's in the chunk of code where you are trying to understand how operator precedence applies to scanr.
22:13:12 <dmwit> So... that seems like a pretty doomed endeavor.
22:13:41 <crestfallen> why was precedence brought up?
22:14:07 <dsal> Because you asked?
22:14:28 <dmwit> No, crestfallen is right, somebody else brought up precedence first.
22:14:50 <dsal> He asked why parens aren't needed.  They're only needed if there's precedence confusion.
22:14:53 <dmwit> I suspect operator precedence was brought up due to very slightly sloppy wording.
22:14:58 <dmwit> It is a problem with word games. =P
22:15:09 * dsal suggests banning English from #haskell
22:15:18 <crestfallen> :)
22:15:33 <dmwit> But "syntax and operator precedence" are two topics that are very often treated together.
22:15:47 <dmwit> So relearning the syntax, you will almost certainly relearn about operator precedence, too.
22:16:01 <dmwit> And, though there's no operators, "precedence" *is* related.
22:16:27 <dmwit> (If you lump associativity in under the term "precedence" anyway.)
22:16:33 <dmwit> Aaaanyway, that's all a bit meta.
22:17:10 <dmwit> The direct answer to "why aren't parentheses needed in `f 7 10`?" is "because the intended interpretation is `(f 7) 10` and function application is left-associative, so `f 7 10` already means that".
22:17:45 <crestfallen> I don't follow. scanr is showing the steps of foldr. right, its meta because its essentially just laying out f 7 10 before it gets fed to f 6 right?
22:18:21 <crestfallen> right I see, thanks dmwit 
22:18:46 <dmwit> What I meant by "that's all a bit meta" was that the discussion about how we got to a slightly wrong bit of wording was a discussion about the discussion, rather than a discussion about the code.
22:19:05 <dmwit> I do not understand what "laying out f 7 10 before it gets fed to f 6" means, so I cannot evaluate the truth of that claim.
22:22:48 <crestfallen> no dmwit (f 7) 10 was a great explanation. I didn't see it I thought it was how scanr worked with :: [Expr]     ( which I had never seen used )
22:28:48 <dmwit> Perhaps some food for thought: how would you test the hypothesis that it was related to scanr?
22:29:19 <crestfallen>   glad I didn't scare you off... well ...
22:29:58 <crestfallen> you mean how precedence relates to scanr?
22:30:09 <crestfallen> or f 7 10
22:30:15 * hackage vector-circular 0.1 - circular vectors  https://hackage.haskell.org/package/vector-circular-0.1 (chessai)
22:31:14 <crestfallen> scanr lists 10, the seed, first at the end of the list [Expr] .. then ..
22:35:37 <ja> you could test that it is related to scanr by swapping in another function with the same signature and see if it is still a problem?
22:37:22 <dmwit> You say, "I thought it was how scanr worked with :: [Expr]". That is your hypothesis. Are there any tests you could do where, if it was not related to scanr, the test would have a chance of revealing that?
22:38:11 <dmwit> I am not requesting that you answer here. Only that you think.
22:39:14 <dsal> Usually when I'm struggling with something, I eventually get so frustrated I have to give up and think.  Then I solve the problem.
22:39:53 <dmwit> But: if you cannot come up with tests that have a chance of proving your hypothesis incorrect, then your hypothesis may not actually be hel.ing you predict or understand the world.
22:40:59 <ja> it could still make you famous and rich, like string theory!
22:42:15 <dsal> or hickey
22:42:57 <ja> haha interesting. what is the claim that hickey makes that is not dis-provable?
22:44:37 <crestfallen> I would have figured that it would have returned [...(f 7) 10, 10]   and not [...,f 7 10, 10] I guess the precedence has to do with adding the :: [Expr] you are showing the left associativity, but before that scanr shows us f 7 10 without indication of left associativity
22:45:23 <dsal> > (div 10) 5
22:45:26 <lambdabot>  2
22:45:45 <dsal> You *can* do that, but I don't see that in the wild.  It's kind of weird.
22:46:44 <crestfallen> that's interesting yeah. so in my garbled language I was getting to that right?
22:48:07 <dmwit> What is th "it" in "it would have returned..."? If the "it" is an expression with scanr in it, then no observation about its output can disprove the claim "that output is related to scanr".
22:49:22 <crestfallen> it's not that "I would have figured .. [...(f 7) 10, 10]  ,  but that if it had returned that I would have understood it right away. the way it does work however seems redundant a bit. 
22:49:46 <dsal> > div 10 5  -- You don't understand this?
22:49:48 <lambdabot>  2
22:49:57 <dmwit> The "I guess the precedence has to do with ..." is yet another hypothesis, and not a discriminating test. It is the opposite of a test: it is an object that introduces an obligation to create more tests.
22:52:21 <crestfallen> dsal no I understand that of course :(
22:53:05 <dsal> Then I'm confused as to why you'd put parens around a partially applied function with immediate parameters.
22:58:14 <crestfallen> but we had f 7 10 so it wasn't immediately apparent that f is a binary operator
22:59:08 <dmwit> What is the other thing besides binary operator that you think it might be?
22:59:51 <crestfallen> well yeah it must be binary
23:00:19 <dmwit> (...can you think of a test you could perform to distinguish the worlds where it is a binary operator from the worlds where it is that other thing?)
23:01:08 <dmwit> whoops, messages crossed. you can ignore my last comment
23:02:33 <crestfallen> > scanr (*) 10 [5,6] :: [Expr]  -- this won't work
23:02:35 <lambdabot>  [5 * (6 * 10),6 * 10,10]
23:04:05 <dmwit> whoops, it worked =P
23:04:25 <crestfallen> is that even what you expected?
23:04:51 <dmwit> Well, it's what *I* expected.
23:12:39 <crestfallen> so its just how its written. so how do you say it?  without parens it has a lower precedence I guess
23:13:23 <dmwit> use the word "it" less
23:17:43 <crestfallen> dmwit but am I correct that ' 6 * 10 ' is a lower precedence? if not, then I don't understand why scanr would return that value. 
23:19:22 <dmwit> lower than what?
23:19:33 <c_wraith> It's not a precedence thing.  Precedence is how ambiguous things are parsed.  There is no ambiguity
23:20:48 <dsal> This is showing you approximately how it's evaluated.
23:20:55 <c_wraith> It's an accurate representation of the expression in an unevaluated form.
23:21:42 <crestfallen> so it's sort of like a trace. you're getting a value that shows a step, but is a tad redundant.
23:22:41 <crestfallen> I see yes its approximation or unevaluated. thanks that is solid
23:22:51 <dsal> > scanr (/) 10 [5,6] :: [Expr]   -- maybe something that doesn't associate would be less distracting.
23:22:53 <lambdabot>  [5 / (6 / 10),6 / 10,10]
23:22:53 <c_wraith> the one thing that isn't clear there is that the two different 6 * 10 expressions would actually be one shared expression
23:25:01 <crestfallen> yeah its more illustrative than straight algebra say. because without parens either operand could be evaluated first wrongly. or is that a stretch?
23:26:20 <dsal> I still like f as an example.
23:26:42 <dsal> > foldr f 0 [1,2,3] :: Expr
23:26:44 <lambdabot>  f 1 (f 2 (f 3 0))
23:31:10 <crestfallen> so c_wraith , the bottom line is precedence is not in play when scanr was being used..
23:31:26 <c_wraith> Nope.  precedence is a parsing thing.
23:32:06 <crestfallen> so, at least you could understand my (slight?) confusion?
23:32:44 <c_wraith> well, scanr is certainly a weird operation.  :)
23:33:28 <dsal> I don't understand why you'd want to add parens to `f a b`
23:34:12 <crestfallen> well, its cool because with recursion you can do a trace, with a fold, I get to see what's happening.
23:36:31 <dsal> It's going to blow your mind when you find out that folds are recursion.
23:38:18 <crestfallen> well I sort of got that, but I was originally asking - thinking - that the seed has a feel of something that is updated dsal. does that make sense?
23:39:12 <crestfallen> I was asking about mutation, and the seed value came to mind, incorrectly I assume.
23:40:09 <dsal> If it helps you to think about it that way, I guess.  I'm not sure I'd think of it that way.
23:40:55 <crestfallen> so then scanr came to mind, because of the hickey video I just watched, and how he was talking about records and logs. so how haskell keeps logs so to speak since things are immutable
23:41:13 <dsal> > foldl const 11 [1..10] :: Expr
23:41:16 <lambdabot>  11
23:42:13 <crestfallen> > scanl const 11 [1..10]
23:42:14 <lambdabot>  [11,11,11,11,11,11,11,11,11,11,11]
23:42:42 <dsal> > scanl (flip const) 11 [1..10]
23:42:44 <lambdabot>  [11,1,2,3,4,5,6,7,8,9,10]
23:43:19 <dsal> > scanl (flip const) 11 [1..10] :: Expr
23:43:20 <lambdabot>  error:
23:43:20 <lambdabot>      • Couldn't match expected type ‘Expr’ with actual type ‘[Integer]’
23:43:20 <lambdabot>      • In the expression: scanl (flip const) 11 [1 .. 10] :: Expr
23:43:25 <dsal> > scanl (flip const) 11 [1..10] :: [Expr]
23:43:27 <lambdabot>  [11,1,2,3,4,5,6,7,8,9,10]
23:43:29 <dsal> heh
23:43:35 <dsal> I should be sleeping.
23:43:55 <crestfallen> thanks a lot dsal that's pretty cool
23:43:58 <dsal> > let sm [] = 0; sm (x:xs) = x + sm xs in   sm [1..5]  :: Expr
23:44:00 <lambdabot>  1 + (2 + (3 + (4 + (5 + 0))))
23:44:08 <crestfallen> thanks c_wraith et al
