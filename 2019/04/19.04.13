00:00:33 <cocreature> you can make a Traversal that targets the Int in both constructors
00:00:45 <jle`> remexre: my usual intuition is to try to represet Foo as some sum `Either Baz ????`
00:00:54 <jle`> if it's not possible, then you cannot
00:01:13 <cocreature> actually reading your question again, I am not entirely sure I understood it correctly.
00:01:49 <cocreature> ah now I got it but yeah that’s not lawful
00:01:52 <remexre> jle`: hm, darn
00:01:53 <suppi> That's a nice rule of thumb jle
00:02:44 <cocreature> "preview l (Bar x a) = Just x" but "review l x = Bar x b" for a fixed b so if a /= b things break
00:03:09 <jle`> it works the same for lenses, Lens' s a implies that s can be represented as (a, ????)
00:05:25 <suppi> The first chapter of thinking with types talks about this, canonical representations
00:08:25 <remexre> hm
00:08:29 <remexre> so what I'm trying to do is
00:08:45 <remexre> I've got a typeclass for a bunch of types
00:09:43 <remexre> I want to be able to have an a -> Maybe (Foo, Maybe Bar), Foo -> a for both data A = B Foo Bar | ... and data C = D Foo | ...
00:10:33 <remexre> Is there an optic that'd work for this?
00:10:49 <remexre> or should I just have two typeclass methods
00:24:05 <lavalike> > let doubleEveryOtherFromEnd xs = snd $ foldr (\x k -> \b -> let (b',xs) = k b in (not b',(if b' then 2*x else x) : xs)) (const (True,[])) xs undefined in (doubleEveryOtherFromEnd [1..5], doubleEveryOtherFromEnd [1..6]) -- golergka, woke up with this idea
00:24:07 <lambdabot>  ([2,2,6,4,10],[1,4,3,8,5,12])
00:35:02 <Squarism> Anyone know how to do "select count(booleanExpression) from tbl" in Esqueleto?
00:42:32 <lavalike> Squarism: it's not clear to me, countRows takes a Value which you can get from Entity via ^. and ?., those together with fmap and comparisons on the inner 'a' of 'Value a' could be the way?
00:44:01 <Squarism> lavalike, thanks. Didnt know you could use fmap in the middle of queries
00:51:26 <lavalike> Squarism: yeah with an embedding of a different language inside of Haskell itself you get the benefit of using *any* Haskell inside it
00:52:26 <Squarism> hmm i just dont get how it would be translated to sql. Cause it needs to be executed in sql right?
00:55:16 <Squarism> my problem is illustrated here : https://pastecode.xyz/view/c446c408
00:55:25 <Squarism> wins and losses just count rows
00:55:33 <Squarism> criteria is neglected
00:57:04 <lavalike> Squarism: oh you're quite right, you have to use their operators for the library to be able to convert it to sql instead of doing it in Haskell-land, fortunately I think those are provided
00:57:51 <lavalike> if you look just below countRows in the hackage docs, there's not_ ==. >=. >. and so forth, all operating on `Value's
00:58:45 <lavalike> ah you already use those
00:59:10 <Squarism> yeah, that gives me booleans. But it seems impossible to convert booleans to an int of sorts. =D
01:01:05 <lavalike> Squarism: hm yeah countRows does not take an argument
01:01:51 <Squarism> there is count though. 
01:02:24 <lavalike> Squarism: I can't find 1 example of a non-* SELECT list in either the docs or the examples O_o
01:02:49 <Squarism> Thanks for trying! Appreciated
01:06:21 <lavalike> Squarism: ah the SELECT list is the last return statement!
01:07:02 <Squarism> yeah
01:07:10 <lavalike> is `select $ ... $ return (count <booleanexpression>)` not doing the trick for you?
01:15:21 <Squarism> lavalike, no it seems not. I just get the rowcount
01:16:00 <lavalike> does the equivalent query give you the expected result directly issued to the sql server?
02:21:16 <fr33domlover> Hmmm more thoughts about MVar and async: The main safety benefits async provides are avoiding accidentally forgetting to stop a thread, and not noticing a thread dying silently due to an exception. But in my case, it's somewhat different: I don't need to catch the exception other than for logging, and if 1 thread fails I need the others to still run. I can still use async, but, ironically, I need to be
02:21:19 <fr33domlover> careful to use it in a way that doesn't propagate exceptions and ends up killing threads and operations that shouldn't be interrupted in my app logic :P
02:55:57 <_ikke_> I'm trying to build ShellCheck for Alpine Linux, and from reading the cabal documentation, it seems like runhaskell Setup.hs configure/build/install seem to be the most suitable for that. The problem is that we do not have any dependency packaged yet, and it seems like a lot of work to get that off. cabal install works, but from what I see it's mostly meant to directly install on the current system. Any way
02:55:58 <_ikke_> I can build it in a way that is easy to package?
03:06:47 * hackage LambdaHack 0.9.3.1 - A game engine library for tactical squad ASCII roguelike dungeon crawlers  https://hackage.haskell.org/package/LambdaHack-0.9.3.1 (MikolajKonarski)
03:18:17 * hackage caster 0.0.1.0 - Multicast, thread-safe, and fast logger.  https://hackage.haskell.org/package/caster-0.0.1.0 (A_kirisaki)
03:21:47 * hackage Allure 0.9.3.1 - Near-future Sci-Fi roguelike and tactical squad combat game  https://hackage.haskell.org/package/Allure-0.9.3.1 (MikolajKonarski)
04:10:06 <epane> Which Haskell plugin (syntax highlighting, code completion, debugging) do you recommend for Visual Studio Code on Kubuntu 18.10?
04:23:09 <zincy> Could you build a Haskell front end framework which targets Webassembly?
04:23:56 <fendor> epane, haskell-ide-engine :)
04:24:16 <fendor> assuming you use stack and project is small to medium sized
04:24:36 <__monty__> zincy: That's basically the same question as "Can you write a compiler in haskell?" The answer is yes.
04:25:04 <zincy> Oh cool
04:25:28 <__monty__> There's no general purpose programming language the answer'd be no for afaik.
04:26:36 <epane> fendor: thx, will try later
04:26:40 <gonz_> Haskell is particularly good for compilers, though, seems to be the general consensus.
04:27:04 <zincy> Yeah I am writing a brainfuck to webassembly one at the moment.
04:27:15 <zincy> The parsing is amazingly easy
04:27:27 <zincy> Not quite sure how do do the next step :P
04:30:31 <zincy> Using javascript as a compile target must be inherently slower than webassembly. Since the browser would be calling precompiled binary.
04:32:52 <lavalike> can ghc spit out llvm ir these days? that representation has some wasm backends iirc
04:35:13 <zincy> Also ... https://github.com/tweag/asterius
05:38:22 <Franciman> Hi
05:38:30 <Franciman> does division by 0 throw any exception?
05:38:39 <Franciman> division between Doubles
05:38:46 <Franciman> I see Infinity in the REPL
05:40:22 <__monty__> Franciman: I think it depends on the settings for flops in your CPU. But usually it doesn't, because floats can represent +/-inf.
05:40:57 <Franciman> I see, thanks
06:09:22 <Axman6> > 1/0
06:09:25 <lambdabot>  Infinity
06:09:36 <Axman6> > 1 `div` 0 :: Integer
06:09:38 <lambdabot>  *Exception: divide by zero
06:17:50 <__monty__> > 1 / 0 :: Rational
06:17:52 <lambdabot>  *Exception: Ratio has zero denominator
06:57:17 <ski> > 1 `mod` 0
06:57:19 <lambdabot>  *Exception: divide by zero
06:57:20 <ski> :/
06:58:24 <ski> (imho, calling `mod' with zero denominator ought to return the numerator, because two integers being congruent modulo zero is the same as them being equal)
07:00:21 <Ariakenom> it is?
07:00:47 <ski> (hm, i heard the other day that when indexing an array in LLVM, you can make it so that the index wraps around, modulo the array length .. and then, if the array is declared with length zero (e.g. at the end of a structure), that could then correspond to not wrapping around at all :)
07:00:57 <Ariakenom> it does break n `mod` m < m
07:01:11 <ski> yes, it's an exception to that
07:01:47 * hackage telega 0.1.8 - Telegram Bot API binding  https://hackage.haskell.org/package/telega-0.1.8 (iokasimovmt)
07:03:43 <ski> ⌜a ≡ b (mod m)⌝ means ⌜m ∣ a - b⌝, iow ⌜∃ k : ℤ. k⋅m = a - b⌝ or ⌜∃ k : ℤ. a + k⋅m = b⌝
07:04:14 <Ariakenom> my font did not like that
07:04:17 <ski> in case ⌜m = 0⌝, that reduces to just ⌜∃ _ : ℤ. a = b⌝, or ⌜a = b⌝
07:04:36 <Ariakenom> but I get the general idea, yeah
07:05:28 <erisco> boo
07:06:34 <ski> ⌜ℤ ∕ (m⋅ℤ)⌝ has ⌜m⌝ inhabitants (being cosets, equivalence classes), in case ⌜m > 0⌝
07:07:45 <ski> if ⌜m = 0⌝, it has one inhabitant for each inhabitant of ⌜ℤ⌝, because ⌜ℤ ∕ (0⋅ℤ)⌝ is isomorphic (as a ring) to just ⌜ℤ⌝
07:11:29 <Ariakenom> now do n `div` 0 = 0
07:12:22 <ski> heh, that should still be undefined :) (because the ⌜k⌝ that exists isn't unique in this case)
07:13:24 <ski> so, if you have an infinite (in both directions, so indexed by ⌜ℤ⌝) sequence, that you know has (fundamental/primitive/basic/prime) period ⌜p⌝ (an integer), then you only need to keep track of ⌜p⌝ components, in case ⌜p > 0⌝ (in case ⌜p = 0⌝, you need to keep track of one component per integer index in ⌜ℤ⌝)
07:20:09 <Ariakenom> relatedly divmod vs quotrem vs mystery contender https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/divmodnote-letter.pdf
07:20:55 <ski> (and e.g. the order of an element ⌜g⌝ of a group (written multiplicatively) is the (fundamental/&c.) period of the sequence of powers ⌜⟨g ͥ⟩ᵢ⌝, being ⌜⟨⋯g⁻³,g⁻²,g⁻¹,1,g,g²,g³,⋯⟩⌝ of ⌜g⌝. in case this sequence never repeats, we say the order is ⌜0⌝, which is consistent with this view)
07:28:53 <ski> (if we instead consider only the "nonnegative" side of this sequence, so ⌜⟨1,g,g²,g³,⋯⟩⌝, then the order of ⌜g⌝ is the least index ⌜i⌝ such that ⌜g ͥ = 1⌝
07:29:30 <ski>  usually people would say "least positive index", but if you consider the (multiplicative) divisibility ordering ⌜(∣)⌝ rather than the ordinary (additive) "comparative" ordering ⌜(≤)⌝, then ⌜0⌝ is the greatest element, and so if ⌜g ͥ = 1⌝ for no nonzero ⌜i⌝, we still have ⌜g⁰ = 1⌝, so that ⌜0⌝ is then the (multiplicatively) least such index)
07:34:11 <ski> (hm, you could also consider such a sequence of powers in any monoid, not just in a group .. but i think possibly one'd want to adapt the definition of order of an element, there, because rather than the powers of an element always generating a cycle (either a finite one, or the infinite one, being a single "strand" that doesn't loop back), ..
07:34:21 <ski>  it could also generate a "cycle with a hair" / "a spool with part of the thread going off it to some distance")
07:36:39 <lavalike> what's ⌜this⌝ ?
07:37:04 <ski> Ariakenom : hm, that reminds me of "Proposal for Division Operators in Scheme" (draft) by Riastradh (Taylor R. Campbell) in 2009,2010,2011 at <https://mumble.net/~campbell/tmp/division.txt>
07:37:12 <ski> lavalike : quote marks ?
07:37:44 <lavalike> as in '"' ?
07:37:54 <ski> e.g., yes
07:38:47 <lavalike> curious
07:39:16 <ski> or as in `(zipWith . zipWith) (+) [[0],[1,2],[3,4,5]] [[],[600],[700,800]]'
07:40:19 <lavalike> here they work as arms, it also got a nose and mouth and sides, just missing the eyes: ⌜(≤)⌝
07:40:23 <ski> hm, i suppose one could argue that we'd still, in one sense have
07:40:24 <ski>   n = d * (n `div` d) + n `mod` d
07:40:28 <ski> if we'd define
07:40:32 <ski>   n `mod` 0 = n
07:46:13 <ski> namely if we define ⌜m ⋅ n⌝ as ⌜∑ᵐ⁻¹ᵢ₌₀ n⌝ (the sum ⌜n + n + ⋯ + n⌝ with ⌜m⌝ summands). because then one could argue that ⌜0 ⋅ n⌝ is well-defined if ⌜n⌝ is well-defined, for each non-negative index less than ⌜0⌝ (there are none)
07:47:12 <ski> > 0 * (7 `div` 0)  -- but multiplication on `Integer' doesn't exhibit a corresponding matching laziness
07:47:14 <lambdabot>  *Exception: divide by zero
07:50:08 <c_wraith> wasn't there an experiment with special cases in Integer arithmetic several versions back in GHC?
07:50:26 <ski> hm, i dunno
07:51:03 <ski> they did add
07:51:04 <ski> > gcd 0 0
07:51:06 <ski> > lcm 0 0
07:51:07 <lambdabot>  0
07:51:09 <lambdabot>  0
07:51:11 <ski> however
07:51:52 <c_wraith> I can justify those, even if they aren't the result of the blind algorithm
07:52:09 <ski> (that `gcd' result clearly uses the divisibility order in the interpretation of "*greatest* common divisor")
07:53:00 <dmwit> The gcd one, at least, is the result of the blind algorithm. A pleasant match between two pieces of mathematics, the algorithm and the sensible generalization.
07:53:03 <c_wraith> Well, "divides" in number theory is usually defined such that 0|0 is defined and true
07:53:13 <ski> right
07:53:41 <ski> in fact ⌜d ∣ 0⌝, for every ⌜d⌝
07:54:13 <ski> (and ⌜0 ∣ d⌝ only if ⌜d = 0⌝)
07:57:10 <ski> lavalike : i think that may be called Quine (or quasi-quotation) quote marks
07:59:24 <dmwit> I wonder if, now that gcd already doesn't match the Report, the core library committee would be open to making it return a positive number more often.
07:59:57 <dmwit> > gcd (-5) minBound :: Int -- like here
08:00:00 <lambdabot>  -1
08:00:41 <ski> interesting
08:00:44 <ski> > gcd (-5) (minBound + 1) :: Int
08:00:45 <dmwit> (It's easy to fix; I just discovered this oddity a few days ago, and did a bit of source diving. The fix would simplify both gcd and lcm.)
08:00:47 <lambdabot>  1
08:00:58 <ski> simplify, how ?
08:01:15 <dmwit> currently, `gcd a b = gcd' (abs a) (abs b)`, so two calls to `abs`.
08:01:35 <dmwit> The fix is `gcd a b = gcd' a b` and then call `abs` once in the base case.
08:01:51 <dmwit> Then in `lcm` we can delete a call to `abs` as well that seems to be there to protect against this oddity.
08:01:52 <ski> > abs minBound < (0 :: Int)
08:01:54 <lambdabot>  True
08:02:12 <dmwit> Yes. gcd called with minBound and either 0 or minBound can't avoid returning a negative number.
08:02:15 <dmwit> But we can fix all the other cases.
08:02:44 <ski> > negate minBound == (minBound :: Int)
08:02:46 <lambdabot>  True
08:03:04 <ski> so i suppose we still have `abs n = max (negate n) n', there
08:03:27 <dmwit> yup
08:03:42 <dmwit> (But why did that property spring to mind for you?)
08:04:37 <ski> well, i tend to use that as the definition of `abs' ?
08:05:16 <ski> (because then i can reason about `abs' using the adjunction property for `max', rather than reasoning by cases)
08:07:21 <ski> (iow, ⌜max(a,b) ≤ y⌝ is equivalent to ⌜a ≤ y ∧ b ≤ y⌝)
08:21:12 <remexre> is there a way with cabal new-* to recheck all modules for warnings?
08:22:21 <ski> (Edsger Wybe Dijkstra mentioned reasoning by such adjunctions, in "Indirect equality enriched (and a proof by Netty)" in 2001-12-14 at <http://www.cs.utexas.edu/users/EWD/ewd13xx/EWD1315.PDF>,<http://www.cs.utexas.edu/users/EWD/transcriptions/EWD13xx/EWD1315.html>)
08:24:17 * hackage effects 0.2.4 - Computational Effects  https://hackage.haskell.org/package/effects-0.2.4 (SjoerdVisscher)
08:24:29 <dmwit> remexre: Not a really good one. You can delete the appropriate build directory under dist-newstyle.
08:24:57 <remexre> darn, ok, thanks
08:29:29 <dmwit> Or you could sed '$s/\(.*\)/\1\n/' **/*.hs ;-)
08:29:58 <dmwit> But deleting the right directory is much cleaner, and will work even if there's .lhs files or alex/happy files or other preprocessed things.
08:38:11 <alunduil> are there any good ways to workaround a non-injective type family defined in a library that I want to preserve the polymorphism of?  I can grab pastes if examples would be useful to see what I'm referring to.
08:46:02 <lavalike> ski: aight (:
09:12:58 <dmwit> alunduil: Work around it how? Some small pastes would probably be helpful for grounding the discussion.
09:13:31 <alunduil> Sure, I'm writing this code (https://bpaste.net/show/0cfd71d56532) and getting these errors (https://bpaste.net/show/66380bfe8842).
09:13:49 <alunduil> In short, I'm trying to hook graphql-api and servant-server together in a more general way.
09:14:02 <alunduil> dmwit: ^
09:19:24 <dmwit> alunduil: :t interpretQuery ?
09:19:50 <alunduil> dmwit: http://hackage.haskell.org/package/graphql-api-0.3.0/docs/GraphQL.html#v:interpretQuery
09:20:19 <alunduil> I think the problem lies in the GraphQL.Handler definition but I'm still learning type level programming.
09:20:53 <dmwit> Okay. You'll have to use ScopedTypeVariables and TypeApplications.
09:21:07 <dmwit> ...which you already have on, great.
09:21:16 <dmwit> So just `interpretQuery @a` everywhere should do the trick.
09:21:24 <dmwit> Except for the error that isn't about injectivity, of course.
09:22:21 <dmwit> Since you're already in TA-land, you can and probably should skip all the Proxy stuff.
09:22:58 <dmwit> (Then use TA instead of Proxy in the modules that use getHandler, postJSONHandler, etc.)
09:23:02 <alunduil> ah, that's where that application needed to go.
09:23:19 <alunduil> That makes way more sense.
09:23:25 <dmwit> Actually, I guess it's probably `interpretQuery @Handler @a` or `interpretQuery @_ @a`.
09:23:47 <dmwit> Ah, no, I had it right.
09:23:50 <alunduil> I'm guessing the latter based on my recent readings.
09:24:00 <dmwit> Haddock needs to improve to show this part of the API. =)
09:24:26 <alunduil> Yeah it does.  I've been dropping into the graphql-api source when needed as well. ;)
09:24:27 <dmwit> (To be 100% clear: `interpretQuery @a` is right, `interpretQuery @Handler @a` is not.)
09:24:35 <alunduil> Oh?
09:25:02 <alunduil> That seems counterintuitive.  I'd expect the former to fill the m variable with a rather than the api variable.
09:25:17 <dmwit> Yes. The type declaration in the source begins `interpretQuery :: forall api m. ...`; since you want `a` to be your `api` and `Handler` to be your `m`, they must come in that order.
09:25:27 <alunduil> Oh, yep.
09:25:39 <alunduil> Haddock should be fixed to make that more obvious.
09:25:44 <dmwit> Right.
09:25:51 <alunduil> dmwit: thanks!
09:25:59 <erisco> boo
09:26:07 * dmwit is scared
09:32:47 * hackage shake-c 0.4.3.0 - Library for building C code with shake  https://hackage.haskell.org/package/shake-c-0.4.3.0 (vmchale)
09:58:53 <nisstyre> omg there are like 5 different "spotify api in haskell" libraries but they all look either unfinished or abandoned or won't build in stack
09:59:06 <nisstyre> I wonder if that's because of spotify changing stuff
09:59:38 <merijn> nisstyre: Probably, also, their API is rather limited and doesn't support a lot of the things you'd like
09:59:54 <nisstyre> merijn: all I really want are playlists
10:00:02 <nisstyre> as in the track names and IDs
10:00:26 <nisstyre> maybe I could just do it myself without a library easily
10:03:33 <dmwit> nisstyre: Hey alright, that's a great idea! Then there will be 6 different "spotify api in haskell" libraries, all unfinished or abandoned. =D
10:04:07 <merijn> dmwit: Only if he publishes it to Hackage, not if he just writes/uses his own stuff in an executable
10:05:33 <remexre> is there a way with PatternSynonyms to declare a set of patterns that are exhaustive?
10:06:14 <nisstyre> merijn: I'll publish it just to make someone else come in here complaining about all of the useless libraries
10:06:52 <Cale> https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/glasgow_exts.html#complete-pragmas
10:08:49 <remexre> Cale: thanks!
10:11:19 <ski> hm, i don't see a corresponding pragma for stating nonoverlap
10:25:48 <dmwit> ski: I don't think there is one. What are you hoping the compiler would do with that information?
10:39:21 <ski> dmwit : not warn with `-Woverlapping-patterns'
10:44:37 <dmwit> % pattern Dup = ()
10:44:37 <yahb> dmwit: ; <interactive>:43:9: error: Not in scope: data constructor `Dup'
10:44:45 <dmwit> % :set -XPatternSynonyms
10:44:45 <yahb> dmwit: 
10:44:48 <dmwit> % pattern Dup = ()
10:44:48 <yahb> dmwit: 
10:44:54 <dmwit> % :set -Woverlapping-patterns
10:44:54 <yahb> dmwit: 
10:45:02 <dmwit> % f () = 3; f Dup = 4
10:45:02 <yahb> dmwit: ; <interactive>:47:11: warning: [-Woverlapping-patterns]; Pattern match is redundant; In an equation for `f': f Dup = ...
10:45:09 <dmwit> Huh, neat! I didn't know it would do that.
10:45:37 <Cale> http://hackage.haskell.org/package/dependent-sum-0.5/docs/src/Data.Some.html#Some
10:45:44 <dmwit> ski: What's an example where the compiler is wrong about that?
10:45:48 <Cale> Some cute use of pattern synonyms there :D
10:47:08 <Cale> (Really we're just sidestepping the apparently needless restriction that newtypes can't be existentials.)
10:55:44 <fen> how do you have a list with a certain number of value of a certain value
10:55:45 <fen> ?
10:56:07 <fen> eg, a list of length n+m with n values True, and m values False
10:56:31 <fen> we have to be able to tell how many members there are of the type? eg Bool has 2
10:59:30 <fen> would it be best to not use a list for this and instead kind of combine the datatypes with the list declaration?
11:00:10 <fen> since this is supposed to be a type level list anyway, is there any advantage or complication because of that
11:00:13 <fen> ?
11:00:46 <Cale> To be honest, that's the sort of constraint that I wouldn't bother trying to encode at the type level in Haskell (but then, I also wouldn't bother trying to encode list length)
11:01:08 <fen> its important for the current project
11:01:12 <Cale> But you can easily make a GADT with two constructors
11:01:19 <fen> otherwise there is no way to write the types over which an instance is
11:01:24 <Cale> or, three rather
11:01:34 <Cale> one for the empty list
11:01:45 <Cale> one for the case of adding a True, and another for adding a False
11:01:51 <fen> the 2 types appearing in the instance need to be calculatable from one antoher, so there is some serious type level things to be happening
11:02:08 <fen> basically want a type (family? or datatype?) that takes 2 nats as parameters, and wraps a list with this number of true/false at type level 
11:02:42 <Cale> This kind of thing ends up being intensely impractical to work with in Haskell just because you almost always end up wanting to do fancier calculations at the type level, and they make everything awkward and frustrating.
11:03:06 <fen> Cale: the current application could use a combination of Bool and List as you describe, but is there a way to do it with just list and bool, and some kind of "retrive the number 2 from they type Bool" family
11:03:50 <Cale> You could use singleton Bool
11:04:36 <fen> Cale: its ok, its not going to be awkward and frustraiting, and if thats your main concern maybe the version just dealing with Bool is better than trying to make it work for... what would it be... something taking a list of nats which is as long as there are different values of the type it containes...
11:04:46 <dmwit> I'd use an Int(eger) plus a smart constructor that guarantees it's between 0 and (m+n) choose n.
11:04:59 <dmwit> If I really wanted to guarantee this property.
11:05:07 <fen> or could just do that way for maximum generality, as it could be useful to develop or utilise the correct machinery 
11:05:35 <dmwit> (You can upgrade to a type-checked Fin analog if you're really scared you didn't implement your smart constructor correctly.)
11:05:52 <fen> dmwit: the order the bools appear in the list is needed for the type level calculation. how would a smart constructor work at type level?
11:06:19 <dmwit> The order the bools appear in the list is needed at the computation level, but not the type level, if your type is only intended to capture "there are this many trues and this many falses in the list".
11:06:58 <dmwit> (And the order is indeed available at the computation level.)
11:07:07 <ski> % let f Dup = 3; f Dup = 4
11:07:07 <yahb> ski: 
11:07:21 <fen> ok, so if Fin is a bounded Nat, what is this thing? a nat is [()] so Fin n is BoundedList n (), and what we have isnt just BoundedList n Bool, it also needs a certain number of Bools...
11:07:32 <dmwit> % let { f Dup = 3; f Dup = 4 }
11:07:32 <yahb> dmwit: 
11:07:34 <dmwit> huh
11:07:34 <fen> ie whatever it is takes 2 nat params, whereas Fin only takes 1
11:08:10 <dmwit> ski: But a non-overlapping pragma wouldn't fix that...?
11:08:20 <ski> % pattern Dupl = ()
11:08:20 <yahb> ski: 
11:08:24 <ski> % let f Dup = 3; f Dupl = 4
11:08:24 <yahb> ski: 
11:08:59 <dmwit> fen: I have encoded my [Bool] as a [()].
11:09:19 <dmwit> fen: The number of ()'s in the list tell where the Trues go. The type tells how many elements there are in the [Bool].
11:09:49 <dmwit> These two facts together are enough to reconstruct the original [Bool].
11:09:59 <ski> dmwit : right, so i misremembered the undesirable behaviour. still, if it did not automatically disregard that warning with pattern synonyms, one'd need such a pragma to be able to state when it'd be safe to disregard it
11:10:24 <dmwit> ski: When is it safe to disregard it?
11:10:32 <dmwit> It doesn't even appear safe *here*, where it is being disregarded!
11:10:40 <ski> when the system knows that the patterns don't overlap
11:10:50 <dmwit> I'm lost.
11:11:00 <dmwit> Are we still claiming that a non-overlapping pragma is a good idea?
11:11:05 <ski> (but let's say the datatype is exported abstractly)
11:12:07 <ski> it would be safe if it could prove, in the context of the module including that pragma, that the indicated pattern synonyms actually didn't overlap
11:12:24 <fen> dmwit: how can the "number of () in the list tell you where the Trues go" !?
11:12:33 <fen> that is wrong
11:12:41 <ski> then that they don't overlap would be part of the interface of the module (while the actual implementation of the pattern synonyms would not)
11:12:53 <dmwit> ski: [failing to warn? warning?] would be safe if [the compiler? the programmer?] could prove ...
11:13:29 <ski> already, with `-Woverlapping-patterns', it still doesn't warn with `f False = ...; f True = ...', because it knows those are not overlapping
11:13:52 <Cale> fen: imo, if you're going as far as using DataKinds and/or singletons, you're already well into the "awkward and frustrating" zone, and the only question is whether you can really derive enough benefit to make that awkwardness and frustration worthwhile.
11:14:18 <Cale> (Quite often the answer will be no, but I'm not willing to entirely rule out the possibility)
11:14:24 <dmwit> fen: I don't see why it should obviously be wrong. 0 means all the Trues go at the end; 1 means the list ends True,False,lots of Trues; 2 means the list ends True,True,False,lots of Trues; 3 means the list ends True,False,True,False,lots of Trues, etc. You can enumerate all the [Bool]s satisfying your constraints, and there's exactly n+m choose n of them, so a number from 0 to n+m choose n is just the 
11:14:30 <ski> i would like the ability to state that a set of pattern synonyms are (pairwise) disjoint / non-overlapping, even if they are pattern synonyms for an abstract data type whose data constructors are not exported
11:14:30 <dmwit> right size.
11:14:55 <ski> (but still, it should warn, when it can't prove they are disjoint)
11:15:10 <dmwit> ski: But why? So far as I can tell you haven't shown a situation where it would incorrectly warn in the first place.
11:15:26 <dmwit> ski: Aha! So you want to warn more often *and* have this pragma. Okay.
11:15:31 <ski> yes
11:15:54 * dmwit ponders
11:16:40 <Cale> Haskell just isn't Coq or Agda or Idris -- it theoretically can do a lot of the same tricks as a proper dependently typed language, but there are also some walls you can slam into pretty hard.
11:17:05 <ski> currently, when using pattern synonyms, it has false negatives. if it didn't disregard `-Woverlapping-patterns' completely with pattern synonyms, i think one would probably get false positives (at least in the abstract data type situation). so i want to be able to explicitly declare disjointness of pattern synonyms, to reduce that
11:17:12 <oo_miguel> is Data.Vector.fromListN always strict and what can I use if I need laziness AND quick random read access?
11:17:18 <dmwit> ski: The only implementation I can imagine that seems like it would fit your vision (without trying to solve an undecidable problem) seems like it would warn a lot more.
11:17:33 <dmwit> For basically every view pattern, e.g.
11:17:46 <ski> view patterns is another thing
11:17:51 <dmwit> Which I guess would indeed make your non-overlapping pragma quite critical.
11:17:59 <cocreature> oo_miguel: strict in what sense?
11:17:59 <ski> harder to do something sensible with
11:18:19 <ski> mere pattern synonyms sound more tractable
11:18:33 <dmwit> What exported patterns (that don't involve views) would need an unsound overlapping analysis?
11:18:55 <oo_miguel> cocreature: It seemt that having somehting like: fromListN 100000 [1..] will create the complete vector even If I only will ever access a few initial elements
11:19:12 <dmwit> Like, what would you hate about the algorithm "just unfold pattern synonyms before checking for the -Woverlapping-patterns warning"?
11:19:38 <ski> unsound meaning that it warns even in cases that actually doesn't overlap ?
11:19:40 <fen> dmwit: what? is there only 1 of these?
11:19:43 <ski> dmwit : separate compilation ?
11:19:52 <fen> an int bounded to be the same length of the list
11:20:07 <fen> to specify the location of one value different from the rest?
11:20:34 <oo_miguel> I want something like infinite list, but with a quick (!!)
11:20:51 <dmwit> ski: Yes, unsound meaning that. The analysis can be module-local, and the pairwise-overlapping status for the patterns of a given type can be put in the interface file.
11:21:00 <fen> oh, your bounding your Fin by (n+m) `chose` m
11:21:10 <ski> oo_miguel : infinite list of progressively longer arrays ?
11:21:16 <fen> yeah, that has the correct information content
11:21:48 <dmwit> fen: (Of course you'd want to use a better type-level representation of your number than unary, to make the div and mod operations not awfully slow. But that's neither here nor there.)
11:21:51 <jle`> oo_miguel: sounds like you want (Int ->)
11:21:57 <cocreature> oo_miguel: it will force the whole vector but not the elements in the vector
11:22:18 <fen> so there would be an isomorphism between `Fin ((n+m) `choose` m) and the BoundedListOfBools n m
11:22:38 <jle`> a vector cannot be infinite length, pretty much by definition
11:23:00 <dmwit> fen: Yes. And specifically I'd make the isomorphism be a newtype wrapper+unwrapper pair.
11:23:06 <oo_miguel> ski: jle`: cocreature: yeah probably my approach is wrong. The root of my question. is that I have an infinite list, where each of the elements is calculated from the preceeding one. And I need to access different values from this list randomly but do not know the maximum index in advance
11:23:07 <fen> no
11:23:09 <jle`> especially since the first byte encodes the length of the vector pretty much
11:23:15 <fen> dmwit: thats totally not the idea!
11:23:34 <dmwit> You don't have a monopoly on declaring what the idea is! It came out of my brain, after all.
11:23:37 <dmwit> =)
11:23:40 <fen> we want to actually use a [Bool] here
11:23:42 <jle`> oo_miguel: if that's the case, then generating elements is O(n)
11:23:51 <jle`> oo_miguel: so an O(n) !! is not going to be a problem for you
11:23:55 <dmwit> Then all hope is lost. You can't do that. [Bool] doesn't mean that.
11:24:11 <dmwit> ([Bool] doesn't even mention an `m` or `n` anywhere!)
11:24:13 <oo_miguel> jle`: but will each !! be n?
11:24:17 <cocreature> jle`: that’s true if you do only one access
11:24:21 <fen> just instead of using a list, something that can be told how many values the type it contains has, and how many of each type
11:24:29 <cocreature> once you do more than one it starts to matter
11:24:38 <fen> basically it just ensures there are the correct number of each Bool
11:24:43 <jle`> ah yeah, didn't think about the second access
11:25:01 <oo_miguel> jle`: I access the list around n times or more
11:25:03 <jle`> actually some of us did run into something similar for Advent of Code this year
11:25:29 <ski> dmwit : hm, perhaps you're right that it can infer it automatically. i'd still want to be able to state it explicitly, so as to get an error if it can't prove it, and to have it in the explicit interface of the module
11:25:32 <jle`> in that case i might use Seq
11:25:54 <dmwit> oo_miguel: You could use a list of full trees, each one deeper than the last.
11:26:15 <jle`> indexing is O(log n), and adding items is O(1)
11:26:16 <dmwit> oo_miguel: Easy to make lazy, plus O(log n) indexing, n being the index.
11:26:40 <dmwit> jle`: But Seq doesn't support infinite sequences, does it?
11:26:41 <fen> (Comlexity x ~ (n :: Nat)) => ?List (BoundedList n Nat) x 
11:26:55 <jle`> it doesn't support infinite sequences, but it looks like oo_miguel only wants unbounded sequences
11:27:13 <fen> eg for x = Bool, ?List (BoundedList 2 Nat) Bool
11:27:31 <jle`> as in, length n for arbitrary large n that is not known up-front
11:27:47 <dmwit> jle`: But spine-strict is already bad, probably.
11:27:57 <dmwit> jle`: Since it forces you to compute n before you get access to the first element.
11:28:03 <dmwit> jle`: Likely that's as bad as computing the whole sequence.
11:28:30 <jle`> it looks like in oo_miguel's case there is no way around computing the whole sequence, up to n
11:28:43 <dmwit> Why do you believe that?
11:28:48 <oo_miguel> jle`: dmwit: yeah this is correct
11:28:49 <dmwit> Nothing he's said so far indicates that to me?
11:28:54 <oo_miguel> I told that each n  depends on n-1
11:29:05 <oo_miguel> but I do not know the max n I will need
11:29:29 <dmwit> Right. But what I'm saying is that if you don't know the max n you will need, Seq is no good, because it can't lazily grow when you discover you need a bigger n.
11:29:32 <dmwit> Whereas my proposal can.
11:29:58 <jle`> oh yeah, you're right, seq can't do that with laziness
11:30:09 <oo_miguel> dmwit: jle`: unfortunatelly I do not understand both proposals :P but I will try to play with them
11:30:41 <oo_miguel> I know `seq` only for forcing the first expression to head normal form  when evaluated
11:30:42 <jle`> so my way doesn't work here
11:30:50 <jle`> ah i'm talking about Seq, from Data.Sequence
11:30:53 <dmwit> oo_miguel: So ask me something! I'm standing right here arguing til I'm blue in the face trying to get you to agree I'm the smartest Haskeller talking, of course I'm going to put in a tiny bit of extra work to make you understand my brilliance...
11:30:56 <jle`> which is unfortunately named
11:31:04 <dmwit> =D
11:32:01 <cocreature> I’m happy to just accept that dmwit is the smartest Haskeller talking without asking them questions :)
11:32:05 <oo_miguel> dmwit: heh, allright. so I create an infinite list (the [] list ?) of trees populating them somehow with my values?
11:32:51 <dmwit> oo_miguel: Right. So the first element in the list has just the first value in your sequence. The next element of the list has the next two elements of the sequence. The next element of the list has the next four elements of the sequence. Then 8, 16, 32, etc.
11:33:01 <oo_miguel> ah
11:33:25 <dmwit> Finding the right tree takes O(log n) time (n the index), and finding the right element in that tree again takes O(log n) time.
11:34:15 <oo_miguel> dmwit: I see, makes perfect sense
11:34:36 <oo_miguel> I will try to build this now, thanks
11:34:41 <dmwit> g'luck!
11:34:56 <ski> @quote create.fire
11:34:56 <lambdabot> mrd says: give a man a fire and you warm him for one day; point a man to oleg's web site, and he will create fire for himself -- in the type system
11:35:04 <ski> @quote OlegFacts
11:35:04 <lambdabot> OlegFacts says: Oleg wrote a function with type (a -> b) -> [a] -> [b] ...and it wasn't map
11:35:11 <ski> @quote OlegFacts
11:35:11 <lambdabot> OlegFacts says: Oleg's first datatype in haskell started like this: data ChuckNorris ...
11:35:20 * ski gives up
11:35:29 <dmwit> hehe
11:35:33 <cocreature> ski: I can do that as well! "notMap _ _ = []"
11:36:08 <dmwit> :t concatMap . (replicate 2 .)
11:36:10 <lambdabot> Foldable t => (a -> b) -> t a -> [b]
11:36:12 <jle`> notMap f = map f . reverse
11:36:26 <ski> @quote added.to.the.shootout
11:36:26 <lambdabot> MyCatVerbs says: The *real* best way to optimize a program is to tell dons that it's been added to the Shootout.
11:36:27 <dmwit> ...there are many ways.
11:36:46 <ski> @quote added.to.the.shootout.is
11:36:47 <lambdabot> Pseudonym says: Telling dons that something has been added to the shootout is the new telling Oleg that it can't be done in the type system.
11:36:56 <ski> @quote standard.operating.procedure
11:36:56 <lambdabot> Pseudonym says: What was considered 100 milli-Olegs of type hackery five years ago is standard operating procedure these days
11:37:14 <jle`> isn't there more than one prominent oleg in haskell? kind of like how we have multiple Simons and Chris's
11:38:18 <cocreature> phadej certainly seems to be the more active Oleg compared the one those quotes probably refer to :)
11:38:23 <Mo0O> hi there
11:39:12 <ski> yes, but which one is more famous/notorious ?
11:39:16 <dmwit> I watch some professional video gamers play video games sometimes. It's pretty humbling to watch them treat things that take me hours to get right once as things you should be able to just do without thinking.
11:39:18 <ski> hello Mo0O
11:40:01 <dmwit> I sometimes wonder whether anybody would feel that way about me doing my profession. Probably doing some type hacking (per the 100 milli-Olegs comment) is as close as I'll ever get to that.
11:41:02 <nshepperd_> The more Olegs the better, i say
11:41:15 <ski> but are they cumulative ?
11:41:30 <erisco> *Anybody* is a low standard dmwit, but good luck :)
11:41:46 <dmwit> ski: It's probably sublinear growth. But that's okay, let's get as close to the asymptote as we can!
11:42:10 <Mo0O> I'm trying to make my vim setup for haskell, my project use cabal sandbox feature so all my dependencies are installed in `./.cabal-sandbox` everything compile perfectly but inside vim my linter complain that it doesn't find de dependencies, it looks like a ghc error: `Could not find module ‘Cli’ Use -v ...` do you know how I can tell ghc to look inside .cabal-sanbox ?
11:42:20 <Mo0O> hello ski 
11:43:29 <dmwit> Mo0O: Use `cabal exec ghc -- args` instead of `ghc args`.
11:43:50 <dmwit> cabal will set up the environment and rewrite the arguments to ghc appropriately.
11:44:19 <Mo0O> thanks a lot dmwit, going to try that
11:44:30 * dmwit crosses his fingers
11:45:26 <dmwit> Depending on how new your cabal is, you may need to use `v1-exec` instead of `exec`.
11:47:05 <Mo0O> looks like there's a `cabal-ghc` provided by ale: https://github.com/w0rp/ale/blob/master/doc/ale-haskell.txt#L36 looks like I should activate this one and desactivate `ghc` checker
12:02:01 <Mo0O> dmwit: thanks a lot, that's perfect
12:02:04 <Mo0O> :)
12:02:17 * hackage yesod-core 1.6.14 - Creation of type-safe, RESTful web applications.  https://hackage.haskell.org/package/yesod-core-1.6.14 (MichaelSnoyman)
12:02:24 <merijn> dmwit: I think that's only in 3.0 and that's not been released yet to the best of my knowledge :p
12:02:57 <merijn> Mo0O: Honestly, at this point in time cabal sandboxes are basically 100% obsoleted by new-build/v2-build
12:03:17 * hackage unliftio-streams 0.1.1.0 - Generalization of io-streams to MonadUnliftIO  https://hackage.haskell.org/package/unliftio-streams-0.1.1.0 (BardurArantsson)
12:04:48 <Mo0O> merijn: I didn't know that, I'm completly new to haskell a looking for a good dev setup
12:04:57 * Mo0O go find some documentation on new-build/v2-build
12:05:01 <Mo0O> thanks
12:05:06 <merijn> Mo0O: https://cabal.readthedocs.io/en/latest/nix-local-build-overview.html
12:05:40 <merijn> Mo0O: It's basically "every project is automatically/transparently sandboxed and sandboxes can share dependencies so you're not always recompiling everything"
12:05:40 <Mo0O> perfect :)
12:06:47 * hackage rio 0.1.9.1 - A standard library for Haskell  https://hackage.haskell.org/package/rio-0.1.9.1 (MichaelSnoyman)
12:07:47 * hackage Allure 0.9.3.2 - Near-future Sci-Fi roguelike and tactical squad combat game  https://hackage.haskell.org/package/Allure-0.9.3.2 (MikolajKonarski)
12:08:09 <dmwit> 100% is too strong. I keep one sandbox around on each machine because it is crazy convenient to be able to just drop into a ghci that has a ton of libraries already available.
12:08:42 <dmwit> People suggest v2-repl --constraint blah, but with a sandbox you can just accrete useful stuff and not have to think about exactly which libraries you plan to use this session.
12:09:03 <fen> dmwit: whats wrong with; (Comlexity x ~ (n :: Nat)) => ?List (BoundedList n Nat) x 
12:09:04 <fen> ?
12:09:17 <dmwit> And if you ever get into dependency hell you just drop the rm bomb and start over.
12:09:47 <fen> is that Complexity family something to do with singletons like Cale suggested? 
12:10:10 <dmwit> fen: The question seems underspecified to me. But syntactically I don't think ? can go there.
12:10:39 <dmwit> I have no idea what Complexity is supposed to be. Nor List, nor BoundedList.
12:11:09 <fen> but you said the Fin ((n+m) `choose` m) version was better
12:11:21 <fen> Complexity Bool = 2
12:11:45 <fen> Complexity (Maybe x) = 1 + Complexity x
12:12:04 <dmwit> You must have misunderstood me. I definitely didn't say Fin ((n+m) `choose` m) was better than... that stuff you posted above.
12:12:08 <fen> ?List, just couldnt think of the name of this thing, its some kind of list
12:12:08 <lambdabot> No module "just couldnt think of the name of this thing, its some kind of list" loaded
12:12:13 <fen> SomethingList
12:12:23 <fen> lol lambdabot
12:13:17 <fen> dmwit: they are supposed to be the same thing though right? why the Fin version? just to have something concrete to compare it too?
12:14:38 <fen> its just supposed to encode at type level the proportion of values it containes
12:14:49 <fen> like, half true, halfe false, etc
12:14:55 <dmwit> I have no idea if they are supposed to be the same thing. I also have no idea whether they *are* the same thing, independently of intention.
12:15:03 <fen> where the position these appear in is not specified at type level
12:15:21 <dmwit> I don't really intend to spend a lot of time thinking about it, either.
12:15:26 <fen> shuffleList?
12:15:41 <fen> dmwit: its offensive!?
12:15:58 <dmwit> Not offensive. I just have... problems more interesting to me to think about at the moment.
12:16:09 <fen> please share!
12:16:16 <dmwit> I'm not trying to be mean or say this isn't worthwhile or whatever. Just transparent about my intentions.
12:16:19 <fen> interesting problems is what we are all about
12:16:37 <dmwit> I'm hacking on github.com/dmwit/mcmario at the moment.
12:17:16 <dmwit> It's a Rocket League handicap recommendation system; currently it only works for 1v1s, and I'm modifying it to accept info and make recommendations about team games.
12:17:17 <fen> who is Dr. Mario!?
12:17:35 <dmwit> Dr. Mario is a video game from the 80s that I like a lot.
12:17:35 <fen> oh, some superhero comic thing... nvm
12:18:05 <dmwit> (mcmario was originally a Dr. Mario handicap recommendation system; I'm working on a fork for Rocket League right now.)
12:18:42 <fen> thought it might have been some ancient repo of awesome code
12:19:40 <fen> anyway, whats the problem with that? seems like smooth runnings 
12:20:14 <dmwit> No conceptual problem at the moment. Just engineering work.
12:20:21 <fen> strange
12:20:43 <fen> how did you manage to find something that wasnt a constant stuggle!?
12:21:11 <fen> or is that just the sign of being proficient at haskell, where you can actually do things you want to with it
12:22:06 <dmwit> I mean. I had to sit and think and do mathematics for a while before I started.
12:22:13 * dmwit shrugs
12:22:49 <dmwit> Eventually you've done enough thinking to know what you want the program to do, and it's a matter of translating for the computer.
12:24:29 <Cale> (It helps not getting mired in recursion schemes and type-level hackery and just writing the program you want in a more straightforward way ;)
12:25:18 <fen> unless those things are essential! 
12:25:26 <Cale> They're never essential.
12:25:30 <fen> sure
12:25:33 <fen> they are!
12:26:02 <fen> how else are you going to generate values which form equivilance classes of the base functors for containers?
12:26:05 <Cale> Recursion schemes are almost always better replaced by other systems of generics when you go to actually engineer real-world things.
12:26:19 <Cale> (at least in my experience)
12:26:21 <fen> it literlly needs both of the things you just said were never essential!
12:26:54 <fen> oh the old "your just reinventing generics" argument
12:26:59 <fen> scrap your generics!
12:27:07 <jle`> i'd say 98% of productive haskellers never use recursion schemes or type-level hackery
12:27:34 <jle`> i made up that number arbitrarily but i don't think it's too far from the reality
12:27:35 <Cale> Type level hackery, well, there are cases where you can use it to solve certain engineering problems, but in my experience, you never want to start from that angle. You want to write a whole bunch of programs which have a particular issue that you want to exclude.
12:27:36 <fen> but i have foldable classes generated by providing the base functor
12:27:48 <fen> and these base functors are generated using type level hackery
12:28:16 <fen> basically the idea of families of foldable and unfoldable classes is impossible without thsi
12:28:43 <jle`> if you're trying to create an abstraction system, then you're going to run into complications because your abstraction is complex, not because of haskell usually
12:28:53 <jle`> if you're just creating an application (like most haskellers), then you won't really run into this :)
12:28:55 <fen> thats literally the issue, and it literally needs that machinery to do it!
12:28:59 <HenryCH> would map (+1) . map (+1) traverse the list twice or would ghc optimize it?
12:29:06 <jle`> right, so your question 'how did you mangae to find something that wasn't a constant struggle ...'
12:29:14 <jle`> the answer is: write an application ;)
12:29:20 <fen> lame!
12:29:20 <Cale> fen: There are lots of other approaches one could take, and also no user ever cared about how you were folding over things. You could always have written such recursion directly.
12:29:33 <jle`> HenryCH: ghc does optimize it, but even without optimization, it would only traverse the list once
12:29:57 <Cale> (and usually, it would end up being much more maintainable, simply because you're not trying to overgeneralise)
12:29:58 <fen> HenryCH: there is a fmap . fmap rule it would use, so the optimisation you describe happens
12:30:22 <Cale> Haskell gives you a lot of rope to hang yourself with when it comes to inventing generalisations you don't need.
12:30:25 <fen> but ther is no (+1) . (+1) = (+2) as a result of some rule
12:30:27 <jle`> even without optimization, it only traverses the list once anyway. laziness is neat like that :)
12:30:34 <Cale> It's important to write real programs first, and then figure out how to make them better.
12:30:35 <HenryCH> jle`: how so?
12:31:04 <fen> Cale: write it directly? isnt the idea to have abstractions?
12:31:12 <fen> like, have the fold class existing at all
12:31:18 <jle`> HenryCH: when haskell sees map (+1) (map (+1) (1:2:3:[]))
12:31:27 <jle`> HenryCH: if you want to evaluate it, you ask "what is the first item?"
12:31:29 <Cale> fen: No, the idea is to have programs which run and do useful things
12:31:45 <Cale> fen: and only then, figure out how to write those programs in better ways
12:31:46 <jle`> HenryCH: to find the first item, you need to expand the definition of map
12:31:50 <fen> we make language choices
12:32:00 <jle`> @src map
12:32:00 <lambdabot> map _ []     = []
12:32:00 <lambdabot> map f (x:xs) = f x : map f xs
12:32:03 <Cale> There's no point in trying to invent abstractions that you think you might need before you know why you need them
12:32:16 <jle`> HenryCH: i recommend trying to step through this by hand, on paper and pencil :)
12:32:21 <Cale> The space of such things is very large and full of useless abstractions
12:32:25 <fen> thats irrelavent, it assumes there was no reason to create this abstration
12:32:28 <fen> im sure there was
12:32:58 <HenryCH> jle`: all right, i think i get what you're trying to say
12:33:11 <Logio> Cale: that's most of mathematics though
12:33:11 <Cale> There are *lots* of pretty-looking abstractions which, unless you're only interested in doing performance art of constructing abstractions, are absolutely not useful.
12:33:22 <jle`> HenryCH: in order to see what the first value of map (+1) (map (+1) (1:2:3:[])), you have to see which 'pattern' the map matches
12:33:29 <Cale> Even in mathematics, abstractions don't just come out of nowhere.
12:33:40 <jle`> HenryCH: and to do that, you have to see what the first value of map (+1) (1:2:3:[]) is
12:33:46 <jle`> so here's how evaluation works:
12:33:49 <fen> keep coding forwards tackling all problems head on, never looking back, and then if and when this process is finally complete, start from the beginning again and find that you have a library in place for every problem you reencounter
12:33:56 <jle`> map (+1) (map (+1) (1:2:3:[]))
12:34:01 <Cale> You want the *right* abstractions which express enough that let you produce many results, while at the same time apply to enough things that you already cared about so that you actually solve problems.
12:34:06 <jle`> map (+1) (2 : map (+1) (2:3:[]))
12:34:18 <jle`> => 3 : map (+1) (map (+1) (2:3:[])
12:34:23 <jle`> etc.; just one traversal :)
12:34:34 <HenryCH> i see, thanks
12:34:50 <jle`> and this is all without any optimization rules or fusion
12:35:23 <fen> the idea is that if you need a particular abstraction that is part of a larger set of such, which requires another abstraction to express, then you could argue it is a waste of time doing that. but if you do, then at least everything is done properly and you have many reusable things as a result
12:35:25 <zachk> looks like function composition to me, (fusion) and some basic arithmetic 
12:35:30 <Cale> Finding those things is not easy, it requires a lot of experience and good design taste.
12:35:44 <dmwit> jle`: eh... I'd call that two traversals.
12:35:50 <jle`> zachk: 'fusion' in the context of ghc lists is something different, but yeah, this is fusion in the abstract sense
12:35:53 <fen> like, you could just use foldable for streams and the not using the basecase might not bother you... but really there is something going on there
12:35:57 <dmwit> jle`: You're traversing [1,2,3], and you're constructing, traversing, then immediately throwing away [2,3,4].
12:36:19 <Cale> It's better to start out by actually solving problems directly, then take your solutions, figure out what they have in common, and abstract just those things.
12:36:28 <fen> and trying to write a default for traversable in terms of get and set really gets to these issues
12:37:08 <fen> well, I wanted a cyclic graph that you could grow while preserving its hamiltonian path for a default comonad in terms of traverse and zipper
12:37:10 <jle`> dmwit: hm, that's a good point
12:37:27 <fen> the traverse and zipper turned out to be get and set, and then there were families of varients of these
12:37:43 <fen> and now the idea is to type hack a parametrisation of these families
12:37:52 <fen> you cant say thats not solving the desired problem
12:38:06 <dmwit> Sure I can. The desired problem is "my games of Rocket League are unfair".
12:38:33 <fen> the algebraic cyclic graph comonad!
12:38:35 <Cale> fen: That sounds like a deep daisy-chain of potential XY problems to me. :D
12:38:54 <dmwit> (Well, that's not the desired problem, actually... my true desired problem is "I have so much money I don't know what to do with it". ;-)
12:38:56 <fen> i would be happy if it already existed but it does not!
12:39:00 <jle`> fen: and what are you using the algebraic cyclic graph comonad to do?
12:39:11 <fen> and it seems these things are nesacary to write it
12:39:12 <Cale> Like, what is the program you're writing supposed to compute, first of all.
12:39:15 <MarcelineVQ> dmwit: I have suggestions.
12:39:20 <dmwit> MarcelineVQ: I'm listening!
12:39:35 <MarcelineVQ> Use a different meaning for the word have.
12:39:37 <dmwit> MarcelineVQ: If the first suggestion is "forget everything you know about what to do with money" I'm going to stop listening.
12:40:17 <jle`> i think what my (and maybe Cale's) point is that the perception you have that Haskell is unproductive might be self-imposed, since people writing applications in haskell are productive all the time. it's just building complex abstractions without a specific application you are writing in mind that would cause one to stall in productivity
12:40:35 <jle`> i wouldn't judge anyone for doing this, because it's something that i, personally, do all the time
12:40:37 <fen> well the reason you need a cyclic graph comonad is for stencil opperations to communicate information across edges for the implementation of graphical algorithms, where a lookup table representation of the graph is not algebraic, ie you cant guarantee it preserves a hamiltonian path depending on how you grow it
12:40:39 <jle`> (build abstractions for fun)
12:40:50 <fen> and thats how the traverse instance works for the comonad
12:41:07 <jle`> but i don't assume that the experience of the average productive haskeller is writing abstractions in a vacuum :)
12:41:08 <HenryCH> is it just one traversal for the specific case of map? i assume filter pred1 . filter pred2 would benefit from just one filter?
12:41:21 <jle`> HenryCH: try writing it out, by hand :)
12:41:51 <fen> you *could* write a traverse instance for a graph using a lookup table representation, and a stencil comonad like that, but basically, that does not use the get/set style of traverse at the heart of this project, and does not give the zipper style comonad "store"
12:42:08 <Cale> fen: The thing I'm hearing here is that it sounds like you're starting with a problem which is *already* a pile of abstract nonsense though.
12:42:19 <dmwit> HenryCH: The naive implementation of Haskell would do two traversals for `map f . map g`, and would do two traversals for `filter f . filter g`.
12:42:28 <Cale> Rather than "I want to write a program that when the user runs it, will do X"
12:42:35 <jle`> right, but this answers your original question "how can you find something that you can do without major problems" -- just do the thing you said you could do ;)
12:42:37 <dmwit> HenryCH: However, GHC is not naive, and will convert the former to `map (f . g)` and the latter to `filter (liftA2 (&&) f g)`.
12:42:45 <fen> if your question is "why does nobody else need such an approach to write graphical algorithms" i would just say that they can but in a less awesome way
12:42:54 <jle`> HenryCH: yes, sorry, i do believe i am incorrect here. i have to go home and rethink many things
12:43:15 <fen> Cale: no, we want to ensure certain properties through the type syste,
12:43:15 <jle`> fen: choosing the less awesome way is usually the first step in being able to solve problems and do productive things :)
12:43:22 <Cale> But why?
12:43:34 <jle`> i find that the less awesome i strive to be, the more productive i am
12:43:34 <Cale> Are there certain bugs that you've written over and over and want to avoid?
12:43:56 <fen> why only allow a graph to be grown in a way that maintains its traversability?
12:44:01 <Cale> Until you know what the bugs look like, you don't know what your types even need to express.
12:44:12 <fen> no just answer that question
12:44:13 <jle`> (i say this in the context of your original question: "how can you find something that you can write in haskell without major conceptual problems?" => the answer is, find less awesome ways)
12:44:29 <Cale> I don't know what your question is even referring to there.
12:44:42 <fen> well thats ok
12:44:50 <fen> we can discount the criticism then
12:44:55 <fen> on i go
12:44:57 <fen> bye!
12:44:59 <Cale> I don't even know why we need a graph in the first place. What is the actual problem that we're trying to solve?
12:45:04 <MarcelineVQ> That does make it easy.
12:45:06 <jle`> the answer to "why only allow ..." is "because it lets you do useful things"
12:45:21 <jle`> oh they left
12:45:25 <MarcelineVQ> Does anyone have tips for conceptualizing software? I've got a program in mind with many moving parts and I'm having trouble holding all of them in my mind at once. I'm leaning towards trying something like https://www.yworks.com/yed-live/ but would like to hear opinions
12:45:38 <jle`> also i might have mixed up my negatives
12:46:00 <MarcelineVQ> That is to say I'd like to lay out hot it works before making it
12:46:03 <MarcelineVQ> how
12:46:13 <Cale> I've seen a fair amount of this mindset in a certain fraction of newer Haskell users lately, and it's a little bit puzzling.
12:46:42 <gonz_> Cale: Which one?
12:46:51 <gonz_> Mindset, that is.
12:47:00 <Cale> There's a lot of variations of it, but mostly it seems that people end up struggling really hard to encode things at the type level before they're really sure why they're doing it.
12:47:16 <MarcelineVQ> What I find puzzling is that someone can hear dr mario and think it's a superhero comic.
12:47:19 <jle`> 'we were so focused on what we could do, that we forgot to think about whether or not we "should" do it'
12:47:58 <MarcelineVQ> jle`: A thought no one kept in mind for jurassic park 3 :>
12:48:09 <jle`> i wonder if i am a part of the problem
12:48:51 <Cale> I think it's kind of a community problem that a lot of the more advanced Haskell programmers are talking about such advanced features of the language in the context of very particular problems that demand it, and not talking enough about boring old practices of functional programming -- despite the fact that when you're actually writing programs, the latter are far more useful.
12:49:12 <Cale> It's only natural of course :D
12:49:28 <gonz_> It's easy to get carried away. There's nothing wrong with it and I'm pretty sure you temper it with time. Don't we all get enthusiastic about type system features that we're comparatively new to and want to try out?
12:49:40 <Cale> The best papers for new Haskell programmers to be reading are the ones from the 1990s
12:49:49 <gonz_> Everyone's first program with Idris is some useless piece of garbage that they probably could do way easier even without dependent types.
12:50:03 <jle`> there's something wrong with it if twists expectations on what actual productive haskelling looks like
12:50:07 <Cale> All the new papers are pretty irrelevant to daily use of Haskell in 99.5% of all cases.
12:50:13 <MarcelineVQ> maybe, but indexed monads are hella cool bro
12:50:17 <Cale> and similarly with a lot of the blog posts
12:50:24 <MarcelineVQ> https://medium.com/the-web-tub/idris-state-machines-in-javascript-apps-b969e2cb6ed2
12:51:20 <jle`> maybe there is a different 'distribution' of beginner/intermediate/advanced haskellers than in other languages
12:51:34 <jle`> but then again, B/I/A might not even be meaningful, because the "tail" in Haskell is so...huge
12:51:51 <jle`> there is no real 'upper limit' in understanding
12:52:09 <MarcelineVQ> jle`: afaik I'm still beginner haskell, so it's kind of a hard thing to quantify even if it was useful to do so
12:52:33 <Cale> Yeah, the problem is that if you look at the communication about Haskell right now, it's very dispropotionately far out on that tail.
12:52:43 <koala_man> _ikke_: do you actually want to build shellcheck for Alpine, or do you just want to run it? There's an official upstream Alpine image with shellcheck preinstalled, and you can also fetch statically compiled bianries
12:53:14 <jle`> the 'cool' topics within the haskell community are ... a race to the tail
12:53:19 <Cale> Yeah
12:53:38 <gonz_> Maybe if the conversation was more consciously directed toward Haskell offering easy maintainability when using the more vanilla features and everyday stuff the situation wouldn't be like that.
12:53:47 <gonz_> But as you've pointed out, no one's talking about that.
12:54:05 <gonz_> Snoyman had a talk about trying to shift the conversation towards something like that
12:54:09 <jle`> i think people talk about it when talking to new haskellers
12:54:15 <jle`> but if a new haskeller peeks into, say, /r/haskell ...
12:54:25 <jle`> or a haskell newsletter
12:54:28 <Cale> I myself am a bit guilty of this: I've been extracting quite a lot of good practical application out of fancy things I can do with GADTs and constraints lately -- but still, that accounts for like 2% of the code I'm writing.
12:54:29 <jle`> they're goign to see a weird distribution
12:54:43 <jle`> heck, even if they come into this channel, heh
12:55:02 <zachk> I remember when I started, this channel talked about monads alot to newcomers 
12:55:15 <_ikke_> koala_man: It would be nice to be able to just install it, but I guess for now that might work as well
12:55:24 <jle`> i think all haskellers care about accessible maintainability. it's just that they've already internalized it as something to take for granted, and something to maybe explain to people new to haskell
12:55:26 <Cale> It's tough, because cool new methods are simply more salient when we think about the code we're writing.
12:55:28 <zachk> or that was where the newbie questions were geared 
12:55:31 <jle`> but it's not something we talk about *to other haskellers*, very often
12:55:42 <Cale> right
12:55:52 <_ikke_> koala_man: similar to how archlinux does it now
12:56:08 <_ikke_> koala_man: but it's a lot of work to do that from scratch
12:56:15 <jle`> we just assume every other haskeller we know already appreciates it, and talk about other things lol. but to outsiders it warps the perspective
12:57:05 <gonz_> I think that's spot on.
12:57:41 <gonz_> Personally, I have a lot of conversations about PureScript & Haskell with my neighbor who's a TypeScript programmer (pretty much exclusively), so I get a lot of mileage out of even the vanilla features.
12:58:38 <gonz_> Showing the practical use of higher kinded types and being able to abstract "one layer higher" is something he's never even seen before
13:00:36 <jle`> i do remember reading something about this when i first got into haskell. so maybe it's something we've been dealing with for a long time, heh
13:09:18 <ski> zachk : how long ago was that, roughly, ooc ?
13:09:52 <zachk> i wanna say 2006-2008 not sure on the exact year 
13:09:59 <ski> okay
13:10:38 <ski> it still happens that i (and others) will go into more basic descriptions of, monads say, or existentials, &c., with newbies, in here
13:13:48 <sm> maybe we should've had a #haskell-practical all these years ?
13:14:04 <MarcelineVQ> questions answered <=> questions asked. if less people are answering about monads then less people are asking
13:14:07 <sm> except it would have failed, the pull of #haskell is too strong
13:14:42 * Lycurgus wonders how many haskellers think truth being ineffable is a defensible position, having found 1
13:16:01 <ski> sm : i kinda like having a mix of practical vs. theoretical, abstract vs. concrete, beginner vs. advanced, in the same channel
13:16:54 <MarcelineVQ> Lycurgus: What does that mean, "Something's too hard to explain so why bother" ?
13:16:57 <sm> #haskell has always tended to skew theoretical and abstract though, driving off some of the practical folks
13:17:20 <ski> that may be the case
13:17:38 * ski isn't sure what to do about that
13:18:09 <Lycurgus> MarcelineVQ, no it refers to a thread of conversation between myself and (at least tdammers) on the ineffability of truth as a concept
13:18:28 <ski> (except perhaps to try to speak up more, then practical, respectively concrete stuff are being discussed ?)
13:18:29 <Lycurgus> (in the pure chat channels)
13:18:49 <sm> ski: I used to practice that too, but kind of gave up
13:19:41 <sm> also there's less "practical" stuff here now I think largely because cabal and stack work better
13:19:52 <Lycurgus> lol
13:19:56 <ski> Lycurgus : ineffable in the sense of Wittgenstein "Wovon man nicht sprechen kann, darüber muß man schweigen." ?
13:19:58 <Lycurgus> but true
13:20:11 <Lycurgus> just so
13:20:18 <Lycurgus> not that thing W recanted
13:20:47 <Lycurgus> the first thing was tight though, the system of the bemerkungen less so
13:21:16 <ski> elaborate on "first thing","system of the bemerkungen" ?
13:21:25 * ski hasn't read much late Wittgenstein, fwiw
13:21:31 <Lycurgus> the system of the tractatus
13:21:47 <Lycurgus> whose final sentence was quoted
13:22:09 <Lycurgus> (by ski)
13:22:11 <ski> so that's the "first thing" you were talking about ?
13:22:23 <ski> and "system of the bemerkungen" would refer to some of his later work ?
13:22:45 <Lycurgus> exactly
13:22:58 <sm> MarcelineVQ, re grokking software I have had some luck with outlining with org, diagramming with plantuml, or just generally documenting it
13:24:38 <sm> also loading it up in the appropriate jetbrains IDE and playing with their tools
13:28:12 <MarcelineVQ> what does a jetbrains ide give you, in your experience?
13:30:15 <Lycurgus> a way better eclipse
13:30:59 <sm> an effective project overview sidebar, good code navigation, class hierarchy, file structure, automatic diagramming, code reformatting, file inspections, project-wide code quality reports, refactoring - stuff like that
13:31:33 <sm> db entity tables.. I'm sure there's more stuff I haven't found
13:32:10 <MarcelineVQ> neat, what lang do you use most?
13:32:30 <MarcelineVQ> with jetbrains
13:33:05 <sm> I use it (phpstorm) for working with a large PHP(+js/smarty/postgres) codebase
13:33:43 <sm> and have tried to use it (IDEA) for working on haskell projects (but haskell support isn't there yet)
13:33:45 <Lycurgus> java stuff here, i still use eclipse for php
13:34:36 <Lycurgus> although actually the best php debugger I know of runs in emacs
13:35:36 <sm> I'm a lifelong emacs user, and always have that running, but for extended sessions I'll fire up the IDE
13:36:38 <sm> (depending on the project, the task, the machine, how much memory I can spare at that moment..)
13:37:51 <sm> ..available power..
13:42:04 <literallyCrevice> Say I have a function of type `String -> String`, and I want to use it with all of stdin. Currently what I do is `main = getContents >>= return . myFunction >>= putStrLn`. Is there a similar way without using `return`?
13:42:55 <MarcelineVQ> ma >>= return . f === fmap f ma
13:44:07 <literallyCrevice> MarcelineVQ: Thanks!
13:44:58 <ski> @src interact
13:44:58 <lambdabot> interact f = do s <- getContents; putStr (f s)
13:45:06 <ski> literallyCrevice ^
13:50:26 <literallyCrevice> ski: Huh, for some reason I was under the impression that interact used getLine. Thanks again!
13:52:44 <MarcelineVQ> me too actually
13:54:24 <ski> how could it use lazy I/O, that way ?
14:11:13 <Bish> hello #haskell i played around with threads the other day and people said using forkIO for doing something like this iss stupid
14:11:26 <Bish> so i listened and used parMap, my code is still darn slow and uses a lot of ram, maybe you can tell me why
14:11:48 <Bish> http://dpaste.com/3KARBMD
14:11:57 <fen> hmm, ShuffleList compiles, but it cant have "cons"
14:12:44 <fen> as this needs the value to determine which type level nat is incremented 
14:13:23 <Bish> fen: this wasn't meant for me i suppose :o
14:14:43 <fen> (True : ([True,False,False,False] :: ShuffleList ([1,3] :: BoundedList (Complexity Bool) Nat))) :: ShuffleList ([2,3] :: BoundedList (Complexity Bool) Nat)
14:15:14 <fen> the [1,3] becomes [2,3] because it was true that was added to the list, so now there are 2 trues and 3 falses
14:15:29 <fen> so the type of the output depends on the value of the input.
14:15:37 <fen> Bish: sorry!
14:15:57 <Bish> fen: all good, was just confusued because the channel was so silent all the time
14:16:27 <fen> huh?
14:16:34 <fen> http://tunes.org/~nef/logs/haskell/19.04.13
14:16:38 <fen> its not...
14:17:29 <Bish> well 10 minutes or whatever
14:17:54 <fen> idk about parMap sorry
14:18:38 <fen> what do you mean "forkIO for threads is slow?"
14:18:45 <fen> are you sure thats what was meant?
14:19:06 <fen> is that not a valid way to run threads in parallel?
14:19:07 <Bish> i meant to say "is bad if the function is pure"
14:19:18 <fen> hmmm
14:19:36 <Bish> because it is threading with IO while you actually don't need iO
14:19:40 <fen> so is this parMap supposed to run pure computations in parallel?
14:19:44 <MarcelineVQ> it's probably not ever going to use less ram to do things in parallel, whether parallel is a speedup or not depends on whether you have enough work to do in each separated part to justify the overhead of splitting it up, this is called granularity.
14:19:45 <Bish> yes
14:19:51 <fen> nice!
14:20:06 <Bish> MarcelineVQ: well, there is no need to have much things in ram
14:20:17 <fen> yeah, you have a multicore processor?
14:20:22 <Bish> sure
14:20:32 <Bish> hello 2019!
14:20:33 <fen> how can you tell which thread is running on which core?
14:20:47 <Bish> fen: well i don't need to?
14:20:55 <fen> like, are you sure parMap is assigning one thread per core?
14:21:04 <fen> i assume thats what you want to happen
14:21:06 <Bish> fen: ah, you're asking how that works
14:21:11 <fen> idk
14:21:15 <Bish> ghc has green threads, managed by RTS
14:21:24 <fen> runtime system?
14:21:27 <Bish> so the haskell runtimes decides where it runs at
14:21:40 <fen> huh, ok
14:21:44 <Bish> and they're supposed to be super cheap, but so far i got no benefit out of it
14:21:56 <MarcelineVQ> Where in this code do you use parallel anything
14:22:10 <Bish> MarcelineVQ: parMap is supposed to
14:22:12 <fen> yeah! show us the code
14:22:17 <Bish> fen: i did
14:22:22 <Bish> http://dpaste.com/3KARBMD
14:22:29 <MarcelineVQ> There is no parmap here
14:22:31 <fen> oh sorry
14:22:42 <fen> yeah but how can you tell what cores its running on?
14:23:17 <Bish> MarcelineVQ: sorry, i changed the parMap to a normal map for tests
14:23:17 * hackage exact-combinatorics 0.2.0.9 - Efficient exact computation of combinatoric functions.  https://hackage.haskell.org/package/exact-combinatorics-0.2.0.9 (WrenRomano)
14:23:33 <Bish> fen: well, i can't but i see the program using multiple threads when being told to
14:23:51 <fen> on top or something?
14:23:55 <Bish> http://dpaste.com/1F05CG7
14:23:56 <Bish> fen: yes
14:24:15 <Bish> just compile my program
14:24:22 <fen> im on windows
14:24:41 <Cale> Bish: have you tried using much larger chunk sizes?
14:25:45 <Cale> You'd ideally want to not be creating too many more sparks than you have available cores (though you certainly can, you're creating more overhead without much benefit if you do)
14:26:15 <Cale> see also http://hackage.haskell.org/package/parallel-3.2.2.0/docs/Control-Parallel-Strategies.html#v:parListChunk
14:26:29 <fen> oh, so thats maybe why it runs slower, because these threads are more memory intensive and just end up running on the same core anyway
14:26:29 <Cale> which should make it so you don't have to do the chunking by hand
14:27:50 <Bish> Cale: yes, i tried tweaking it, but right now it crashes my pc :D
14:27:53 <Bish> because its eating my ram
14:28:01 <Bish> Cale: and i wasn't really impressed by its speed
14:28:15 <Bish> fen: windows will also tell you about logical-core usage
14:28:23 <Bish> if you tell the task-managert o
14:28:26 <fen> so is Bool a singleton? and does that mean you can check the value to determine the type of the output which depends on the value of the input?
14:28:45 <fen> what is this task-managert?
14:28:57 <Bish> task-manager, the thingy of windows
14:31:36 <fen> huh, there are like collums in the "details" section
14:31:55 <fen> but noththing that is "number of cores the process is using" or "threads per core" or something
14:32:07 <Bish> well there is tab showing the graph of the usage
14:32:15 <Bish> you an right click that and tell it to show all logical cores
14:33:14 <fen> oh yeah, and the process moiter has these aswell
14:34:00 <Bish> Cale: chunk size does not seem to matter much... while 8 being the fastes.. obviously, since i have 8 cores
14:34:27 <Bish> but its eating a lot of ram, and it will crash if my combinations go over 5
14:34:33 <fen> if i type [1..] in ghci it causes all the cpu cores to be used almost completely 
14:34:43 <MarcelineVQ> It is eating a huge amount of ram, even with r0
14:34:58 <Bish> fen: that be true for a c program for (;;) i++; also
14:35:08 <fen> yeah?
14:35:10 <Bish> MarcelineVQ: what does r0 mean?
14:35:17 <MarcelineVQ> https://hackage.haskell.org/package/parallel-3.2.2.0/docs/Control-Parallel-Strategies.html#v:r0
14:35:22 <fen> how does it know to split the work among the cores?
14:35:31 <Bish> fen: oh, thats what u mean, no it doesn't
14:35:34 <Bish> 1 core should peg
14:35:39 <fen> ?
14:35:44 <Bish> MarcelineVQ: ah, one sec
14:35:56 <fen> haskell seems to manage to split the work of counting over 4 cores, how!?
14:36:00 <Bish> MarcelineVQ: yes
14:36:12 <Bish> but a bit less i supposed, will check that
14:36:18 <fen> that seems like something that would be difficult to paralelise 
14:36:31 <Cale> Bish: That's not obvious at all from the 8 cores, because it does nothing to determine how many chunks will be available to work on at any moment.
14:36:48 <Bish> well in my simple example it should be, doesn't it?
14:36:54 <Cale> Bish: Ideally, you'd be able to split the work 8 ways, not split it into chunks of size 8
14:36:55 <Bish> i mean the program is not doing anything else
14:37:14 <fen> ohhh, i think its basically using all of 1 core and everything else is being shifted to the other cores
14:37:23 <Bish> fen: uhm, yeah
14:37:31 <fen> because one is completely used and the others are equally not quite totally used
14:37:31 <Cale> (the fact that your third core is working on a chunk of size 8 doesn't say much about what the other cores are doing)
14:37:43 <Bish> fen: but thats basicially the promise haskell makes, but that doent work really much right now for me
14:38:23 <Bish> the code is really small & was easy to parallelize, but there is a lot of voodoo involved :D
14:38:37 <fen> you mean the chinks should be length L/8 instead of length L? 
14:39:11 <Bish> Cale: well, how would i split calculation of hashes of words into 4 codes if not creating in 4 chunks?
14:39:16 <fen> hmmm
14:39:28 <fen> maybe I could wrap the Bool in a IsTrue or IsFalse wrapper
14:39:40 <MarcelineVQ> oh.. this list is like, super, duper long
14:39:43 <fen> and throw an error if those are constructed badly
14:39:58 <Bish> MarcelineVQ: sure it is, it's all combinations of a-bA-B0-9
14:40:20 <Bish> but somehow i figured things would be garbage collected... since i filter them
14:40:21 <MarcelineVQ> the memory use problem might be the same that you encounter with mapM vs map, there you can run out of mem because sequencing needs the whole list to finish
14:41:09 <Bish> MarcelineVQ: i had this problem without parallelization, too, thats where the "repM" functionc ame in
14:41:27 <Bish> that solved it for that part.. becausue from that point on the garbage collector was able to throw away things needed
14:41:39 <Bish> things uneeded*
14:42:09 <Bish> and concat works on endless lists, i can't see how the program needs the rest of the list
14:43:54 <Bish> the program gets faster if i let it use less cores
14:43:55 <Bish> qq
14:44:23 <Bish> its fastest on one core :D
14:45:26 <fen> data IsTrue where  IsTrue :: (x ~ Bool) => x -> IsTrue
14:45:36 <Cale> Bish: That's a sign that your chunk size isn't large enough
14:45:36 <MarcelineVQ> be sure to (re)read https://www.oreilly.com/library/view/parallel-and-concurrent/9781449335939/ch03.html if you haven't recently
14:45:55 <fen> is there some way to use the value of x in the constraint?
14:46:42 <Cale> Bish: It might be better to split up the domain into 8 parts -- perhaps just based on the first symbol of the password
14:47:12 <Bish> i don't follow :/
14:47:45 <fen> or do i have to write;
14:47:55 <fen> toTrue :: (x ~ Bool) => x -> IsTrue
14:48:03 <fen>  toTrue False = error "toTrue False" 
14:48:03 <fen> toTrue True  = IsTrue
14:48:11 <fen> toTrue True  = IsTrue True
14:48:26 <fen> is that a "smart constructor" ?
14:49:02 <Cale> Bish: e.g. one process will try to find the first matching password starting with 'a' through 'g', and then the next will try to find a password starting with 'h' through 'n' and so on.
14:50:05 <Cale> fen: What is "IsTrue"?
14:50:24 <Bish> Cale: is the idea behind that to reduce the overhead by creating green threads?
14:50:50 <Bish> i have a feeling fen is a bad troll
14:51:47 <Cale> Bish: Somewhat -- if you're using Control.Parallel, then it's not threads, but sparks, but similarly, creating lots of tiny sparks will result in more time wasted scheduling sparks than doing actual work.
14:52:01 <fen> data IsTrue where  IsTrue :: Bool -> IsTrue
14:52:26 <Cale> fen: ... okay? So it's a trivial wrapper around Bool?
14:52:56 <fen> yeah, but the constructor ensures that the value is actually True
14:53:00 <Cale> fen: What's the point?
14:53:10 <fen> as it seems to be something that cant be enforced at type level
14:53:17 <Cale> You might as well just use () in that case?
14:53:30 <fen> the type of my function depends on the value of the bool its given 
14:53:32 <Cale> since it seems like you want to exclude False
14:53:46 <Cale> and if you removed False from Bool, then you'd be left with a one-constructor datatype
14:53:50 <fen> well its supposed to take Bool, but now there has to be 2 functions
14:53:57 <Cale> of which () is the canonical example
14:54:24 <fen> well, no need to convert to and from, can just store the input bool
14:55:11 <fen> hmm, thought you might be right, the IsTrue type need not actually store anything at all
14:56:26 <takanuva> could anyone please help me? is it possible to "tie the knot" on two monadic actions on the lazy state monad? I want to use first >>= second, but "first" should receive the (monadic) result of second as argument
14:56:50 <Cale> yes
14:57:07 <takanuva> I'm an experienced programmer already, but I'm having a hard time implementing this one
14:57:15 <ski> takanuva : look into `MonadFix',`mfix', and the `mdo' and `do .. rec ...' extensions
14:57:26 <Bish> Cale: i always figured green threads are super low cost
14:57:47 <Cale> do rec { v <- first w; w <- second v }; return w
14:57:50 <Bish> maybe not, "spawn a thread for every addition" low-cost
14:57:51 <takanuva> wow, recursive do!
14:58:27 <Cale> Bish: You're not spawning threads, there are a certain number of "capabilities" which are pulling from a work-stealing queue of "sparked expressions"
14:58:29 <Bish> in fact using no parMap at all even makes the program x2 faster
14:58:48 <Bish> Cale: i can't follow that, sorry
14:58:57 <Cale> Bish: and they try to make their way through that queue, doing work to evaluate the expressions on it
14:59:05 <Bish> where is the difference between spark and a green thread
14:59:11 <Cale> Okay, so the primitive that all this stuff is built out of is par
14:59:17 <Cale> par :: a -> b -> b
14:59:45 <Cale> par x y causes x to be added to a queue of expressions to be evaluated (if there is available time) before resulting in y
15:00:05 <Cale> That is, when you evaluate par x y, that's what happens.
15:00:21 <Bish> yeha, but there should be nothing else to do in my program
15:00:31 <takanuva> ski: it worked! thank you VERY much! :D
15:00:38 <Bish> and that also doesn't explain the ram explosion, i mean i only spawn 4 at a atime
15:00:48 <Bish> EXCEPT, haskell keeps going without them being evaluated
15:01:16 <Cale> Semantically, par x y is identical to y, but the performance may be different because one of the processes which is taking from the queue and evaluating the expressions it finds may evaluate x before it is needed (before it occurs somewhere inside y)
15:01:49 <Bish> confusing
15:02:33 <Cale> So, for example, we might try to define  parAdd x y = y `par` (x + y)
15:02:48 <Cale> and then if we parAdd two expressions which each take quite a long time to evaluate
15:03:09 <Cale> well, I should be careful, and write it like this:
15:03:18 <Bish> i think i understand...
15:03:21 <Cale> parAdd x y = y `par` x `pseq` x + y
15:03:26 <takanuva> Cale: thank you as well; it worked as expected :)
15:03:35 <Cale> takanuva: nice :)
15:03:47 <takanuva> this saved me hours of refactoring
15:03:54 <Bish> things will be evaluated if needed.. if i tell rts to do it parallel, it gets added to a queue instead (and evaluated accoridng to the strategy used)
15:04:15 <Bish> BUT, i only spawn 4 at a time
15:04:25 <ski> takanuva : sometimes staging might be a better solution. but would require more refactoring
15:04:43 <Cale> Bish: yeah, so there will be, say, 4 or 8 of these "capabilities" drawing stuff from the queue, and hopefully getting to these expressions before the main thread hits them and evaluates them first
15:04:53 <ski> (and it's not clear it would even work in your case)
15:04:56 <Cale> If the thing they find is already evaluated, they just grab another thing
15:04:59 <takanuva> ski: what do you mean by "staging"?
15:05:08 <Bish> oh, so that can even hapen. things getting "double-evaled" ?
15:06:02 <Cale> Bish: It's extremely rare for work to be duplicated, but there is actually a window of one or two clock cycles where if the main thread and a worker thread both try to evaluate the same expression, they'll duplicate the work
15:06:21 <Cale> Bish: That's typically statistically insignificant though
15:07:43 <Cale> In the threaded runtime, the first thing that happens when a thunk (runtime representation for an expression) begins evaluating is that the pointer to code which represents it is overwritten with a pointer to a "grey hole" which blocks, waiting for the result of the expression to become available
15:07:44 <Bish> so this is not whats happening in my cod?
15:08:18 <Cale> So typically, if a worker thread already started working on a spark, and the main thread needs the result, it will just block and wait for the result.
15:08:51 <Bish> yeah, but that would be totally fine?
15:08:56 <Cale> yes
15:09:00 <Bish> that would be like spawning 4 threads and c and joining them
15:09:07 <Bish> but that does not seem to be whats happening for my case
15:09:10 <Bish> normal map is way faster
15:09:54 <Cale> Well, your particular program spends most of its time allocating memory for strings
15:10:17 * hackage jammittools 0.5.5.2 - Export sheet music and audio from Windows/Mac app Jammit  https://hackage.haskell.org/package/jammittools-0.5.5.2 (mtolly)
15:10:36 <Bish> yeah, and why should it.. thats what i dont understand
15:10:42 <Bish> in my "map-only" version it doesnt do that either
15:11:44 <Bish> why would the threaded version allocate string space more?
15:11:52 <Bish> the only explaition i would have for it:
15:11:54 <ski> takanuva : let's say you write `foo :: A -> D; foo a = d where (c,d) = bar a (frob c); bar :: A -> B -> (C,D); bar a b = ..a..b..; frob :: C -> B; frob c = ..c..'. sometimes you can then reformulate `bar :: A -> B -> (C,D)' into `baz :: A -> (C,B -> D)' so that you can say `foo a = b_d (frob c) where (c,b_d) = baz a', without a cycle
15:12:19 <johnw> I have to say, one place where conduit/pipes really shines as being the Right Way to do things is when writing streaming client/server endpoints with servant
15:12:35 <Bish> that "concat" actually doesnt wait for the sparks to be evaluated
15:12:46 <Bish> and just keeps going adding threads.. which ofcourse "bind" the input string
15:12:52 <Cale> It doesn't.
15:13:05 <ski> @wiki Tying the Knot
15:13:05 <lambdabot> https://wiki.haskell.org/Tying_the_Knot
15:13:15 <ski> takanuva : also cf. that ^, if you haven't seen it before
15:13:24 <Bish> Cale: okay, then i dont get it at all :/
15:13:26 <Cale> parMap will go through and spawn sparks for every damn thing in the list, and as many of them as possible will be evaluated at once until you need them
15:13:51 <Bish> yeah, then i don't get whats in my ram
15:14:13 <Bish> i wrote the program like that in the intention of taking 4 words, calculating their hash.. and flatten the resulting list, so it gets created in parallel
15:14:13 <Cale> Well, mostly hashes of future passwords you might need to look at
15:14:45 <Cale> Bish: When the program runs single threaded, it kind of works okay because each hash and password becomes garbage the moment it's considered
15:15:12 <Cale> Bish: But you're basically telling your program to run ahead and compute as many of the hashes ahead of time as possible, before we get to comparing them against the target
15:15:36 <Cale> Bish: and so more and more of your memory is getting filled up with hashes of strings that need to eventually be checked against the target
15:16:02 <fen> ok, have done the ShuffleList gist
15:19:01 <Bish> Cale: but i tell it to calculate 4? how is it running ahead?
15:20:11 <Cale> Let's see your program again?
15:20:20 <Cale> (with the parallelism stuff left in)
15:21:51 <Bish> http://dpaste.com/2P9GZWS
15:22:01 <Bish> i chunk into 8 there
15:22:39 <Bish> and pardon my horrible code
15:23:13 <fen> Cale: https://gist.github.com/fen-hs/430e8d31b6ebb96e4c9cc3fcdd65f400#file-typelevel-shufflelist-hs
15:23:21 <Cale> So yeah, this is going to result in an arbitrarily large amount of garbage being created ahead of the main thread which is checking the strings to see if they match the target
15:23:45 <Cale> It's like you have 4 cores or 3.5 or something working on hashing strings
15:24:00 <fen> shuffle list!
15:24:07 <Cale> and then one core which is chugging along through the allocated queue of strings which is getting longer and longer ahead of where it is
15:24:41 <Bish> so.. there are more than 8 sparks "active" ?
15:24:47 <Bish> because it will run ahead and create more?
15:24:54 <Cale> fen: looks kinda silly to me
15:25:02 <fen> the problem is this way of IsTrue IsFalse and the many functions, one for each value, isnt going to be easy to do for things other than bool....
15:25:06 <Bish> it is, you should ignore him
15:25:10 <fen> Cale: thanks!
15:25:18 <fen> any tips on making it less silly?
15:26:08 <Cale> fen: Well, what is your program supposed to do?
15:26:12 <fen> ...
15:26:18 <fen> shuffle list!
15:26:25 <fen> for the thing with the other stuff before
15:26:52 <fen> generating inductive recursive instances 
15:27:00 <fen> Recursive*
15:27:02 <Cale> Is there a reason why consTrue and consFalse take an argument other than the list at all?
15:27:09 <fen> Corecursive* !
15:28:16 <Cale> also, these type families...
15:28:18 <Cale> Why?
15:28:24 <fen> ...
15:28:50 <fen> sure the IsTrue etc seems redundant as there is one function for each
15:28:57 <fen> so it could just take ()
15:28:59 <Cale> Like, here's what I'd expect, if we were going to be doing this thing... which I don't even think is remotely useful in the first place but whatever
15:29:00 <fen> or nothing at all
15:29:04 <Cale> Let me write some code
15:29:19 <fen> haha, thanks
15:29:25 <fen> it could be useful
15:29:40 <fen> maybe it doesnt need to be apparently useful in order to be actually useful
15:30:18 <fen> hmm, it would be generous not to just use that argument any time a challenge of expected utility is made
15:30:34 <Bish> Cale: so, are there 8+ of my chunks in queue?
15:30:41 <Bish> waiting to be evaluated?
15:31:30 <fen> Bish: wouldnt that depend on how fast they were put into the queue and how fast they were taken from it?
15:31:57 <Bish> fen you should get a better hobby than trolling inside here
15:32:16 <fen> eh?
15:32:33 <fen> am i supposed to know what that means?
15:32:44 <fen> or like, engage you on it or something
15:32:57 <fen> maybe so that any response would be off topic and prove you right?
15:32:59 <fen> piss off
15:33:11 <Bish> so i was right in thhe end. thats something
15:33:33 <Cale> Bish: fen isn't trolling, just... down a really deep rabbit hole of impracticality
15:33:51 <Bish> he is a regular customer?
15:34:13 <Cale> yes
15:34:27 <geekosaur> regular, and has a tendency to not understand the difference between theory and actual hardware
15:34:34 <geekosaur> which isn't unique
15:34:49 <Bish> well, but why does he do that.. i tend to not be able to have empathy there
15:34:55 <Bish> it always feels like acting a role to me
15:35:03 <geekosaur> it's not
15:35:20 <geekosaur> I spent >10 years working with EE grad students, they often have the opposite problem
15:35:33 <Bish> i read a lot of irony in what he writes.. like that he is talking aobut usefulness of programs.. putting things into a queue and out.. which sounds really procedual
15:35:36 <geekosaur> when you spend a lot of time learning <x>, you tend to think in it even when it's not appropriate
15:35:48 <Bish> geekosaur: but these usually don't join irc channels 
15:36:07 <MarcelineVQ> he's just, really bad at asking for help
15:36:09 <geekosaur> that very much depends
15:36:15 <MarcelineVQ> it is getting a little old though
15:37:53 <Bish> Church.hs, getting real templeOS vibes there
15:39:24 <Bish> geekosaur: yeah i am having that exact problem
15:40:05 <Bish> would you be so kind and answer that question i asked: if there are actually more than 8+ chunks waiting to be evaluated in my runtime?
15:40:08 <Bish> that would explain everything
15:40:35 <Cale> fen: Here's a more straightforward approach to what you're doing, I guess: https://gist.github.com/cgibbard/4c441e4314b9fd8e377dcdb5f6ea1625
15:40:57 <fen> Im having a problem with microphone latency... the "buffer overrun" error occurs when the latency is reduced by reducing the buffer size, this seems to mean the mic cant put data into the buffer as fast as it is retrived... or something
15:41:06 <Cale> fen: I still don't think it's a wise thing to be doing in the first place, under 99.9995% of all circumstances.
15:41:27 <fen> ok, Cale, your concerns are noted
15:41:33 <fen> thanks for your help
15:42:12 <Cale> I feel like we've been answering the wrong questions repeatedly, and would love to know where this all started
15:42:29 <fen> right, yeah sorry, forgot how to use singletons
15:43:25 <fen> Cale: its a long project, and its difficult to describe simply how it all hangs together. it will just be easy to assume it might be useful 
15:43:36 <Cale> It's not easy to assume that of this
15:44:15 <fen> if I have to recount the whole project, to justify each unorthodox aspect it utilises, this will be needlessly time consuming
15:45:19 <geekosaur> Bish, I didn't catch the context of that question beyond that it involves sparks and parMap. don't assume I tap some hivemind to know what's going on in here...
15:46:13 <fen> the very briefest summary is; we are growing a cyclic graph in a way that preserves its Hamiltonian path. itse represented as a tree, which is Free Nonempty. Nonempty is Foldable1. we discover a bunch of Foldable-like classes, and with to generate these algorithmically, requiring type level programming. 
15:46:34 <Bish> geekosaur: didn't mean you neccasserily, i just need to know this :D
15:46:44 <Bish> Cale was with meon that all th time
15:47:11 <Cale> Bish: There will be an arbitrarily large number of chunks waiting
15:47:22 <Cale> Bish: parMap sparks all the elements of the list
15:47:47 <Cale> Bish: in the hopes that some thread will get to evaluate each one before it's needed
15:47:49 <Bish> so.. i can imagine concat running away from me?
15:48:13 <Bish> putting new things in to the evaluation queue, while the previous ones are not yet evaluated?
15:48:28 <Cale> So, it's not even the concat, but the fact that immediately, you're enqueuing a very very large number of expressions
15:48:45 <fen> hmm, maybe it was "buffer underrun" cant recall
15:48:51 <Cale> and then 4 or 8 cores go to work evaluating those, and each of them does allocation of a ByteString
15:49:03 <Cale> and you eat memory really quickly
15:49:21 <Bish> somehow i expected parMap to "lock and wait"
15:49:33 <Cale> No, it just sparks the whole list
15:49:57 <Bish> but i thought i specify the strategy and use rdeepseq because it forces it to be evaluated
15:50:03 <Bish> thats what i thought alteast
15:51:08 <geekosaur> parMap is the do-it-all version. there are strategies that let you chunk things, which is a way to limit the number of sparks, and various other ways to control it
15:51:35 <geekosaur> or you can switch to explicit thread pooling and thereby directly limit the number of worker threads in the pool
15:51:56 <Bish> so i could fix my code by saying, there should be only >8 sparks?
15:52:06 <Bish> which would be a dirty way to do it, but still?
15:52:37 <Bish> <8*
15:53:41 <fen> oh, that paste shouldt have used BoundedList like that... its just a fixed length list... not a fixed length list below a sertain length
15:54:06 <Bish> how do i unzip a list? like, cutting [1..] into 4 peices [[1,5..],[2,6...],[3,7...],[4,8..3]]
15:54:30 <fen> wasnt that what this was for Bish? http://hackage.haskell.org/package/parallel-3.2.2.0/docs/Control-Parallel-Strategies.html#v:parListChunk
15:54:30 <Bish> is there functon that does that?
15:54:58 <Bish> oh, that looks cool, will try thhat out, thanks fen 
15:55:30 <Bish> but how do decide for a n?
15:56:06 <fen> what? to choose the blocks?
15:56:42 <fen> you dont want to allow too much into the buffer?
15:58:38 <fen> your trying to draw on the resources equally?
15:59:45 <Cale> Bish: I think what you actually want is a bit of concurrency, so that you can not only compute things in parallel, but also obtain the first result without having to trawl through all the failures
16:00:25 <Cale> btw, do you know what length of password this target is?
16:00:31 <Cale> (Is it 5?)
16:02:02 <Bish> 4
16:02:03 <Bish> it's 9999
16:02:26 <Bish> let me write that whole thing again
16:03:06 <Cale> haha, found it :)
16:03:18 <Cale> let's see how much time that took
16:03:25 <Bish> but how would i actually put concurrency in there, just out of curiousity
16:03:29 <Cale> 14 seconds
16:03:37 <Bish> yeah same for me :D
16:03:43 <Bish> the map only version takes 5
16:04:06 <Cale> oh
16:04:14 <Cale> hah, I was giving -N before
16:04:20 <Cale> But if I give -N8
16:04:27 <Cale> then it finishes in 1.9s
16:05:17 <Cale> https://gist.github.com/cgibbard/1ce9fbc455fd530032638b29978301ce
16:05:19 <Bish> oh interesting, it gets worse for me, the more core i say it should use
16:05:53 <Cale> Well, my program is optimised for 8 specifically
16:06:00 <Cale> I should getNumCapabilities :)
16:06:12 <Bish> phew, let me go through that
16:07:33 <Bish> when i came here.. i specifially wanted a forkIO version and got mocked for it..
16:07:57 <Cale> Nah, there are things you can do with forkIO which you can't really do with pure parallelism
16:08:14 <Cale> But it's worth trying pure paralellism a lot of the time
16:08:22 <Bish> but youre not saying i could write a version that does pure parallelism as fast?
16:08:27 <Bish> i couldn't*
16:08:37 <Cale> In this case, the nature of your search makes it hard
16:08:54 <Bish> why did u chose to get the "words" in such a weird way?
16:08:57 <Cale> You don't care which result you get, you just want the first one
16:09:09 <Bish> dropping, checking for not null? is that neccessary for optimization reasons?
16:09:10 <Cale> Because I wanted to split up the search space evenly
16:09:26 <Cale> Oh, that's just a way to cut the alphabet into n lists
16:09:38 <Cale> er, lists of length n
16:09:39 <Cale> rather
16:11:07 <Bish> but crazy beautiful code
16:11:13 <Bish> except for the weird block creation
16:11:32 <Cale> > ['A'..'Z']
16:11:34 <Bish> but that thread spawning put is pretty, everyone could read that
16:11:34 <lambdabot>  "ABCDEFGHIJKLMNOPQRSTUVWXYZ"
16:11:41 <Cale> > iterate (drop 8) ['A'..'Z']
16:11:43 <lambdabot>  ["ABCDEFGHIJKLMNOPQRSTUVWXYZ","IJKLMNOPQRSTUVWXYZ","QRSTUVWXYZ","YZ","","","...
16:11:52 <Cale> > takeWhile (not . null) . iterate (drop 8) $ ['A'..'Z']
16:11:54 <lambdabot>  ["ABCDEFGHIJKLMNOPQRSTUVWXYZ","IJKLMNOPQRSTUVWXYZ","QRSTUVWXYZ","YZ"]
16:12:03 <Cale> > map (take 8) . takeWhile (not . null) . iterate (drop 8) $ ['A'..'Z']
16:12:04 <lambdabot>  ["ABCDEFGH","IJKLMNOP","QRSTUVWX","YZ"]
16:12:17 <Cale> ^^ that's all it does
16:12:32 <Bish> yeah i understand it.. but thats a lot of stuff for something like that, isn't it?
16:12:34 <Cale> I consider that to be an idiom. I suppose we have Data.List.Split in scope
16:12:35 <geekosaur> this might be a place to note chunksOf
16:12:39 <geekosaur> yeh
16:12:40 <Cale> so I could have used chunksOf 8
16:12:40 <Bish> doesn't "chunksOf" do the same thing
16:12:43 <Cale> yes
16:12:45 <geekosaur> and cleverer stuff
16:12:49 <Cale> I just didn't notice it was in scope :)
16:12:56 <Cale> and reimplemented it in-place
16:16:11 <Cale> https://gist.github.com/cgibbard/1ce9fbc455fd530032638b29978301ce -- here's a new version
16:17:19 <Cale> I probably shouldn't worry about that bit of arithmetic
16:17:38 <Cale> well, it helps if we don't just lose a thread because we don't have enough work on one of them
16:19:22 <Cale> It still kind of allocates a lot...
16:19:36 <Cale> when I generalise it to search longer and longer strings
16:20:40 <Cale> oh, I was making decisions in the wrong order, nevermind :)
16:21:21 <Cale> https://gist.github.com/cgibbard/1ce9fbc455fd530032638b29978301ce -- there, now it searches for arbitrarily long passwords :D
16:23:10 <fen> Cale: you entitle the paste you addressed to me "foolishness" and have repeatedly asked for explanations of how this code is required, by what motivation... is it that I could ask also of you the justification for needing this? you write things like "tpye level stuff is normally unnesacary, hard to work with" etc as justifications, but other difficult things are not so commonly discouraged, so why take this demeaning tac?
16:23:20 <monochrom> I thought the project was due a week ago.
16:28:21 <Cale> fen: Well, because whatever it is, I think we can come up with a more reasonable and direct solution to it
16:28:50 <Cale> fen: Which probably doesn't put quite as much information at the type level
16:29:23 <fen> but its the argument to a class
16:29:38 <fen> its a parameter, that makes it use a different instance
16:29:43 <fen> thats exactly what a type is
16:29:50 <Cale> sure...
16:29:54 <Cale> But does it need to be?
16:29:56 <fen> we need to generate types in order to generate families of classes
16:30:05 <Cale> Or can we just have a term-level parameter?
16:30:15 <fen> the type checker could not use that
16:30:34 <fen> it would not serve as an equivalence class of containers if it were not visible at type leve
16:30:38 <fen> level
16:30:58 <Cale> It might though?
16:31:08 <Cale> You don't *need* to encode sets of things as types
16:31:12 <fen> wheres that hyloList paste, ahng on
16:31:20 <Cale> You can use Data.Set for instance
16:31:35 <fen> https://pastebin.com/raw/mDdjGUKL
16:31:44 <Cale> Or just enumerate an appropriate list
16:32:33 <fen> class Folding y f where  folding :: f a -> y a b -> b  
16:32:33 <fen> instance Folding CoState [] where
16:32:33 <Cale> That's a remarkably elaborate way to get [1,2,3]
16:32:49 <fen> data CoState a b = CoState (Maybe (a,b) -> b)
16:33:10 <fen> type Foldable_ f = Folding CoState f
16:33:23 <fen> here we see that the normal "Foldable" class is recovered
16:33:33 <fen> by supplying a *type* to the Folding class
16:33:46 <Cale> ... sure
16:33:53 <fen> we need to generate these types, to generate the corresponding classes and instances
16:33:57 <Cale> ... but why?
16:33:57 <Bish> holy crap.. last $ repM 8 alphetbet
16:34:02 <Bish> starts by printing "\""
16:34:08 <Bish> because it knows its a string? :D
16:34:10 <Cale> yes
16:34:14 <Bish> haha hilarious
16:34:19 <Bish> what a little detail
16:34:35 <Bish> i have nooooo clue what it is, but let me print " because im sure its a string
16:34:41 <fen> [00:33] <Cale> ... but why?
16:34:53 <Cale> fen: Why do we need all these instances?
16:35:08 <fen> this is a discouraging and unnecessary digression
16:35:10 <Cale> fen: This code looks more than a bit baroque to me as well
16:35:30 <Bish> oh i get it.. thats not even speiciaally coded... it's just the show function of string
16:35:38 <fen> that is a bizzare and incomprehensible comment!
16:35:41 <Bish> impressive.
16:35:57 <fen> am i supposed to give credence to these rebuttals!? 
16:36:19 <Cale> fen: What I mean is that anything we could accomplish by way of generating a Foldable instance for some weird type, we could also accomplish directly.
16:36:34 <fen> directly?
16:36:39 <fen> using recursion schemes?
16:36:41 <Cale> Like, we could just write the appropriate foldMap for the type that we need straight away
16:36:45 <Cale> Not using recursion schemes
16:36:56 <fen> its the same idea 
16:37:01 <ski> > let s = show s in s
16:37:03 <fen> you mean to give one class to each type
16:37:03 <lambdabot>  "\"\\\"\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"\\\\\\\\\\\\\...
16:37:08 <Cale> (or whatever part of Foldable you're interested in)
16:37:17 <Cale> I mean, maybe don't even use Foldable at all
16:37:23 <Cale> Just write a simple function
16:37:28 <Cale> which does the thing you wanted to do
16:37:28 <fen> the whole idea is to gather "similar" types
16:37:32 <fen> thats the point of a class
16:37:38 <Cale> Yeah, kinda
16:37:39 <monochrom> Bish: Yeah. Generally, print is based on show, and show is usually coded up (especially the built-in ones) to use programmer-friendly notation rather than enduser-friendly notation.
16:37:50 <Cale> Except how many similar types do you actually have? What are they?
16:37:59 <fen> by asking, as recursion schemes does, to give a unique "base functor" to each container
16:38:21 <Cale> In the end, which base functor are you actually plugging in?
16:38:22 <fen> is essentially giving the kind of parametrised "Folding" class of the paste
16:38:32 <fen> but with one "Folding" class per container
16:38:34 <Cale> Or is it somehow not determined?
16:39:12 <fen> your pluggin in a base functor that is isomorphic to the base functors of many Corecursive intstances
16:39:26 <Cale> I mean, for the program you're actually trying to produce
16:39:36 <Cale> which presumably does some sort of graph search things
16:39:54 <Cale> It's hard to imagine that there are all that many actual instances to worry about
16:40:07 <Cale> and we could just write the computation in a more direct way
16:40:09 <fen> we can cover all possible base functors, first by doing this, seeing which are equivalent to some type, and then seccond, by generating these "cannonical base functor types" using type level programming
16:40:12 <Cale> without needing all these abstractions
16:40:20 <fen> how you expect to achieve that without type level programming!?
16:40:38 <monochrom> People feel betrayed because beginners start with, and usually stop at, Int, and programmer-friendly notation for Int coincides with enduser-friendly notation, i.e., both look like 123.
16:40:38 <fen> its the only way
16:40:43 <Cale> I mean, I'm not convinced that we need to "cover all possible base functors"
16:41:22 <Cale> What happens to all this if we specialised to the one or two in particular that we care about?
16:41:34 <Cale> Or else, why do we care?
16:41:55 <Cale> Like, if we really do need it to work for all choices of functor, why is that?
16:42:22 <fen_> you seem to be encouraging a discussion of "why do we bother doing things better"
16:42:46 <fen_> either that or encouraging the precise description of the better approach being devised
16:42:56 <fen_> and one of those is significantly more useful
16:43:07 <Cale> In my experience, any case that recursion scheme stuff comes up, it turns out *so* much nicer if you avoid the abstract version, and just program the particular recursion schemes at each type, which lets you customise the production of the algebra, basically.
16:43:25 <fen_> your saying just use generics
16:43:28 <Cale> Or just don't use it at all, and just write the recursive function.
16:43:36 <Cale> No, not generics
16:43:38 <fen_> thats "dont use classes"
16:43:54 <Cale> Just hand-written definitions of the hylomorphism you're interested in
16:43:57 <Cale> or whatever
16:44:18 <Cale> It's going to be not as much code as all this setup, I guarantee :)
16:44:21 <fen_> so because generating these classes using type families is "difficult" you would rather not use classes at all, or generate all the boilerplate using generics
16:44:44 <Cale> I'd rather solve the actual problem of interest rather than fucking around with types :D
16:44:52 <fen_> im not convinced either of those arguments are really worth taking seriously 
16:45:17 <Cale> Until there's a good reason to believe this is an abstraction that we need to make, why make it?
16:45:54 <monochrom> Approximately the only important recursion schemes are catamorphism and anamorphism. The rest are ad-hoc.
16:45:57 <Cale> I mean, maybe it's fun to play around with all this stuff, but you yourself seemed to be getting frustrated with how absurd this Haskell programming had become
16:46:01 <fen_> basically, you dont like the idea of generating classes of types. you dont like recursion-schemes to begin with, and so see no point in improving on that abstraction
16:46:09 <fen_> this is not helpful
16:46:23 <fen_> families of classes*
16:46:26 <Cale> Well, are you writing a library for recursion schemes, or are you writing a program that actually does something?
16:46:39 <fen_> you dont like the idea of families of classes
16:46:49 <fen_> your only criticism is that it "does not do anything"
16:46:51 <fen_> wtf
16:46:53 <Cale> I kept hearing about extending Hamiltonian paths in graphs, which sounds more like something an actual program would do...
16:47:14 <fen_> ok, we can go over that *again*
16:47:16 <Cale> I don't hate the idea of using type classes when it makes sense to use them
16:47:26 <Cale> But I want to have a reason to do it in the first place
16:47:36 <Cale> Like, there should be at least 5 or so instances
16:47:39 <monochrom> Moreover, 90% of those ad-hoc recursion schemes were because they were futile attempts at solving the problem that Ralf Hinze really solved with catamorphism (or anamoprhism) plus adjunction.
16:47:50 <Cale> that I care about and that will save me the trouble of writing code over and over
16:47:57 <fen_> i dont like the idea of a discussion on the pretence of seeking answers and then rejects them when they are presented
16:48:50 <fen_> and if you dont like the idea of criticism to the line of questioning you take, and will insist on it, then you are simply being unhelpful
16:49:13 <Cale> fen_: Well, it feels to me like you started out with this problem, and then you found one abstraction which might be useful to apply to it, but not quite, so you got involved in another problem surrounding that, and then another, and we're a few levels deep into this thing when we should have taken a step back and approached the entire initial problem differently or something
16:49:40 <Cale> But I don't know what your overall goals are
16:49:55 <fen_> monochnom: the current appraoch simply seeks to strengthen the way we represent those things
16:50:00 <monochrom> Hey I gave Hamitonian paths on my exam!
16:50:20 <monochrom> It was really an excuse to make my students code up permutation.
16:50:41 <fen_> Cale: there are many instances, one for each depth of nesting
16:51:00 <fen_> an infinite number, required for a inductive instance
16:51:15 <fen_> requiring 
16:51:21 <Cale> fen_: But I dunno, that's just my take on it -- you're writing a lot of really weird code which I would be unhappy to see in production, and I kind of think that if your real goal is a practical one, we might be able to approach it a different way.
16:52:08 <Cale> I mean the *real* outermost goal, not just the one immediately one step up on the stack :)
16:52:11 <fen_> the real point we are discussing is why would you be unhappy to see this code in production
16:52:37 <fen_> with some calls to reexamine the fundamental thing to do with the cyclic graphs
16:52:45 <Cale> I'd be unhappy to see it because it's deeply unidiomatic to the point of being obfuscation.
16:52:48 <monochrom> My real outermost goal is to enjoy steak and desugared grape juice.  (Guess what I mean by that! :) )
16:53:01 <fen_> no, its to avoid lookup table representations of graphs
16:53:19 <monochrom> (P.S. One day I may also want to try evaporated/concentrated desugared grape juice.)
16:53:28 <fen_> jam?
16:53:36 <Cale> Well, there are really several different reasons I'd hate to see it
16:53:46 <fen_> because its never nesacary!
16:54:06 <fen_> because if you see it, it means that the programmer should reexaimine what they are trying to do
16:54:25 <fen_> because if you dont understand their explanation it must be their fault
16:54:39 <fen_> those are not good reasons!
16:55:04 <fen_> dry wine?
16:55:15 <fen_> mmm!
16:55:23 <Cale> The type level programming with singletons stuff, well, I've seen that kind of code cause so much grid-lock for no good reason, where someone enforces a bunch of constraints at the type level that guarded against bugs that nobody was worried about in the first place, and which resulted in multiplying the amount of development effort required for everything for the rest of the project.
16:56:02 <Cale> So, I'm always really suspicious as soon as the types get really fancy
16:56:11 <Cale> There *are* good uses for fancy types
16:56:11 <monochrom> If you invoke multiple XY abstractions just to get a 1st-year graph algorithm, then you're just doing a glorified version of "f x = g x; g x = h x; h x = i x; i x = 2*x+3".  I would then ask "why so many intermediate functions?".  My student would answer "because I thought I a few helper functions would help.".
16:56:16 <fen_> oh, because its difficult
16:56:18 <Cale> but there are plenty more bad uses
16:56:48 <fen_> and there are consequences of that for teams of people that find things difficult
16:56:48 <fen_> ok
16:57:20 <fen_> i can see the criticism of doing things that are difficult because of the possible negative consequences of doing things that are difficult
16:57:30 <fen_> thats fair
16:57:40 <fen_> but, its absent of context
16:57:48 <fen_> and ignores the advantages
16:58:07 <Cale> right, and then when the context is another "I'm doing something that's possibly more complicated than it needs to be"
16:58:17 <Cale> then I get *really* suspicious
16:58:24 <adamCS> fen, Cale: This conversation reminds me of the general tension between solving just the thing you have and abstracting and how far.  And Haskell is good because it encourages, invites, almost demands, abstraction.  And that way greatness and efficiency lie, But Haskell is also bad, because sometimes that abstraction is has lower power-to-weight ratio and sometimes because you ride it to a not-useful place, to an abstraction
16:58:24 <adamCS>  that is rarely more useful than your concrete case.
16:58:25 <fen_> so encouraging the reexamination of the project and its priorities
16:58:46 <fen_> which when are presented - are recieved with "that looks complicated though"
16:59:21 <fen_> forming a loop, where the explanation of a difficult concept is met with the criticisms of things which are difficult and a request for a repeated explanation
16:59:24 <fen_> ok
16:59:26 <Cale> adamCS: It's great because it won't prevent you from introducing the abstractions once you work out what they ought to be. It's bad because it gives you an almost infinite amount of rope with which to hang yourself in abstractions.
16:59:32 <adamCS> My experience--which is limited!--is that this is a difficult thing, like good writing or whatever.  It's hard to see clearly when you're in the middle of it.
16:59:33 <fen_> now why is that frustraiting
16:59:46 <fen_> you can criticise my difficult code, and i can be annoyed by that
16:59:50 <fen_> perfect
17:00:05 <Cale> I always recommend writing the program, or some simplified version of it, and getting it working end to end
17:00:14 <adamCS> Cale: yes. And when you are spooling out the rope it's hard to see that you are doing that and not solving something beautiful.  
17:00:21 <fen_> Cale: yes you do
17:00:30 <Cale> and only then starting to think about ways that you can introduce abstractions to avoid repetition
17:00:35 <fen_> you think that because things can be done, that there is no need to do them differently
17:00:43 <Cale> or admit the kind of adaptation that you're going to need
17:00:46 <fen_> and you cant justify that as a good approach
17:01:02 <Cale> adamCS: Yes!
17:01:26 <adamCS> Cale:  Been there.  Am often there.  Might be there right now.
17:01:28 <fen_> why would you presume that existing abstractions being insufficient wasnt the main reason more sophisticated abstractions were sought?
17:01:56 <fen_> why do you need a rundown of all the design choices to believe that there is the possibility it is worthwhile
17:02:12 <DigitalKiwi> monochrom: grape juice and steak is offtopic
17:02:14 <fen_> and then, when these are presented to you, say "that sounds difficult, its probably a bad idea"
17:02:28 <Cale> fen_: Well, unlikely claims demand larger amounts of evidence :D
17:02:41 <fen_> every day!?
17:03:15 <Cale> fen_: If you say "I ate a BLT sandwich for lunch today", I'm inclined to believe you without much evidence, because that's a reasonably common thing to eat
17:03:37 <adamCS> fen: Presumably someone's frustration is why we have such nice foldable and traversable abstractions.  And Lenses.  But also lenses.  And we tend not to see the failure cases as much because we don't publish those on hackage as much.  That being said, if you love chasing what you're chasing, keep chasing it!  
17:03:43 <adamCS> fen_: ^
17:03:51 <Cale> If you say "I ate an alligator's leg for lunch"... I might ask for some proof :D
17:04:27 <fen_> yeah, but then on seeing this proof to just ask for it again because it does not fit your way of thinking is stubborn
17:04:45 <Cale> I haven't seen anything particularly convincing yet
17:04:45 <adamCS> But I know I've discovered more than once that something I thought was such a nice way to generalize had led me to something so very complex that it coldn't possibly be useful.  And that was *always* hard to see until I had gotten pretty far in.
17:05:02 <fen_> or you could say "i dont believe this proof" and we are into some bizzare philophy thing
17:05:20 <fen_> and the i say, why are you so determined to convince me you dont understand what im doing?
17:05:23 <fen_> its not helpful
17:05:28 <fen_> thats the only conclusion
17:05:48 <DigitalKiwi> Cale: they have alligator farms it's not that rare but i don't know that you can say it was or was not an arm
17:05:55 <Cale> It's like I asked for proof that you ate an alligator's leg for lunch, and you provided me a drawing of an alligator that's the size of an office building.
17:06:13 <Cale> and then what am I supposed to think? :)
17:06:51 <DigitalKiwi> usually get it fried like chicken fingers/nuggets at a bar
17:07:16 <fen_> adamCS: every time i try to seek the higher level abstraction, which requires more sophisticated language machinery, which Cale seems afraid of, this machinery exists and manages to represent the desired abstraction. its one of the best things about haskell.
17:07:50 <fen_> Cale: if you want to understand the abstraction, saying abstractions are often unnecessary is an unhelpful approach. 
17:08:02 <DigitalKiwi> do alligators have arms or legs
17:08:37 <Rembane> Both imo
17:08:56 <gonz_> They
17:09:02 <gonz_> They're all legs, no?
17:10:19 <DigitalKiwi> no wonder they get used for shoes, each one has 2 pairs!
17:10:31 <fen_> so, specifically, which part of the justification of this abstraction are you not getting here?
17:11:58 <fen_> or do you just want to talk about alligators? 
17:13:12 <randomWinNoob> Hello. Im a noob concerning Haskell. I dont know how to import System.Random under Windows. Under Debian the following code works fine : https://pastebin.com/HsdpC30W
17:15:31 <fen_> [00:59] <adamCS> Cale: yes. And when you are spooling out the rope it's hard to see that you are doing that and not solving something beautiful. 
17:16:06 <Bish> Cale: thanks for all your help, appreciate it
17:16:08 <fen_> if he is indeed simply being contrary, for no good reason - the only valid response is to call him out on it
17:16:19 <fen_> clear?
17:16:49 <DigitalKiwi> can we talk about alligators instead of being rude to cale
17:16:58 <randomWinNoob> im new to haskell. can someone here help me with a problem please?
17:17:05 <Rembane> randomWinNoob: You need the random package. How did you install Haskell?
17:17:16 <Bish> randomWinNoob: just ask :)
17:17:27 <fen_> out DigitalKiwi out
17:17:39 <DigitalKiwi> u first
17:17:49 <fen_> why thank you!
17:17:51 <randomWinNoob> i got it from here: https://www.haskell.org/platform/windows.html the 64bit version
17:19:34 <Rembane> randomWinNoob: Cool. I wonder if that includes cabal, I haven't used Haskell on Windows for ages. 
17:20:40 <Bish> Cale: one last question, how would that solution look if i was searching for multiple hashes?
17:21:09 <Bish> sure u can read from a channel multiple times, but how do i know "it's over"?
17:21:22 <randomWinNoob> on the same site you can download the cabal package source package :random-1.1.tar.gz . do I need this?
17:22:08 <Bish> do this:
17:22:10 <Bish> win+r
17:22:11 <Bish> cmd.exe
17:22:14 <Bish> cabal install random
17:25:46 <Cale> Sorry, dinner arrived and I had to go eat
17:27:06 <Cale> Bish: You could perhaps store the set of hashes that hadn't yet been solved in an IORef or something, and use Set.member instead of an equality test
17:30:04 <joebobjoe> I feel like everybody knows functional programming these days so there is no point to pursue my interest in it anymore
17:30:36 <Rembane> joebobjoe: Do you only learn things that nobody knows?
17:32:21 <Cale> nevermind that isn't remotely true
17:33:26 <randomWinNoob> You have to type something like "cabal v2-install..." now. There seems to be a new project style.
17:34:14 <Rembane> Almost everything is the same except for the name of the commands.
17:34:19 <Solonarv> the old style is still there, but, well, there's a reason we have a new set of commads
17:35:44 <randomWinNoob> but I still dont get the command right. it says https://pastebin.com/ga2FYxKV
17:36:42 <randomWinNoob> if I run the programm it still puts the same error. But thx so far!
17:36:52 <Solonarv> the error message suggests a fix already: add --lib to the command
17:36:59 <Solonarv> i.e: cabal install --lib random
17:38:16 <randomWinNoob> yeah I tried it already like you suggested and in other possible ways but its not right. :cabal: unrecognised command: instal (try --help)
17:39:15 * Clint golfclaps.
17:39:16 <jle`> can we get a version of the cabal binary with only new-style commands
17:39:20 <jle`> maybe call it cabal-new
17:39:22 <Solonarv> randomWinNoob: that's a typo
17:39:30 <jle`> so we can write cabal-new install instead of cabal new-install
17:39:33 <Solonarv> jle`: cabal-head defaults to the newstyle commands
17:39:36 <jle`> :O
17:39:42 <Solonarv> i.e. with cabal head 'cabal foo' means 'cabal v2-foo'
17:39:48 <jle`> this is amazing
17:39:56 <Solonarv> it is!
17:40:09 <monochrom> You just need a commuting digram so that " " and "-" commute over "new".
17:40:34 <Solonarv> also fixes some really annoying things on windows, like installing exes and scripts basically not working at all
17:40:48 <Solonarv> randomWinNoob: you typed instal instead of install
17:40:50 <jle`> new-cabal new-install
17:41:04 <randomWinNoob> ok guys I found a solution: I used v1 instead and there it worked without a problem! thank you all
17:41:17 <monochrom> I still like v1.
17:41:33 <Solonarv> monochrom: really? what shortcomings do you see in v2?
17:41:40 <Clint> nix-style builds, for one
17:41:48 <jle`> v1 is still useful, but not as a default
17:42:03 <monochrom> docs are scattered.
17:42:06 <Clint> v1 is crucial for traditional packaging
17:42:07 <dmwit> randomWinNoob: You just wrote `instal` instead of `install`...
17:42:08 <randomWinNoob> first time i used so 100% more effective compared to v2 xD
17:42:27 <randomWinNoob> ups im sorry
17:42:32 <jle`> it's like why we use both vectors and quaternions
17:42:37 <jle`> different uses for different situations :)
17:42:37 <monochrom> Disk usage goes against my partitioning scheme.
17:43:08 <monochrom> I.e., I want /home to stay small, /usr/local to stay large.  v2 is against this.
17:43:30 <monochrom> YES THAT'S RIGHT I ACTUALLY WANT CABAL V1-INSTALL --GLOBAL
17:43:39 <Solonarv> I guess you could use symlinks, but then you'd end up with a rat's nest of symlinks
17:43:46 <Solonarv> and that seems like a Bad Idea
17:44:06 <randomWinNoob> thx dmwit. now I know v2 works also just fine
17:44:40 <Bish> Cale: thanks :*
17:44:45 <randomWinNoob> good night. what a great first impression of this irc
17:46:26 <monochrom> I gusss .cabal/config contains enough variables to redirect v2 to /usr/local or whevever you/I want.
17:47:03 <monochrom> However the completely discoherent ad-hoc scattered doc location is still a problem not a solution.
17:47:51 <fen_> how can you discourage leading responses which encourage off topic discussion?
17:48:01 <monochrom> Clearly, this is because the whole bloody premise of v2 is discoherent ad-hoc scattered.
17:48:02 <fen_> without doing so..
17:48:20 <fen_> is there an acronym for this?
17:49:52 <fen_> that a simple question of channel moderation right?
17:49:59 <fen_> its not offtopic as such?
17:50:00 <Clint> an... acronym?
17:50:03 <fen_> idk
17:50:14 <fen_> some really short way of saying something, a name
17:50:40 <fen_> gtfo or something
17:51:10 <dmwit> Discussions of channel moderation are not really on-topic here, no.
17:51:39 <fen_> does that count? can i say gtfo here?
17:52:09 <fen_> well im not going to argue about it so...
17:52:10 <dmwit> I would recommend that you do not say "gtfo" here.
17:52:25 <fen_> talk to the hand
17:53:05 <fen_> there must be some way to not encourage long and pointless discussions on the validity of argument
17:53:45 <jle`> there's an option if you are one of the participants; you can withdraw your participation
17:54:02 <fen_> my alligator is hungry brb
17:54:06 <dmwit> Let's return to Haskell-related topics now.
17:54:15 <fen_> thats a good one
17:54:41 <fen_> yeah, but then the person encouraging a frustrated but haskell related thing would be exempt
17:55:03 --- mode: ChanServ set +o dmwit
17:55:45 <dmwit> Let me be as clear as I can be: that was not intended to be a discussion point of how to do moderation. It was intended to be moderation. Discussions of how to do moderation can go elsewhere; in here, the topic is Haskell.
17:56:03 <fen_> understood
17:57:57 --- mode: dmwit set -o dmwit
17:59:00 <fen_> in the spirit of de-escalation i shall leave, to return later with more-awesome code
17:59:09 <fen_> sry for the fruckus
17:59:46 <Clint> is there a non-awkward way to use fold on a Nothing to get a 0?
17:59:57 <dmwit> > sum Nothing
17:59:59 <lambdabot>  0
18:00:20 <Clint> dmwit: cool, thanks
18:59:57 <Welkin> we lost 450 people
19:00:43 <Solonarv> huh, wonder what happened
19:00:58 <Welkin> the weekend maybe
19:01:26 <fen> mass civil disobedience in response to the assange arrest? 
19:01:27 <Welkin> https://haskell.jp/
19:01:29 <Welkin> best website ever
19:01:33 <Rembane> Matrix down? 
19:01:46 <Welkin> a rabbit and a frog are making mochi
19:03:57 <fen> the "generics approach" is that you would never need two isomorphic datatypes. the approach of taking the quotient under this isomorphism to give equivalence classes of datatypes, specifically instances of Recusive and Corecursive, is to serve as alternative to this, so that isomorphic datatypes can use common functions via 
19:04:13 <fen> type parametrised classes using the cannonical base functor which defines the equivalence class - namely, the base functor of the cannonical "generic" alternative datatype
19:07:52 <OmegaDoug> I'm seeing a type error when trying to obtain a TChan in my main function. the TChan is in the STM monad and main is in the IO monad and it's conflicting: https://gist.github.com/DouglasBrunner/1e5e12a4f330baf953f4fed70307555f
19:08:08 <OmegaDoug> Any pointers on how to resolve this would be appreciated
19:09:10 <Rembane> OmegaDoug: Can you run the STM monad? 
19:10:02 <OmegaDoug> Rembane: It's all my code, so I can. How would I go about that? Is that not what "atomically" is doing?
19:10:09 <Welkin> OmegaDoug: here's some pointers (void* a) (int* b) (char* c)
19:10:39 <Rembane> Welkin: You are being way too funny.
19:11:01 <OmegaDoug> Welkin: That one was so funny I forgot to laugh
19:11:11 <fen> :t forkIO
19:11:12 <Rembane> OmegaDoug: It is exactly what atomically does. 
19:11:13 <lambdabot> error: Variable not in scope: forkIO
19:11:56 <fen> % :t forkIO
19:11:56 <yahb> fen: IO () -> IO ThreadId
19:12:16 <Rembane> OmegaDoug: Put atomically before newTChan and that line will type check. 
19:12:32 <geekosaur> but that newTChan also wants to be in an "atomically". that said, there's newTVarIO as a special case for this particular usage (actually a different one where atomically won't work, but that doesn't make it unusable here)
19:13:10 <geekosaur> er, TChan,. wait
19:13:23 <geekosaur> that one won't have the special case, I think, so use atomically
19:13:51 <geekosaur> except it looks like there is one for that too
19:14:00 <geekosaur> but atomically is the correct way to do it
19:14:15 <OmegaDoug> Rembane: Ok. I made the changes and it works
19:14:16 <OmegaDoug> THanks
19:14:29 <Rembane> OmegaDoug: Sweet! Good luck! 
19:18:29 <fen> % :t atomically
19:18:29 <yahb> fen: STM a -> IO a
19:18:55 <fen> % :t atomically newTChan
19:18:55 <yahb> fen: IO (TChan a)
19:19:06 <fen> hmm
19:19:09 <fen> whats that for?
19:19:12 <Rembane> It's a natural transformation. 
19:19:20 <Rembane> Puts STM into IO
19:19:22 <fen> oh yeah, so it is
19:19:57 <fen> so how come TChan is returned into an STM monad?
19:20:08 <fen> and here is to be used in and IO monad
19:20:17 <fen> is STM like a more restricted version of IO?
19:20:32 <fen> and to use IO functions it needs to be run "atomically" ?
19:21:23 <fen> maybe it has something to do with what is it a TChan
19:22:05 <fen> maybe the better question is what we can do within the STM monad
19:23:03 <fen> its nice how "forever" seems to produce a stream
19:23:14 <Rembane> I find the STM docs to be quite good to explain this: https://hackage.haskell.org/package/base-4.12.0.0/docs/GHC-Conc.html#v:atomically
19:26:39 <fen> Welkin: why is the rabbit about to hit the frog with a long mallet? 
19:26:55 <Welkin> hahahaha
19:27:00 <Welkin> they are making mochi
19:27:08 <Welkin> that's how they do it
19:27:23 <Welkin> a wooden pot and a large wooden mallet
19:27:40 <fen> thats so cruel!
19:27:47 <Welkin> one person flips/stretches the mochi with their hand and then pulls away quickly before the mallet comes down on it
19:28:04 <fen> oh so the frog is safe
19:28:12 <Welkin> https://www.youtube.com/watch?v=tmSrULDVRPc
19:28:20 <Welkin> fast mochi making
19:28:45 <fen> i like the "events" one best, "and more" is quite good too
19:29:58 <fen> baaahhhhh!!! that video! the dude makes no effort at all to not get hit with the mallet!!!
19:31:50 <fen> not quite sure how thats a good depiction of the process of a collaborative wiki contributions
19:32:42 <fen> i suppose their random shouting is supposed to coordinate the not getting hit with the mallet, but it seems basically ineffective! 
19:32:53 <Welkin> japanese websites are the best
19:33:02 <Welkin> also, japanese people love mascots
19:33:06 <Welkin> they really do make everything better
19:33:58 <Welkin> I've noticed japanese websites tend to have a lot of character compared to the soul-less "theme I bought on themeforest" websites in english
19:34:13 <Welkin> or "corporate blue and white" websites
19:34:36 <fen> those are indicative of companies without employees 
19:35:21 <Welkin> the web is so sterile these days, not vibrant like in the 90s
19:35:34 <Welkin> walled gardens don't help
19:36:15 <fen> scrolling style "impact - business - streamlined - etc" and then a footer with "contact, empolyment" and you click employment and its like "this is a fake company, you cannot work for us"
19:37:34 <fen> and your like, "but i thought this was the website of the US state department"
19:38:24 <Squarism> lavalike, FYI, i seem to have found the way. I could do "sum_ " on booleans. 
19:50:42 <fen> @quote fen
19:50:42 <lambdabot> fen says: how slow are you trying to be!?
19:50:47 <fen> oh
19:51:43 <fen> @quote fen
19:51:43 <lambdabot> fen says: unnecessary complexity could be discouraged, but that might encourage an otherwise unnecessary justification.
21:30:50 <nshepperd> I used cborg+serialise packages a little today
21:31:12 <nshepperd> seems nice, pretty efficient
21:31:27 <glguy> I'm also a fan
21:31:38 <nshepperd> I like that i can now use cbor-tool to inspect the structure of my serialised stuff
21:31:45 <glguy> I've been using cborg to make it easy to do cross-language serialization, too
21:31:47 <nshepperd> instead of it just being an opaque mystery
21:32:04 <glguy> between Rust, C++, and Haskel
21:32:05 <glguy> l
21:32:16 <clever> nshepperd: i should look into cbor-tool, ive been using the npm cbor library to convert junk to json, lol
21:32:35 <clever> and the problem is that cbor has 2 ways to encode a list, and my decoder only accepts one, but npm cbor only makes the other!
21:32:36 <nshepperd> @hackage cbor-tool
21:32:36 <lambdabot> http://hackage.haskell.org/package/cbor-tool
21:34:27 <slack1256> Is it and alternative to asn1?
21:34:58 <glguy> asn1 is significantly more flexible
21:35:05 <glguy> but there are some areas of similarity
21:35:18 <slack1256> it is a restricted subset of the functionally of json it seems, nice
21:36:04 <clever> one issue ive had in cbor, is that you can encode a list as either <list of 2 items><item1><item2>
21:36:15 <clever> or <list of unknown><item1><item2><end-of-list>
21:36:55 <clever> the parser i was working with only accepts the 2nd type, and cant accept both because then it would be capable of mutating data when you encode(decode(x)) and hashes wont align right
21:37:21 <clever> and it makes sense for an encoder in haskell to use the 2nd form for lists, because they are linked lists, and the length can be unknown when starting
21:37:47 <clever> which is also why it makes sense for a JS encoder to use the 1st form, since lists arent linked, and the length IS known ahead of time
21:37:55 <clever> and then the conflict arises!
21:38:10 <clever> i just gave up on JS and did everything in haskell :P
21:38:34 <slack1256> hahah
21:39:00 <slack1256> woulnd't the first form make sense for fixed length `Vector`s?
21:39:26 <nshepperd> so you need a canonical encoding?
21:39:43 <nshepperd> yeah, i think the haskell instance for Vectors puts the length up front
21:39:48 <clever> slack1256: which form my codebase uses, depended on the types
21:40:02 <clever> for [], it used the list of unknown size encoding
21:40:09 <clever> for maybes, it was either a list-of-0, or a list-of-1
21:40:33 <clever> i think tuples where a list of known size also
21:40:43 <clever> so (a,b) became a list of 2
21:41:02 <clever> so it heavily depends on the types being (en|de)coded
21:49:51 <nshepperd> the one mistake i made so far with it is forgetting to make sure that the `encode` in my Serialise instance encodes my stuff as a single object
21:52:07 <nshepperd> ie. if my thing has multiple fields, write 'encode (x, y, z)' instead of 'encode x <> encode y <> encode z'
21:53:06 <nshepperd> former writes it as a list of three items
22:06:39 <fen> here is more-awesome code;https://gist.github.com/fen-hs/430e8d31b6ebb96e4c9cc3fcdd65f400#file-staten-hs
22:07:08 <fen> it is the "cannonical base functor"
