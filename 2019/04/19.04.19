00:25:17 <hololeap> ski: can you explain what you mean by *>
00:27:10 <bahamas> is it correct to say that effects are the way your program interacts with systems outside itself?
00:29:54 <Cale> hololeap: basically  data c *> t where WithDict :: c => t -> (c *> t)
00:30:31 <Cale> bahamas: sure, though the term "effect" means a lot of things in different contexts
00:32:07 <Cale> bahamas: So while that statement is almost always going to be true to some degree, it might not be sufficient to define the term "effect"
00:32:32 <bahamas> Cale: can you tell me what's missing? or clarify in any way you see fit
00:32:53 <Cale> For example, people often consider the nondeterminism of the list monad to be an "effect"
00:33:08 <Cale> > do x <- [1,2,3]; y <- [4,5]; z <- [6,7,8]; return (x,y,z)
00:33:10 <lambdabot>  [(1,4,6),(1,4,7),(1,4,8),(1,5,6),(1,5,7),(1,5,8),(2,4,6),(2,4,7),(2,4,8),(2,...
00:33:25 <Cale> Here, we're picking x, y, and z in all possible ways from each of the corresponding lists
00:33:42 <bahamas> Cale: and what is the effect here?
00:33:48 <Cale> The "picking"
00:33:55 <bahamas> I guess I don't know what nedeterminism means in this context
00:34:00 <bahamas> *non
00:34:13 <Cale> It means "doing things in all possible ways"
00:34:25 <Cale> e.g. like a nondeterministic Turing machine
00:35:07 <bahamas> but why is it nondeterministic if it will always return the same list?
00:35:40 <bahamas> I'm using the definition here https://en.wikipedia.org/wiki/Nondeterministic_algorithm maybe nondeterministic Turing machine means something else
00:36:10 <Cale> Yeah, the first few sentences there are misleading
00:36:29 <Cale> "This property is captured mathematically in "nondeterministic" models of computation such as the nondeterministic finite automaton. In some scenarios, all possible paths are allowed to run simultaneously."
00:37:20 <Cale> Or, I should have included the sentence immediately prior to that
00:37:27 <Cale>  If a deterministic algorithm represents a single path from an input to an outcome, a nondeterministic algorithm represents a single path stemming into many paths, some of which may arrive at the same output and some of which may arrive at unique outputs. 
00:38:19 <bahamas> still, in the case of the list monad above, you always get the same result no?
00:38:24 <Cale> yes
00:38:41 <Cale> You get the same list of possible results
00:39:11 <Cale> x, y, and z are selected differently for each
00:40:20 <bahamas> anyway, I feel we're going down a deep rabbit hole. my motivation for asking to question was to compare how haskell handles effects as opposed to imperative languages
00:40:26 <Cale> > do x <- [1..20]; y <- [x..20]; z <- [y..20]; if x^2 + y^2 == z^2 then [(x,y,z)] else []
00:40:28 <lambdabot>  [(3,4,5),(5,12,13),(6,8,10),(8,15,17),(9,12,15),(12,16,20)]
00:40:39 <Cale> anyway, sure
00:41:15 <bahamas> so can we say that all interactions of a program with other systems are done through effects but not all effects are interactions of a program with outside systems?
00:41:47 <Cale> I suppose, though I still feel a bit awkward about that
00:42:11 <Cale> Maybe it's fine
00:43:24 <Cale> bahamas: Perhaps I should just start by introducing the IO type
00:45:23 <Cale> In Haskell, given any type of values t, we have a type (IO t) whose values are essentially programs that describe a program that may do just about anything your computer can do, before resulting in a value of type t
00:46:04 <Cale> You can imagine that the values of type IO t are effectively the syntax of those programs, or the binary for them, or something like that.
00:46:45 <Cale> Simply evaluating an expression for a value of type IO t won't do much but heat up your CPU and use some memory
00:47:04 <Cale> But if it's then executed, which is separate, it will carry out the described effects
00:47:10 <Cale> :t getLine
00:47:11 <lambdabot> IO String
00:47:15 <Cale> :t putStrLn
00:47:16 <lambdabot> String -> IO ()
00:47:26 <bahamas> Cale: what's the difference between evaluation and execution?
00:47:48 <Cale> evaluation is the process of reducing expressions to values (for the primary purpose of pattern matching on the result)
00:48:17 <Cale> Execution refers to carrying out the actions described by these values of type IO t.
00:49:27 <Cale> If you've ever heard anyone say that Haskell "doesn't have side effects", what they're referring to is that evaluation doesn't cause anything to happen apart from a result being computed.
00:49:54 <Cale> (It doesn't read from or write to the network, disk, or terminal, for example)
00:50:17 <Cale> Execution of IO actions is where all that stuff happens
00:50:54 <bahamas> Cale: yes, this part I understand. that side effects are encoded in types. or, to use your words, side effects are the actions described by some values. is that right?
00:51:26 <Cale> Well, it's almost improper to call them side effects at that point, because the sole purpose of those values is to encode actions to be performed
00:51:57 <Cale> The evaluation of IO actions still doesn't accidentally cause the effects to go off -- you really have to tell them to be executed
00:52:18 <bahamas> ok, so effects instead of side effects :)
00:52:18 <Cale> The way that happens is that, well, initially, the IO action called 'main' will get executed
00:52:35 <Cale> and then in turn, IO actions can say "execute this other IO action, and do something with its result"
00:52:38 <Cale> yeah
00:52:50 <Cale> For example, I can write a program like
00:52:52 <Cale> main = do
00:52:55 <Cale>   x <- getLine
00:53:00 <Cale>   putStrLn (reverse x)
00:53:21 <Cale> The do-expression is a way of gluing multiple actions together into a single one
00:53:42 <bahamas> so getLine and putStrl return two different values. the runtime knows how to execute these actions. but they both belong to the IO type
00:54:11 <Cale> Well, getLine simply *is* a value, which describes the action of getting a line of text
00:54:32 <Cale> 'x <- getLine' means "execute the action getLine, and call its result x"
00:54:45 <bahamas> isn't getLine a function that returns the value or functions are values?
00:54:54 <Cale> no, getLine isn't a function
00:54:55 <Cale> :t getLine
00:54:56 <lambdabot> IO String
00:55:03 <Cale> Its type doesn't have an -> in it
00:55:08 <bahamas> oh, right
00:55:16 <Cale> putStrLn is a function however
00:55:29 <Cale> It takes a String, and produces the action for printing that string on the terminal
00:55:34 <Cale> :t putStrLn
00:55:35 <lambdabot> String -> IO ()
00:55:45 <Cale> (which is an action that produces an empty tuple when executed)
00:55:50 <bahamas> are there values that don't have actions associated?
00:56:01 <Cale> What do you mean?
00:56:38 <Cale> I think the answer will be "yes", but I'm not entirely sure what question you're asking :)
00:56:54 <Cale> For example, we can write a perfectly sensible function like:
00:57:03 <Cale> square :: Integer -> Integer
00:57:07 <Cale> square x = x * x
00:57:29 <bahamas> above I asked about the difference between evaluation and execution. you said that evaluation reduces expressions to values. execution carries out the actions described by the values
00:57:30 <Cale> and this is simply a function, independent of any I/O
00:57:45 <Cale> ah, yeah, described by IO action values specifically
00:58:03 <Cale> There are many other types of values which don't describe IO actions (most of them)
00:58:29 <Cale> e.g. functions, integers, lists of strings, ...
00:58:34 <bahamas> IO actions are the only kind of actions? that's what I'm trying to understand
00:58:48 <Cale> ah, there are in fact other types of actions
00:59:19 <juri_> what is (# a,b #) ? 
00:59:24 <Cale> as well as a common interface for things which behave like "actions of some sort that can be glued together"
00:59:31 <Cale> An unboxed pair
00:59:42 <juri_> Cale: thanks.
01:00:04 <bahamas> Cale: can you give me some examples of oher types of actions? the interface you're referring to is the monad, I assume
01:00:07 <Cale> https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/glasgow_exts.html#unboxed-tuples
01:00:14 <Cale> bahamas: Right, yeah
01:01:22 <Cale> bahamas: For example, some libraries define a type  Parser t  for things which can consume some initial portion of an input string, and, providing they succeed, produce a value of type t as the result of parsing
01:02:00 <Cale> and depending on the parsing library, there might be nondeterminism involved, and there will certainly be some keeping track of the remaining input
01:03:10 <Cale> bahamas: Another flavourful one is that we have a library for what's called software transactional memory, or STM, for helping threads communicate
01:04:10 <bahamas> Cale: yes, I've heard of it
01:04:12 <Cale> The idea of STM is to provide some "transactions" which either run to completion in such a way that their effect appears like it happened at a single point in time with respect to all other threads (i.e. it is "atomic"), or they may fail
01:04:46 <Cale> or they may ask to be retried at a later point in time, when something they read along the way has changed
01:05:16 <Cale> In this sort of thing, it's *very* important that STM transactions can't do anything which isn't reversible
01:05:30 <bahamas> Cale: one other thing I wanted to ask. are IO actions the only ones implemented in the runtime or are there are other kinds?
01:05:43 <Rembane> Fire ze missiles
01:05:49 <Cale> So, we have a special STM monad where the only effects you can express are memory effects on special sorts of mutable variables and arrays
01:06:06 <Cale> bahamas: STM is also implemented at a primitive level
01:06:17 <bahamas> aha, ok
01:06:57 <bahamas> I guess that's why Haskell has a runtime, as opposed to PureScript. or I guess PureScript's runtime is the browser (or node)
01:07:02 <Cale> So, there are other monads we can build out of things like IO and STM, and also other monads, like Parser, which don't need either of those, and are just built out of ordinary values
01:07:15 <Cale> Well, we could also define IO in Haskell itself
01:07:28 <Cale> The bit which would be tricky and circular would be defining the execution of IO actions
01:07:47 <Cale> I can define a type for describing just terminal I/O like this:
01:08:05 <Cale> data TermIO a =
01:08:19 <Cale>     Done a -- Perhaps the action is finished with a result of type a
01:09:00 <Cale>   | PutChar Char (TermIO a) -- Perhaps it starts off by writing a single character to the terminal, and then continuing with another action
01:09:25 <Cale>   | GetChar (Char -> TermIO a) -- Or perhaps it starts off by reading a character from the terminal, and deciding how to continue based on that
01:09:57 <bahamas> and something needs to handle these different values: PutChar and GetChar. Done feels different, because it's what that something would return, no?
01:10:20 <Cale> Well, we can write a program which interprets these TermIO actions into IO actions:
01:10:26 <Cale> execute :: TermIO a -> IO a
01:11:02 <Cale> execute (Done a) = return a -- NB:  return :: a -> IO a  is a function which produces an action that "does nothing" except to return the given value
01:11:23 <Cale> execute (PutChar c x) = do putChar c; execute x
01:11:41 <Cale> execute (GetChar f) = do c <- getChar; execute (f c)
01:11:47 <bahamas> right. that's why return doesn't exist in PureScript and `pure` is preffered
01:12:00 <Cale> Oh, well, that's just a naming convention
01:12:18 <bahamas> naming is a convention :D
01:12:36 <Cale> return *does* make sense as a name for that function
01:12:53 <Cale> It's just that it might be surprising if you expected it to have an additional control-flow effect
01:13:14 <bahamas> you mean the meaning of return from imperative languages?
01:13:16 <Cale> (but really, that control flow effect it has in many imperative languages is a pretty surprising thing to begin with)
01:13:18 <Cale> yeah
01:13:48 <Cale> In imperative languages, it often calls the continuation of the procedure that's currently being defined
01:14:16 <Cale> So to a Haskeller, it feels a bit like everything has been implicitly wrapped in callCC :D
01:14:27 <bahamas> ok, I feel that I'm reaching information overload. I'll have to mull over this and make better sense of it.
01:14:39 <bahamas> thank you for taking the time to explain all this
01:14:40 <Cale> but anyway, yeah
01:14:57 <Cale> You can imagine that the IO type itself is some more elaborate description of what external calls to make
01:15:09 <Cale> and there's some executor which pattern matches and carries them out
01:15:37 <bahamas> is the executor for IO implemented in C?
01:15:40 <Cale> In GHC, they don't do it like that, but that's just a matter of choice -- turns out if you cheat, you have a good bit less compiler to write
01:17:07 <Cale> In GHC, internally, actions of type IO a are implemented internally as functions of some type which looks like  Token -> (Token, a)
01:17:30 <Cale> but then the "functions" there are simply allowed to have impurities, and their evaluation will cause effects
01:17:53 <Cale> and the passing around of these 0-byte tokens is used to string them together and make sure they don't cause trouble too soon
01:18:23 <Cale> which is a bit of an awful hack, but it means that the compiler can leverage all its existing dependency analysis and such
01:19:03 <geekosaur> bahamas, the runtime is partly in C and partly in cmm (and a few things in asm). IO itself is in Haskell, but makes use of compiler internals (primops) whose implementation is generally in C or cmm
01:19:03 <Cale> It doesn't need to know special rules for simplifying compiled IO actions
01:19:14 <Cale> yeah
01:19:36 <geekosaur> and which live in the runtime, because many of them manipulate its state in some fashion
01:19:59 <geekosaur> and quite a few OS service calls are simply foreign calls to libc functions
01:20:13 <bahamas> yeah, I assumed that something has to make those syscalls
01:21:34 <Cale> But yeah, it would be a perfectly good mental model to imagine that there's some C code, or machine code, or previously-compiled Haskell code, which is in the runtime, and pattern matching on your IO actions and interpreting them
01:21:52 <Cale> (apart from the fact that the compiler is able to do better optimisation than you might expect from that)
01:23:19 <Cale> That pattern matching would then kick off all sorts of evaluation, and once each bit of the program was evaluated enough to match a pattern for some primitive, it would carry it out, and recurse
01:24:10 <Cale> (but yeah, GHC disposes with that layer of abstraction -- IO actions are opaque anyway, so they can stick whatever hackery they want in there)
01:24:26 <bahamas> Cale: since you mentioned evaluation again, you reminded me of another source of confusion. what are expressions? you mentioned them in opposition to values. from what I can understand expressions are reduced to values. what are they composed of then?
01:25:35 <Cale> Well... most things, in some sense :)
01:25:54 <geekosaur> executable code, often containing its own patterns that get pushed onto the graph reduction engine's pattern stack to be further reduced. when an expression finally evaluates to a value, that value replaces the executable code, and if it's required again later the computed value is used
01:25:57 <Cale> Expressions are the things you type in which you'd expect to have a value
01:25:59 <geekosaur> (which is enabled by purity)
01:26:42 <Cale> For example, 5 * 27
01:26:47 <bahamas> so you have values and operations on values. functions would be an example of operations on values
01:26:51 <Cale> or  (\x -> x^2 + x + 1)
01:27:02 <Cale> functions are values too
01:27:06 <Cale> (but not all values are functions)
01:28:11 <Cale> When we say "values", properly, we're talking about the abstract mathematical things that our programs correspond to
01:28:42 <Cale> and then "terms" or "expressions" are the bits of program code that we use to identify those things
01:28:44 <bahamas> ok, I think an expression is something that is meaninful for the compiler, right? like the AST type has Expr as value. and Expr is also a type with a bunch of values
01:28:48 <Cale> yeah
01:28:55 <Cale> It's exactly that
01:29:14 <Cale> It's a syntactic thing
01:29:37 <Cale> Now, since evaluation is lazy, it's very natural to think of those expressions as actually persisting through to runtime
01:29:49 <Cale> and being rewritten repeatedly
01:30:31 <Cale> What actually happens is that there's a runtime representation of expressions called "thunks"
01:30:47 <Cale> which are, in GHC, represented as pointers to code
01:31:05 <bahamas> aha. I've seen that word a lot of times and never understood what it meant
01:31:21 <Cale> the first time you jump into that code, it first rewrites the pointer to point at a bit of code which throws an exception (infinite loop detection)
01:31:44 <Cale> then it computes the value of the expression (typically an integer tag and pointers to arguments for the data constructor)
01:32:09 <Cale> and then it rewrites the pointer to point at a shorter bit of code that returns the computed result immediately, before returning it itself
01:32:42 <Cale> so the next time you need the value of that expression, it will be available already
01:33:00 <bahamas> I would really understand this if I implemented it. even a simple version of it
01:33:02 <Cale> and the primary reason you need the value of an expression is to be able to deal with a case expression
01:33:28 <Cale> (pattern matching)
01:33:31 <bahamas> right
01:34:03 <Cale> (Or maybe you're dealing with a function application and need to figure out what function you have, reducing it until it's a lambda)
01:34:25 <Cale> (which would be represented by another data structure called a closure)
01:34:59 <Cale> Usually we like to stay up in the world of functions and expressions, rather than trying to think about closures and thunks, whenever possible.
01:35:51 <Cale> If you imagine Haskell evaluation as just being expression rewriting, using the rules provided by the program to successively substitute expression trees, you won't be so far off
01:36:03 <Cale> If you can think of them as expression *graphs*, you'll be closer still
01:36:37 <Cale> i.e. where you're allowed to have cycles and multiple references to the same thing due to a variable being repeated
01:36:55 <Cale> > let ones = 1 : ones in ones
01:36:57 <lambdabot>  [1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1...
01:37:06 <Cale> ^^ this infinite list takes a small finite amount of space, really
01:38:23 <Cale> When I reason about performance of my programs, I almost *never* have to think about closures and thunks -- it's all done up in the world of graph rewriting
01:39:03 <Cale> http://www.cas.mcmaster.ca/~kahl/HOPS/ANIM/index.html -- there have been programming languages which make that graph rewriting more visual :)
01:40:04 <Cale> If I'm thinking about closures and thunks, it's much more probably because there's some bug in the compiler I'm using ;)
01:40:30 <Cale> But it's good to learn about that stuff anyway
01:40:47 <Cale> https://www.microsoft.com/en-us/research/wp-content/uploads/1992/04/spineless-tagless-gmachine.pdf
01:41:14 <Cale> This is an old paper which has a very good introduction to how to implement lazy evaluation
01:41:33 <Cale> It's not all 100% accurate with respect to the way that GHC presently does things, but it's a really good start :)
01:41:55 <Cale> (It was once true in the past)
01:42:22 <bahamas> graph theory is something I know very little, but I feel it would be useful to know more of
01:42:31 <Cale> The whole "exploring the design space" part is really good reading even if you don't care about the details though
01:42:41 <bahamas> since many relationships between systems can be modelled with graphs
01:42:43 <jophish> Can GHC unpack members which have a type imported from an hs-boot file?
01:43:04 <Cale> jophish: Sorry, have no idea about that one
01:43:29 <jophish> Cale: it doesn't seem to give a warning when I try to, but perhaps it's silently not unpacking
01:43:33 <Cale> jophish: Maybe you can inspect the core?
01:43:38 <jophish> good idea
01:44:46 <Cale> anyway, really late here, I should get some sleep
01:44:54 <Cale> g'night and good luck :)
01:46:12 <jophish> thanks! good night
01:47:42 <jophish> There doesn't seem to be any difference in the core
01:48:32 <geekosaur> .hs-boot I think should not matter. there might be a question of whether it's unpackable for other reasons
01:50:05 <jophish> geekosaur: I think I might be misunderstanding exactly how (or when) GHC determines the memory layout for objects: but how could GHC unbox something that it doesn't know the size of yet?
01:50:26 <jophish> it's only seen this type to be unboxed from the boot file which doesn't have the RHS of the data decl
01:52:10 <geekosaur> hypothetically, if it knows that it has a fixed size but doesn't know that size yet, it could fill that in later (more precisely, let the assembler do so since it should have that information). but if you think about ut, it should have enough to figure out if it's safe or it can't do *anything* with values of the type
01:53:21 <jophish> I'm reading through ddump-simpl and don't recognize anything which tells me if a field is unboxed or not, not really sure what to look for
01:53:54 <geekosaur> hm, I'm not sure either, I think that detail might only show at lower levels than core
01:53:55 <jophish> geekosaur: sorry, I must not be thinking enough, how could it know if it's safe or can't do anything?
01:54:07 <jophish> had a look at stg too, nothing jumping out there
01:54:22 <jophish> all it knows is that there's a type with that name
01:54:31 <geekosaur> right, which means it can't do anything
01:55:46 <jophish> oh right, I see what you meant
01:55:48 <geekosaur> it has to fill in the details somehow, or it might as well not have the type. .hs-boot stops it from getting into an import loop, but at some point it needs to know more than the .hs-boot provides
01:56:25 * geekosaur is insomniac and probably shouldn't be trying to think thsi through right now...
02:02:20 <jophish> geekosaur: well, I can get the object and interface file even when the .hs-boot file's .hs counterpart is missing
02:03:13 <DigitalKiwi> insomniacs of the world unite incoherenly at 3am in their respective timezones
02:52:34 <jusss> what is a typeclass?
03:22:47 * hackage haskoin-node 0.9.10 - Haskoin Node P2P library for Bitcoin and Bitcoin Cash  https://hackage.haskell.org/package/haskoin-node-0.9.10 (xenog)
04:08:17 * hackage haskoin-store 0.14.1 - Storage and index for Bitcoin and Bitcoin Cash  https://hackage.haskell.org/package/haskoin-store-0.14.1 (xenog)
04:27:09 <mikorym> sup
04:30:36 <mikorym> haven't been on irc in ages
04:35:33 <Akii> is there a cabal config that lets me include additional files? I have some migration sql scripts I need to include in my nix derivate and I'd assume it honours the cabal config
04:39:47 * hackage telega 0.2.3 - Telegram Bot API binding  https://hackage.haskell.org/package/telega-0.2.3 (iokasimovmt)
04:40:31 <bor0> I have `apply n f v = v : apply (n - 1) f (f v)` and `apply 0 _ _ = []` as the base case. I can do stuff like `apply 5 (+1) 1` which is [1,2,3,4,5] and `apply 5 (+20) 1` which is `[1,21,41,61,81]`. is there a Haskell built-in for `apply`, or a cleaner way to write it? it basically repeats function application on every recursive step
04:41:18 <bor0> something tells me I can use folds.. I'm checking that right now
04:43:19 <Akii> "data-files" has done something :3
05:02:17 * hackage haskell-lsp-types 0.9.0.0 - Haskell library for the Microsoft Language Server Protocol, data types  https://hackage.haskell.org/package/haskell-lsp-types-0.9.0.0 (AlanZimmerman)
05:03:17 * hackage haskell-lsp 0.9.0.0 - Haskell library for the Microsoft Language Server Protocol  https://hackage.haskell.org/package/haskell-lsp-0.9.0.0 (AlanZimmerman)
05:17:43 <lyxia> > take 5 $ iterate (+ 20) 1
05:17:45 <lambdabot>  error:
05:17:45 <lambdabot>      Ambiguous occurrence ‘take’
05:17:45 <lambdabot>      It could refer to either ‘Data.List.take’,
05:18:08 <lyxia> > take 5 $ iterate (+ 20) 1
05:18:10 <lambdabot>  [1,21,41,61,81]
05:18:16 <lyxia> bor0: ^
05:20:32 <bor0> ah, there's iterate. thanks! :)
05:20:47 * hackage termbox-banana 0.1.1 - reactive-banana + termbox  https://hackage.haskell.org/package/termbox-banana-0.1.1 (mitchellwrosen)
05:44:47 * hackage lsp-test 0.5.1.1 - Functional test framework for LSP servers.  https://hackage.haskell.org/package/lsp-test-0.5.1.1 (AlanZimmerman)
07:30:34 <vijaytadikamalla> Can someone please answer this question
07:30:36 <vijaytadikamalla> https://stackoverflow.com/questions/55763395/no-instance-for-fromyaml-text-arising-from-a-use-of-decodestrict
07:36:45 <Cale> vijaytadikamalla: The explicit package qualifier in the error message there makes it seem as if there might be more than one version of Text that's in play somehow
07:37:18 <Cale> vijaytadikamalla: How are you compiling this code?
07:37:39 <vijaytadikamalla> loading in ghci
07:37:57 <vijaytadikamalla> Yeah I faced multiple version problem
07:38:18 <Cale> Try:  ghc-pkg list text
07:38:21 <Cale> at the terminal
07:38:41 <vijaytadikamalla> text-1.2.2.2 and text-1.2.3.1
07:39:14 <Cale> Yeah... hmm, how to get things sorted out for you...
07:39:50 <vijaytadikamalla> I want text-1.2.3.1 for HsYAML
07:40:16 <Cale> It appears that HsYAML got built against text-1.2.2.2 since that's what it says the instance is for
07:40:48 <vijaytadikamalla> but parsec is taking text-1.2.2.2
07:41:23 <Cale> Some version of parsec will certainly work with any version of text
07:42:00 <Cale> (the current version of parsec will work with anything from text-0.11.3 up to <1.3)
07:42:19 <monochrom> This may be one of those times either sandboxing or cabal new-build is absolutely better.
07:42:33 <Cale> Yeah, I think just starting your cabal file now would be a good plan :)
07:43:09 <Cale> (or even if you're just experimenting, making a sandbox will help)
07:43:16 <monochrom> I am a fan of v1-install but I totally keep an eye on uniqueness for it to work.
07:43:43 <maerwald> sandboxes are underrated
07:43:56 <vijayphoenix> After I faced this issue, I installed HsYAML in sandbox
07:44:00 <Cale> You should try to pass as many packages to cabal install as you can at the same time
07:44:27 <vijayphoenix> But still the problem remains
07:44:34 <Cale> because it will solve their dependencies together that way, and tell you if there's a fundamental problem
07:44:58 <Cale> Let me stare at the deps for HsYAML for a moment, since it's the only thing I'm unfamiliar with in this picture
07:46:48 <Cale> It doesn't appear to be problematic in any way, there ought to be a solution
07:47:07 <monochrom> I can't reproduce the dependency problem.
07:47:33 <vijaytadikamalla> sandbox isn't helping
07:49:48 <Cale> vijaytadikamalla: Did you give all the packages you're using to cabal install on the same commandline when installing the packages in the sandbox?
07:50:50 <vijaytadikamalla> cabal install --dependencies-only used this
07:50:54 <Cale> oh, also, use cabal repl to load your code
07:51:25 <monochrom> Which other packages are involved? Because HsYAML alone cannot cause this.
07:52:00 <Cale> > decodeStrict str :: Either String [Text]
07:52:00 <Cale> Right []
07:52:02 <lambdabot>  error:
07:52:02 <lambdabot>      Not in scope: type constructor or class ‘Text’
07:52:06 <Cale> I was able to load your code
07:52:13 <Cale> in a newly constructed sandbox
07:52:23 <Cale> I did cabal repl, and then :l foo.hs
07:53:01 <Cale> Yeah, if there are other dependencies involved, maybe there's something which really wants the older text package
07:53:27 <Cale> (though in that case, I would expect the install command to have failed to find a solution)
07:54:18 <monochrom> Also "sandbox isn't helping" is ambiguous.
07:56:11 <Cale> Another option, if you don't mind a sledgehammer solution, is to install the nix package manager, and do something like:
07:56:17 <Cale> nix-shell -p 'haskellPackages.ghcWithPackages (pkgs: with pkgs; [ HsYAML parsec text ])'
07:56:24 <orzo> Cale: did you discover anything I was doing wrong with the lookAtRotate problem?
07:56:28 <Cale> (might be good to make an alias)
07:56:43 <Cale> orzo: Ah, sorry, I wandered off there!
07:57:11 <Cale> orzo: Peeking at it, I wasn't quite able to make sense of what was going on in that code.
07:58:13 <Cale> vijaytadikamalla: Anyway, if you *do* have nix, that command will download some pre-compiled stuff and drop you into a shell with ghc and those packages installed.
07:58:37 <Cale> It's quite handy for experimenting with different packages that you're not sure you want to use yet :)
07:58:42 <orzo> I'm trying to write the equivelent of lookAtRotate as a call to lookat on suitable arguments
07:58:51 <vijaytadikamalla> okay I will try....thanks for your help
07:59:07 <Cale> orzo: I'm not sure that in your position I wouldn't just give up and do quaternions :D
07:59:41 <Cale> (though they don't really answer the question at hand)
08:02:06 <Cale> I guess you want to represent affine maps in the end anyway, so that suggestion won't fully help. Maybe try working entirely with the projective 4x4 matrices as much as you can? It would be easier to sort out what was going on if there wasn't so much conversion between representations going on in the midst of the same code.
08:06:26 <klntsky> is there a library to perform eta reduction on TemplateHaskell terms? Or at least variable substitution
08:08:19 <Cale> klntsky: If you find something like that, let me know. I've been doing (limited) unification of types by hand.
08:09:06 <Cale> klntsky: There really ought to be way more in the way of TH support libraries, but instead, we just end up doing the same stuff over and over. I really don't know why.
08:15:08 <klntsky> Cale: OK. If there is no solution, I should probably implement it in a separate package, to be reusable. Could you share a link to your implementation of type unification?
08:16:12 <Cale> https://github.com/obsidiansystems/aeson-gadt-th/blob/develop/src/Data/Aeson/GADT/TH.hs#L147 -- it's not quite in a reusable form
08:17:09 <Cale> also, oh haha, this reminds me I have a bunch of updates to this code sitting on my machine, waiting to be finished and merged in
08:17:41 <Cale> There are some very glaring cases it doesn't attempt to deal with
08:17:46 <Cale> some of which are quite easy
08:17:55 <Cale> like, for example, the list type constructor
08:18:16 <Cale> It's just that at the time, I wrote exactly what I needed
08:19:27 <Cale> I have a version which does a complete match there, and takes care of ForallT and deals with the name capture which comes from that
08:21:10 <vijaytadikamalla> When loading foo.hs all other loaded modules get unload
08:21:25 <klntsky> The most sad thing in implementing ad hoc term-manipulating functions in TH is the feeling of being too lazy to write down the assumptions in the comments :)
08:23:04 <Cale> vijaytadikamalla: Try putting a * before the name of the module to load
08:24:07 <vijaytadikamalla> did :l *foo.hs but still everythong gets unloaded
08:31:59 <bor0> :src iterate
08:32:03 <bor0> @src iterate
08:32:03 <lambdabot> iterate f x = x : iterate f (f x)
08:37:17 <shapr> @where cis194
08:37:17 <lambdabot> https://www.seas.upenn.edu/~cis194/spring13/lectures.html
09:11:07 <mikorym> hi all
09:11:45 <vijaytadikamalla> How to load a new file in cabal repl without removing old ones
09:12:15 <mikorym> There are many online references for haskell -> cat theory. Are there any good resources for learning haskell if you know category theory?
09:14:03 <gonz_> vijaytadikamalla: `import ModuleName`?
09:14:22 <gonz_> AFAIK you can't *load* several modules.
09:14:47 <gonz_> Because loading puts the entire context of that module in scope, which implies unloading of other modules
09:15:00 <gonz_> Importing it will simply put the things inside of it in scope, as if you imported it from other modules.
09:15:28 <glguy> How about, :add [*]<module> ...        add module(s) to the current target set
09:15:51 <glguy> (I forget if that only updates the import list or not)
09:17:05 <Cale> You can also generally :l *ModuleName
09:17:56 <monochrom> "import" and ":module" are the ones who update the import list only. Also, only good for modules from non-main packages.
09:18:05 <Cale> Oh, right, nevermind me, I'm misremembering what * means
09:18:13 <Solonarv> you can do :m + *Foo IIRC
09:18:20 <Cale> * means "load as bytecode even if compiled object code exists"
09:18:25 <monochrom> ":load" and ":add" are for modules from the main package
09:19:00 <Solonarv> IIRC * also has the side effect of bringing into scope everything that's in scope in that module, rather than only its exports
09:20:26 <Cale> Yeah, that's just because compiled object code won't have corresponding information in its .hi about non-exported things, and the code may not even properly exist (e.g. it might all have inlined)
09:20:58 <Solonarv> yup
09:21:07 <Solonarv> it's a very convenient side effect, though!
09:22:04 <Cale> yep
09:22:36 <monochrom> * insists on bytecode precisely for bringing more things into scope
09:40:25 <monochrom> I was reading the && || associativity thread in haskell-cafe. It's pretty depressing.  After 30 years of Haskell and a lot of experts on board (including dependent typing experts!), people are still confused, including said experts.
09:43:49 <Cale> What's the thread about?
09:44:12 <Cale> Why || has lower precedence than && ?
09:44:14 <monochrom> Why is && infixr in Haskell, infixl in all other languages
09:44:20 <Cale> oh
09:45:04 <Cale> Seems obvious to make it infixr to me, since it makes sense for it to bail out early in a left-to-right fashion
09:45:17 <monochrom> The beauty is that infixr is exactly right for Haskell, infixl is exactly right for other languages.  But people think muddly and with presumptions, they don't see why.
09:45:26 <Cale> yeah
09:45:35 <Cale> People not used to lazy evaluation, I guess
09:45:47 <infinisil> Why is infixl right for other languages?
09:45:53 <infinisil> or, imperative ones
09:45:59 <infinisil> Or strict ones?
09:46:03 <monochrom> Actually more than that.  Also not used to how non-lazy-language programmers think.
09:46:05 <Cale> Because they evaluate innermost-first
09:46:39 <Welkin> why is there no implementation of quicksort in vector-algorithms?
09:46:50 <monochrom> To a large extent, if you are writing an optimizing compiler, you don't really care, you already know the right code to generate.
09:46:58 <glguy> How might I notice the associativity of && in something like C?
09:47:24 <Welkin> glguy: you carefully read the manual I suppose
09:47:37 <klntsky> glguy: by observing side effects order?
09:48:12 <glguy> klntsky: OK, what's an example that differentiates the associativity of && in C?
09:49:00 <Cale> glguy: something where each conjugand prints to the terminal
09:49:11 <Cale> glguy: remembering that effects should happen innermost-first
09:49:50 <Cale> If it was right associating, you'd get the stuff at the end happening first.
09:49:54 <monochrom> But the real problem is how do you explain to the average C programmer (who hasn't seen lazy evaluation, who believes in the highschool story that evaluation order is bottom-up on the parse tree) that "0!=0 && 3/0 && 4/0" is well-defined?
09:50:36 <nshepperd> doesn't C's short-circuiting semantics mean that the effects would happen in left-to-right order either way
09:50:36 <Welkin> add parentheses
09:50:49 <Welkin> lisp solved this 70 years ago :D
09:50:59 <monochrom> Suppose you tell them "it means 0!=0 && (3/0 && 4/0)".  They will think "OK 3/0 && 4/0 happens first. I get an error. Abort".
09:51:36 <glguy> Cale: Could you write an example that shows the difference?
09:51:38 <monochrom> Therefore you cannot tell them that.  You have to tell them the fake story "it means (0!=0 && 3/0) && 4/0".
09:52:36 <Welkin> why is there no implementation of quicksort in vector-algorithms?
09:52:59 <monochrom> Welkin: But did they provide another sorting algorithm?
09:53:10 <Welkin> huh?
09:53:14 <Welkin> there are many
09:53:25 <Welkin> but quicksort is a standard, widely used algorithm
09:53:36 <Welkin> I'd expect it to be there along with the other ones
09:53:39 <monochrom> OK well too bad then.
09:54:13 <monochrom> This is what you get for FOSS. Everyone is always waiting for someone else to do it.
09:54:16 <nshepperd> printf("a") && (printf("b") && printf("c")) produces "abc"
09:54:18 <Welkin> including many I have never heard of, like introsort and american flag sort
09:54:30 <monochrom> haha
09:54:41 <Welkin> sure, I have an implementation of quicksort from a gist by dmjio
09:54:46 <Welkin> it's really short
09:54:52 <Welkin> so I don't understand why it isn't in there
09:54:56 <monochrom> This is also what you get for FOSS. What gets done is always what someone else fantasizes about.
09:55:41 <monochrom> Clearly, no one fantasizes about mundane quicksort.  Everyone fantasizes about american flag sort and dependently typed sort.
09:55:51 <nshepperd> the only way i can think of to observe the association order would be to go into C++ and redefine operator&& to not be algebraically associative any more
09:56:29 <glguy> nshepperd: I can't think of a way, either
09:56:43 <Cale> glguy: Ah, apparently I didn't understand the semantics of how it sequences effects well enough. Perhaps it's not possible.
09:56:59 <monochrom> Yeah my comment/opinion is also after thinking along the line of glguy and nshepperd.  You can't actually observe the difference.
09:57:22 <monochrom> But you still have to tell end-users a story they understand.
09:57:25 <glguy> && itself can't show you and && is on its own associativity level, so you can't use other operators to check
09:58:23 <Cale> But that's interesting. It seems to perform all the effects independently of how the expression is associated.
09:59:52 <Solonarv> well, of course. If you evaluate function arguments before calling the function and do this in a left-to-right fashion, then f(g(x, y), z) and g(x, f(y, z)) will evaluate x, y, z in that order always
10:00:10 <Solonarv> (including in the special case where f = g = (&&))
10:00:38 <Cale> ah, apparently in C, they just leave it entirely ambiguous which order the effects occur in
10:00:44 <Cale> so it may or may not be observable
10:01:12 <Cale> (and even if it does make an observable difference with the compiler you're using, that difference is somewhat meaningless)
10:01:22 <Welkin> it's also strange that there is no sort function whatsoever in the vector package
10:01:27 <Welkin> I'd expect at least one
10:01:31 <Cale> But with GCC at least, it seems to perform the effects from left to right, regardless of associativity
10:01:48 <Cale> (at least, insofar as I've tried)
10:01:56 <Welkin> like all of the other collection types that come with a de facto sorting function
10:03:11 <Tuplanolla> This is called a "sequence point" in N1570, Cale.
10:04:06 <monochrom> Here is my take.  The real story you want to tell everyone is this: x&&y&&z means the same as Scheme's (and x y z), and it means you evaluate from left to right looking for the first false.
10:05:11 <monochrom> To some people, it feels like "left associating" because "we care about the left more". So you tell them "yeah I agree!"
10:05:36 <monochrom> To some other people, it feels like "right associating" because "we ignore the right more". So you tell them "yeah I agree!"
10:05:45 <Cale> yes :)
10:06:58 <monochrom> "care about the left more" feels like left associating if you think like highschool algebra, i.e., in the parse tree, you do bottom-up evaluation order, i.e., the opposite of lazy.
10:07:19 <monochrom> And "ignore the right more" feels like right associating if you think lazy evaluation.
10:07:42 <monochrom> Either way as you have observed, a compiler writing actually doesn't care.
10:35:04 <johnw> does anyone know of tricks to discover why "bytes allocated in the heap" is a large number for my program (1G) when all the other numbers are so small (max residency 78k, for example)
10:36:03 <Cale> johnw: I guess the ordinary profiler?
10:36:15 <Cale> That's more about time performance than space.
10:36:17 <johnw> it's not very illuminating
10:36:23 <johnw> here's the code: https://github.com/jwiegley/simple-ltl/blob/master/src/LTL.hs
10:36:47 <Cale> If all the other numbers are acceptable, I wouldn't worry about it, since it's not a very meaningful metric
10:37:02 <johnw> well, the regular profiling also mentions 2G allocated 
10:37:27 <Cale> Does the breakdown say where it's all happening?
10:37:35 <johnw> I believe it's due to values of type Machine being constructed and deconstructured continually
10:37:38 <johnw> no, it doesn't break it down at all
10:37:44 <Cale> oh
10:37:55 <johnw> everything is inlined, so my report summary is just:
10:37:58 <johnw> main         Main            test/Main.hs:(10,1)-(16,3)            62.6   46.3
10:37:59 <Cale> ahhh
10:37:59 <johnw> foldTestTree Test.Tasty.Core Test/Tasty/Core.hs:(351,1)-(366,76)   37.3   53.6
10:38:04 <Cale> fun
10:38:07 <Cale> haha
10:38:15 <johnw> if I don't inline, things get drastically worse
10:38:59 <johnw> but I'm not even sure if those numbers are GC allocations or just heap allocations (they make perfect sense if the latter)
10:39:40 <johnw> i tried changing out Machine for a finally encoding representation
10:39:45 <johnw> but oh my, that is not better
10:40:21 <Cale> oh
10:40:23 <johnw> due to the way laziness is an advantage for this code, the final encoding appeared to remove the ability to avoid work
10:40:35 <Cale> haha, I was about to suggest doing a cps-newtypey thing
10:40:41 <johnw> :)
10:41:20 <johnw> here's what that version looks like: https://github.com/jwiegley/simple-ltl/blob/final/src/LTL.hs#L41
10:41:27 <johnw> although it makes combine and select very hard to read
10:42:23 <Cale> There really ought to be a pragma for doing that
10:42:32 <johnw> I wrote a `recursors` library that does it automatically
10:42:45 <johnw> but we'd need a pragma to automatically turn case matching into its equivalent
10:45:06 <monochrom> w00t LTL
10:46:09 <johnw> Cale: I just tried the CPS'd version again
10:46:33 <Cale> Something mysteriously different?
10:46:45 * Solonarv mumbles about pattern synonyms
10:46:49 <johnw> bytes allocated in the heap is the same, but max residency goes up from 78k to 604M, and copied during GC is up from 600k to 3.7G
10:46:55 <Cale> oh, lovely
10:47:17 <Solonarv> oh, actually a GHC plugin might be able to auto-CPS things
10:48:09 <Cale> There's probably some way in which the functions making up the Machines are retaining more values now.
10:48:21 <Cale> Or retaining more Machine-thunks rather
10:49:04 <Cale> It's not totally clear to me
10:49:34 <monochrom> CPSing can kill some laziness, yeah.
10:50:36 <johnw> yes, and laziness is essential here, to preserve the idea that "next" doesn't do anything at all until we have more input
10:51:09 <johnw> well, even with big "allocated in the heap numbers", I have a productivity above 99%, so I suppose there's nothing to worry about?
10:51:25 <Cale> Yeah, I think just don't worry about it :)
10:52:24 <Akii> lol I've set the nixpkgs to master for one of my packages.. been compiling packages all day
10:52:40 <Cale> Akii: yeah...
10:52:48 <Akii> I think it compiles GHC atm
10:52:59 <johnw> on macOS those are brutal rebuild
10:53:00 <johnw> s
10:53:05 <Akii> xD
10:54:30 <Cale> johnw: I think I've finally figured out more clearly why I'm constantly infuriated any time I have to edit a .nix file. It's that in order to derive most of the benefits of pure functional programming, you need to be able to know the definitions of things you're working with.
10:54:43 <johnw> true!
10:54:46 <Akii> true.
10:54:50 <Cale> The convention of piling everything together into giant sets is horrible.
10:55:02 <Cale> Giant sets that are then recursively edited
10:55:03 <johnw> giant fixed point, self-recursive sets
10:55:05 <Akii> if only there was a way to fix this
10:55:11 <johnw> whenever I see "extensible-self", I cry
10:55:17 <Akii> like if you could like tag expressions what they return or smth
10:55:19 <Cale> Let's remove sets from the nix language :D
10:55:37 <royal_screwup21> anyone familiar with parser combinators know how <$ works? I understand how <$> works: we take a parser parameterized on a function that takes a string, return [(a, String]), and then fmap over the that resultant to get [(b, String])
10:55:38 <Cale> I'm almost not joking
10:55:39 <Cale> lol
10:55:54 <johnw> so much of what Nix is is built around attrsets, though
10:55:56 <Cale> royal_screwup21: It's just fmap with a constant function
10:56:02 <johnw> I almost feel like they came first
10:56:17 <Cale> v <$ x = fmap (const v) x
10:56:25 <johnw> I love <$
10:56:34 <Akii> maybe the confusion is about the effects that f holds
10:56:39 <Akii> at least that's what I'm thinking right now
10:56:51 <Akii> because with parsers that means consuming input or not?
10:56:54 <royal_screwup21> so, given two args, const returns the first arg...
10:57:15 <royal_screwup21> not sure I understand what's going on there in that def. :(
10:57:27 <Cale> johnw: hnix will almost certainly help... at least, if it results in the ability to tab complete and browse what's available without having the repl shit all over my terminal and then crash
10:57:40 <johnw> royal_screwup21: for any f a, substitute out one 'a' for another
10:58:00 <johnw> Cale: yes, just free up Ryan's time to work on it with me more :)
10:58:17 <Akii> Cale another mess are versions. Hate how it's basically non existent as a concept in nix
10:58:26 <Cale> johnw: He's been sucked into implementing concurrent FRP for backends :D
10:58:31 <johnw> so I hear
10:58:33 <royal_screwup21> is <$ built  into the stdlib? 
10:58:36 <Akii> I just pick master and hope it half way aligns with what stackage thing I'm on
10:59:00 <Akii> royal_screwup21 I think that comes from Data.Functor
10:59:01 <Cale> Akii: another thing is that derivations are fundamentally non-compositional
10:59:02 <johnw> Cale: he was pondering whether my ltl stuff could have a use in FRP systems
10:59:39 <Cale> Akii: I can nix-env a couple things, but try and get a derivation that gets both of them and suddenly it's an arbitrarily difficult puzzle
10:59:39 <johnw> I've already been thinking about it as a context-aware filter for lists in general. I.e., "give me the prefix of the list that satisfies the formula"
11:00:12 <conal> johnw: oh! is there a _continuous_ form of LTL?
11:00:19 <conal> I guess there could be.
11:00:20 <johnw> hmm
11:00:41 <reallymemorable> I am trying to use this module `Data.GADT.Compare.TH` but I can't figure out howo to find what I need to put in my build-depends to make it work.
11:00:53 <reallymemorable> Where does one look for such information?
11:00:55 <johnw> conal: I avoided solving my Coq problems temporarily by just implementing it in Haskell
11:01:06 <Cale> reallymemorable: That's from dependent-sum
11:01:12 <Cale> google
11:01:23 <conal> johnw: :) 
11:01:29 <reallymemorable> weird, i have dependent-sum in my build depends
11:01:37 <royal_screwup21>  could someone  give me a toy example of how to use <$? :)  I'm looking here but I don't see a concrete example
11:01:40 <royal_screwup21> https://stackoverflow.com/questions/14087881/what-is-the-purpose-of-in-the-functor-class
11:01:47 * hackage simple-ltl 0.1.0.0 - A simple LTL checker  https://hackage.haskell.org/package/simple-ltl-0.1.0.0 (JohnWiegley)
11:01:49 <reallymemorable> oh its compiling now...i guess it was some HIE thing
11:02:11 <Cale> > 5 <$ [1,2,3,4]
11:02:13 <lambdabot>  [5,5,5,5]
11:02:51 <Cale> It's more useful in parsers (where you want to try to parse something, then throw away what you parsed and produce a constant result)
11:03:05 <royal_screwup21> ahhh I see, thanks
11:03:06 <johnw> I think of it as the "en passant" operator
11:03:10 <Cale> or in FRP, where you want to replace all the occurrences of an Event with a constant value
11:03:24 <johnw> I'll often do stuff like 10 <$ put x, where put x is just something I want to happen along the way to returning 10
11:03:36 <Cale> haha
11:03:43 <Cale> I'm not sure I'd go that far
11:03:54 <Cale> You mean at the end of a do-block?
11:04:53 <royal_screwup21> > fmap (const 5) [1,2,3]
11:04:55 <lambdabot>  [5,5,5]
11:05:10 <royal_screwup21> see I don't get it: doesn't const takes *2* args? 
11:05:17 <Solonarv> it's being partially applied
11:05:28 <Solonarv> we can do this with any function in Haskell
11:06:08 <Solonarv> (const 5) is the same as (\x -> const 5 x) which is the same as (\x -> 5)
11:06:46 <tabaqui> doesn't (\x -> 5) evaluate x?
11:06:52 <Solonarv> no, it does not
11:07:00 <Solonarv> > (\x -> 5) undefined
11:07:02 <lambdabot>  5
11:07:05 <tabaqui> ok
11:07:22 <Solonarv> (why would it evaluate x ?)
11:07:45 <tabaqui> dunno, I avoid any symbols as far as possible
11:08:08 <johnw> (\x -> 5) means: ignore argument, 5
11:08:14 <Solonarv> symbols?
11:08:18 <tabaqui> I would write (\_ -> 5) here. Well, I would use pointfree actually, but still
11:08:19 * Solonarv scratches head
11:08:36 <tabaqui> I dunno, variable is not correct word for haskell
11:08:39 <Solonarv> tabaqui: well, me too, but I was explaining partial application
11:08:41 <tabaqui> in Lisp it is symbol
11:09:01 <Solonarv> variable is the correct word as far as the haskell report is concerned
11:09:24 <johnw> tabaqui: in compiled Common Lisp, I think they wouldn't have a symbol there either
11:09:24 <Solonarv> it doesn't vary over time, but it does vary between different invocations of that function
11:09:40 <tabaqui> but variables are not able to variate
11:09:52 <Solonarv> s/over time/within one function invocation/
11:10:14 <tabaqui> ok, it doesn't matter actually
11:10:32 <Solonarv> if I write: let f x = x + 2 in f 3 * f 5; then 'x' takes on a different value in the two calls to f
11:11:07 <tabaqui> johnw: from gnu.org, for example: The let expression is a special form in Lisp that you will need to use in most function definitions. let is used to attach or bind a symbol to a value in such a way that the Lisp interpreter will not confuse the variable with a variable of the same name that is not part of the function.
11:11:26 <tabaqui> symbol and variable are both correct
11:11:36 <johnw> binding should be a compile-time matter
11:11:49 <johnw> as are variables in Haskell
11:11:58 <johnw> they are just "names for things"
11:12:04 <johnw> and an unneeded name should cost nothing
11:12:28 <tabaqui> ah, I get what do you mean
11:12:56 <Solonarv> and indeed '\x -> foo' and '\_ -> foo' are equivalent, provided that 'x' does not occur free in 'foo'
11:12:59 <tabaqui> I think that "symbol" in Lisp is not the same as "symbol" in shared libraries
11:13:09 <johnw> true!
11:13:23 <johnw> in evaluated Lisp, a symbol is akin to a unique constant
11:13:49 <johnw> we don't really use the idea of gensym in Haskell, though a similar service does exist (Data.Unique)
11:14:21 <johnw> but that's not what "x" is here
11:14:35 <johnw> no symbols were harmed in the evaluation of this lambda abstraction
11:15:53 <tabaqui> oh, I do not remember much about Common Lisp, but gensym disappears after macro expanding, right?
11:16:28 <tabaqui> and in case of "let" we get just the same "variables" as in non-macro lambda expression
11:16:31 <johnw> it should, yes
11:16:52 <johnw> it turns into just a name after expansion
11:17:03 <tabaqui> right
11:17:05 <johnw> however, you can treat symbols as values too
11:17:16 <tabaqui> uhm, okey
11:17:17 <johnw> as we do in haskell when we have data Foo = Foo | Bar | Baz
11:17:31 <johnw> here, Foo, Bar and Baz are somewhat similar to Lisp symbols
11:17:40 <johnw> they are unique constructions that we can match against for identity
11:17:54 <tabaqui> waait, it is enumerates, defined as 'foo, 'bar', 'baz
11:18:10 <tabaqui> *'bar
11:18:16 <johnw> 'Foo' is a value that only compares equal to 'Foo'
11:18:30 <johnw> I should have said: data MyType = Foo | Bar | Baz
11:18:31 <tabaqui> it is nothing about "let"/"lambda"
11:18:35 <johnw> MyType is the enumerable
11:18:43 <johnw> correct
11:24:36 <Berengal> Is there any good way to shuffle something lazily?
11:25:10 <tabaqui> should it work for infinite lists?
11:25:34 <Berengal> Not infinite, no, but possibly large
11:25:46 <Berengal> I want to do a random depth-first search
11:25:54 <tabaqui> well, actually if you get random elements from the middle of the list, then you have to eval all elements before
11:26:35 <tabaqui> not the elements itself, but the Cons constructors
11:26:59 <Ariakenom> are lists relevant?
11:27:13 <Berengal> It doesn't have to be lists
11:27:21 <johnw> hah, was just going to say that :)
11:27:22 <fendor> is there a function to remove trailing whitespace for a text?
11:27:38 <tabaqui> dropWhile isSpace
11:27:42 <tabaqui> or something similar
11:27:44 <johnw> an IntMap keyed on random indices would be much more efficient than a list
11:27:52 <fendor> strip might also work
11:28:08 <Berengal> I was thinking of a map or array, but I would still have to shuffle the indices
11:28:44 <Berengal> Unless there's a way to generate a random list of indices that doesn't shuffle?
11:29:14 <merijn> Berengal: Look up Fisher-Yates shuffle
11:29:47 <merijn> Berengal: What do you need it for?
11:30:27 <Berengal> I'm doing a depth first search, but I want to do it in random order
11:30:42 <royal_screwup21> what package is <*> located in?
11:30:48 <Cale> base
11:30:59 <Cale> It's in the Control.Applicative module
11:31:08 <Cale> (or the Prelude now)
11:31:36 <royal_screwup21> Cale ah hmm, I've imported that module, tried :t <*> -- but it's throwing an erro: error: parse error on input ‘<*>’
11:31:37 <Berengal> So I basically want the list monad, but >>= to give elements in a random order
11:31:47 <Cale> royal_screwup21: Try :t (<*>)
11:32:00 <royal_screwup21> ah that works :)
11:32:04 <Cale> royal_screwup21: If you're not using an infix operator infix, you have to put it in parens to refer to it
11:32:06 <merijn> Berengal: How many elements are we talking?
11:32:16 <merijn> Berengal: millions, billions, trillions?
11:32:49 <Berengal> Maybe millions, but probably a couple thousand
11:33:01 <johnw> infinity quadrillions
11:33:28 <Berengal> But there's many of them
11:33:39 <merijn> Berengal: Honestly, for that number I'd just get an unboxed vector and shuffle in place via Fisher-Yates, then you can do just do "toList" to get back into the list monad
11:34:11 <Berengal> Something like a tree a couple thousand deep with each node having 100-1000 child trees
11:34:19 <Solonarv> fendor: there is a dropWhileEnd in Data.Text IIRC
11:34:22 <merijn> Berengal: People dramatically overestimate the cost of "millions of integers in a vector"
11:34:49 <fendor> Solonarv, yeah, I think that is what I am looking for, thanks!
11:34:56 <Solonarv> a million integers in an unboxed vector should only take about 8 MB (on a 64-bit system(
11:34:59 <merijn> Berengal: Napkin math: unboxed vector of 1,000,000 64bit integers is only about 7.5 MB of memory :)
11:35:02 <Solonarv> ))
11:35:44 <Solonarv> a plain old non-unboxed Vector should be up to twice as much
11:35:44 <Berengal> Thanks
11:36:01 <Solonarv> (could be less than that if there are repeats and you get proper sharing)
11:36:13 <Solonarv> clarifying: less than twice as much
11:44:17 * hackage midi-util 0.2.1 - Utility functions for processing MIDI files  https://hackage.haskell.org/package/midi-util-0.2.1 (mtolly)
11:48:28 <royal_screwup21> so I'm trying to wrap my head around <*>, in the context of parser combinators but I'm trying to use it with simple lists first...I'm looking at the definition, but it's not quite clear to me how I can use it. Could someone give me a pointer in the right direction? :0
11:48:31 <royal_screwup21> :)*
11:48:57 <royal_screwup21> also, what is the name of this operator?? Googling <*> parses it as wildcard so I'm not getting any useful result
11:49:18 <Cale> > [id, (+2), (*2), (^2), (2^)] <*> [5,7]
11:49:20 <lambdabot>  [5,7,7,9,10,14,25,49,32,128]
11:49:46 <Cale> royal_screwup21: It's sometimes pronounced "ap" for "apply"
11:50:16 <Berengal> a <*> b is "do a, then do b, then apply the result of a to the result of b"
11:50:30 <Berengal> if you think of a and b as computations
11:50:47 <Cale> So in the case of lists, you're picking an element from each, and applying the function you picked to the argument you picked
11:51:30 <royal_screwup21> right I'm follwing
11:51:59 <Cale> In the case of parsers, you're parsing the beginning of the input to get a function, and the beginning of the remainder of the input to get an argument for that function, and producing the result of applying the function to the argument
11:52:49 <Cale> A common pattern is something like  SomeDataConstructor <$> parseArg1 <*> parseArg2 <*> parseArg3 ...
11:54:09 <Cale> Which is like a generalised version of liftA2/liftA3/etc. or liftM2/liftM3/etc. if you're familiar with those
11:56:11 <royal_screwup21> damn...so the def of Parser that I'm looking at is: newtype Parser a = Parser (String -> [(a, String)])
11:56:44 <royal_screwup21> when appying <*>, we assume that one of the parses is paramterized on a function that returns [(function, String)]
11:56:45 <Cale> A parser for things is a function from strings to lists of pairs of things and strings.
11:57:06 <Cale> no, that's the implementation level
11:57:07 <royal_screwup21> parsers*
11:57:29 <Cale> You're not supposed to look at how the sausage is being made :D
11:57:44 <royal_screwup21> haha I see 
11:57:56 <Cale> When you're using <*> your first parser is producing a function where you have 'a' there
11:58:05 <royal_screwup21> I'm trying to "port" it to java/kotlin/rust in my head
11:58:19 <Cale> i.e. your first parser has type  Parser (a -> b)
11:58:23 <royal_screwup21> yup
11:58:26 <Cale> and your second parser has type  Parser a
11:58:32 <royal_screwup21> right
11:58:43 <tabaqui> LeiraSarvo
12:00:43 <royal_screwup21> ok, so we first parser with first parser, get something of type [(a->b, String]), parse that unconsumed string to get [(a, String)], then apply that function we got onto the a type to get ([b, String])...
12:00:48 <geekosaur> I think trying to understand some of this in terms of languages that don't work the same way will be of only limited help
12:01:20 <geekosaur> because you might be able to translate fragments, but when the languages work differently combining the fragments together won't work well
12:02:04 <geekosaur> I mean, you can force it, but it won't be idiomatic and that will get in the way of understandiing
12:02:05 <royal_screwup21> yup :)  Just discovered yesterday it's not possible to simulate Fix in rust or any jvm langauge 
12:02:42 <Cale> royal_screwup21: For *each* element of that first list, we take the function f of type a -> b, and depleted string, and we parse the depleted string with the second parser, getting some x and even-more-depleted string
12:02:43 <geekosaur> it'll look like doing things the hard way… because it is the hard way, in Rust etc. it's the easy / natural way in Haskell
12:02:58 <Cale> royal_screwup21: and then we produce f x and the corresponding even-more-depleted string
12:03:23 <Cale> Using the list monad, it would look like this:
12:03:42 <Cale> p <*> q = Parser $ \s -> do
12:03:53 <Cale>   (f, s') <- runParser p s
12:04:04 <Cale>   (x, s'') <- runParser q s'
12:04:11 <Cale>   return (f x, s'')
12:10:40 <zachk> can I safely compile with a newer ghc if I only have 2 gigs of ram? 
12:15:29 <koala_man> zachk: when I had 1.75GB ram I had to add swap to compile 
12:15:53 * zachk hears the *thrashing* of swap 
12:16:05 <zachk> did you use -j1? 
12:16:34 <koala_man> no, but the machine only had one cpu so I imagine that's the default
12:19:49 <zachk> dunno about that 
12:21:33 <monochrom> royal_screwup21: I usually motivate liftA2 first. And then "what if you want liftA3, liftA3, liftA5?" which motivates <*>.  <*> and liftA2 are convertible to each other, but with <*> you can now do liftA3, liftA4, etc.
12:22:19 <monochrom> liftA2 can be motivated by "fmap is liftA1, what if you want liftA2".
12:23:06 <bonjour> HELLO
12:23:10 <shapr> BONJOUR
12:23:16 <bonjour> SLT
12:23:22 <shapr> Are you here for the best of Haskell programming?
12:23:31 <shapr> c'est trop chouette!
12:24:04 <shapr> bonjour: haskell?
12:24:24 <bonjour> Comment on fait de la gestion d'erreur sur un readFile ? 
12:25:46 <shapr> well dang
12:25:54 <shapr> I was very slowly coming up with an answer to that :-(
12:25:57 <monochrom> Pretty sure that question contained no information.
12:26:46 <Solonarv> it was "how does one handle errors with readFile?", which is an entirely reasonable question
12:26:55 <Solonarv> (just, y'know, in french)
12:27:21 <monochrom> OK sorry.
12:28:56 <Solonarv> well, they left anyway :/
12:40:38 <shapr> my french is not fast
12:40:59 <johnw> ah, but how do you handle them in French
12:42:48 <shapr> je ne sais pas :-(
12:47:52 <Solonarv> you tell them to speak english, dangit!
12:49:25 <shapr> or Swedish, I can do Swedish!
13:09:52 <Solonarv> I mean, I speak french&german but I still find it weird to talk about programming in anything other than english
13:14:17 <shapr> Solonarv: funktionelle programmering!
13:14:23 <shapr> at least, that's what I remember from Swedish
13:14:37 * Solonarv doesn't speak swedish but can usually sort of guess what words mean
13:14:50 <shapr> Swedish sounds very much like German
13:19:17 * hackage safe-json 0.1.0 - Automatic JSON format versioning  https://hackage.haskell.org/package/safe-json-0.1.0 (nideco)
13:19:18 <Solonarv> indeed it does!
13:21:21 <Akii> it's still compiling stuff
13:22:00 <Akii> funny because all I need to verify is that 2 files are copied into the derivate 
13:37:23 <monochrom> If thrashing happens, it's going to be dog slow.
13:38:03 <Akii> been compiling literally all day
13:38:23 <Akii> I'll just do it on the server now and take the downtime
14:19:55 <flebron> Hi folks. Looking for a neat way of doing some list manipulation. I've got a list of numbers, li :: [Int]. I then have some indices into this list, fronts :: [Int]. I have some other indices into this list, nice :: [Int]. Both `fronts` and `nice` are ascending. I'd like move the indices specified by `fronts`, to the front of li, producing a permutation li' of li. Easy enough. However, how can I modify 
14:20:01 <flebron> `nice`, producing `nice'`, such that [li' !! n | n <- nice'], are still my "nice" numbers?
14:25:40 <Cale> flebron: Well, something like...
14:26:45 <Cale> translate fronts n = case lookup n (zip fronts [0..]) of Just k -> k; Nothing -> n + length fronts
14:27:13 <Cale> You may wish to define length fronts outside this function and reuse it, since it's going to be costly to calculate repeatedly
14:28:07 <Cale> However, any case in which you care a bunch about the indices of elements of a list is one where you might want to reconsider your data structure choices
14:28:35 <Cale> Lists are really primarily good at one access pattern: beginning to end.
14:28:41 <flebron> That's fair. I guess it isn't really idiomatic Haskell to work with indices at all?
14:28:50 <Cale> yeah, at least in the case of lists
14:29:05 <Cale> We sort of treat lists as if they're loops waiting to happen
14:29:20 <Cale> Putting stuff in a list is like saying "I intend to iterate over these things in this order"
14:30:23 <Cale> xs !! n necessarily costs O(n) time
14:31:00 <Cale> (and on top of that, can throw an ugly and obnoxious to catch exception if your index is out of bounds)
14:31:32 <Cale> So yeah, possibly IntMap? Or Data.Sequence is an option...
14:31:47 <flebron> About the snippet above, I'm not totally sure I follow. If x = (n - 1) was in nice, for example, where n = length li, then I'm saying translate fronts x is x + length fronts = n - 1 + length fronts? That might now go out of bounds.
14:32:31 <Cale> oh, oops, yeah, that was just wrong on my part
14:32:54 <Cale> We need to subtract the number of elements of fronts which come before x
14:33:10 <Cale> kind of awkward
14:33:14 <flebron> Yeah :s
14:34:18 <flebron> But I think I can use what you mentioned. My incoming data structure is kinda fixed by my caller, but I *think* can get rid of `nice`, and just giving my next stage a list of the nice things, and a list of the non-nice things.
14:34:21 <Cale> One way to make it nice would be to actually build a permutation type, and multiply together cycles of the form (1..k) for each k in fronts.
14:34:44 <flebron> (Instead of giving my next stage the entire list, and indices for "which things are nice")
14:35:00 <Cale> ah, cool
14:35:09 <Cale> yeah, that sounds much more natural
14:35:24 <flebron> Thanks :)
14:35:26 <Cale> :t partition -- might be useful
14:35:27 <lambdabot> (a -> Bool) -> [a] -> ([a], [a])
14:41:45 <Welkin> how is this non-exhaustive? `f : Maybe Char -> String -> String -> Bool; f mchar (x:xs) (y:ys) = ...; f _ _ [] = ...`?
14:42:33 <Welkin> looks fine to me
14:43:24 <Solonarv> hm, weird
14:43:45 <Welkin> ghci says this
14:43:46 <Welkin> *** Exception: OneAway.hs:(17,5)-(24,30): Non-exhaustive patterns in function compareChars
14:43:50 <bdesham[m]> (y:ys) will match a single-element string, right?
14:43:51 <bdesham[m]> with ys = []?
14:43:59 <Welkin> yes
14:43:59 <jle`> Welkin: compareChars or f ?
14:44:06 <Welkin> f = compareChars
14:44:29 <jle`> hm what are you calling it with?
14:44:35 <jle`> it might be a pattern match inside the definition?
14:45:17 <flebron> f Nothing [] [1]?
14:45:33 <flebron> Or rather, f Nothing "" "a"
14:45:44 <Welkin> https://bpaste.net/show/09ff46f5e600
14:46:20 <jle`> ah yeah, flebron got it i think
14:46:29 <jle`> where the first list is empty but the second list is non-empty
14:47:14 <Welkin> that one is wrong
14:47:15 <Welkin> https://bpaste.net/show/9c05c040f57e
14:47:17 * hackage libffi-dynamic 0.0.0.1 - LibFFI interface with dynamic bidirectionaltype-driven binding generation  https://hackage.haskell.org/package/libffi-dynamic-0.0.0.1 (JamesCook)
14:47:30 <jle`> in the future you can debug something like this with an extra case that logs the input, like compareChars _ (traceShowId->xs) (traceShowId->ys) = undefined
14:47:46 <Welkin> that doesn't make sense
14:48:11 <Welkin> case 1 is that both lists are non-empty, case 2 is that anything matchs on list 1 and list 2 is empty
14:48:37 <flebron> Welkin: What should compareChars Nothing "" "a" be? Why?
14:48:58 <Welkin> okay, I see
14:49:23 <bdesham[m]> Welkin: x:xs won't match an empty list
14:50:21 <Welkin> of course not
14:51:15 <jle`> think of it as f True True = ...; f _ False = ...
14:51:39 <jle`> you're missing the False True case. although maybe this angle doesn't make it any more clear
14:51:46 <Welkin> I got it
14:52:51 <Welkin> thanks
14:53:35 <Welkin> my logic would guarantee that would not happen
14:53:39 <Welkin> so I didn't consider it
14:53:45 <dmwit> > let f xs ys = drop 1 xs' == ys' || xs' == drop 1 ys' where (xs', ys') = unzip (dropWhile (uncurry (==)) (zip xs ys)) in f "abcdefg" "abcefg"
14:53:47 <lambdabot>  False
14:53:48 <Welkin> but the compiler doesn't know
14:55:01 <dmwit> Oh, whoops, that zip will never do.
14:55:29 <Welkin> this is part of a function for calculating the levenshtein distance if it is equal to 0 or 1
14:55:32 <Welkin> pretty stupid function
14:56:21 <dmwit> > let f (x:xs) (y:ys) | x == y = f xs ys; f xs ys = drop 1 xs == ys || drop 1 ys == xs in f "abcdefg" "abcefg"
14:56:23 <lambdabot>  True
14:56:52 <dmwit> Welkin: ^
14:57:03 <flebron> Nice use of drop to save yourself some pattern matching :)
14:57:05 <dmwit> Since you have already checked which one is longer, you can even avoid the (||).
14:57:06 <jle`> Welkin: well, if your logic guarantees that it would not happen, it wouldn't have come up as a runtime error ;)
14:57:32 <jle`> but also in the -Wall case it's good to add a catch-all pattern at the end with a proper error message too
14:58:59 <flebron> As a war story, some months ago I was profiling some code. We were spending quite a bit of CPU computing edit distances, and comparing the edit distance with a constant. The constant was... 0.3. The entire thing was just string equality :)
14:59:37 <dmwit> Heh, whoops.
15:03:15 <Welkin> dmwit: that's clever
15:03:25 * dmwit takes a bow
15:03:27 <Welkin> I like that better than my solution
15:03:36 <Welkin> too much pattern matching and additional checks
15:04:05 <Welkin> it's 10 lines less code
15:06:19 <Welkin> actually 12 lines less
15:06:27 <Welkin> I had a special case for when the lengths were equal too
15:06:42 <Welkin> \a b -> (<= 1) . length . filter (== False) $ zipWith (==) a b
15:07:23 <dmwit> You still need something like that if I read your code right.
15:08:07 <dmwit> Or, like... you could add to the base case. `f xs ys = drop 1 xs == drop 1 ys || {- the old base case -}`
15:32:08 <RedNifre> This is almost kleisli, is there a better way to write this?: https://pastebin.com/qSyAeVD2
15:32:50 <RedNifre> (I decided against using a parsing library and instead opted for understanding parser combinators instead by implementing one myself)
15:36:58 <Cale> RedNifre: That's liftM2 (,), but also you should make Parser into a Monad instance
15:37:08 <Cale> and not use type synonyms
15:43:54 <MarcelineVQ> Cale: :>
15:48:08 <neysofu> Hi, I'm writing a binary λ-calculus interpreter using only Python lambdas (figured it would be useful to learn Church encodings). I've got most pieces right, but call-by-name reduction is tricky. Does anybody know how I should approach it?
15:48:15 <neysofu> https://github.com/neysofu/lambda/blob/master/main.py#L156
15:52:50 <monochrom> What madness is that?!
15:54:20 <neysofu> Yes, it's not really beautiful
15:55:01 <monochrom> But generally the way to encode call by name in a call-by-value language is to manually rewrite "f(1, 2)" to "f(one(), two())" where "one" is a function that returns 1, e.g., "def one():  return 1", for example.
15:57:16 <monochrom> Yes this means the resulting code will be double as lengthy, tedious, ugly.
15:57:34 <monochrom> I recommend giving up.
15:59:31 <neysofu> I don't understand. I'm not dealing with Python functions, so I don't have to worry about Python being strict evaluated
16:00:06 <neysofu> Besides using a call-by-value Y instead of the classic lazy one
16:00:23 <monochrom> Are you saying that "Python lambda" is not "Python function"?
16:01:23 <neysofu> Sorry, I didn't mean it that way. Just saying that Python semantics count very little here... I think. I'm trying to find a generic algorithm to perform call-by-name reduction
16:02:47 * hackage wai-secure-cookies 0.1.0.4 -   https://hackage.haskell.org/package/wai-secure-cookies-0.1.0.4 (alaminium)
16:06:11 <neysofu> I think I have to pass the de Bruijn index of the currently bound term and scan all subterms to find and replace variables.
16:09:29 <Zer000> If I'm trying to parse a json-like AST with parsec, I need to convert that ast into a Stream right? So basically a flat list? How would I do that though?
16:10:00 <monochrom> What is an example of "json-like AST"?
16:10:07 <moet> parsec, iirc, lets you parse directly into your target data structure
16:10:08 <Zer000> it's bencoded data
16:10:21 <Zer000> i'm using the bencoding library
16:10:39 <monochrom> I am too lazy to learn bencoding.
16:10:45 <Zer000> monochrom, it's literally json
16:10:52 <Zer000> but serialized different into a string
16:11:09 <monochrom> But parsec is best for linear sequential flat input, not already-tree-structured input.
16:11:18 <Welkin> you need to rub it with the fleeb
16:11:24 <Welkin> because the fleeb has all of the fleeb juice
16:11:38 <Zer000> moet, are there any examples of this?
16:11:42 <Welkin> json is not an AST
16:11:47 <Welkin> it's pure data
16:12:18 <Zer000> But it's still one tree-like structure
16:12:24 <Welkin> sure
16:12:34 <monochrom> Unless you say "I just mean using parsec to parse json". Then sure you can do that, but aeson already does that for you. So point your dependency at aeson rather than parsec.
16:15:26 <monochrom> moet's computer crashed trying to give you an example.
16:16:14 <Zer000> monochrom, I still don't get it :( I have this data (BValue): http://hackage.haskell.org/package/bencoding-0.4.5.1/docs/Data-BEncode.html#t:BValue  and I'm trying to write code to make sense of it to build my own data structure based on rules
16:16:57 <Welkin> he forgot to rub it with the fleeb
16:18:01 <monochrom> What is "fleeb"?
16:19:12 <Welkin> it's how you make a plumbus
16:19:29 <Welkin> https://www.youtube.com/watch?v=eMJk4y9NGvE
16:19:32 <zachk> plumbus?
16:20:00 <monochrom> Oh wow thank you.
16:20:51 <monochrom> I think I'm entitled to spew out obscure Dream of the Red Mansion references too.
16:21:08 <Zer000> I can write a function :: BValue -> Value and just use aeson, but then I would needlessly convert a bytestring to Text and then back, that's the only drawback
16:21:41 <Welkin> monochrom: is that your favorite book?
16:21:48 <monochrom> No.
16:21:51 <Welkin> it was mao zedong's
16:21:52 <Welkin> communist!
16:24:08 <monochrom> I agree you should stick to BValue then.
16:24:25 <monochrom> So just write a recursive function that does structural recursion on your BValue.
16:24:43 <monochrom> No I won't give examples because you gave me none when I asked for it.
16:24:59 <RedNifre> Cale Parser a = String -> ParseResult a and ParseResult a is just Either String (String, a), so it's already a monad. Or did you mean that the parser function itself should be a monad? Can functions even be monads?
16:25:33 <monochrom> Also I don't want my computer to crash.
16:26:27 <monochrom> RedNifre: Cale means "newtype Parser a = Parser (String -> Either String (String, a))" has many benefits.
16:26:40 <Welkin> type aliases are useless/dangerous
16:26:48 <monochrom> (Many benefits compared to type synonym "type Parser a = ...".)
16:29:00 <RedNifre> Hmmmmmmm....
16:29:05 <RedNifre> Why?
16:29:55 <Solonarv> for one, you can actually give it the Functor/etc instances you want
16:30:05 <RedNifre> I mean, I started with Either String (String, Element) and when I did String -> Either String (String, Element) I noticed that the first is a parser result and the second one is a parser.
16:30:11 <Zer000> monochrom, I did give you an example! You asked for a json-like ast and I linked to the BValue definition. If that's not what you wanted then my bad
16:30:31 <monochrom> You can make an Alternative instance.
16:30:41 <RedNifre> What's an Alternative instance?
16:30:54 <RedNifre> Sounds like Either? 
16:30:56 <Solonarv> @src Alternative
16:30:56 <lambdabot> class Applicative f => Alternative f where
16:30:56 <lambdabot>     empty :: f a
16:30:56 <lambdabot>     (<|>) :: f a -> f a -> f a
16:31:51 <Solonarv> > Left "boo" <|> Right 42 -- Either does indeed have an Alternative instance
16:31:53 <lambdabot>  Right 42
16:32:07 <RedNifre> What's the sematic of <|> ?
16:32:10 <monochrom> But "String ->" will get into the way.
16:32:59 <monochrom> Also IIRC lambdabot's Alternative instance for Either is from a non-standard package.
16:33:10 <Solonarv> needs to be associative - i.e. x <|> (y <|> z) = (x <|> y) <|> z
16:33:10 <Solonarv> and empty needs to be a unit for it - i.e. empty <|> x = x = x <|> empty
16:33:39 <monochrom> <|> is EBNF's |
16:33:53 <Solonarv> oh, indeed - looks like that instance isn't in base
16:34:00 <Solonarv> I wonder why
16:34:56 <monochrom> It can get controversal what <|> should do for Either.
16:36:00 <monochrom> One stance is "Left foo <|> Left bar = Left foo".  The other stance is "Left foo <|> Left bar = Left (foo <> bar)".
16:36:07 <RedNifre> Well, the parsing should be unambiguous, so if I throw three parsers at one thing like a <|> b <|> c only one of them should produce a Right, so it should all work out.
16:36:25 <Solonarv> what's the definition of empty for the first of these instances?
16:36:33 <RedNifre> > Right "oh" <|> Right "dear"
16:36:35 <lambdabot>  error:
16:36:35 <lambdabot>      • Ambiguous type variable ‘a0’ arising from a use of ‘show_M412542501359...
16:36:35 <lambdabot>        prevents the constraint ‘(Show a0)’ from being solved.
16:36:38 <monochrom> Oh, right, nevermind.
16:36:51 <Solonarv> > Right "oh" <|> Right "dear" :: Either () String
16:36:53 <lambdabot>  error:
16:36:53 <lambdabot>      • No instance for (Control.Monad.Trans.Error.Error ())
16:36:53 <lambdabot>          arising from a use of ‘<|>’
16:37:04 <Solonarv> huh
16:37:08 <RedNifre> > Left "can't" <|> Left "parse this"
16:37:10 <lambdabot>  Left "parse this"
16:37:15 <RedNifre> hm
16:37:28 <Solonarv> grrr
16:38:13 <Solonarv> I don't like that instance >:(
16:38:32 <monochrom> Ah OK I was wrong about "non-standard".  Error e => Alternative (Either e) is in Control.Monad.Except etc.
16:38:57 <monochrom> Told you so! >:)
16:39:14 <monochrom> Anyway (->) String stands in the way.
16:41:27 <RedNifre> As always with Haskell/Idris, I have to decide between getting something done and learning more stuff that will make my code much better.
16:41:57 <monochrom> Yes but not this case.
16:42:48 <monochrom> Alternative actually came from this parser business.  You will need it.  All you can claim is you prefer a different name.
16:43:11 <monochrom> In fact I get to say the same of Idris too.
16:43:18 <monochrom> Err nevermind you already did.
16:43:39 <monochrom> OK I get to say the same of Javascript too.
16:44:34 <monochrom> I could handcode my own tramoplining and get something done immediately, or I can take a week to learn node.js or something.
16:44:47 <RedNifre> Well, it depends on how much you know about a language. It's entirely possible to solve something in a language perfectly and not learn anything doing it, if you already know the language very well.
16:45:17 <monochrom> newtype and Alternative requires only 10 minutes of learning.
16:45:48 <RedNifre> My Haskell/Idris knowledge on the other hand is so mediocre/minimal, that every quest frazzles into many subquests.
16:46:06 <RedNifre> yeah, but it's 2am already =)
16:46:30 <RedNifre> My understanding of newtype is that it's like type but incompatible instead of just a synonym
16:47:49 <EvanR> newtype is like data, except restricted to 1 constructor with 1 slot, and has no runtime cost
16:51:04 <EvanR> actually it seems to have very little in common with "type"
16:51:06 <Welkin> it's an anime magazine
16:51:11 <Welkin> a good one
16:51:15 <Welkin> that was discontinued
16:51:25 <Welkin> remember when we had all these great magazines?
16:52:10 <RedNifre> > empty :: Either String String
16:52:12 <lambdabot>  Left ""
16:52:24 <RedNifre> Hmmmm...
16:52:49 <RedNifre> > empty :: Either Float Float
16:52:52 <lambdabot>  error:
16:52:52 <lambdabot>      • No instance for (Control.Monad.Trans.Error.Error Float)
16:52:52 <lambdabot>          arising from a use of ‘empty’
16:53:06 <RedNifre> So some Either are Alternative and others aren't?
16:53:16 <geekosaur> only Either String
16:53:27 <geekosaur> because it only supports one type parameter whereas Either requires two
16:53:43 <geekosaur> so one has to be "pinned down" to qualify for Alternative
16:54:17 <RedNifre> Uh huh.
16:54:54 <Welkin> > empty :: Sum Int
16:54:55 <lambdabot>  error:
16:54:55 <lambdabot>      • No instance for (Alternative Sum) arising from a use of ‘empty’
16:54:55 <lambdabot>      • In the expression: empty :: Sum Int
16:55:27 <RedNifre> > Just "first" <|> Just "second"
16:55:29 <lambdabot>  Just "first"
16:55:38 <RedNifre> > Right "first" <|> Right "second"
16:55:40 <lambdabot>  error:
16:55:40 <lambdabot>      • Ambiguous type variable ‘a0’ arising from a use of ‘show_M865607715701...
16:55:40 <lambdabot>        prevents the constraint ‘(Show a0)’ from being solved.
16:55:50 <Welkin> you can play with lambdabot  in private messages
16:56:06 <RedNifre> Right, sorry.
17:00:58 <Welkin> what happened to the Mutable vector docs????
17:01:10 <Welkin> they are impossible to find and not listed anywhere in the vector docs
17:01:21 <Welkin> I had to manually search through the html files to find it
17:01:27 <glguy> Welkin: What module?
17:01:35 <Welkin> Data.Vector.Mutable
17:01:53 <glguy> It's right there on the index
17:01:55 <glguy> http://hackage.haskell.org/package/vector
17:02:00 <Welkin> ah I see
17:02:09 <Welkin> I was looking at the list at the top
17:02:12 <Welkin> in the description
17:03:18 <Welkin> maybe it should be added to that list
17:03:25 <Welkin> so others don't get confused like I did
17:05:43 <monochrom> Or delete the top description altogether.
17:06:17 <monochrom> "monockam's razor"
17:07:14 <skeuomorf> I imagine many peopole use Haskell to build Financial Software, specifically for things like transaction settlement, immutable logs, and so on, are there any high-quality open-source libraries that do these kinds of things?
17:09:26 <hpc> are there any in any language?
17:09:56 <johnw> immutable logs tend to clog up my fireplace
17:09:59 <hpc> i am sure you'll find it's all kept in-house because it's a competitive advantage
17:10:15 <skeuomorf> johnw: hah!
17:10:28 <skeuomorf> hpc: Yeah, figures.
17:10:39 <Solonarv> there's some blockchain/cryptocoin stuff iirc
17:11:02 <skeuomorf> Yeah, they tend to be very tightly coupled to a certain protocol
17:11:32 <skeuomorf> Also, the threat model is vastly different, so they tend to be very inefficient for "normal" use-cases
17:14:53 <skeuomorf> johnw: Are you John Wiegley?
17:14:58 <johnw> I am
17:15:58 <skeuomorf> Man, I love ledger, thanks for making that!
17:16:07 <johnw> you're most welcome!
17:16:49 <skeuomorf> Also, thanks for maintaining emacs! Currently IRCing from it :)
17:16:58 <johnw> same
17:17:20 <skeuomorf> \-/
17:27:40 <johnw> skeuomorf: what have you been up to with Haskell?
17:30:08 <skeuomorf> johnw: I am still a noob in Haskell (have written toy programs, and a tiny interpreter for a subset of scheme based on the wikibook), but I am going to build a piece of financial software soon and I was thinking about using it. I also sometimes play with logics, and implementing a logic + inference rules seems quite compelling with Haskell.
17:30:28 <johnw> yes, the logic stuff should be great fun in Haskell
17:30:34 <johnw> I was playing around with LTL today
17:30:49 <johnw> I'm rather new to modal logics
17:31:40 <skeuomorf> Nice, same, I mostly got interested in Haskell as a language for implementing logics when I saw Dan Piponi's https://github.com/dpiponi/provability
17:32:46 <johnw> i've been able to effectively use Löb's theorem exactly once in real code
17:33:09 <skeuomorf> haha, what did you use it for?
17:33:20 <johnw> it's part of the core evaluator for hnix
17:33:46 <johnw> since hnix relies on lazily self-referential structures, Löb's theorem is a cheap way to make the dependency graph implicit
17:34:27 <skeuomorf> Nice!
17:35:21 <johnw> i.e., you end up evaluating (Map Key Value -> Value) -> Map Key Value
17:35:30 <MarcelineVQ> johnw: you should be in ##dependent if you like modalities, people are always yacking about that stuff
19:07:30 <OmegaDoug> Are there any functions which take a string/text/bytestring and return a timestamp in milliseconds?
19:08:40 <OmegaDoug> I'd like to convert directly from a string to a timestamp, without having to have a level of indirection by, say, convering to UTC time first
19:09:25 <glguy> time package's parseTimeM should be able to
19:10:16 <glguy> Not certain if there's an instance for that or not
19:13:45 <OmegaDoug> That looks great, thanks
19:55:47 * hackage shh 0.4.0.0 - Simple shell scripting from Haskell  https://hackage.haskell.org/package/shh-0.4.0.0 (lukec)
19:59:21 * aquarial sent a long message:  < https://matrix.org/_matrix/media/v1/download/matrix.org/CPewIWiJNGYkWxGGGjlPVMeH >
19:59:40 <glguy> aquarial: That message was too long, it shows up as a pastebin entry
20:00:08 <aquarial> Can I write "cabal-version: >=1.10" in project.cabal
20:00:28 <Solonarv> you can
20:00:31 <aquarial> Or should I use "cabal-version: 1.18"
20:00:35 <Solonarv> but you probably want to write the latter
20:00:57 <Solonarv> also, newer cabal versions have useful features (common stanzas!), which you might want to use
20:01:11 <aquarial> Won't that pin cabal-version to 1.18?
20:01:33 <aquarial> Or is that a lower bound
20:02:31 <Solonarv> it says which version of cabal you wrote your package description in
20:02:37 <Solonarv> it doesn't pin anything
20:02:49 <Solonarv> so, effectively a lower bound
20:03:42 <aquarial> Makes sense. My cabal file is https://github.com/aquarial/discord-haskell/blob/master/discord-haskell.cabal
20:06:00 <Solonarv> oh, that's your library! I've been meaning to try it
20:06:17 * hackage discord-haskell 0.8.2 - Write bots for Discord in Haskell  https://hackage.haskell.org/package/discord-haskell-0.8.2 (Aquarial)
20:07:18 <Solonarv> short summary (taken from the info text of 'cabal init'):
20:07:18 <Solonarv> Please choose version of the Cabal specification to use:
20:07:18 <Solonarv>  * 1) 1.10   (legacy)
20:07:18 <Solonarv>    2) 2.0    (+ support for Backpack, internal sub-libs, '^>=' operator)
20:07:18 <Solonarv>    3) 2.2    (+ support for 'common', 'elif', redundant commas, SPDX)
20:07:20 <Solonarv>    4) 2.4    (+ support for '**' globbing)
20:08:38 <Solonarv> I'm not sure how much value there is in sticking to pre-2.0 versions tbh
20:10:38 <glguy> I need a pretty good reason to not use at least 2.2
20:11:09 <Solonarv> same
20:11:31 <Solonarv> I use especially ^>= and common stanzas all the time
20:14:13 <ddellacosta> love common stanzas
20:14:17 <aquarial> K. I'll change the src but hold off publishing to hackage until there's changes for a minor release
20:14:46 <Solonarv> :D
20:15:01 <Solonarv> keep in mind that cabal-version: FOO needs to be the first line in your cabal file
20:15:12 <Solonarv> starting at whichever version, I don't remember
20:26:18 <aquarial> :q
20:26:24 <aquarial> that was supposed to be in a vim window
20:36:49 <aquarial> discord-haskell is usable but the interface is too imperative.
21:34:49 <fen> if a Tree is Free Nonempty, what is a Free Tree?
21:35:02 <fen> and then, what is Free of this, and so on
21:42:52 <fen> for some perspective on this, a similar question is "what is the use of a Tree", as it is simply Free Nonempty? While it seems difficult to imagine the use of Free Tree, it is the opposite to imagine simply using Nonempty, as the Free version (Tree) is so commonly used.
21:54:12 <jusss> where is this wrong https://paste.ubuntu.com/p/68mnPRfC4K/
21:57:42 <slack1256> jusss: can you post the error too?
21:58:29 <slack1256> jusss: althought I would guess it is at class definition, that `b` came from thin-air
21:59:10 <jusss> slack1256 https://paste.ubuntu.com/p/7fYFYJtf8W/
22:00:34 <slack1256> Yep, it is the class definition the problem, what should `value (V (3 :: Int))` return?
22:00:59 <slack1256> from the class definition it should return a `forall b` for any type b
22:01:43 <jusss> value (V 3) should return 3
22:02:05 <fen> jusss: you mean it should fix b == a ?
22:02:41 <fen> basically it can only return undefined right?
22:02:47 <slack1256> jusss: just a sighly of topic, you can do this easily with Data.Coerce
22:02:55 <jusss> fen I don't know, can I use T a as a in class ?
22:03:35 <fen> sure, thats not the problem, its that you cant return an arbitrary value of any type from that
22:03:57 <fen> well you could, but it would be undefined, or error or something
22:04:07 <fen> :t undefined 
22:04:08 <slack1256> % (coerce (Identity (5 :: Int)) :: Int
22:04:08 <yahb> slack1256: ; <interactive>:52:37: error: parse error (possibly incorrect indentation or mismatched brackets)
22:04:08 <lambdabot> a
22:04:24 <slack1256> % (coerce (Identity (5 :: Int))) :: Int
22:04:24 <yahb> slack1256: 5
22:04:37 <slack1256> ^ jusss
22:05:10 <fen> slack1256: but the class Coerce has 2 parameters
22:05:41 <slack1256> fen: Sure, the class Coerce establishes a relationship between those two parameter
22:05:44 <jusss> slack1256 fen actually, I just want to learn the typeclass
22:06:00 <fen> jusss: is it ok to provide the specific type `b' in the instance?
22:06:09 <jusss> I don't know
22:06:12 <slack1256> jusss class fails in that it doesn't establishes the relationship between the `a` type and the `b` type
22:06:39 <slack1256> `b` comes from thin air
22:06:49 <slack1256> usual solution for this is functional dependencies
22:06:55 <jusss> like data T a = V a, if this (T a) as the instance of typeclass in `class Whatever (T a) where` ?
22:07:32 <slack1256> I am doing a paste
22:07:35 <jusss> so that b must be in the left of '=' first?
22:07:36 <fen> jusss: if you just want to learn how to write typeclass it might be worth having a specific goal in mind, otherwise, if you just want to understand the error your seeing here, its simply that `b' is unspecified - it should appear alongside `a' as a parameter of the class
22:08:15 <jusss> fen how I can use a to express b? represent ? sorry my English is not good
22:08:28 <fen> if you wrote a class with a -> Int as the type of the function, that would work
22:09:40 <fen> jusss: if the class has both `a' and `b' as parameters, the instances serve to define a correspondence between these 2 types
22:10:49 <fen> class MyCoerce a b where value :: a -> b
22:11:46 <fen> then eg instance MyCoerce Int Bool where value a = True
22:11:51 <slack1256> jusss: https://paste.ubuntu.com/p/tmyJVynt2s/
22:11:56 <slack1256> We can discuss it 
22:12:08 <siraben> Is there a timeline for adding dependent types to Haskell?
22:12:20 <siraben> When would the first experimental version be released?
22:12:49 <slack1256> siraben: Sure, you just have to travel to the future and read Richard's blog for the actual dates :-)
22:12:51 <fen> the version slack1256 gave has a functional dependency saying that "a determines b" which means there can only be one instance for each seperate `a'
22:13:08 <siraben> slack1256:  which Richard?
22:13:15 <slack1256> einsberg
22:13:48 <MarcelineVQ> Richard A. Eisenberg :>
22:13:59 <slack1256> ^
22:15:25 <fen> wow, what are partial type constructors? https://cs.brynmawr.edu/~rae/papers/2019/partialdata/partialdata.pdf
22:15:52 <fen> what does it mean "Functional programming languages assume that type constructors are total." ?
22:18:40 <fen> it says " a constraint in the type definition does not itself provide a proof of this constraint in functions"
22:19:26 <fen> not quite sure how that is something to do with it being "partial", or to give meaning to this term...
22:22:12 <fen> oh it must mean that its not "total" because it only is defined over a subset of possible types, which instantiate the constraint
22:22:55 <slack1256> fen: you just wanted to make us read that paper right? :-D
22:24:51 <fen> haha, was looking for where the dependent haskell stuff is... seems like a lot of the groundwork is here; http://www.cis.upenn.edu/~sweirich/publications.html
22:25:29 <jusss> slack1256 https://paste.ubuntu.com/p/S6SN8R5znM/
22:25:47 <fen> slack1256: hey, you dont have any idea why we dont encounter Free Tree more commonly?  
22:26:39 <fen> jusss: you need to add {-# Langage MultiParamTypeClasses #-} to the top of your code
22:26:44 <fen> Language*
22:27:01 <slack1256> I think it `Free Tree` is isomorphic to `Free NonEmpty`
22:27:25 <fen> hmm, so its just the Nonempty not being Free Something
22:27:34 <jusss> ok
22:27:39 <fen> thanks
22:28:01 <slack1256> as `Free Tree` is isomorphic to `Free (Free NonEmpty)` and you can collapse the Free as they don't add any extra context (they are free), so that last one is isomorphic to `Free Nonempty`
22:28:01 <jusss> slack1256 fen I have to go, thanks 
22:29:30 <fen> slack1256: the partitioning isnt a way to contain more information somehow?
22:30:18 <fen> otherwise it seems like you could make the same argument and just "flatten" the Free thing down since it is a monoid (MonadPlus...)
22:31:01 <fen> like, that "extra information from partitioning" seems to be all that Tree gives us over Nonempty in the first place
22:31:31 <fen> so then why do we never encounter this next level up, or any of the upper levels still
22:32:01 <fen> Free (Free (Free Nonempty))) etc
22:32:34 <fen> is there even a name for Free Tree!?
22:33:07 <slack1256> to answert he first statement, yeah, the Free gives you partioning as it basically a tree. But you can simulate more partioning making a tree deeper and branching at each step
22:33:17 <slack1256> so there is no extra information actually
22:33:22 <fen> or is the argument basically that "a tree of a tree is just a tree" and that makes it always flattened 
22:33:30 <slack1256> exactly
22:34:14 <slack1256> Think what exactly does Free do, it gives you a way to decorate a recursive step
22:34:21 <fen> no extra information? are you sure? what if you used two different definitions of Free and this somehow "coloured" the subtrees somehow
22:35:29 <fen> it seems like the fact that Tree is Free might be why it is such a pervasive type
22:36:42 <fen> there is this kind of puzzling paper which seems just by the fact it exists, to mean that there must be some kind of higher order version of a tree
22:36:43 <fen> https://www.cs.le.ac.uk/people/akurz/Papers/CALCO-07/GK07.pdf
22:36:52 <fen> "higher dimenstional trees" they call it
22:37:11 <fen> these wouldnt even be a thing if there was no extra information right?
22:37:27 <slack1256> I would think either the two colored version would fail somehow or it would be really hard coded on the way of coloring. I mean  on Free `join` is trivial
22:37:33 <slack1256> it must be trivial
22:37:58 <fen> can you explain what you mean by this?
22:38:41 <slack1256> yep, the usual definition for `bind f` is `join . fmap f`
22:39:05 <fen> oh right, join as in, if the same definition of Free was used for both coulours
22:39:19 <slack1256> The only constrain on a `Free f` monad instance is that `f` is a Functor so fmap make sense
22:39:38 <slack1256> The Free ""decorator"" makes the `join` part trivial
22:39:50 <slack1256> so it really has to be trivial for it to work
22:40:30 <slack1256> on you case of the two colour, in my head (without example code) it should either fail, or somehow be trivial (via hard coding, and knowing the way of assigning color)
22:40:30 <fen> but it wouldnt be join if there were 2 different definitions of Free being used deliberately to encode this colouring
22:40:45 <fen> Functor f => Free1 (Free2 f)
22:41:06 <slack1256> Yeah, no chance, unless you prove that `Free1 ~ Free2`
22:41:28 <fen> thats not supposed to have join work over it
22:41:33 <fen> that would be "lossy"
22:41:39 <fen> it would discard some information
22:42:03 <slack1256> Well `join` can discard information, but it wouldn't be trivial then
22:42:08 <fen> that was the idea of the example, to show that these higher dimensional trees must encode more information that regular trees
22:42:17 <slack1256> and the part of `join` being trivial is what really makes `Free` free!
22:43:12 <fen> slack1256: you mean that it would be also trivial to pick any depth to "split" the tree into a tree of trees?
22:43:28 <fen> as the inverse of the join
22:43:51 <fen> like, the "no extra information" aspect of this triviality means this must be so
22:44:06 <slack1256> Mmmm I don't know what do you mean by that
22:44:41 <fen> the idea is that Tree1 (Tree2 a) would not have such a reversible join, as it would discard information
22:45:11 <fen> hmm, actually, the join isnt reversable since it forgets where the nesting was
22:45:19 <slack1256> exactly
22:45:23 <slack1256> what do you mean by reversible?
22:45:28 <slack1256> it can return to be a lot of things
22:46:07 <fen> not sure, just trying to understand why "higher dimensional trees" are a thing, despite what you say about them being isomorphic to Tree
22:46:18 <slack1256> I would read the paper
22:46:31 <fen> it is not a simple idea
22:46:41 <slack1256> maybe they are using really specific terms, but the names are generic
22:46:51 <fen> it says something about the "index" being a tree of one order lower
22:47:07 <fen> which is utterly mind-boggling 
22:47:17 <fen> and so have never really understood this idea
22:49:19 <fen> data Rose f a = Rose a (Maybe (f (Rose f a)))
22:49:29 <fen> type Tree f a = Maybe (Rose f a)
22:49:35 <fen> data Rose0 a = Rose0 a -- = \X -> X
22:49:42 <fen> type Rose1 = Rose Rose0 -- = \X -> Listˆ+(X)
22:49:48 <fen> type Rose2 = Rose Rose1 -- = \X -> Rose(X)
22:49:54 <fen> type Rose3 = Rose Rose2
22:50:09 <fen> so that Rose3 is this type that is a "tree of trees"
22:50:51 <fen> oh, they manage to give list as a lower dimensional version of tree
22:51:09 <fen> that was the point about "well why not just flatten the tree down to list"
22:51:25 <fen> apparently list can be written using something like free...
22:52:23 <fen> and then, the rest of the paper is basically using inaccessible language
22:54:25 <slack1256> I think I get the problem
22:54:37 <fen> you do!?
22:54:42 <slack1256> you think the dimensionality the paper talks about it at the level of data structures
22:54:51 <slack1256> it a term defined by this rogers guy
22:55:08 <fen> oh no, that sounds bad!
22:55:12 <slack1256> he is concerned is a dimensionality of a grammar (a term he defined on his paper)
22:55:31 <slack1256> and how to model that correctly he defines a dimensionality tree
22:55:42 <fen> well, it has the Free of Free idea as in the example above
22:55:53 <slack1256> well usually grammars are structured as tree, but the levels are not explicit
22:56:11 <slack1256> The idea of you linked paper is to give a categorial treatment to the rogers formulation
22:56:25 <fen> thats why its hard to read
22:56:37 <slack1256> here is the deal, you can "represent" different index of tree by stacking Free (Free (Free))) etc
22:56:43 <slack1256> so the levels are explicit
22:56:59 <slack1256> plust you can give a generic `do notation` for all the levels
22:57:18 <slack1256> but really the dimensionality stuff is a property of the grammar modelled via Free
22:57:51 <slack1256> in no way it is saying something about the "efficient" or "succint" representation of the data structure that is (Free (Free (Free)))
22:57:57 <fen> still not sure about this idea of "index"
22:58:24 <slack1256> Well, read the original paper, scihub works wonky from my end.
22:58:50 <fen> oh, you used the term, thought maybe it was something that could be common
22:58:53 <slack1256> also, this is my read of the situation, it is by no mean authoritative
22:59:38 <fen> well yeah, certainly sounds about right, using category theory as a way to write about the idea, without needing to be able to comprehend it
23:00:32 <fen> but "trees of trees" or "clusters of clusters" (graphs of graphs) etc, seems almost comprehensible 
23:00:40 <fen> yet there are very few examples
23:01:00 <fen> we seem to just use lists and trees
23:02:13 <fen> despite the same equivalence between lists vs trees, free lists vs free trees, and free trees vs free (trees of trees) etc...
23:02:45 <fen> it seems somewhat arbitrary that we *dont* flatten trees to lists, but we flatten everything more free than trees down to trees
23:03:24 <fen> thats the confusing thing
23:03:47 * hackage salak-yaml 0.2.9.1 - Configuration Loader for yaml  https://hackage.haskell.org/package/salak-yaml-0.2.9.1 (leptonyu)
23:03:50 <fen> do human brains somehow reject the idea?
23:04:23 <fen> i cant understand why this apparently arbitrary choice prevails so
23:04:47 * hackage salak 0.2.9.1, salak-toml 0.2.9.1 (leptonyu): https://qbin.io/br-quit-usrd
23:07:36 <fen> i understand the point from before that Functor f => (Free (Free f)) is basically just (Free f), but that code snippet from the paper shows that list is almost the same... they manage to make the Cofree Maybe into a Free by having a Maybe at the top or something 
23:08:23 <fen> hmm, maybe its that difference at the basecase that gives us this artefact of Tree and List being all we ever use
23:09:05 <fen> anyway, all this totally messes up the idea of "higher order state unfolds".... apparently we just need to be able to "unfold a tree" and never bother with anything higher order
23:09:25 <fen> this makes some recent ideas being developed redundent maybe
23:10:13 <fen> it would be really good to be able to understand if this was the case or not, because at the moment its not clear if its something to do with things being confusing or if there is a mathematical thing at the heart of it
23:10:47 <fen> free is the one thing it doesnt makes sense to do free too... confusing or what!?
23:11:54 <fen> and it seems to be having some baring on a making some abstraction about "higher order state unfold" redundant or just a bad idea or something
23:12:07 <fen> but its really complicated and it would be great to get some feedback!
23:14:46 <fen> ah, no, thats not quite the point... you can "unfold a nested thing" like a list of lists or a nonempty of lists of nonempty or something... thats fine.. but now there is some extra complication because instead of just having alternating list and nonempty as the only choices (and stream?) but there are also Trees now aswell!!! 
23:15:28 <fen> and the confusion with "higher dimensional trees" is the question about if this can safely be truncated at that point without having to consider also Trees of Trees... 
