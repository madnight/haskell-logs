00:18:47 <araujo> Hello.
00:25:51 <cm> yo
01:29:55 <shapr> I was reading the GHC-SMP paper, and I realized that often there are multiple thunks that could be eval'd at once...
01:30:52 <shapr> For example, 1 + 2 + 3 could be ((1 + 2) + 3) or (1 + (2 + 3))
01:31:18 <shapr> But I suspect that thunks are 'overordered' upon compilation.
01:32:25 <shapr> But maybe not.. does anyone know more?
01:32:39 <Saulzar> The overhead would be too much for something simple...
01:33:13 <shapr> I'm trying to think of ways to integrate concurrency into Haskell. I think ideas from Erlang, E, and Nesl/Sisal are worthwhile.
01:33:30 <Saulzar> I thought that was what the parrallel ghc thing was about, that you explcitly insert `par` where you know it would be useful
01:34:03 <shapr> Maybe something like par$ to replace $ ?
01:37:17 <Saulzar> I have only glanced over it briefly, but I thought that they had "par" equivalents for "seq" (or they could be made without too much trouble)
01:38:40 <shapr> I've discovered that I want concurrency as part of Haskell. I'd really like a single concept that integrates both multicpu parallelism and across the network distribution.
01:38:57 <Saulzar> Certainly Ocaml can be beaten on concurrency, the word there was that it can never be multithreaded :)
01:40:17 <shapr> So my current theory is that each thunk should include E style permissions, and the language should not depend on a thunk returning, thereby dealing with network or component failure.
01:40:24 <shapr> On the downside, that doesn't sound like Haskell at all.
01:44:46 <JohnMeacham> the network should not be transparent. it is the fatal flaw in distributed component models.
01:45:23 <JohnMeacham> you can't abstract away the network, accessing remote objects is inherently different than accessing local ones. I could back this up, if I wern't falling asleep.
01:46:13 <sieni> can't you treat the local objects as remote objects ;-)
01:46:16 <sieni> ?
01:46:30 <shapr> I don't think the network should be transparent, I think everything should act like it's across the network.
01:47:13 <shapr> Because local access can fail just as much as remote access can fail, especially once we leave the single cpu viewpoint.
01:48:08 <shapr> Or maybe I'm claiming that failure and time delay is the inherent property of both remote via network and remote via smp.
01:48:46 <shapr> Are there any significant differences between computations distributed via SMP and network?
01:49:25 <shapr> I wonder if moving delay and failure into the basic viewpoint of a language implies a dataflow architecture.
01:49:46 <Saulzar> Hmm, everything is exaggerated by a large fator.
01:50:10 <shapr> Synchronicity is one assumption of most of the languages we use today, but dataflow languages are inherently async.
01:50:17 <Saulzar> Chance of failure is many many times higher, delay is many many times higher
01:50:37 <pejo> shapr, if your claim is "SMP adds delay" then that goes for cache-misses on UP too. 
01:51:02 <tuomov> swap adds delay
01:51:06 <shapr> pejo: Good point, do you see any connection for failure on UP?
01:51:28 <Saulzar> Well, chance of failure is nothing for SMP depending on your code.
01:51:39 <shapr> For both network and SMP a request may not ever return.
01:52:08 <Saulzar> Only if your code never returns, and you can make sure of that 
01:52:11 <shapr> SMP failure rates may change when you have 64 cores on a die.
01:52:45 <Saulzar> True :)
01:52:59 <shapr> Hm, hardware flaws do happen, but software in general pretends that they don't. Maybe languages should be aware of failure?
01:53:00 <pejo> shapr, is there any OS/Hardware that just maps away the broken CPU silently?
01:53:26 <tuomov> all the unices for number-crunching hardware?
01:53:28 <shapr> Not silently, events are sent via hotplug.
01:53:45 <Saulzar> I don't think transparently anything really handles hardware failure
01:53:50 <shapr> tuomov: I've never used them, how do they handle failure?
01:54:14 <pejo> tuomov, which are those?
01:54:24 <tuomov> just stop using the broken cpu/unit? 
01:54:34 <tuomov> I don't really _know_ anything, though. just citing what I've heard.
01:54:37 <shapr> von Neumann was interested in reliable computations on unreliable hardware near the end of his life.
01:54:57 <shapr> Seems like the two important pieces are failure and delay.
01:55:14 <shapr> Though maybe failure is a special case of delay =)
01:55:21 <pejo> Solaris just panics when it detects a CPU failure on the cheapass machines. I have no idea how it behaves on the more expensive hardware. 
01:55:31 <Saulzar> and errors, if you're dealing with the network
01:55:39 <tuomov> it should stop using the broken cpu/mobo unit on high-end hardware
01:55:49 <shapr> Errors are part of failure/delay.
01:55:52 <tuomov> it's usually not just the cpu afaik; every cpu has its own mobo
01:56:07 <shapr> I wonder if classing failure as infinite delay is a good convention?
01:56:20 <tuomov> and it should be possible to change the broken unit on the fly
01:56:26 <shapr> That would assume that the system has some way of measuring how much delay is too much, and can retry a computation.
01:56:28 <Saulzar> errors certainly aren't an infinite delay :)
01:56:50 <cm> "fail early" :)
01:56:53 <tuomov> it's really no different from changing a broken machine in a cluster
01:56:55 <shapr> They do delay the result of the computation for an infinite about of time.
01:57:05 <Saulzar> Hmm
01:57:05 <tuomov> that's how those high-end machines work. they have a very fast internal network
01:57:06 <pejo> tuomov, adding stuff to a partition during runtime is easy, it's the removal of the old stuff that is hard.
01:57:10 <tuomov> (that gigabit ethernet is catching up on?)
01:58:00 <pejo> tuomov, the gigabit ethernet still adds plenty of delay, even if you might start to compare raw bandwidth.
01:58:17 <flux__> shapr, but usually you don't know if a calculcation is going to stop given finite time or not, when as with failures you would
01:58:29 <tuomov> http://en.wikipedia.org/wiki/Non-Uniform_Memory_Access
01:58:30 <flux__> shapr, it'd be a shame to waste that piece of information
01:58:31 <shapr> Ok, so the simple model I have right now is: integrate delay as a basic part of the language. Call errors and failure infinite delays. 
01:59:04 <shapr> flux__: errors as out of band results?
01:59:10 * shapr thinks about that.
01:59:27 <shapr> "It's a result, but not as we know it."
01:59:30 <flux__> shapr, well I didn't think of replacement, but atleast just thinking them as infinite delays would seem wasteful
01:59:41 <flux__> shapr, there'd be a mechanism to abort a function then?
01:59:51 <flux__> or what would be the mechanism to retry it
02:00:10 <shapr> I'm thinking you have a timeout&retry mechanism.
02:00:14 <pejo> tuomov, even the small machines have different memory latencies depending on if your memory is local to the CPU board or on a neighbour. I have no idea if people care about that though.
02:00:19 <shapr> This would give you eager scheduling for free too.
02:00:40 <flux__> so what if you have a very long calculation going on, and because it hasn't completed in a minute, you just retry it? never getting the result.
02:00:56 <shapr> flux__: What calculations cannot be broken into chunks?
02:01:13 <flux__> raytracing 1x1 image of a complex scene?-)
02:01:38 <tuomov> none. every cpu instruction takes a fixed time to complete.
02:01:42 <flux__> should the function caller/function itself somehow know how long evaluaiting the function is going to take?
02:01:44 <shapr> tuomov: exactly
02:02:14 <kagy> shapr, why not look at Mozart/Oz too for inspiration it has lightweight concurrency / dataflow variables / transparent distribution of language entities and a simple model of distributed faults.  (obdisclaimer: I am biased!)
02:02:19 <pejo> shapr, there's a hidden efficiency problem in there too which you might have abstracted away from. 
02:02:19 <shapr> flux__: hm.. what would that do?
02:02:21 <cm> shapr, as long as the default timeout is "infinite", fine with me.
02:02:35 <cm> shapr, but then, I don't want failure to be modelle as infinite delay :-)
02:02:58 <cm> shapr, "Specifying timeouts considered premature specification" ;-)
02:03:08 <shapr> cm: If there's a default value for 'retry' that's relatively low, any delay over the magic retry number is the same as failure.
02:03:26 <shapr> kagy: I've been meaning to buy CTM and hit Mozart/Oz, just haven't gotten there yet.
02:03:33 <flux__> hmm, actually.. maybe if each function would know how long it is going to take, the compiler could establish some sort of worst case time for most functions
02:03:41 <shapr> pejo: What's the hidden problem?
02:03:56 <shapr> flux__: How would you do that?
02:04:06 <tuomov> you must restrict the class of functions or else you've solved the halting problem :)
02:04:22 <flux__> I didn't say all functions ;)
02:05:17 <pejo> shapr, not all problems are better off with more paralellism, breaking it up in too small 'chunks' will make the computation "go slow".
02:05:38 <flux__> the simplest way would be to sum all the times of functions called outside loops, then make an educated upper bound guess about how the length of the loops depend on the input parameters, etc ;)
02:06:19 <flux__> I guess it might not work that great for the interesting functions though
02:06:23 <cm> then you tweak the parameters at runtime, oops, this is PGO ;)
02:06:39 <cm> s/at runtime/using profile information
02:07:06 <cm> -e +ing
02:07:23 <cm> brb
02:07:38 <kagy> shapr:  I haven't read it, but maybe http://www.ps.uni-sb.de/PapersOz/ProgrammingSysLab/ngc98.pdf will give you a taste (and its cheaper than the book)
02:07:46 <pejo> shapr/flux, you might be interested in Y. A. Liu and G. Gomez. Automatic accurate cost-bound analysis for high-level languages.
02:10:02 <pejo> shapr/flux, or maybe Accurate step counting by Hope and Hutton. 
02:10:47 <pejo> Lunchtime though, bbl.
02:10:54 <shapr> pejo: What about setting a high timeout for large chunks?
02:11:01 <shapr> Anyway, thanks for the discussion.
02:11:17 * shapr goes back to being a web developer writing Python
02:22:55 <stesch> Next year I'm going back to PHP. |sigh|
02:24:20 <shapr> stesch: Why?
02:26:13 <shapr> dons: Hey, what about a HWN plugin for lambdabot? Then I could do @hwn add "release of sha256/sha512 library" http://davidmercer.nfshost.com/projects/shaskell/shaskell.html
02:26:31 <shapr> dons: On the downside, the irc limit of 512 chars may not be long enough for a HWN entry.
02:26:38 <stesch> Because I'm living in Germany. No (better) Jobs and very hard to found a startup.
02:26:45 <shapr> stesch: You could move.
02:27:20 <stesch> No, I couldn't.
02:27:56 <shapr> If you say so.
02:28:30 <shapr> I moved from Alabama to Finland some six years ago. I'm still having my great European adventure.
02:30:12 <shapr> I wish the Simons would use an email client that doesn't break mailing list threads.
02:30:23 <Saulzar> I would like to think I'll not get stuck doing anything related to web devel.
02:31:31 <shapr> I would very much like to get another job.
02:32:19 <Saulzar> I guess being keen on Haskell is not the way to succeed in that respect :)
02:33:18 <shapr> Can you guess how interviewers respond when you say you like to write compilers for fun?
02:33:27 <Saulzar> Hehe.
02:33:39 <Saulzar> Microsoft would probably employ you then :P
02:33:52 <shapr> I'd love to hack on GHC =)
02:34:31 <Saulzar> A friend of mine at Uni who has been writing a haskell compiler for .NET got job offers from both Apple and Microsoft
02:34:48 <Saulzar> Mind you, he hates haskell and bags it at every oppertunity
02:34:55 <shapr> Whyfor?
02:35:32 <shapr> I have the opposite problem. I despise languages that keep me from using my awesome powers.
02:35:37 <Saulzar> He has rather a strong entrenched belief in certain ways of doing things I think :)
02:36:05 <shapr> I have a strong entrenched belief that there are better ways of doing things that I do not yet understand.
02:36:25 <Saulzar> If he didn't constantly rant about the evils of haskell I would never have botherd to take a look :)
02:37:42 <Saulzar> Yep, that's my view. 
02:38:22 <Saulzar> But, he likes writing compilers for fun - I imagine he might tell interviewers that too
02:39:04 <_darius>  bring your compiler in to demo
02:42:16 <Saulzar> He's often around here, you could ask him, "olliej" I don't think it can compile anything with floating point numbers for some reason
02:42:54 <pejo> Saulzar, aren't we all here due to our beliefs?
02:44:31 <Saulzar> Either that or from practical issues, eg. work, I suppose :)
02:45:42 <shapr> Not too many of those here.
02:46:48 <shapr> I'd guess only ten or fifteen commercially employed Haskellers here. I think the researchers and the hobbyists are equally represented on #haskell.
02:46:52 <Saulzar> I guess quite a few encounter haskell at University though
02:47:47 <pejo> Saul, which is a good thing!
02:48:47 <pejo> Been discussions in here about what they teach in the first course of functional programming at universities though. A lot of people get the impression that it's all useless since "you can't do anything real" with it.
02:49:38 <shapr> Yeah, I spoke to some www.ltu.se graduates who said "Oh, that Haskell language only has nested lists, right? I learned that during my CS degree."
02:49:48 <shapr> hiya Andrew_P 
02:50:09 <Andrew_P> Hello
02:50:16 <shapr> How's code?
02:50:38 <Saulzar> I went right the way through and never saw Haskell at all :)
02:50:53 <shapr> pejo: Sadly, these LTU grads did not want to hear about my mailing list archive search webapp, nor FLM, nor any other code I've written.
02:51:26 <pejo> shapr, if we're talking LTU specifics there is a lot that could be done. It's just that someone has to do it. 
02:52:13 <shapr> pejo: I once found some monad notes on a whiteboard near the CS department, and immediately annotated those notes with descriptions of how arrows could improve the code, and urls to further info.
02:52:52 <shapr> Saulzar: I never got in...
02:53:07 <pejo> shapr, boggle. Someone wrote down monad notes? I don't remember much of the fp course but I have no memory what so ever about Monads even being mentioned.
02:53:37 <shapr> In that case, whoever came back to look at the notes must have been doubly surprised.
02:53:47 <Andrew_P> I'm watching SICP lectures http://swiss.csail.mit.edu/classes/6.001/abelson-sussman-lectures/ these days and i found them to be rather useful, though examples is not in Haskell :)
02:53:49 <Saulzar> Anyway, graduates have enough problems with writing 4 lines of Java code...
02:54:30 <shapr> pejo: I'm moving to Stockholm over the summer, can you point me to good type theory gatherings in that city?
02:54:40 <shapr> I know FaxÃ©n is at KTH, do you know anyone else?
02:54:55 <pejo> shapr, he's the only guy I'm aware of there too. 
02:55:13 <shapr> I guess I'll have to venture to GÃ¶teborg on the weekends.
02:55:36 <shapr> Andrew_P: Yeah, SICP and lectures are way cool. I only wish they had the MPG lectures on a torrent.
02:56:04 <pejo> shapr, going to participate in FC Z or are you doing something else there? ;)
02:56:17 <shapr> Saulzar: I still think that math classes would improve my understanding of many things, so I'll probably end up going to uni at some point.
02:56:22 <shapr> FC Z?
02:56:33 <shapr> heh, hockey?
02:56:43 <pejo> shapr, soccer team put together by ZTV this year. They're doing it for hockey next year. 
02:57:01 <shapr> I'll probably play unicycle hockey in sthlm, does that count? :-)
02:58:11 <shapr> I wish cyclecomponents.se would send my new tire! I'm so impatient!
02:58:19 <pejo> shapr, will you end up on TV in a 10 week sequel? :-)
02:58:52 <shapr> If they offer me better money than I'm making now (which should be easy ;-), yeah sure.
02:58:53 <pejo> (The soccer team consisted of the worst soccer players ever, but a lot of 'different' personalities for sure). 
03:00:27 <shapr> For extra fun, you can light the puck on fire: http://www.unicyclist.com/gallery/albup81
03:03:04 <shapr> I doubt I fit into the worst soccer players, I'm merely bad.
03:03:55 <pejo> Yeah, you're probably too ordinary when it comes to the social side as well, or else you'd be without customers. :-)
03:04:31 <shapr> Well, I don't exactly have a huge number of customers...
03:05:19 <Saulzar> They would never put the worst soccor players on TV, I'm sure there are plenty of elderly who couldn't even move the ball
03:06:01 <pejo> Saulzar, they have age constraints too, 20-35 or something. Apparently there are similar shows in denmark for example.
03:06:56 <Saulzar> Competing to be the worst never works quite right :)
03:07:02 <pejo> shapr, oh btw, you might have people on SICS who are interesting. Not sure where Armstrong is employed nowdays, maybe Ericsson research?
03:07:33 <pejo> shapr, and MDH has Lisper with his grad students. And a bunch of real time stuff with their real time center (Västerås?).
03:08:29 <shapr> ah, good point
03:10:39 <shapr> I wonder where Urban Boquist is working these days.
03:11:38 <pejo> shapr, can't help you there, sorry. He doesn't seem to reply to email sent to his chalmers adress, thats all I know. 
03:11:49 <pejo> shapr, have you tried asking Lennart?
03:12:23 <shapr> good idea.
03:12:29 <shapr> I'll ask him when he's online.
03:12:31 <shapr> @seen lennart
03:12:32 <lambdabot> I saw lennart leaving #haskell 7 hours, 50 minutes and 18 seconds ago.
03:33:08 <MaxCohen> guys i have a problem installing the compiler GHC.. and i don;t have a running GHC.. is there and solution.. to it.. i am trying to install it on Linux-Slackware
03:33:25 * adept . o O (use real distro)
03:33:27 * adept hides
03:33:35 <adept> MaxCohen: are you compiling from sources?
03:34:07 <MaxCohen> eys
03:34:08 <MaxCohen> yes
03:34:27 <adept> MaxCohen: you might be better off by taking binary packages for RH/debian/... and using them, or by perusing build infrastructure from gentoo
03:34:29 <MaxCohen> yes i am using the original source from Haskell.org
03:34:50 <MaxCohen> ghc.6.4.1.tar.bz2
03:34:58 <MaxCohen> i never use binaries
03:35:08 <Saulzar> You can install from the generic glibc 2.3 package very easily from here http://haskell.org/ghc/download_ghc_641.html
03:35:32 <Saulzar> They might save you some trouble
03:36:07 <MaxCohen> thanks will check.. 
03:36:13 <adept> MaxCohen: because (IMO) building ghc is on par with building gcc+libc (in terms of difficulty)
03:36:43 <MaxCohen> hmm.. yeah can be true.. adept 
03:36:46 <MaxCohen> :P
03:40:56 <MaxCohen> adept looks like i have to build whole thing.. from scratch.. 
03:41:39 <xerox> Did you see pycontest.net?
03:43:07 <pejo> MaxCohen, or use the generic binary as Saulzar suggested.
03:45:17 <MaxCohen> pejo binaries suck.. downloading source from scratch.. used to compile from source. thanks
03:46:24 <adept> xerox: cool. now we just need python backend for ghc
03:47:20 <xerox> adept: haha, that's right.
03:47:21 <MaxCohen> all you guys using hashell ??
03:48:02 <xerox> MaxCohen: http://www.haskell.org/ghc/survey2005-summary.html
03:48:03 <xerox> :-)
03:48:36 <MaxCohen> wow thanks xerox 
03:48:38 <xerox> Well, this one: http://www.haskell.org/ghc/survey2005/
03:48:41 <xerox> You're welcome.
03:50:40 <MaxCohen> hmm.. well most of them on Windows eh!
03:51:09 <MaxCohen> 47%
03:51:49 <xerox> Well... :)
03:51:53 <sieni> http://os.newsforge.com/article.pl?sid=05/05/18/2033216
03:55:07 <Saulzar> 47% is probably fairly low even for software developers
03:56:55 <MaxCohen> Saulzar nah.. it great coz it's 47 out of 100 :P
03:58:32 <Saulzar> xerox, That looks like a fun contest...
04:07:52 <Saulzar> I had an exam question once for wiring up a 7 segment display, I couldn't figure out what a "5" was meant to look like
04:09:25 <Saulzar> So I drew a funny looking shape and labelled it "pretend this is a 5"
04:11:22 <MaxCohen> Saulzar even i had one too.. 
04:11:31 <MaxCohen> prolly on 8086
04:19:32 <stesch> MaxCohen: I'm not using Haskell. :-)
04:22:20 <MaxCohen> stesch wow!..
04:22:45 <MaxCohen> but still for kicks this is good channel... the ideas behind haskell are great
04:27:22 <sieni> well, lazy evaluation causes some lazy evaluation
04:37:22 <MaxCohen> sieni that's true :)
04:37:44 <xerox> Saulzar: well, I tried, but py is _so_ boring.
04:43:30 <Saulzar> xerox, Well, I meant the concept mainly ... I don't know any python myself
04:44:06 <Saulzar> Not too hard, but plenty of room for crazyness
04:45:59 <Saulzar> xerox, I can see that without arrow combinators you'd be lost :)
04:47:39 <xerox> Saulzar: that's right.
04:48:30 <xerox> Well, it doesn't have things like 'const', or guards, or...
04:48:51 <xs_> what's const?
04:49:29 <Saulzar> @type const
04:49:30 <lambdabot> forall a b. a -> b -> a
04:49:50 <Saulzar> > const 5 10
04:49:52 <lambdabot> 5
04:50:00 <xs_> hm, sorry i meant. what's the point of const?
04:50:16 <xs_> isn't it just let const a _ = a?
04:50:37 <Saulzar> Looks like it
04:51:16 <xerox> xs_: yes, defining a constant function.
04:51:42 <xerox> > let makePrivacy = map (const 'x') in makePrivacy "secret password"
04:51:45 <lambdabot> "xxxxxxxxxxxxxxx"
04:51:48 <xerox> ;-)
04:52:10 <xs_> heh! :)
04:52:13 <sieni> xs_: It gives you a constant function
04:53:58 <sieni> > sum $ (\x -> map x [1..10]) $ sin
04:54:00 <lambdabot> 1.4111883712180104
04:54:06 <sieni> sum $ (\x -> map x [1..10]) $ const 5
04:54:15 <sieni> > sum $ (\x -> map x [1..10]) $ const 5
04:54:16 <lambdabot> 50
04:54:24 <sieni> > sum $ (\x -> map x [1..10]) $ 5      
04:54:25 <lambdabot>  add an instance declaration for (Num (a -> b))
04:54:35 <xerox> Hmm.
04:54:48 <xerox> Why're you writing it that way?
04:55:00 <xerox> Oh, nice one, anyway.
04:55:17 <xerox> > sum $ (`map` [1..10]) sin
04:55:18 <lambdabot> 1.4111883712180104
04:55:20 <xerox> > sum $ (`map` [1..10]) const 5
04:55:22 <lambdabot>   The function `(`map` ([1 .. 10]))' is applied to two arguments,
04:55:22 <lambdabot>   but its type `(a -> b) -> [b]' has only one
04:55:28 <xerox> > sum $ (`map` [1..10]) $ const 5
04:55:29 <lambdabot> 50
04:55:46 <sieni> xerox: forgot about currying ^_^
04:59:10 <xerox> Well no, it's cool :)
05:05:20 * xerox mumbles
05:06:32 <xerox> Is there a monadic iterate?
05:07:00 <xerox> @hoogle (a -> m a) -> a -> m a
05:07:02 <lambdabot> Data.Generics.Schemes.everywhere :: (a -> a) -> a -> a
05:07:02 <lambdabot> Data.Generics.Schemes.everywhere' :: (a -> a) -> a -> a
05:07:02 <lambdabot> Prelude.($) :: (a -> b) -> a -> b
05:07:17 <xerox> @hoogle m a -> a -> m a
05:07:18 <lambdabot> Control.Parallel.par :: a -> b -> b
05:07:18 <lambdabot> Control.Parallel.seq :: a -> b -> b
05:07:18 <lambdabot> GHC.Conc.par :: a -> b -> b
05:07:30 <xerox> @hoogle m a -> m a -> m a
05:07:31 <lambdabot> Prelude.asTypeOf :: a -> a -> a
05:07:31 <lambdabot> Control.Parallel.par :: a -> b -> b
05:07:31 <lambdabot> Control.Parallel.seq :: a -> b -> b
05:07:38 <xerox> @hoogle Monad m => m a -> m a -> m a
05:07:39 <lambdabot> No matches, try a more general search
05:10:24 <Lemmih> @type Control.Monad.Fix.fix (\i f a -> f a >>= \a' -> Control.Monad.liftM (a:) (i f a'))
05:10:26 <lambdabot> forall a (m :: * -> *). (Monad m) => (a -> m a) -> a -> m [a]
05:13:01 <xerox> !
05:19:54 <xerox> ...aaand, is there a monadic fold?
05:20:35 <Saulzar> foldM
05:21:21 * Saulzar dozes off.
05:22:05 <xerox> @index foldM
05:22:06 <lambdabot> Control.Monad, Control.Monad.Reader, Control.Monad.Writer, Control.Monad.
05:22:06 <lambdabot> State, Control.Monad.RWS, Control.Monad.Identity, Control.Monad.Cont,
05:22:06 <lambdabot> Control.Monad.Error, Control.Monad.List
05:22:12 <xerox> @type Control.Monad.foldM
05:22:13 <lambdabot> forall a (m :: * -> *) b.
05:22:13 <lambdabot> (Monad m) =>
05:22:13 <lambdabot> (a -> b -> m a) -> a -> [b] -> m a
05:22:16 <xerox> Well, ok.
05:58:21 <araujo> Good morning.
05:59:25 <xerox> Oi.
05:59:26 <carp> anyone know what's happened to glass.cse.ogi.edu (where the ghc-devel darwinport gets ghc from)?
06:28:06 <basti_> what are "imprecise exceptions"?
06:34:45 <xerox> Could not find module `System.Console.Shell':
06:34:49 <xerox> oh my.. what is it now?
06:34:55 <xerox> (building lambdabot)
06:35:16 <basti_> a missing module?!
06:35:24 <xerox> Yes but where is it?
06:42:21 <Pupeno> Hello.
07:11:08 <MaxCohen> clear
07:14:36 <basti_> over and out maxcohoen, over and out.
07:18:18 <basti_> any way i can make programs link quicker?
07:18:44 <musasabi> basti_: praying/faster machine
07:18:56 <basti_> >.<
07:22:45 <ndm|bristol> basti_: yhc
07:23:50 <basti_> hmmk
07:24:22 <ndm|bristol> it has runtime linking, which is pretty much instant
07:24:58 <basti_> the problem is gtk2hs... it needs about 5 second to link or something
07:25:18 <ndm|bristol> not on my computer, takes more like 20 seconds
07:25:34 <ndm|bristol> i want gtk2hs for yhc at some ponit in the future, so gyhe can be compiled with yhc
07:28:28 <dcoutts__> ndm|bristol: that'd be cool
07:28:53 <ndm|bristol> yes, definately]
07:29:02 <dcoutts__> basti_: you mean static linking takes 5sec +
07:29:03 <ndm|bristol> and you'll get much smaller binaries, since it doesn't need to statically link
07:29:41 <dcoutts__> we get binaries 30% bigger than ghc's "hello world" now.
07:29:53 <dcoutts__> (using ghc's split-objs feature)
07:30:06 <dcoutts__> but yhc would be orders of magnitude smaller again
07:30:25 <ndm|bristol> indeed, gtk hello world should be ~1Kb with yhc
07:30:35 <dcoutts__> that'd be great
07:30:38 <basti_> dcoutts__: thats true. i thought dynamic should be faster, but i couldnt link dynamically
07:30:54 <dcoutts__> basti_: you mean with ghci? 
07:31:01 <dcoutts__> are you running win32?
07:31:04 <ndm|bristol> when yhc is ready for gtk, i'll let you know - i'm sure i could use a bit of help on it
07:31:32 <dcoutts__> ndm|bristol: yeah, It'll take a little porting and #ifdef'ing
07:31:43 <basti_> no, on linux
07:31:52 <basti_> and with ghc, not with ghci
07:31:58 <ndm|bristol> dcoutts__: i guessed, but it will be worth it
07:32:20 <ndm|bristol> the hope is to get all libraries in a yhc'able state
07:32:29 <dcoutts__> basti_: oh I see what you mean. If we could make ghc packages into .so dynamic libs then linking would be quicker. Yeah.
07:32:57 <dcoutts__> ndm|bristol: tell me when you start on that, I'd like to help.
07:33:06 <dcoutts__> ndm|bristol: probably wait 'til we've got our darcs going
07:33:37 <ndm|bristol> dcoutts__, will do, we'll have to wait til we have debugging and 100% haskell98 conformance first
07:41:01 <dcoutts__> ndm|bristol: right, and full(ish) FFI support
07:41:22 <basti_> xerox: are you there?
07:41:44 <dcoutts__> ndm|bristol: in the mean time I can try and make all modules build without -fglasgow-exts
07:41:59 <ndm|bristol> dcoutts__, ffi support is mainly done, just not checked in
07:42:18 <ndm|bristol> not requiring -fglasgow would be a very good first step
07:42:23 <dcoutts__> ndm|bristol: how about the export "wrapper" stuff?
07:42:32 <ndm|bristol> but if there are any extensions you really can't live without, let us know
07:42:48 <ndm|bristol> export wrapper? i'm not that well up on how gtk2hs works
07:42:51 <dcoutts__> ndm|bristol: the fglasgow-exts is mostly for unsafeCoerce# which is only for an optimisation
07:43:12 <dcoutts__> ndm|bristol: export "wrapper" is part of the FFI spec
07:43:17 * dcoutts__ looks it up
07:43:26 <ndm|bristol> if its part of the FFI spec, we'll implement it fully
07:43:29 <dcoutts__> it used to be known as dynamc export
07:43:48 <ndm|bristol> exporting functions back to C?
07:43:56 <dcoutts__> right
07:44:00 <ski> closures
07:44:06 <ndm|bristol> that might be tricky
07:44:15 <dcoutts__> yes, closures
07:44:37 <ndm|bristol> i'm sure it can be done
07:44:52 <ndm|bristol> Tom has FFI under control, he says
07:45:01 <dcoutts__> ndm|bristol: that was my suggestion about libffi
07:45:04 <ndm|bristol> i saw some initial demos of the FFI, seemed to be pretty flawless
07:45:14 <ndm|bristol> dcoutts__, yes, libffi has been investigated
07:45:23 <dcoutts__> the lib that allows one to construct C calls dynamically at runtime
07:45:26 <ndm|bristol> looks like it might not work on windows with visual studio
07:45:29 <dcoutts__> ndm|bristol: oh, and?
07:45:37 <dcoutts__> hmm
07:45:41 <dcoutts__> possible
07:45:49 <ndm|bristol> yhc builds natively, and happily, using visual studio
07:46:00 <dcoutts__> it's quite possibly GNU C only
07:46:09 <ndm|bristol> it does seem to be
07:46:31 * dcoutts__ wonders how python does foreign calls
07:47:15 <ndm|bristol> i'll make sure tom is aware that closures are required
07:48:08 <dcoutts__> ndm|bristol: actually most gtk2hs stuff does not strictly need dynamic export
07:48:21 <dcoutts__> the other alternative is a C api to make calls to Haskell
07:48:25 <ndm|bristol> if its in the FFI spec, we should implement it
07:48:38 <dcoutts__> that's actually what we do now with ghc
07:48:41 <ndm|bristol> dcoutts__, we will have a C api for haskell soon
07:48:48 <dcoutts__> and signal/event callbacks
07:49:00 <ndm|bristol> thats pretty easy with our implementation
07:49:11 <dcoutts__> that allows you to build and execute calls?
07:49:27 <dcoutts__> eg mkHsChar, mkHsInt, mkHsApp etc?
07:52:04 <ndm|bristol> probably, yes
07:52:18 <basti_> anyone got a clue about gtk2hs applications?
07:52:47 <dcoutts__> basti_: yes :-)
07:52:51 <basti_> ^^
07:53:04 <basti_> I'm doing drawingAreaNew
07:53:11 <dcoutts__> right
07:53:21 <basti_> then i try drawingAreaGetWindow with the result, and it breaks
07:53:39 <basti_> (Main:9826): GLib-GObject-CRITICAL **: g_object_ref: assertion `G_IS_OBJECT (object)' failed
07:54:06 <dcoutts__> basti_: it should really return Maybe I guess
07:54:12 <basti_> oh
07:54:24 <dcoutts__> the thing is that the window for the draw area is not available when the widget is not realised
07:54:54 <basti_> hmmm i see
07:55:06 <dcoutts__> you can't draw to it untill the widget is realised, that's when the widget allocates itself the X windows resources
07:55:29 <basti_> when is it "realized"?
07:56:08 <musasabi> how was foreign export "wrapper" different from foreign export ?
07:56:09 <basti_> that explains at least why this sometimes breaks, and sometimes doesn't
07:56:22 <dcoutts__> basti_: Gtk.onRealize canvas $ do ...
07:56:29 <basti_> hmm i see.
07:56:39 <basti_> now, i'm at a spot where i'm sure it's realized
07:56:43 <dcoutts__> basti_: it's realised just before it is shown
07:56:53 <basti_> during onExpose processing
07:56:59 <basti_> i try to draw, but nothing happens
07:57:09 <dcoutts__> basti_: it is always realised by the time it is exposed
07:57:19 <basti_> yes I'm talking about a different problem now ^^
07:57:24 <basti_> i was debugging quite a while
07:57:28 <dcoutts__> so if you get the window at that point then you'll be ok
07:57:44 <basti_> the window is drawn, and during onExpose i try to draw
07:57:48 <basti_> the DA stays blank though
07:58:46 <dcoutts__> ndm|bristol: btw about the C interface, this is what we do for ghc at the moment: http://haskell.org/gtk2hs/darcs/gtk2hs/glib/System/Glib/hsgclosure.c
07:58:57 <dcoutts__> ndm|bristol: rather than export "wrapper"
07:59:00 <ski> (musasabi : latter exports a toplevel haskell value, former makes a haskell function that if passed a closure, constructs a c function that will call that closure .. iirc)
07:59:04 <dcoutts__> basti_: code?
07:59:42 <basti_> sure
07:59:42 <dcoutts__> basti_: you've seen the drawing area demo?
07:59:44 <dcoutts__> http://haskell.org/gtk2hs/darcs/gtk2hs/demo/graphic/Drawing.hs
07:59:51 <basti_> lisppaste2: @url
07:59:51 <lisppaste2> To use the lisppaste bot, visit http://paste.lisp.org/new/haskell and enter your paste.
08:00:16 <dcoutts__> and the cairo version:
08:00:21 <dcoutts__> http://haskell.org/gtk2hs/darcs/gtk2hs/demo/cairo/Drawing.hs
08:00:45 <lisppaste2> basti_ pasted "renderWithDrawable does nothing?" at http://paste.lisp.org/display/15059
08:01:23 <basti_> I'm ignoring the Event value (matching against _). Is that evil?
08:01:38 <dcoutts__> basti_: no, not evil
08:03:00 <dcoutts__> basti_: so you're using cairo. You're not stroking the line you've built.
08:03:16 <dcoutts__> basti_: moveTo _ _; LineTo _ _; stroke
08:03:22 <musasabi> ski: thanks
08:03:27 <basti_> >.<
08:03:29 <basti_> OUCH
08:03:46 <Pupeno> is there a channel on freenode about functional programming ?
08:03:50 <basti_> thanks dcoutts.
08:04:08 <dcoutts__> you see LineTo etc just extend the current drawing region. Then fill & stroke actually do the drawing.
08:04:17 <ski> Pupeno : several
08:04:24 <Cale> Pupeno: this channel is about functional programming :)
08:04:24 <basti_> yes i see.
08:04:31 <dcoutts__> basti_: if you're doing multi-segment lines then you can do all the lines and then stroke it at the end.
08:04:44 <Pupeno> a language-agnostic functional programming channel I mean.
08:04:46 <basti_> u huh
08:04:59 <dcoutts__> basti_: there isn't really a good cairo intro yet sadly.
08:05:07 <dcoutts__> even on the cairo site itself :-(
08:05:14 <basti_> the stupid thing is: i tripped about that at least once
08:05:21 <dcoutts__> so these things are not very obvious
08:05:43 <dcoutts__> it actually has a nice well thought out drawing model, but it is not explained at all.
08:05:58 <basti_> no it's not.
08:06:05 <basti_> you need a wiki.
08:06:46 <dcoutts__> ndm|bristol: it's FFI 4.1.1, http://www.cse.unsw.edu.au/~chak/haskell/ffi/ffi/ffise4.html#x7-160004.1
08:06:55 <Cale> dcoutts__: you really should add xerox's nice little graphical shell to the library :)
08:07:25 <dcoutts__> basti_: well we're hoping the cairo devs will make a tutorial, since there are bits even we don't understand yet!
08:07:53 <dcoutts__> Cale: oh yes? I've not seen that. Where is it?
08:07:56 <basti_> heh
08:07:58 <ndm|bristol> dcoutts__, thanks - i'll record that and pass is on
08:08:05 <ndm|bristol> @wiki Yhc/Gtk2Hs
08:08:06 <lambdabot> http://www.haskell.org/hawiki/Yhc/Gtk2Hs
08:08:44 <Cale> I'm pretty sure you have seen it, though I might not be referring to it in a way that makes sense :)
08:08:49 <Cale> let me find it
08:09:12 <Cale> http://haskell.org/gtk2hs/CairoDrawing.hs
08:09:13 <dcoutts__> oh, ok :-)
08:09:31 <dcoutts__> oh that
08:09:47 <Cale> I've extended it to do animation, though I should really improve its performance.
08:10:01 <dcoutts__> that was written by axel, not xerox (which is why I was confused!)
08:10:04 <Cale> oh
08:10:12 <Cale> I wasn't aware of that :)
08:10:13 <Cale> hehe
08:10:23 <dcoutts__> np
08:10:43 <dcoutts__> Cale: it'd be cool to have a pure layer on top of the cairo api, and animation too
08:11:31 <Cale> Yeah, currently I don't have anything reactive going on -- just plug in a function  Time -> Render ()  and it displays it.
08:11:47 <dcoutts__> right
08:12:20 <dcoutts__> I'd like something with a pure api, ie functions for constructing drawings
08:12:38 <dcoutts__> then a monadic IO/Render function for actually doing the drawing
08:12:49 <dcoutts__> but all the construction would be pure
08:13:02 <dcoutts__> then a time parameter to that would be fairly easy I'd expect
08:13:53 <dcoutts__> something like the SOE api (which is mostly pure), but with the capabilities of cairo
08:13:59 <dcoutts__> that'd be great
08:17:12 <Cale> Graphical user interfaces really seem pretty object oriented, but with that view, there are still a lot of fiddly details to fill in, especially in terms of handling events. I wonder if there's some more equational way of handling things.
08:18:05 <dcoutts__> I'm not sure they're that OOPy
08:18:14 <dcoutts__> it's just the traditional view
08:18:33 <Cale> Well, the whole widget thing is.
08:18:59 <dcoutts__> the widget is certainly an OOP abstraction
08:21:40 <dcoutts__> I'd like to describe the relation between modifiable user state in an equational way
08:21:55 <Cale> If you were to design a program for drawing simple graphs, and you wanted the user to be able to drag the vertices around, and add/remove vertices and edges, it really seems like you're talking about objects. However, I wonder if a lot of the trouble of dealing with objects could be avoided if they could be created equationally. So rather than handling the realignment of the edges with a vertex, say, you defined an equation on the proper
08:21:55 <Cale> ties of the vertices and edges incident with it.
08:22:56 <Pupeno> Does NewBinary work in big-endian all the time ?
08:23:06 <musasabi> yes
08:23:08 <Cale> I'm not quite sure how to deal with creation and deletion, but that would be a start :)
08:23:35 <dcoutts__> Cale: a lot of the difficulty I think is in naming/identity issues
08:23:41 <ski> (.. realignment ?)
08:23:48 <Pupeno> musasabi: really ?
08:24:08 <Cale> ski: as the user drags a vertex, the edges incident with that vertex need to be moved
08:24:26 <dcoutts__> Cale: describing a widget template is not too hard to do in a pure way, but it's the idenity of multiple instances of a widget template that's tricky.
08:24:56 <musasabi> Pupeno: really.
08:24:58 * ski dictionary-looks-up 'incident'
08:25:21 <Cale> ski: touching
08:25:31 <Cale> adjacent
08:25:33 <Pupeno> I was discarding NewBinary because it 'ignores' endiannes, well, I'll take a second look at it.
08:25:40 <dcoutts__> Cale: that's perhaps one of the advantages of the OOP view. Objects naturally have identity that is distinct to their "value".
08:26:24 <ski> Cale : do you mean edges which have this vertex as end-point ?
08:26:28 <Cale> ski: yes
08:26:52 <musasabi> Pupeno: look for "
08:27:00 <ski> (touching sounds like the edge just happens to lie over the vertex)
08:27:00 <musasabi> "instance Binary Word32 where" in http://www.n-heptane.com/nhlab/repos/NewBinary/NewBinary/Binary.hs
08:27:35 <Cale> ski: well, I was trying to describe the meaning of "incident"
08:28:06 <musasabi> however NewBinary does not produce the same binary stream for the same data on different platforms allways
08:28:31 <Pupeno> musasabi: what's the exception ?
08:29:22 <musasabi> e.g. whether Int is serialized as 4 or 8 bytes.
08:29:31 <mjl69_> if I define a new type like this:
08:29:33 <mjl69_> data MyList a= MyCons a (MyList a) | MyNil
08:29:56 <mjl69_> and I want a function that takes a list ( the built in kind) and returns a MyList of the same elements
08:29:57 <musasabi> which also makes list serialization unportable.
08:30:25 <Pupeno> musasabi: oh, that's ok, I'll always work with IntN where N is 8, 16 or 32 and ASCII Strings (can I handle those, right ?)
08:30:32 <mjl69_> f (x:xs) | (x:xs) == [] = MyNil
08:30:32 <mjl69_>          | otherwise = MyCons(x f(xs)) -- doesn't work
08:30:47 <musasabi> yes, those work. But you have to serialize the lists by hand.
08:30:48 <Cale> dcoutts__: yeah, I'm just considering ways to improve on that view. We can probably deal with identity just as well with sufficient combinator libraries, but as useful as a concept of identity is, it's also a big pain to use
08:31:07 <musasabi> replicateM for decoding and mapM_ for encoding.
08:31:41 <Pupeno> musasabi: how does that affect me ? (I don't see how)
08:32:01 <musasabi> Pupeno: well with DNS you have to encode strings/arrays etc
08:32:07 <xs_> mjl69_: don't you want MyCons x (f xs) instead?
08:32:13 <musasabi> Pupeno: I think the most serious issue is the compression.
08:32:19 <Cale> dcoutts__: You always end up having to define all these relationships between the objects in your system where the relationships are common, but the code can't be shared, because it's all tangled up in event handlers.
08:32:35 <Pupeno> musasabi: oh, yes. Probably.
08:32:45 <dcoutts__> Cale: yes, if we can sidestep the issue of widget identity, that'd be good
08:33:01 <dcoutts__> Cale: hmm, not quite sure what you mean
08:33:05 <Pupeno> well, I have to go to work now. Thank you.
08:33:28 <Cale> dcoutts__: well, take my graph editor as an example
08:33:30 <mjl69_> xs_:  yeah, that's what it was supposed to be.  I still get Couldn't match `MyList a' against `t -> t1' ...
08:33:48 <mjl69_> Probable cause: `MyCons' is applied to too many arguments in the call
08:33:48 <mjl69_>         (MyCons x f (xs)) ...
08:33:52 <musasabi> and that does not work with NewBinary
08:33:53 <dcoutts__> Cale: ok, so what can't be shared/reused?
08:34:23 <Cale> We end up adding code to the drag event handlers that essentially just ensures that the x-coordinate and the y-coordinate of certain objects remains the same
08:34:54 <Cale> This is a common pattern, of just equating a piece of data in one object with a piece of data in another.
08:35:10 <dcoutts__> Cale: right
08:35:16 <Cale> But you always end up writing this stupid code which actually physically does that mutation
08:35:22 <mjl69_> how is it a problem that I give the constructor a value and a function that returns a MyList and it wants a value and a MyList?
08:35:36 <dcoutts__> Cale: right, and it's not declerative
08:35:37 <Cale> rather than being able to write the "equate" wire once and just hooking it in when needed
08:35:55 <musasabi> mjl69_: like foobar :: a -> MyList a -> MyList a ?
08:36:01 <dcoutts__> Cale: it's doing things in response to certain events rather than declaring an equational relationship
08:36:09 <Cale> yeah
08:36:22 <dcoutts__> Cale: that is certainly something I'd like to be able to do
08:36:38 <Cale> The thing which always troubles me is what to do with command buttons
08:36:44 <xs_> mjl69_: (x:xs) can never match [].
08:36:52 <dcoutts__> Cale: how so?
08:37:29 <Cale> Well, they're triggers for specific actions in a way which is hard to describe in a declarative way.
08:37:31 <Cale> er
08:37:49 <Cale> well, you know what I mean :)
08:37:50 <dcoutts__> hmm
08:37:55 <mjl69_> xs_: oh yeah, fixed that too...
08:38:08 <dcoutts__> they're not continuous state
08:38:12 <Cale> yeah
08:38:27 <dcoutts__> they really are events
08:38:27 <xs_> mjl69_: with those fixed, it works for me..
08:38:40 <dcoutts__> well a mixed model is not necessarily so bad
08:39:08 <dcoutts__> Cale: the "do this IO action when that button is pressed" is not so bad for buttons
08:39:41 <dcoutts__> Cale: otherwise it's a continuous signal of type Bool, or a discrete state of type ()
08:39:56 <dcoutts__> discrete signal I mean
08:40:43 <Cale> dcoutts: yeah, I suppose you do really want something imperative there
08:41:55 <dcoutts__> the continuous signal style fits the "instant apply" model well
08:42:30 <Cale> yeah
08:42:36 <dcoutts__> where instead of a dialog box having "apply" or "ok" buttons, you've just got "close" and each setting gets applied as soon as it's changed
08:42:53 <dcoutts__> gnome uses this style heavily
08:43:48 <mjl69_> xs_:  wow, cool it did work!
08:43:59 <mjl69_> just stupid mistakes on my part.
08:45:31 <mjl69_> so when I didn't do (f xs), it was counting f and xs as two separate args
08:45:53 <mjl69_> I thought that the f would bind strongly to the xs
08:47:08 <Cale> dcoutts__: hmmm... I wonder if there's some nice abstraction we could make to automatically turn instant-apply interfaces into delayed-apply ones.
08:48:14 <mjl69_> a function applied to it's args is the strongest association of all, but here I have two functions fighting it out.  without the ()'s, the outer one won?
08:48:38 <Cale> mjl69_: hm?
08:49:03 <Cale> mjl69_: function application is left associative
08:49:15 <mjl69_> g x f(xs) as apposed to g x f xs
08:49:23 <Cale> those are the same
08:49:40 <Cale> g x (f xs) as opposed to g x f xs
08:49:41 <Cale> ?
08:49:52 <dcoutts__> Cale: I expect so
08:50:38 <Cale> g x (f xs) = ((g x) (f xs)), whereas g x f xs = (((g x) f) xs)
08:50:49 <musasabi> JohnMeacham: ping
08:52:56 <mjl69_> Cale: I see it now.  Sorry, lack of sleep is making me sloppy in my thinking.  I get it and see that my last example had nothing to do with the problem
08:53:37 <mjl69_> now, to work my way back up to understanding (==<) after I get a little sleep.
08:53:53 <mjl69_> @type (==<)whoops
08:53:54 <lambdabot> Not in scope: `==<'
08:53:54 <lambdabot>  
08:53:54 <lambdabot> <interactive>:1:5: Not in scope: `whoops'
08:54:00 <Cale> @type (=<<)
08:54:00 <mjl69_> @type (>>=)
08:54:01 <lambdabot> forall b (m :: * -> *) a. (Monad m) => (a -> m b) -> m a -> m b
08:54:01 <lambdabot> forall (m :: * -> *) b a. (Monad m) => m a -> (a -> m b) -> m b
08:54:02 <mjl69_> help
08:54:05 <mjl69_> :-)
08:54:11 <dcoutts__> Cale: it's probably just a matter of delaying all the events/actions until the dialog is closed, or something like only delivering events for the final state of each changed widget in the dialog box.
08:54:17 <mjl69_> need sleep
08:54:21 <mjl69_> &
08:54:35 <Cale> dcoutts__: I wonder if what we're really talking about is a way to functionally combine UML diagrams :)
08:55:00 <dcoutts__> eek!
08:55:04 <Cale> (not that I know all that much about UML)
08:55:11 <dcoutts__> I hope not :-)
08:55:13 * dcoutts__ neither
08:55:52 <Cale> But it seems like people draw these graphs with the components of their system represented as boxes, and the lines of communication described as arcs
08:56:09 <Cale> and, well, the boxes are nothing more than labelled vertices
08:56:31 <dcoutts__> right
08:57:16 <Cale> We'd like a way to be able to write the arcs in a modular way.
08:57:36 <Cale> I keep repeating words :)
08:57:42 <Cale> Perhaps I need more coffee :)
08:57:50 <dcoutts__> heh
09:02:31 <gour> dcoutts__: how about your conference-paper?
09:10:36 <Cale> Well, the coffee was finished, but tea is now being prepared :)
09:11:41 <Cale> dcoutts__: so do you suppose that perhaps these arcs are nothing more than arrows?
09:11:47 <pejo> Cale, lets just hope it's not green tea, or you'll lose out on the caffeine!
09:11:56 <Cale> Orange Pekoe
09:12:08 * ski wonders what arcs we're talking about ..
09:12:22 <Cale> ski: the arcs connecting components of an OO design
09:12:36 <ski> UML arcs ?
09:12:39 <Cale> yeah
09:12:51 * ski has tried not to learn UML
09:13:04 <Cale> Well, I don't really know UML either
09:13:39 <xerox> Suggest me exercises to give to an Haskell newbie who didn't see IO yet :)
09:13:50 <Cale> but basically, I'm just thinking that we'd like a way to abstract the means by which objects communicate with one another. Imperative event handlers are tedious.
09:13:56 <ski> (weren't you talking about vertices and graphs and guis before ?)
09:14:06 <Cale> ski: well, I was, that was one case study :)
09:14:33 <Cale> hmm, and would you look at that, what we want there is slightly more than an arrow
09:14:36 <ski> xerox : what kind of exercises ?
09:15:01 <Cale> xerox: folds for various user-defined types?
09:15:22 <Cale> and showing how you can do pretty cool things with treeFold and foldr
09:15:25 * ski associated with constraints before .. and adaptive/incremental
09:16:02 <Cale> ski: I think we can express constraints as specific pairs of arrows.
09:16:08 <Cale> (a b c, a c b)
09:16:20 <musasabi> OBDD could be a fun exercise
09:16:33 <ski> saying two things should be equal when underlying things change is a bit like constraints, it seems
09:16:34 <xerox> Well, something Tree based is cool
09:16:40 <xerox> I was thinking of something even more basical
09:16:50 <ski> s/it seems/it seems to me/
09:16:51 <xerox> Like implementings Prelude things, but I can't come up with interesting things
09:17:00 <xerox> Well, I gave him to implement reverse :)
09:17:05 <ski> xerox : generate permutations ? :)
09:17:14 <xerox> haha
09:17:17 <xerox> I can't correct that exercise!
09:17:19 <xerox> ^_^
09:17:31 <musasabi> implement map in terms of 1) foldl, 2) foldr
09:17:58 <xerox> Something more fun? heh.
09:18:00 <ski> Cale : hmm
09:18:26 <ski> Cale : is that supposed to be an iso ??
09:19:39 <tromp_> map f = foldl (\a x -> a++[f x]) []
09:20:20 <Cale> ski: not in general, and it may have IO side effects
09:21:08 <tromp_> map f = foldr ((:).f) []
09:21:08 <Cale> ski: the idea is just that the system is responding to a signal on one side by running the arrow in order to interface with the other side
09:21:52 <Cale> ski: In a simple case where we want to declare equality of two properties, the relevant pair would be something like (arr id, arr id)
09:23:02 <ski> map f xs = foldl (\a x -> a . (f x :)) id xs []
09:23:20 <ski> @pl map f xs = foldl (\a x -> a . (f x :)) id xs []
09:23:21 <lambdabot> map = flip flip [] . flip foldl id . flip ((.) . (.)) . ((:) .)
09:23:43 <Cale> hehe
09:24:05 <Cale> I really like  map f = foldr ((:) . f) []
09:24:55 * ski 's version is more or less a "leaned left so far as to come back as right"
09:25:30 <ski> Cale : hm
09:25:46 <Cale> but being a foldl it still has bad properties wrt laziness
09:26:35 <ski> m
09:26:52 <Cale> sort of funny about foldl
09:27:15 <Cale> It's got really good performance if you want strictness, and really poor performance if you want laziness
09:27:36 <ski> monolithic
09:27:59 <blazold> re
09:28:05 <Cale> wb
09:28:14 <ski> Cale : would your two arrows be used back and forth, until a fixedpoint is reached ?
09:28:39 <Cale> ski: hmm.. I'm not sure if you'd want that
09:29:20 <Cale> sometimes it seems like it would be a good thing
09:29:37 <Cale> on the other hand, if we allow the arrows to have side effects too, it could get messy
09:30:00 <ski> not sure i understand the point of your pair, then ..
09:31:37 <Cale> ski: well, when one side updates (perhaps only from somewhere other than this wire) then the relevant arrow is applied, and the other end updated with the result.
09:32:00 <Cale> But we might want to explicitly avoid feedback loops along the same wire
09:32:54 <ski> iiuc, we want something to simulate continuous linked parts ..
09:32:56 <ski> yes ?
09:33:05 <Cale> for one
09:33:29 <Cale> we want something which can handle both continuously updating things, and discrete signals
09:33:58 <ski> yes .. but possibly those two should be handled in different ways ..
09:34:18 <Cale> well, it would certainly be nice if they could at least somewhat be unified
09:34:47 <ski> maybe
09:36:14 <ski> hm .. one way in the vertex + edge case is to have the edge contain a reference to the vertex
09:36:22 <Cale> For continuous things, you want something like an arc or edge with two ends that can be connected to an object's properties/signals. For discrete things, you want some side effecting action that can just be plugged in at one point.
09:36:54 <Cale> or do you?
09:36:55 <Cale> hehe
09:37:09 * ski can't think very clearly atm
09:37:30 <tromp_> how to read an int in octal?
09:38:07 <ski> > read "0o10" :: Int
09:38:09 <lambdabot> 8
09:38:41 <tromp_> thx, ski
09:39:49 <Cale> there's also readOct in Numeric, which gives a ReadS for a non-annotated octal number.
09:40:03 <araujo> Hello aR0und!
09:40:12 <Cale> (though plain read seems more convenient)
09:42:04 <ski> Cale : anyway .. this reminded my of adaptive/incremental computation in some way .. when some initial conditions are changed, (e.g. a vertex moved) stuff that depend on that are recomputed
09:43:11 <ski> (hm, reminds me .. i should try to get Fudgets running ..)
09:48:22 <Cale> ski: yeah -- my major annoyance with the way that things are traditionally done in OO is that you have all these common bits of code which are written into event handlers which do nothing more than enforce some simple invariant, usually equality of a pair of cells.
09:49:11 <Cale> and it always feels wrong to write such code because there's no modularity to it at all.
09:49:33 <ski> m
09:50:28 * ski wonders how often it'd be feasable to instead have both those cells be references pointing to a single cell
09:50:54 <Cale> Well, that should be possible too
09:51:01 <Cale> You really just want to be able to declare that one thing is a function of another.
09:51:50 <dcoutts__> I started writing a "notification var" thingie
09:52:12 <Cale> So yeah, in some sense, you need an "isomorphism", though what's being preserved isn't the same thing in every case.
09:52:12 <dcoutts__> the idea is that you can register to have an action run whenever the value changes
09:52:37 <Cale> dcoutts__: that would be a great start :)
09:52:50 <dcoutts__> then these notification vars can be tied together
09:52:53 <dcoutts__> and can be made proxies of widget states
09:52:54 <Cale> yeah
09:53:02 <musasabi> isn't this essentially like in an attribute grammar (with the grammar changing) ?
09:53:24 <dcoutts__> that should allow one to build an abstraction where you can set widgets to share state, or provide translations between states (via functions)
09:53:28 <ski> musasabi : that's the adaptive/incremental dimension of this i think, yes
09:58:37 <Cale> heh, out of the 314 items in my home directory, 118 are .hs files, mostly silly little testing scripts, but it's hard to decide what to keep and what to throw away.
09:59:15 <xerox> I can feel your pain
09:59:58 <basti_> i don't want to know what the 800 and something items in my home are.
09:59:59 <tromp_> i once accidentally created over 3 million files in my dir
10:00:07 <basti_> i know that i keep .hs's in an extra subdir
10:00:07 <Cale> hehe
10:00:18 <tromp_> was quite a challenge removing them
10:00:52 <Cale> I once had a mail server running for 4 years in a completely broken state, sending mail to root all the time.
10:00:56 <tromp_> linux has problems expanding * into 3 million items:(
10:01:19 <basti_> tromp_: :/
10:01:36 <Cale> tromp_: the trick is to use xargs
10:01:37 <basti_> i always knew the unix shell approach to pattern expansion is broken.
10:01:47 <tromp_> what's xargs?
10:01:49 <Cale> map
10:01:51 <Cale> :)
10:01:53 <Cale> sort of :)
10:02:26 <Cale> It processes its input into arguments for some command, which is run repeatedly.
10:02:33 <Cale> (usually, once per line)
10:02:35 <tromp_> i ended up moving all other files (less than a hundred) out of that dir
10:02:46 <tromp_> and doing a rm -r on the whole dir
10:02:56 <Cale> what FS?
10:03:22 <tromp_> how do i tell?
10:03:43 * ski had trouble removing a file that started with '-', once
10:04:01 <Cale> oh, type 'mount' and it should say
10:04:27 <tromp_> reiserfs
10:04:33 <Cale> ah, nice :)
10:04:42 <ski> it kept thinking the filename was a flag to the program :/
10:04:45 <basti_> ski: heh
10:04:54 <basti_> ski: most programs think you're done with flags after a -- flag
10:05:03 <Cale> at least your filesystem is built to be able to handle that many files then :)
10:05:20 <ski> (hm, iirc, it started with two '-')
10:05:22 <Cale> I'm running reiser4 on my new drive, but not on my root.
10:05:24 <tromp_> yes, it handled it very well
10:05:25 <basti_> ski: even then
10:05:41 <basti_> its not nice at all to put a file named -rf somewhere btw.
10:05:43 <ski> basti_ : m .. didn't know about that then ..
10:05:58 <tromp_> especially not "-rf *"
10:06:05 <basti_> ski: like "rm -- -stupidfile-"
10:06:15 <basti_> tromp_: the * won't be effective.
10:06:33 <ski> basti_ : iirc, i removed it by using '?' or something ..
10:06:54 <basti_> hmm.
10:06:57 * ski tries to remember
10:07:54 <ski> (yes, i think that was it)
10:08:01 * basti_ shrugs
10:09:30 <musasabi> the trick is rm -rf / tmp/stupidfile (don't try that at home as root)
10:12:32 <basti_> always give dangerous options last in the line
10:12:50 <basti_> that way you minimize the risk of hitting enter and doing something you did not want at all
10:22:18 <Oejet> klskfcÃ¸af
10:22:22 <Oejet>  m,bj-
10:23:37 <basti_> Oejet: yup.
10:24:18 <Oejet> basti_: Hello. :-)
10:24:43 * basti_ waves
10:43:12 <Cale> "The language and elementary results of category theory have now pervaded a substantial part of mathematics. Besides the everyday use of these concepts and results, we should note that categorical notions are fundamental in some of the most striking new developments in mathematics." -- Nathan Jacobson
10:43:26 <Cale> "Category theory is bunk." -- Marshall Cohen
10:44:00 <ski> yes
10:44:02 <Cale> That's the way that this book on category theory starts off. :)
10:44:11 <ski> nice
10:44:22 <araujo> Cale, which book?
10:44:38 <ski> (Cale : hm, btw, you've seen ccard, right ?)
10:45:03 <Cale> araujo: "A Categorical Primer" by Chris Hillman
10:45:07 <Cale> ski: no
10:45:54 <ski> Cale : http://www.verify-it.de/sub/ccard/
10:46:09 <astrolabe_> Who is Marshall Cohen?
10:46:24 <araujo> food's home!
10:47:58 <Cale> http://www.math.cornell.edu/People/Faculty/cohen.html
10:50:59 <ski> (Cale : whaddya think ?)
10:51:13 <astrolabe_> interesting point of view
10:52:48 <Cale> ski: neat :)
11:10:58 <xs> @pl \(_,a,b) -> (a, b)
11:10:59 <lambdabot> (line 1, column 6):
11:10:59 <lambdabot> unexpected ","
11:10:59 <lambdabot> expecting letter or digit, operator or ")"
11:10:59 <lambdabot> ambiguous use of a non associative operator
11:11:03 <xs> hmm.
11:11:36 <palomer> hrmph.
11:12:06 <palomer> is there a tree searching monad?
11:12:31 <palomer> rather, a tree searching type class which is a subclass of Monad
11:12:59 <xs> flatten in the order you want, then use list?
11:16:35 <Cale> palomer: hmm... maybe MonadPlus is sort of what you're thinking of?
11:18:25 <palomer> you can think of tree searching as monadic, no?
11:18:42 <basti_> that really depends
11:19:14 <basti_> if you want a monadic computation that traverses the tree you're rather looking for a tree fold with >>=
11:19:17 <basti_> or something similar
11:19:44 <palomer> or, rather, I start with a value and I have to build a tree from it
11:19:52 <palomer> and I have to backtrace
11:20:06 <palomer> barktrack
11:20:12 <palomer> backtrack
11:20:55 <basti_> you know that list monads give you backtracking-like execution order?
11:21:10 <basti_> (depth first, due to lazy eval)
11:21:26 <palomer> depth first is fine
11:21:54 <palomer> how does it work?
11:21:57 <basti_> hmm
11:22:01 <basti_> maybe like: (typing)
11:23:18 <basti_> > head $ filter (\x->(x `rem` 3) ==0) (do {x<-[2,3,4];y<-[x+1,x*3];[y+2,y*5,y^2]})
11:23:19 <lambdabot> 15
11:23:48 <basti_> thats a bit stupid, but it shows the principle
11:24:05 <basti_> you'd list-monadically compute all results and filter out the ones that don't interest you
11:24:15 <palomer> oh, no
11:24:26 <palomer> I want something like Maybe
11:24:35 <palomer> but, if it fails, it tries something else
11:24:51 <basti_> you can have that
11:25:04 <basti_> observe that ++ is mplus for lists
11:25:21 <basti_> (mplus is somewhat like an alternative)
11:25:29 <basti_> you can also have that in Maybe
11:25:50 <basti_> > Just 3 `mplus` Just 5
11:25:51 <lambdabot> Just 3
11:25:53 <basti_> > Nothing `mplus` Just 5
11:25:54 <lambdabot> Just 5
11:26:02 <xerox> > Just 3 `mplus` Nothing
11:26:04 <lambdabot> Just 3
11:26:09 <basti_> this is lazily evaluated btw.
11:26:22 <basti_> so the second expression never is computed when the first succeeds
11:26:39 <xerox> > foldr (mplus) mzero [Just 1, Just 2, Nothing, Just 4]
11:26:40 <lambdabot> Just 1
11:26:57 <palomer> oh, so I'd do map mplus [list of possible values]
11:27:09 <basti_> no, rather fold
11:27:15 <xerox> > foldl (mplus) mzero [Just 1, Just 2, Nothing, Just 4]
11:27:16 <palomer> oh, right, fold
11:27:16 <lambdabot> Just 1
11:27:30 <palomer> yeah, shouldn't I use foldl?
11:27:36 <basti_> hmm
11:27:41 <basti_> this is a little hard now
11:27:43 <palomer> no, wait, foldr
11:27:54 <basti_> but i think I'd use foldl 
11:28:02 <basti_> hmmm
11:28:05 <basti_> no i'd not
11:28:11 <palomer> foldl would force you to evaluate everything, no?
11:28:22 <basti_> foldr, because if you have (Just 4) `mplus` (...)
11:28:30 <basti_> then ... needs not to be evaluated
11:28:34 <palomer> foldl (mplus) [Just 1, error "hello"]
11:28:43 <palomer> foldl (mplus) [Just 1, error "hello"]
11:28:45 <palomer> > foldl (mplus) [Just 1, error "hello"]
11:28:46 <lambdabot>  add an instance declaration for (Show ([[Maybe a]] -> [Maybe a]))
11:29:02 <palomer> eh?
11:29:05 <basti_> with foldl, all the `mplus` need to be evalled, I'd say.
11:29:07 <xerox> foldl1 (mplus) [Just 1, Nothing]
11:29:12 <xerox> >foldl1 (mplus) [Just 1, Nothing]
11:29:15 <xerox> > foldl1 (mplus) [Just 1, Nothing]
11:29:17 <lambdabot> Just 1
11:29:17 <xerox> Well, ok.
11:29:26 <basti_> palomer: you forgot the base case.
11:29:37 <palomer> foldl1 (mplus) [Just 1, error "hello"]
11:29:40 <palomer> > foldl1 (mplus) [Just 1, error "hello"]
11:29:41 <lambdabot> Just 1
11:29:46 <palomer> > foldr1 (mplus) [Just 1, error "hello"]
11:29:48 <lambdabot> Just 1
11:29:51 <basti_> the result is the same
11:29:55 <basti_> but the computation is not
11:30:00 <palomer> > foldr1 (mplus) [Just 1, error "hello", error "hello"]
11:30:02 <lambdabot> Just 1
11:30:07 <palomer> > foldl1 (mplus) [Just 1, error "hello", error "hello"]
11:30:08 <lambdabot> Just 1
11:30:13 <palomer> ugh, which to use
11:30:16 <basti_> with foldl you got ((((x `mplus` ...) `mplus` ... ) `mplus` ...))
11:30:17 <basti_> etc.
11:30:28 <basti_> so you'll have to evaluate a tower of mplusses
11:30:48 <palomer> so foldr is what I want
11:30:52 <basti_> with foldr you just have to evaluate as much as you need
11:31:19 <palomer> now generating this list of possible values is another matter
11:31:35 <basti_> why?
11:32:08 <basti_> i mean, it is, but why is it hard?
11:32:17 <palomer> I never said it was hard
11:32:23 <basti_> ah
11:32:26 <palomer> ok, I'm going to prove sequents
11:32:37 <palomer> if the sequent is provable, I want it to return the proof term
11:32:44 <basti_> u huh
11:32:54 <basti_> youre aware that this is possibly undecideable?
11:33:02 <palomer> it's very much undecidable
11:33:18 <basti_> it might not be for 0th order logic, no?
11:33:19 <palomer> if it's not provable, I want it to change the sequent slightly and try again
11:33:20 <basti_> .)
11:33:36 <palomer> well, of course it's decidable for propositional logic
11:33:42 <palomer> that's SAT
11:33:48 <palomer> well, generalized SAT
11:33:51 <basti_> yea
11:34:22 <palomer> so I'd have to write a function Search that returns a Maybe proof term
11:34:48 <palomer> and if it fails, it tries again with another sequent
11:35:03 <basti_> if you employ a monad, then you'll automatically get a monadic value
11:35:31 <palomer> the Monad would be maybe
11:35:34 <palomer> and the list would be:
11:35:42 <basti_> "the list"?
11:35:47 <palomer> [is,f(is),f(f(is)),...]
11:35:53 <basti_> thats iterate.
11:35:59 <basti_> @type iterate
11:36:00 <lambdabot> forall a. (a -> a) -> a -> [a]
11:36:13 <palomer> but, erm, I want it to iterate a finite number of times
11:36:23 <palomer> so I need like a maybe version of iterater
11:36:37 <basti_> take n . iterate?
11:36:41 <basti_> :D
11:36:45 <palomer> of type: forall a. (a -> Maybe a) -> a -> [a]
11:37:03 <palomer> basti_: I don't know n before hand
11:37:04 <basti_> huh?
11:37:32 <basti_> then you just evaluate as many times as you need
11:37:38 <palomer> filter isJust (iterate f lst)
11:39:12 <palomer> where are the functions for dealing with maybe?
11:39:30 <basti_> inside Data.Maybe i think.
11:39:43 <palomer> @hoogle Data.Maybe
11:39:44 <lambdabot> Prelude.undefined :: a
11:39:44 <lambdabot> Test.QuickCheck.Batch.bottom :: a
11:39:52 <palomer> doesn't exist
11:40:00 <palomer> isn't Data.Maybe a datatype?
11:40:12 <basti_> type :b Data.Maybe in ghci
11:40:20 <palomer> it's actually the Maybe module!
11:40:29 <basti_> yes.
11:40:35 <palomer> oh, they're the same
11:40:40 <palomer> Data.Maybe == Maybe ?
11:40:51 <basti_> @type Data.Maybe
11:40:53 <lambdabot> Couldn't find qualified module.
11:40:53 <lambdabot> Maybe you're using the wrong syntax: Data.List.(\\) instead of (Data.List.
11:40:53 <lambdabot> \\)?
11:40:54 <basti_> @type Data.Maybe.Maybe
11:40:55 <lambdabot> Not in scope: data constructor `Data.Maybe.Maybe'
11:40:58 <basti_> uh
11:41:10 <basti_> Data.Maybe is the module name
11:41:14 <palomer> is there a type class for deriving isConstructor functions?
11:41:20 <basti_> the type is called Data.Maybe.Maybe i think.
11:41:24 <basti_> isConstructor?
11:41:35 <palomer> yeah
11:41:35 <palomer> isJust
11:41:37 <palomer> isNothing
11:41:51 <palomer> @hoogle Maybe
11:41:52 <lambdabot> Data.Maybe.Maybe :: Maybe a
11:41:52 <lambdabot> Data.Maybe.maybe :: b -> (a -> b) -> Maybe a -> b
11:41:52 <lambdabot> Prelude.Maybe :: Maybe a
11:41:57 <basti_> there it is
11:42:05 <palomer> yeah, you're right
11:42:13 <basti_> well, every data is constructed by a constructor
11:42:20 <basti_> some constructors are primitive though
11:42:58 <basti_> that is, implicit
11:43:03 <palomer> I still think a way to derive is<constructorName> would be useful
11:43:05 <basti_> like in the value "42"
11:43:17 <basti_> well you can't.
11:43:29 <basti_> you know that your value has to be constructed by a constructor of it's type
11:43:41 <palomer> instead of doing isC x = C _ _ _ -> True ; isC _ = False
11:43:50 <basti_> then you can either match against all the constructors, or you don't
11:43:53 <palomer> isC _ _ _ -> False
11:44:17 <basti_> every.value.is.constructed.with.help.of.a.constructor.
11:44:26 <palomer> basti_: sure
11:44:26 <basti_> (you just don't see them sometimes)
11:44:27 <palomer> and?
11:44:35 <basti_> the function "isConstructor" is always true
11:44:39 <palomer> no
11:44:39 <palomer> I meant
11:44:47 <palomer> is<insert your favourite constructor here>
11:44:56 <palomer> is1, isFalse, isNothing
11:44:59 <basti_> uhm
11:45:03 <basti_> you don't need that
11:45:07 <basti_> that's pattern matching
11:45:18 <palomer> yeah, but it's so much shorter than writing
11:45:28 <basti_> let f (Just x) = "hooh" ; f Nothing = "booo"
11:45:30 <palomer> that function which I wrote above
11:45:39 <basti_> > let f (Just x) = "hooh" ; f Nothing = "booo" in f (Just 4)
11:45:40 <lambdabot> "hooh"
11:45:42 <palomer> basti_: imagine doing that for every constructor you ever write?
11:45:42 <basti_> > let f (Just x) = "hooh" ; f Nothing = "booo" in f Nothing
11:45:44 <lambdabot> "booo"
11:45:50 <basti_> i do it.
11:45:53 <palomer> basti_: that's what I wrote above!
11:46:04 <palomer> I'm just saying it would be convenient to derive these sorts of things
11:46:13 <basti_> how would you want to derive that?
11:46:20 <palomer> well, you can't right now
11:46:31 <basti_> lets say we have   data .... deriving (Constructors)
11:46:35 <basti_> what would Constructors give you?
11:46:45 <palomer> unless you have dependent types:O)
11:46:47 <basti_> functions of which types
11:46:48 <basti_> etc.
11:46:58 <palomer> you can't in haskell!
11:47:03 <palomer> actually, maybe in template haskell
11:47:07 <basti_> can you in Coq?
11:47:19 <palomer> but it wouldn't be in a typeclass
11:47:26 <palomer> I'm not sure about Coq
11:47:39 <basti_> it has type dependencies
11:47:48 <basti_> you can derive haskell programs from coq proofs.
11:48:01 <palomer> sure
11:48:09 <basti_> or are you actually looking for a generalized fold?
11:48:21 <basti_> that is in Functor i think
11:48:25 <palomer> but the morphism you'd use would have to be applied to all your terms
11:48:40 <basti_> with List, Tree and Maybe implementing Functor
11:49:08 <palomer> is :: (#^n -> B) -> B -> bool
11:49:27 <basti_> #^n?
11:49:46 <basti_> are you trying to program, or do you want to invent a new programming language?
11:49:52 <palomer> this isn't haskell
11:49:57 <palomer> this is in a language of dependent types!
11:50:03 <palomer> I said it can't be done in haskell!
11:50:10 <basti_> ^^
11:50:43 <basti_> why don't you try a generalized fold for now?
11:50:48 * basti_ ducks and runs
11:50:53 <palomer> how would that solve my problems?
11:51:11 <basti_> first, i don't see the problem at all. Either you pattern match, or you don't.
11:51:24 <basti_> if you match patterns you'll have to enumerate constructors
11:51:25 <basti_> so what
11:51:34 <palomer> yeah, I simply said "wouldn't it be nice if haskell did some of this mind numbing work for me"
11:51:40 <basti_> "mind numbing"?
11:51:50 <basti_> what exactly is mind numbing about enumerating a few constructors?
11:51:51 <palomer> let's say I have constructors A through Z
11:52:12 <basti_> how do you want to generalize this if it makes any difference?
11:52:13 <palomer> isA A = True ; isA _ = False; isB B = True; isB _ = False; .....
11:52:24 <palomer> that's 52 lines of code!
11:52:30 <basti_> hmm
11:52:35 <basti_> why do you want to do that at all?
11:52:47 <basti_> i mean you'll end up in checking for nearly every case anyway
11:52:56 <basti_> i just about never use isJust
11:53:31 <palomer> basti_: for my maybeIterate I needed it
11:53:37 <palomer> @type iterate
11:53:38 <lambdabot> forall a. (a -> a) -> a -> [a]
11:53:52 <basti_> > take 10 $ iterate (+1) 5
11:53:53 <lambdabot> [5,6,7,8,9,10,11,12,13,14]
11:53:55 <palomer> maybeIterate f a = filter isJust (iterate f a)
11:55:05 <basti_> and you expect to filter for every of your 26 hypothetical constructors?
11:55:30 <palomer> at some point, sure
11:55:47 <basti_> :-o
11:56:00 <basti_> i don't know why, but something about that sounds broken ^^
11:56:42 <palomer> it's one of those things which is often useful
11:57:10 <basti_> honestly, i rarely (if at all) use isJust.
11:57:22 <basti_> and i never thought "dang now i need to type all these isC's"
11:57:39 <basti_> either we program different things, or you're doing something wrong.
12:00:16 <palomer> well, time to write a theorem prover
12:00:22 <basti_> -> Coq
12:00:23 <basti_> ^^
12:00:48 <palomer> no no
12:00:52 <palomer> first propositional
12:01:05 <basti_> coq can do propositional as well
12:09:41 <palomer> well, yes, since propositional is a subset of first order (for some definition of subset)
12:09:55 <basti_> yea
12:11:53 <palomer> is there a haskell parser with left recursion?
12:12:24 <Cale> palomer: Well, did you look at Parsec's "buildExpressionParser"?
12:12:29 <Cale> it might be suitable
12:13:02 <palomer> expression grammars support left recursion?
12:13:27 <Cale> yeah, you just set the associativity of the operators
12:13:43 <Cale> and it manages the problem of constructing the right parse tree
12:15:04 <Cale> Trying to organise my home directory is so distracting. I just spent half an hour on a sidetrack reading about elliptic curve factoring methods due to an assignment question on a Math 145 assignment that was sitting in my homedir.
12:16:19 <xerox> Well done! ;)
12:21:23 <tennin> yeah, I've made three attempts this year to organize the books in my apt., all of which failed for that reason
12:21:35 <Cale> hehe
12:22:34 <Cale> I've actually managed to organise the math books on my computer fairly well.
12:25:03 <ihope> Are there any Haskell forums out there?
12:26:14 <Cale> there are two major mailing lists and a bunch of smaller ones
12:26:32 <ihope> Nothing along the lines of PHPbb, then?
12:26:42 <tuomov> why would anyone want a web bb?
12:26:48 <ihope> :-(
12:26:51 <ihope> *:-)
12:27:06 <Cale> well, mailing lists are just as convenient, especially due to Gmane
12:27:16 <tuomov> it's very inconvenient to read multiple "forums" when they all are different
12:27:19 <ihope> What's Gmane?
12:27:27 <tuomov> a mail2news gateway
12:28:37 <ihope> let {ack 3 n = 2^n*8-3; ack m 0 = ack (m-1) 1; ack m n = ack (m-1)  (ack m (n-1))} in ack 4 2
12:29:09 <ihope> > let {ack 3 n = 2^n*8-3; ack m 0 = ack (m-1) 1; ack m n = ack (m-1) (ack m (n-1))} in ack 4 2
12:29:11 <lambdabot> 200352993040684646497907235156025575044782547556975141926501697371089405955
12:29:11 <lambdabot> 631145308950613088093334810103823434290726318182294938211881266886950636476
12:29:11 <lambdabot> 154702916504187191635158796634721944293092798208430910485599057015931895963
12:29:11 <lambdabot> 952486337236720300291696959215610876494888925409080591145703767520850020667
12:29:11 <lambdabot> 156370236612635974714480711177481588091413574272096719015183628256061809145
12:29:13 <lambdabot> [23 @more lines]
12:29:15 <ihope> 'Ere we go.
12:29:32 <basti_> congratulations ihope 
12:29:35 <basti_> ^^
12:29:36 <ihope> :-)
12:29:37 <xerox> :-D
12:29:42 <xerox> @karma+ ihope 
12:29:42 <lambdabot> ihope's karma raised to 1.
12:30:50 <ihope> So where are these mailing lists?
12:31:23 <xerox> @where haskell-cafe
12:31:23 <lambdabot> I know nothing about haskell-cafe.
12:31:26 <ihope> > error ("foo" ++ (error "bar")) :: Int
12:31:28 <lambdabot> bar
12:31:30 <xerox> Duh.
12:31:34 <ihope> Aww.
12:31:37 <xerox> O_O
12:31:40 <PupenoL> @google haskell mailing lists
12:31:42 <lambdabot> http://www.haskell.org/mailinglist.html
12:31:42 <xerox> > error "foo"
12:31:43 <lambdabot> Add a type signature
12:31:46 <xerox> > error "foo" :: Int
12:31:47 <lambdabot> Exception: foo
12:31:55 <xerox> Oh, I didn't know it did do that.
12:31:57 <PupenoL> @where haskell@haskell.org
12:31:58 <lambdabot> I know nothing about haskell@haskell.org.
12:32:04 <PupenoL> @where haskell-cafe@haskell.org
12:32:05 <lambdabot> I know nothing about haskell-cafe@haskell.org.
12:32:14 <PupenoL> @where mailing lists
12:32:14 <lambdabot> I know nothing about mailing.
12:32:21 <ihope> > error "foo" :: ()
12:32:23 <lambdabot> Exception: foo
12:32:30 <xerox> Well, we could point @where haskell-cafe to some interface, or the subscription page?
12:32:43 <ihope> Special case?
12:33:42 <binary42_> @type error
12:33:43 <lambdabot> forall a. [Char] -> a
12:34:05 <PupenoL> http://haskell.org/mailman/listinfo/haskell-cafe
12:34:13 <ski> @syntax error
12:34:14 <lambdabot> Unknown command, try @listcommands.
12:34:21 <xerox> @where+ haskell-cafe http://haskell.org/mailman/listinfo/haskell-cafe
12:34:22 <lambdabot> Done.
12:35:27 <ihope> ski: eh?
12:35:51 <PupenoL> @where mailinglists
12:35:52 <lambdabot> http://www.haskell.org/mailinglist.html
12:36:43 <ihope> > Just (error "foo") >>= error
12:36:44 <lambdabot> Add a type signature
12:36:54 <ihope> > Just (error "foo") >>= error :: Int
12:36:55 <lambdabot> Couldn't match `Int' against `Maybe b'
12:37:04 <ihope> > Just (error "foo") >>= error :: Maybe Int
12:37:05 <lambdabot> foo
12:37:35 <xerox> Well...
12:37:41 <xerox> That's strange.
12:37:54 <xerox> ihope: tell dons about this, if you happen to see him online.
12:38:33 <ihope> ...MemoServ?
12:38:54 <xerox> Well.. mail? :-)
12:39:38 <ihope> > error "foo" >>= error
12:39:39 <lambdabot>  add an instance declaration for (Show (m b))
12:39:48 <ihope> > error "foo" >>= error :: Maybe Int
12:39:49 <lambdabot> Exception: foo
12:40:31 <ski> good evening esap
12:40:52 <esap> evening ski!
12:41:11 <esap> what's up?
12:41:25 <esap> other than the obvious (christmas :-)
12:41:32 <ski> not much
12:42:30 * esap has been trying to build a syntax for my language. It's harder than I thought. I ended up passing type information in the parser.... :-)
12:42:51 <ski> hm, which lang was that, now ?
12:43:00 <ski> (the CT related thing ?)
12:43:05 <esap> ski: yes.
12:43:51 * ski somewhat recently pondered an arrow (i think) without 'first'
12:44:34 <esap> ski: that would be something like a homset, I suppose?
12:45:14 <ski> \a b -> (s -> a) -> (s -> b)
12:46:59 <Heffalump> ski: I think Hughes discusses that idea in his paper
12:47:27 <ski> hm .. which one ?
12:47:35 * ski goes and checks
12:47:46 * esap has only thought about the case where you leave out 'arr'.
12:48:46 <ihope> > 'a' : repeat 'r'
12:48:47 <lambdabot> "arrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
12:48:47 <lambdabot> rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
12:48:47 <lambdabot> rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
12:48:47 <lambdabot> rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
12:48:47 <lambdabot> rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
12:48:49 <lambdabot> [23 @more lines]
12:49:14 <Heffalump> ski: removing first
12:49:30 <ski> anyway .. i was considering an arrow from '(a,s)' to 'Stream a', in this case
12:49:51 <Heffalump> I think there's a discussion about why the operation in ArrowChoice is not part of the basic Arrow class but first is.
12:49:57 <Heffalump> But I can't quite remember.
12:50:31 <esap> I suppose it's because variables generally need finite products.
12:51:25 <ihope> > fix error
12:51:30 <lambdabot> Terminated
12:51:43 <ski> > error ""
12:51:44 <lambdabot> Add a type signature
12:51:48 <ski> > error "" :: ()
12:51:49 <lambdabot> Exception:
12:52:05 <ski> > error (fix id) :: ()
12:52:06 <lambdabot> Exception: <<loop>>
12:52:18 <ski> > error (repeat '!') :: ()
12:52:20 <Trevion> @type fix
12:52:32 <ski> hm
12:52:40 <ski> @hello
12:52:43 <ihope> > fix id
12:53:05 <ski> ops
12:53:07 <xerox> > error (repeat '!') >>= error :: Maybe Int
12:53:19 <xerox> ...
12:53:48 <ski> this seems bad
12:54:16 <xerox> Err, yes.
12:54:36 <Cale> hehe
12:54:46 <xerox> @botsnack
12:55:05 <ihope> @arr
12:55:16 <xerox> I'm sorry ^_^
12:55:28 <Cale> nice bug though
12:55:29 <ihope> :-)
12:55:40 <ski> xerox : don't be sorry
12:55:49 <palomer> why is if not a function?
12:55:59 <ski> who knows ?
12:56:01 <xerox> I'd like to understand why it does behave like thate
12:56:34 <ihope> \x y x -> if x then y else z
12:56:50 <ihope> ...That second x should be a z, of course.
12:57:51 <musasabi> Has anyone benchmarked the SHA code on the ml?
12:58:07 <neologism> is there any nice descrtiption of parallel haskell?
12:58:20 <musasabi> parallel or concurrent?
12:58:31 <neologism> whats the difference?
12:58:39 <xerox> Euclid knows
12:59:08 <ihope> Well, there's http://www.macs.hw.ac.uk/~dsg/gph/
12:59:30 <ihope> I don't get it.
12:59:36 * xerox hides
13:00:01 <neologism> I meant.. description for total newbie
13:00:16 <basti_> neologism: did you hear about STM?
13:00:19 <ihope> Parallel Haskell: the same as regular Haskell, almost.
13:00:42 <neologism> shared transactional memory?
13:00:46 <basti_> "software"
13:00:56 <basti_> first there is forkIO, which does just what its name implies.
13:00:59 <basti_> @type forkIO
13:01:01 <lambdabot> Not in scope: `forkIO'
13:01:04 <basti_> hmm
13:01:12 <flux__> @index forkIO
13:01:13 <lambdabot> Control.Concurrent
13:01:21 <ihope> @type fix
13:01:21 <lambdabot> Not in scope: `fix'
13:01:29 <ihope> > fix
13:01:30 <lambdabot>  add an instance declaration for (Show ((a -> a) -> a))
13:01:36 <basti_> @type  Control.Concurrent.forkIO
13:01:37 <lambdabot> IO () -> IO GHC.Conc.ThreadId
13:01:42 <flux__> @compose type index forkIO
13:01:43 <lambdabot> Couldn't find qualified module.
13:01:43 <lambdabot> Maybe you're using the wrong syntax: Data.List.(\\) instead of (Data.List.
13:01:43 <lambdabot> \\)?
13:01:47 <flux__> ;(
13:01:48 <neologism> cannot arrows be used for automatic parallelization of program?
13:01:53 <musasabi> neologism: concurrent = concurrent things inside the same machine (i.e. MVars, STM, etc), parallel haskell = transpararent parallel computation on a cluster
13:02:12 <neologism> then I wanted parallel
13:02:19 <basti_> ihope: fix is recursive monadic stuff.
13:02:46 <ihope> Monadic?
13:03:06 <neologism> what about the arrows?
13:03:20 <ihope> @index fix
13:03:20 <lambdabot> Control.Monad.Fix, Control.Monad.Reader, Control.Monad.Writer, Control.
13:03:20 <lambdabot> Monad.State, Control.Monad.RWS, Control.Monad.Identity, Control.Monad.
13:03:20 <lambdabot> Error
13:04:13 <basti_> ihope: it's a bit like "the paradox operator" with a monadic type
13:04:25 <basti_> neologism: well, why arrows, why automatic? ^^
13:04:30 <neologism> just an idea
13:04:40 <neologism> I dont know
13:04:52 <basti_> it'd be nice to have automatic parallelism, yes
13:04:57 <ihope> basti_: it doesn't look monadic to me
13:05:14 <neologism> with referential transparency it should be that hard to do that
13:05:25 <neologism> I (naively) think
13:05:37 <basti_> ihope: oh sorry, mfix is the monadic one
13:05:51 <xerox> @index mfix
13:05:52 <lambdabot> Control.Monad.Fix, Control.Monad.Reader, Control.Monad.Writer, Control.
13:05:52 <lambdabot> Monad.State, Control.Monad.RWS, Control.Monad.Identity, Control.Monad.
13:05:52 <lambdabot> Error
13:05:55 <basti_> neologism: yes, referential transparency helps a lot with concurrency and paralellism
13:06:00 <xerox> @type Control.Monad.Fix.mfix
13:06:01 <lambdabot> forall (m :: * -> *) a.
13:06:01 <lambdabot> (Control.Monad.Fix.MonadFix m) =>
13:06:01 <lambdabot> (a -> m a) -> m a
13:06:12 <musasabi> For automatic parallelism strategies and par are important,
13:06:14 <xerox> ...and how do you feed it the 'a' ?
13:06:25 <ihope> It feeds itself the a.
13:06:32 <xerox> Well..
13:06:38 <neologism> basti_: so.. is there anything which exploits that?
13:06:58 <basti_> neologism: you mean, like, parallel programming and concurrency becoming very easy in haskell?
13:07:04 <xerox> @type mfix (>>=)
13:07:05 <lambdabot> Not in scope: `mfix'
13:07:10 <neologism> I mean.. normal haskell program
13:07:11 <xerox> @type Control.Monad.Fix.mfix (>>=)
13:07:13 <lambdabot> forall (m :: * -> *) b.
13:07:13 <lambdabot> (Control.Monad.Fix.MonadFix ((->) (b -> m b)), Monad m) =>
13:07:13 <lambdabot> (b -> m b) -> m b
13:07:18 <xerox> Woo..
13:07:20 <neologism> which is executed auto-parallely
13:07:44 <basti_> neologism: i don't know about such a thing, but it should be possible.
13:07:52 <basti_> neologism: usually, parallelism is explicit in haskell
13:08:01 <ihope> > (head . show . error) "testing"
13:08:01 <lambdabot> Add a type signature
13:08:02 <neologism> yeah.. but what about implicit
13:08:05 <ihope> > (head . show . error) "testing" :: Int
13:08:06 <lambdabot> Couldn't match `Int' against `Char'
13:08:08 <neologism> that would be nice thing to have
13:08:12 <ihope> > (head . show . error) "testing" :: Char
13:08:12 <lambdabot> Add a type signature
13:08:15 <basti_> I doubt, however, that implicit parallelism is a silver bullet or anything
13:08:16 <ihope> !
13:08:35 <neologism> silver bullet? my english fails
13:08:38 <esap> making things implicitly parallel is not easy even with referential transparency.
13:08:49 <basti_> It'll be an important technique in like 20 years maybe.
13:08:58 <basti_> but right now i just don't see it
13:09:16 <neologism> with multi-core cpus today I think it would be.. beneficial
13:09:22 <neologism> and might push functional languages more into the mainsteam
13:09:24 <flux__> basti_, you mean because of practical problems or because nobody needs it?
13:09:26 <neologism> mainstream
13:09:48 <basti_> flux__: i mean, because of the lack of situation in which it would help more than a tiny bit, or where it would pay its expenses
13:10:01 <esap> The problem is with efficiency. Nobody wants concurrency that's too slow to be useable. And making things both efficient and concurrent requires that you divide the problem properly.
13:10:11 <basti_> neologism: i think dataflow languages are even better suited to parallelism.
13:10:28 <neologism> seems so
13:10:35 <flux__> esap, could the problem be divided dynamically?
13:10:50 <flux__> (well, yeah, it obviously could be, but is there too much overhead, has that been researched)
13:10:59 <basti_> i think it'd be pretty hard to find a good splitting heuristic
13:11:03 <esap> flux: I doubt it would be efficient.
13:11:35 <basti_> but let's wait until cpu's come with dozens of cores usually and are connected with extremely fast lines
13:11:59 <flux__> like a haskell program could run for some time, gather profiling and load distribution info, and then recompile an optimized binary which would incorporate that information and continue from where it left off :)
13:12:12 <flux__> basti_, well, how about that sun's 32-threaded machine
13:12:14 <ihope> Dozens? I'd rather have a core for each kilobyte of memory.
13:12:17 <neologism> maybe speculative forward evaluation or something
13:12:40 <flux__> (n cpu's with m cores with k threads)
13:12:55 <ricebowl> doesn't Amdahl's Law still apply?
13:13:08 <ricebowl> problems can only parallelize up to a point
13:13:23 <basti_> this way or that, our cpu's are two-fold currently
13:13:40 <basti_> maximally. except for very special computers.
13:14:10 <flux__> well, if you have enough parallel cpu's, you could for example sort in linear time.. simply make all permutations of the data, one for each cpu, and then check which one of them is sorted ;)
13:14:40 <neologism> flux__: enough :)
13:14:46 <basti_> >.<
13:14:47 <esap> The problem is efficient use of resources. You can't waste resources if you want efficiency.
13:14:54 <basti_> you can't?
13:15:05 <flux__> esap, sometimes wasting may be ok, if you still outperform single-threaded performance?
13:15:09 <basti_> thats why our processors are 2.4 GHz and we have gigs of ram yes?
13:15:10 <basti_> ^^
13:15:25 <ricebowl> flux__ - if you want to sort more than about 150 items or so, you need more processors than there are atoms in the entire universe
13:15:54 <flux__> ricebowl, you could still use that algorithm with for example merge- or quick-sort, when the block size is sufficiently small ;)
13:16:03 <ihope> Well, we could fit multiple processors into each atom.
13:16:19 <flux__> it wouldn't be O(n) anymore, though ;(
13:16:23 <xerox> (Goodnight)
13:16:26 * basti_ slaps ihope with a massively parallel trout
13:16:35 <ricebowl> er
13:16:40 <ricebowl> far less than 150 items, sorry
13:16:42 <esap> flux: well ok, it depends on what is important. Sometimes you can waste memory for getting better performance (or vice versa). But in general, wasting resources is a bad thing.,
13:16:44 * TuringTest points to http://en.wikipedia.org/wiki/Grover%27s_algorithm
13:16:45 <ricebowl> I was thinking 2^150, but it's a factorial
13:17:01 <TuringTest> "
13:17:21 <Philippa> esap: as a rule of thumb, one resource is your bottleneck at any given time...
13:17:21 <basti_> if we got quantum computers, haskell will be outdated anyway
13:17:21 <TuringTest> That's O(N^(1/2)) time
13:17:49 <Heffalump> basti_: or it might turn out to be the closest existing language that's useful for working with them
13:17:52 <ihope> Outdated? Eek
13:18:06 <esap> philippa: true.
13:18:37 <Philippa> also, if you're getting something for your expenditure, it's not a "waste" :-)
13:18:46 * TuringTest points to http://arxiv.org/abs/quant-ph/0501151 : "Structuring quantum effects: superoperators as arrows" in Haskell
13:19:02 <basti_> >_<
13:19:18 <basti_> yes please
13:19:52 <sethk> I have a "package is broken" for libhgc6-hunit-dev in ubuntu.  Anyone know whether this is a known problem or I really have a bad install of hunit?
13:20:06 <sethk> actually it says "half configured"
13:20:28 <xs> haskell+qc seem to go together well.
13:20:54 <esap> philippa: OTOH, often resource consumption is relatively uniformly distributed in the software [depending on the architecture, of course].
13:21:07 <basti_> TuringTest: hurts.
13:21:33 <TuringTest> I find arrows usually hurt.  I no longer feel pain from dealing with Quantum Mechanics, though.
13:22:35 * Heffalump needs to play with arrow notation more
13:23:12 <Philippa> arrow notation often reads a lot like transliterated C code
13:23:19 <ihope> C?
13:23:43 <Philippa> foo <- bar -< baz vs foo = bar(baz)
13:24:37 <TuringTest> I have never used arrows, but they look useful for making "really smart parsers" and "Yampa: Functional Reactive Programming".  Any other known applications?
13:24:43 <musasabi> too often it looks like
13:24:54 <musasabi> "foo <- bar -< ()" which is not very pretty
13:24:57 * esap goes to watch television. bbl.
13:25:25 <Philippa> yeah, a better way to do nullary ops would be good
13:25:42 <ihope> Nullary?
13:25:49 <Philippa> TuringTest: there's a few more in Hughes' original paper. Anything where not being ArrowApply lets you do funky pseudo-static analysis, I guess
13:26:14 <humasect> @find <<
13:26:15 <lambdabot> parse error on input `<<'
13:26:15 <Philippa> ihope: "no parameters"
13:26:16 <TuringTest> Representing arbitrary graphs as programming syntax is not a matter of syntactic sugar.  It needs a LabView style GUI interface.
13:27:00 <TuringTest> Philippa: It was added to GHC in anticipation of being useful for many things.  But I was wondering where it has been applied.  It does not seem to be useful for network programming, for example.
13:27:13 <ihope> Philippa: sorta like getLine?
13:27:19 <Philippa> ihope: right
13:27:40 <Philippa> TuringTest: I think it turns out most of the fun examples are also monads, which sorta wrecks things a bit
13:27:41 <TuringTest> Oh...I remember another application that uses Arrows: one of the XML libraries.
13:28:00 <TuringTest> Philippa: exactly (or co-monads)
13:31:13 <ihope> @type Just "test" >>= const
13:31:14 <lambdabot>   Expecting a function type, but found `m b'
13:31:14 <lambdabot>   Expected type: [Char] -> Maybe b
13:31:28 <ihope> Eh?
13:31:48 <ihope> Oh
13:31:56 <ihope> @type Just "test" >>= (return . const)
13:31:57 <lambdabot> forall b. Maybe (b -> [Char])
13:48:18 <TuringTest> @djinn (a-> m b) -> (b -> m c) -> (a -> m c)
13:48:18 <lambdabot> -- f cannot be realized.
13:48:39 <TuringTest> @type f g x = (f x) >>= g
13:48:40 <lambdabot> parse error on input `='
13:49:21 <TuringTest> @type  (f x) >>= g
13:49:21 <lambdabot> Not in scope: `f'
13:49:21 <lambdabot>  
13:49:21 <lambdabot> <interactive>:1:3: Not in scope: `x'
13:49:24 <ihope> @type \f g x -> (f x) >>= g
13:49:25 <lambdabot> forall (m :: * -> *) a t b.
13:49:25 <lambdabot> (Monad m) =>
13:49:25 <lambdabot> (t -> m a) -> (a -> m b) -> t -> m b
13:49:28 <TuringTest> ah
13:50:02 <TuringTest> @pl ( \f g x -> (f x) >>= g)
13:50:03 <lambdabot> flip . ((>>=) .)
13:50:29 <ihope> Also, you don't need the parentheses around f x, as that'll take precedence anyway.
13:50:38 <ihope> @pl \x y z -> x z (y z)
13:50:38 <lambdabot> ap
13:50:42 <TuringTest> @pl ( \f g x -> g <<= f x)
13:50:43 <lambdabot> flip ((.) . (<<=))
13:50:59 <ihope> @type ap
13:51:00 <lambdabot> Not in scope: `ap'
13:51:13 <TuringTest> @type Control.Monad.ap
13:51:14 <lambdabot> forall b (m :: * -> *) a. (Monad m) => m (a -> b) -> m a -> m b
13:51:37 <CosmicRay> does bjorn bringert irc?
13:51:56 <ihope> @djinn m (a -> b) -> m a -> m b
13:51:56 <lambdabot> -- f cannot be realized.
13:52:48 <TuringTest> @djinn (Monad m) =>  m (a -> b) -> m a -> m b
13:52:49 <lambdabot> Cannot parse command
13:53:08 <ihope> @type \x y -> do {a <- x; b <- y; return x y}
13:53:08 <lambdabot> forall (m :: * -> *) a b.
13:53:08 <lambdabot> (Monad ((->) (m a)), Monad m) =>
13:53:08 <lambdabot> m b -> m a -> m b
13:53:14 <ihope> That's not right.
13:53:39 <TuringTest> @type \x y -> do {a <- x; b <- y; return a b }
13:53:40 <lambdabot> forall (m :: * -> *) a b.
13:53:40 <lambdabot> (Monad ((->) a), Monad m) =>
13:53:40 <lambdabot> m (m b) -> m a -> m b
13:54:04 <TuringTest> @type \x y -> do {a <- x; b <- y; return.a b }
13:54:05 <ihope> @type \x y -> do {a <- x; b <- y; return (x y)}
13:54:05 <lambdabot> forall (m :: * -> *) b a a1.
13:54:05 <lambdabot> (Monad m, Monad ((->) a)) =>
13:54:05 <lambdabot> (a -> a1 -> a -> b) -> (a -> a1) -> a -> m b
13:54:06 <lambdabot>   Occurs check: cannot construct the infinite type: t = t -> t1
13:54:06 <lambdabot>   Expected type: t
13:54:13 <palomer> check this out: I want to do proof search, and this involves state (for the variables). so I want to search, if it fails I want to throw away the new state accumulated, and if it passes I want to pass the state to further computation
13:54:15 <palomer> how do I do this?
13:55:01 <TuringTest> That is more deterministic that the usual depth first search.
13:55:31 <palomer> this is easy to do with depth first search
13:55:56 <ihope> > flip const undefined 3
13:55:56 <palomer> but, like, I want to do this with a monad
13:55:57 <lambdabot> 3
13:57:43 <humasect> 6.5 must be the version to support multibyte source files / string lits
13:57:43 <TuringTest> Monads are good models for DFS.  But you want to implement memory, correct?  So the search knows which ones have passed and this alters the algorithm?
13:58:01 <palomer> well, as soon as a single one passes it returns
13:58:09 <palomer> with a new state
13:58:11 <palomer> if none passes it fails
13:59:00 <palomer> oh wait, that doesn't work
13:59:06 <palomer> hrm, I'll have to think about it some more
14:01:02 <ihope> data Badger = Badger Badger Badger | Mushroom
14:01:22 <basti_> OW.
14:01:36 <mauke> that made me laugh out loud
14:01:46 <palomer> oh boy
14:02:12 <Philippa> Pattern match failure: Snake!
14:03:28 <ihope> data Snake = Ohh | Its | A Snake
14:04:08 <ihope> Might as well make that "It's".
14:05:45 <mjl69_> so that would be 2 nullary constructors and a unary constructor recursively using it's own type as an argument?
14:05:54 <ihope> Yes.
14:05:54 <Cale> argh, nautilus seems overly picky about file types
14:06:05 <Cale> Cannot open Roots.lhs: The filename "Roots.lhs" indicates that this file is of type "Literate haskell source code". The contents of the file indicate that the file is of type "HTML page". If you open this file, the file might present a security risk to your system.
14:29:43 <TuringTest> It's file magic is silly.  What does "file Roots.lhs" say, for comparison?
14:30:47 <Cale> Roots.lhs: HTML document text
14:31:06 <Cale> It is a valid HTML document, but of course, it's actually literate Haskell.
14:31:30 * TuringTest wonders if file extension could override magic detection
14:32:23 <Cale> ugh, BeOS had this right, why can't we copy them
14:33:42 <Cale> Keep a mime type as an extended attribute in the file system, and do a magic detection once if it doesn't exist. Let the user change it.
14:34:13 <TuringTest> We don't do that on OS X because Jean-Louis GassÃ©e and Jobs ended up not liking each other.
14:34:19 <Cale> heh
14:34:48 <TuringTest> The latest OS X has some special file metadata typing API hocus pocus UUID quasi-MIME thing.
14:35:06 <Cale> I found file types on OSX utterly confusing.
14:35:15 <dons> @arr
14:35:16 <lambdabot> Har de har har!
14:35:21 <dons> hmm...
14:35:38 <Cale> I don't think I ever did figure out where it was keeping the file handler association data.
14:36:12 <tuomov> on linux it's of course in 10 different places because gnome and kde must reinvent the wheel all the time
14:36:58 <tuomov> it is contrary to their philosophy to use the existing *nix solution
14:37:04 <Cale> tuomov: and of course, the one piece of information you're actually interested in, the actual mime type, isn't actually stored anywhere.
14:37:12 <tuomov> because, as l'caza pointed out, unix does all wrong and we should learn from m$
14:37:14 <dons> shapr, I like the idea of @hwn. that would save much time.
14:39:45 <musasabi> just using an extended attribute "mime-type" and being done with it could work.
14:39:57 <musasabi> (and that works with NFSv4 too)
14:40:18 <TuringTest> palomer: are you still there?
14:40:43 <tuomov> and then there's the associations from mime-types to apps that are in a zillion places
14:40:50 <humasect> http://quux.org/devel/magic-haskell ^_^
14:41:27 <tuomov> the proper place being .mailcap but gnome and kde of course not using this
14:41:47 <TuringTest> Apple's new UTI : http://developer.apple.com/macosx/uniformtypeidentifiers.html
14:43:01 <tuomov> xml is so fucking unreadable..
14:43:44 <TuringTest> Especially as a programming language: http://fxsl.sourceforge.net/articles/FuncProg/2.html#List_processing
14:44:13 <TuringTest> (You have to scroll down for the xslt version)
14:44:30 * TuringTest is surprised they fit on one screen
14:45:05 <tuomov> xml is only suitable for "protocol data" that no-one would even image editing by hand
14:45:14 <tuomov> and even there it's not the most efficient format
14:45:47 <musasabi> tuomov: whitespace issues make that tricky, not to start with the "should we stuff binary data here and how" thing.
14:46:23 <tuomov> darcs changes --xml is one such use where the syntax doesn't matter so much
14:46:35 <tuomov> but it could use some other universal config syntax
14:47:26 <tuomov> \foo[key=value]{bar baz quk} ==> <foo key=value>bar baz quk</foo> would already make things much more readable for some uses
14:48:08 <tuomov> i.e. documents, but not for configuration files
14:48:55 <tuomov> for configuration simple key=value pairs and lists in some format is the best format
14:50:17 <basti_> my "piano roll" display pre-trial in gtk/cairo can display a subset of 200000 notes without twitching, due to careful construction
14:51:52 * ADEpt is desperate. Anyone perchance have experience in Openoffice macros?
14:57:26 <basti_> http://deimos.dynalias.org/pianoroll-1.gif < porn ^^
14:58:57 <musasabi> that is evil. now I clicked it and didn't get any porn.
14:59:31 <basti_> its metaphorical porn
15:02:06 <humasect> haskell libraries are DSLs.. 
15:02:15 <Philippa> not all are
15:02:23 <basti_> DSL is a rubber term
15:02:29 <Philippa> not for suitably meaningful values of 'language', anyway
15:02:30 <humasect> rubber? elastic?
15:02:35 <humasect> ah
15:16:18 <fieldy> Hello, good night
15:16:22 <fieldy> I've a question
15:16:36 <fieldy> I have some troubles with FFI and Haskell
15:16:49 <fieldy> trying to import C functions into a Haskell program
15:19:48 <musasabi> Which implementation are you using and what kind of error message do you get?
15:20:17 <musasabi> if it is ghc did you add "-ffi" to the command line, if it is hugs did you use "ffihugs"
15:22:03 <fieldy> ok
15:22:10 <fieldy> I typed -fffi
15:22:12 <fieldy> :s
15:22:39 <Cale> -fffi is right
15:23:05 <Cale> fieldy: what problem are you having exactly?
15:24:13 <fieldy> -fffi  doesn't work, but -ffi works perfect
15:24:14 <fieldy> :s
15:24:19 <fieldy> dunno why...
15:25:02 <Cale> oh, apparently they're both supposed to work
15:25:33 <Cale> and they're both implied by -fglasgow-exts
15:25:34 <fieldy> Thank you very much
15:25:47 <fieldy> ;)
15:25:56 <fieldy> I'll continue working
15:25:57 <fieldy> thanks
16:09:38 <mjl69__> @eval [1,2,3] >>= \x->x*x:[]
16:09:39 <lambdabot> [1,4,9]
16:09:48 <mjl69__> :)
16:11:45 <mjl69__> @eval let f x = x*x in concat(map f [1,2,3]:[])
16:11:46 <lambdabot> [1,4,9]
16:12:20 <mjl69__> it's starting to sink in a little...
16:13:05 <jberg> i ordered the SoE book today. hope its good
16:13:17 <mjl69__> a list is like a monad with training wheels :)
16:16:43 <Cale> hehe
16:25:25 <int-e> @type f x y = (concat (map x y), concatMap x y, y >>= x)
16:25:26 <lambdabot> parse error on input `='
16:25:33 <int-e> @type \x y -> (concat (map x y), concatMap x y, y >>= x)
16:25:34 <lambdabot> forall a a1. (a1 -> [a]) -> [a1] -> ([a], [a], [a])
17:30:21 * SamB thinks stella ought to let you set a breakpoint on a pixel
17:37:16 <SamB> hmm, apparantly it DOES!
18:08:43 <syntaxfree> let life = 99 in odd life
18:08:52 <syntaxfree> > let life = 99 in odd life
18:08:54 <lambdabot> True
18:08:57 <syntaxfree> Life is odd.
18:09:11 <runehol> life is 99.
18:09:16 <syntaxfree> Life is odd.
18:09:19 <syntaxfree> > odd life
18:09:20 <lambdabot>  Not in scope: `life'
18:09:37 <syntaxfree> I need an existential monad.
18:09:46 <syntaxfree> > mapM odd life
18:09:47 <lambdabot>  Not in scope: `life'
18:10:09 <syntaxfree> Haskell doesn't know about the realy important things.
18:10:11 <syntaxfree> * sigh * 
18:11:20 <syntaxfree> @hoogle friendship
18:11:21 <lambdabot> No matches found
18:12:38 <Saulzar> > odd $ length "life"
18:12:39 <lambdabot> False
18:12:43 <syntaxfree> why do I develop my deepest friendships with women? 
18:12:51 <Saulzar> Nothing odd about the length of life :)
18:13:00 <syntaxfree> odd $ sum $ map ord "Life"
18:13:15 <syntaxfree> > odd $ sum $ map ord "Life"
18:13:16 <lambdabot> False
18:13:27 <syntaxfree> > odd $ sum $ map ord "life"
18:13:28 <lambdabot> False
18:13:34 <syntaxfree> ok, I give up. Life is even.
18:13:46 <int-e> > ord 'l' - ord 'L'
18:13:47 <lambdabot> 32
18:14:22 <syntaxfree> > let hotOrNot = odd . sum . map ord in hotOrNot "pursuit of happiness"
18:14:23 <lambdabot> False
18:14:45 <syntaxfree> >  let hotOrNot = odd . sum . map ord in hotOrNot "Kant's categorical imperative"
18:14:46 <lambdabot> False
18:14:56 <syntaxfree> >  let hotOrNot = odd . sum . map ord in hotOrNot "Promiscuity"
18:14:58 <lambdabot> False
18:15:08 <syntaxfree> >  let hotOrNot = odd . sum . map ord in hotOrNot "Morality"
18:15:09 <lambdabot> True
18:15:13 <syntaxfree> >  let hotOrNot = odd . sum . map ord in hotOrNot "God"
18:15:14 <lambdabot> False
18:15:22 <syntaxfree> >  let hotOrNot = odd . sum . map ord in hotOrNot "Truth"
18:15:23 <lambdabot> True
18:15:31 <syntaxfree> >  let hotOrNot = odd . sum . map ord in hotOrNot "Panta rhei"
18:15:32 <lambdabot> False
18:15:37 <Saulzar> lambdabot is wiser than you think :)
18:15:43 <syntaxfree> >  let hotOrNot = odd . sum . map ord in hotOrNot "Permanent things in life"
18:15:44 <lambdabot> False
18:15:52 <syntaxfree> >  let hotOrNot = odd . sum . map ord in hotOrNot "Constant change"
18:15:53 <lambdabot> False
18:15:59 <int-e> > (odd . sum . map ord) "lambdabot"
18:16:01 <lambdabot> False
18:16:02 <syntaxfree> * sigh * 
18:16:50 <sproingie> wow.  just discovered Frag
18:16:52 <syntaxfree> > (odd . sum . map ord) "Did I screw up my relationship for good"?
18:16:53 <lambdabot>  parse error on input `}'
18:16:56 <Saulzar> It's easy to avoid constant change, become a hermit :)
18:17:01 <syntaxfree> > (odd . sum . map ord) "Did I screw up my relationship for good?"
18:17:01 <sproingie> i looked at the source to Frag and my brane all asplode
18:17:02 <lambdabot> False
18:17:03 <int-e> > (odd . sum . map ord) "strive for happiness"
18:17:05 <lambdabot> True
18:17:16 <syntaxfree> > (odd . sum . map ord) "Do I get one more chance?"
18:17:17 <lambdabot> False
18:17:23 <Saulzar> > (odd . sum . map ord) "Haskell is for Hermits"
18:17:24 <lambdabot> True
18:17:31 <syntaxfree> > (odd . sum . map ord) "Do I get many more chances if I wait?"
18:17:32 <lambdabot> True
18:17:43 <syntaxfree> > (odd . sum . map ord) "Does she still love me?"
18:17:44 <lambdabot> False
18:17:55 <syntaxfree> > (odd . sum . map ord)  "Will she ever love me again?"
18:17:56 <lambdabot> True
18:18:24 <Saulzar> lambdabot probably does as good as any other fortune teller you know :)
18:18:33 <int-e> > (odd . sum . map ord) "should we replace odd with even?"
18:18:34 <lambdabot> True
18:18:55 <Saulzar> > (even . sum . map ord) "should we replace even with odd?"
18:18:56 <lambdabot> False
18:19:11 <Saulzar> Ok I guess you need to reverse all the answers above :)
18:19:40 <CosmicRay> you all have way too much time on your hands ;-)
18:19:54 <syntaxfree> what is strange is that I just had the deepest conversation with the one woman that could replace my ex.
18:20:03 <Saulzar> > (even . sum . map ord) "Lambdabot is a great oracle"
18:20:04 <lambdabot> True
18:20:07 <int-e> (odd . sum . map ord) "do we have too much time at our hands?"
18:20:12 <carp> Cosmic Ray: you're here too!
18:20:16 <int-e> > (odd . sum . map ord) "do we have too much time at our hands?"
18:20:17 <lambdabot> True
18:20:27 <CosmicRay> heh
18:20:29 <Saulzar> Only the even lambdabot is correct, though
18:20:33 <syntaxfree> how do I use the Random monad from Lambdabot?
18:20:51 <Saulzar> mkStdGen should work
18:20:53 <syntaxfree> so I produce ((if rnd > .5  then odd else even) . sum . map ord )
18:20:55 <sproingie> meh.  ghc deb for amd64 is still broken
18:20:58 <syntaxfree> > mkStdGen
18:20:58 <lambdabot>  add an instance declaration for (Show (Int -> StdGen))
18:21:06 <syntaxfree> > mkStdGen :: Double
18:21:07 <lambdabot>   Expecting a function type, but found `Double'
18:21:07 <lambdabot>   Expected type: Double
18:21:07 <lambdabot>   Inferred type: Int -> StdGen
18:21:14 <CosmicRay> Igloo: isn't uploading ghc because of some tex thing
18:21:16 <int-e> > mkStdGen 100
18:21:17 <lambdabot> 101 1
18:21:17 <CosmicRay> I don't quite understand it
18:21:24 <Saulzar> mkStdGen just gives you the generator
18:21:27 <Saulzar> From a seed
18:21:30 <syntaxfree> > mkStdGen 1
18:21:31 <lambdabot> 2 1
18:21:34 <syntaxfree> ?
18:21:45 <Cale> syntaxfree: lambdabot has a Random monad?
18:21:55 <syntaxfree> askell has a random monad.
18:21:56 <int-e> a random number generator is a pair of numbers
18:22:00 <syntaxfree> so does Haskell.
18:22:10 <syntaxfree> > x <- mkStdGen 1
18:22:10 <lambdabot>  parse error on input `<-'
18:22:18 <Cale> syntaxfree: er, not in the standard
18:22:23 <CosmicRay> > mkStdGen 1 >>= print
18:22:24 <lambdabot> Couldn't match `m a' against `StdGen'
18:22:25 <syntaxfree> !!
18:22:37 <Cale> and not in GHC
18:22:38 <Saulzar> > randomR (0.0, 10.0) (mkStdGen 0) 
18:22:39 <lambdabot> (9.872770354820595,1601120196 1655838864)
18:22:42 <syntaxfree> Standard Haskell doesn't have a Random monad?
18:22:46 <Saulzar> > fst $ randomR (0.0, 10.0) (mkStdGen 0) 
18:22:47 <lambdabot> 9.872770354820595
18:22:53 <Saulzar> > fst $ randomR (0.0, 10.0) (mkStdGen 1) 
18:22:55 <lambdabot> 3.5925462466181597
18:23:06 <syntaxfree> > fst $ randomR (0.0, 1.0) (mkStdGen 0) 
18:23:07 <lambdabot> 0.9872770354820595
18:23:14 <syntaxfree> > fst $ randomR (0.0, 1.0) (mkStdGen 0)
18:23:15 <lambdabot> 0.9872770354820595
18:23:21 <syntaxfree> that is not good.
18:23:32 <Cale> you're giving it the same seed
18:23:34 <sproingie> that is pseudorandom
18:23:38 <sproingie> that's quite expected
18:23:48 <sproingie> true random is I/O
18:23:59 <Cale> it's supposed to give the same pseudorandom value for the same seed
18:24:10 <syntaxfree> > let F x = fst $ randomR (0.0, 1.0) (mkStdGen ((map . sum . ord ) x) in F "I hope this works"
18:24:11 <lambdabot>  parse error on input `in'
18:24:22 <syntaxfree> > let F x = fst $ randomR (0.0, 1.0) (mkStdGen ((map . sum . ord ) x)) in F "I hope this works"
18:24:23 <lambdabot>  Not in scope: data constructor `F'
18:24:23 <sproingie> just read /dev/random if you're not into portability
18:24:24 <Saulzar> second pair of lambdabot (9.872770354820595,1601120196 1655838864) gives you a new stdGen to use ..
18:24:39 <syntaxfree> > let f x = fst $ randomR (0.0, 1.0) (mkStdGen ((map . sum . ord ) x)) in f "I hope this works"
18:24:40 <Cale> > let f x = fst $ randomR (0.0, 1.0) (mkStdGen ((map . sum . ord ) x)) in f "I hope this works"
18:24:40 <lambdabot> Couldn't match `[a]' against `Int'
18:24:40 <lambdabot> Couldn't match `[a]' against `Int'
18:24:46 <int-e> > take 10 $ unfoldr (Just . randomR (0.0,1.0)) $ mkStdGen 1
18:24:48 <lambdabot> [0.35925462466181596,0.33062336659771935,0.13648524126887446,0.538640014836
18:24:48 <lambdabot> 248,0.9018299466468929,0.5010485614186732,0.7347716559271262,0.205166710472
18:24:48 <lambdabot> 00608,0.4206756984164649,0.9360782023137617]
18:25:15 <syntaxfree> > take 1 $ unfoldr (Just . randomR (0.0,1.0)) $ mkStdGen 1
18:25:16 <lambdabot> [0.35925462466181596]
18:25:31 <Cale> > let f x = fst $ randomR (0.0, 1.0) (mkStdGen ((sum . map ord) x)) in f "I hope this works"
18:25:32 <lambdabot> 0.7701354167121778
18:25:37 <syntaxfree> > let f x = head $ take 1 $ unfoldr (Just . randomR (0.0,1.0)) $ mkStdGen x in f "I hope this works"
18:25:38 <lambdabot> Couldn't match `Int' against `[Char]'
18:26:25 <syntaxfree> >  (let f x = fst $ randomR (0.0, 1.0) (mkStdGen ((sum . map ord) x))) > .5) in f "I hope this works"
18:26:26 <lambdabot>  parse error on input `)'
18:26:36 <syntaxfree>  >  ((let f x = fst $ randomR (0.0, 1.0) (mkStdGen ((sum . map ord) x))) > .5) in f "I hope this works"
18:26:43 <Saulzar> True random is no good for this oracle buisiness anyway, otherwise it keeps on telling you a different story :)
18:26:54 <syntaxfree> Life is random.
18:28:05 <syntaxfree> why do my deepest friendships develop with women?
18:28:30 <syntaxfree> > take 1 $ unfoldr (Infinite . wisdomAboutTheWorld) "Why oh why?"
18:28:31 <lambdabot>  Not in scope: `wisdomAboutTheWorld'
18:28:40 <syntaxfree> Haskell needs work.
18:29:09 <Korollary> no, lambdabot does
18:29:35 * int-e thinks that using  take 1 $ unfoldr  makes very little sense.
18:29:59 <Korollary> why take 1 from infinite wisdom anyways...
18:30:18 <syntaxfree> @type Life
18:30:19 <lambdabot> Not in scope: data constructor `Life'
18:30:36 <syntaxfree> > sort "What is the meaning of life?"
18:30:37 <lambdabot> "     ?Waaeeeffghhiiilmnnostt"
18:31:17 <sproingie> haskell.  making infinite loops fast since 1990
18:31:41 <syntaxfree> @hoogle meaning
18:31:42 <lambdabot> No matches found
18:31:51 <syntaxfree> @hoogle experience->wisdom
18:31:52 <lambdabot> Control.Monad.Error.throwError :: MonadError e m => e -> m a
18:31:52 <lambdabot> Prelude.id :: a -> a
18:31:52 <lambdabot> Prelude.undefined :: a
18:32:01 <syntaxfree> @hoogle [experience]->wisdom
18:32:02 <lambdabot> Data.List.head :: [a] -> a
18:32:02 <lambdabot> Data.List.last :: [a] -> a
18:32:02 <lambdabot> Prelude.head :: [a] -> a
18:32:30 <syntaxfree> what's the diff'rence between Data.List.head and Prelude.head?
18:32:49 <syntaxfree> > undefined Lifwe
18:32:49 <lambdabot>  Not in scope: data constructor `Lifwe'
18:32:53 <sproingie> prelude predates the hierarchical libraries
18:32:55 <syntaxfree> > undefined "Life"
18:32:55 <lambdabot> Add a type signature
18:33:05 <syntaxfree> sproingie: I  know. But why rewrite head?
18:33:26 <Cale> head is surely still part of the prelude
18:33:32 <int-e> syntaxfree: it's not rewritten. it's just re-exported.
18:33:43 <syntaxfree> why not just leave it to the prelude?
18:33:46 <sproingie> syntaxfree: presumably for the day when haskell grows a module system
18:33:54 <sproingie> syntaxfree: makes API documentation easier too
18:33:59 <int-e> Prelude> :info Prelude.head
18:33:59 <int-e> head :: [a] -> a        -- Imported from GHC.List
18:33:59 <int-e> Prelude> :info Data.List.head
18:33:59 <int-e> head :: [a] -> a        -- Imported from GHC.List
18:34:03 <sproingie> syntaxfree: and you can work without the prelude if you choose
18:34:05 <syntaxfree> @hoogle a->[a]
18:34:06 <Cale> Haskell has a module system, it's not too bad :)
18:34:06 <lambdabot> Data.List.repeat :: a -> [a]
18:34:06 <lambdabot> Prelude.repeat :: a -> [a]
18:34:06 <lambdabot> Data.List.replicate :: Int -> a -> [a]
18:34:18 <syntaxfree> replicate 2 3
18:34:23 <syntaxfree> > replicate 2 3
18:34:24 <lambdabot> [3,3]
18:34:29 <sproingie> Cale: certainly it's better documented than ocaml's
18:34:37 <syntaxfree> replicate a b = take a $ repeat b ?
18:35:12 <syntaxfree> The advantage of ML over Haskell -- besides eager evaluation, which is a "not the point issue" -- is the type system, right?
18:35:21 <int-e> @pl replicate a b = take a $ repeat b 
18:35:22 <lambdabot> replicate = (. repeat) . take
18:35:40 <syntaxfree> @pl replicate b a = take a $ repeat b
18:35:40 <lambdabot> replicate = flip take . repeat
18:35:47 <syntaxfree> proper argument order does wonders.
18:36:22 <syntaxfree> isn't replicate a b = flip (flip take . repeat)) then?
18:36:33 <int-e> syntaxfree: anyway, that's how it's implemented, yes.
18:36:33 <Cale> syntaxfree: I'd say Haskell's type system had more cool stuff in it. SML and ocaml have neat module systems though.
18:36:34 <syntaxfree> @pl f a b = a + 2*b
18:36:34 <lambdabot> f = (. (2 *)) . (+)
18:36:40 <sproingie> syntaxfree: ML is merely different.  not wearing a hair shirt of laziness and monads tends to make some things more straightforward in ML
18:37:03 <syntaxfree> I know Ocaml is "impurely" functional. But are all MLs?
18:37:22 <CosmicRay> I believe so.
18:37:36 <sproingie> ocaml describes itself as multiparadigm
18:37:41 <CosmicRay> Cale: actually I disagree that ocaml has a nice module syustem.
18:37:50 <CosmicRay> I generally think of its module system as a trainwreck.
18:37:53 <sproingie> i find ocaml's ML and OO are kind of weakly glued together
18:38:02 <syntaxfree> so doesn't the whole "O'Caml faster than C++" deal just mean "O'Caml is really fast at imperative programming"?
18:38:04 <sproingie> in fact the O in ocaml always seems to be deprecated
18:38:11 <Cale> CosmicRay: really? I've heard only good things about it up until now.
18:38:34 <CosmicRay> Cale: yeah, I switched from ocaml to haskell.  it is incredibly difficult to write a portable build script for a simple ocaml library.
18:38:35 <sproingie> syntaxfree: it's quite fast in general.  FP is easier to make slow accidentally, that's all
18:38:52 <CosmicRay> there is also a lot of unnecessary complexity.
18:38:58 <CosmicRay> some of that is optional.
18:39:00 <CosmicRay> some of it, not so much.
18:39:15 <CosmicRay> ocaml is fast, but then it is also not lazy.
18:39:17 <sproingie> ML's lack of overloading is my main gripe.  lack of machine types is another.
18:39:20 <Cale> Laziness is really important to functional programming. I don't know how people get by without it.
18:39:38 <syntaxfree> sproingie: is it faster than Haskell at doing, say, "vector multiplication" using sum $ zipWith (*)  ? 
18:39:39 <CosmicRay> ocaml also is less full-featured than haskell in many ways
18:39:39 <sproingie> i guess there's machine types if you use a module, just like haskell.  but there's barely any literal representation
18:39:41 <CosmicRay> especiall I/O
18:39:49 <Korollary> Cale: why is lazyness so important in fp?
18:40:00 <sproingie> syntaxfree: you'll find that using lists for vector math is excruciatingly slow
18:40:09 <syntaxfree> sproingie: yes, I've found that.
18:40:12 <CosmicRay> syntaxfree: ocaml lists are, if memory serves, pretty close to machine lists (or maybe linked lists)
18:40:15 <syntaxfree> sproingie: but fixed-lengthness gets in my way.
18:40:25 <sproingie> syntaxfree: it's a vector.  you should know the length.
18:40:27 <CosmicRay> syntaxfree: they are most definately not lazy nor can they be infinite.
18:40:35 <CosmicRay> syntaxfree: that does buy them some speed advantage however.
18:40:44 <CosmicRay> syntaxfree: more fair isto compare an ocaml stream to a haskell list
18:40:44 <Saulzar> The ocaml libraries came off as rather "ad-hoc" to me
18:40:59 <CosmicRay> syntaxfree: but then, ocaml streams are not well supported by most code, since people code for lists.
18:41:05 <CosmicRay> Saulzar: especially regarding I/O
18:41:07 <sproingie> Korollary: google for "wearing the hair shirt", which justifies haskell's design decisions like laziness and purity
18:41:12 <Cale> Korollary: well, I'm writing an article on this at the moment, if you's like to see what I have so far, you're welcome to, but the paper http://www.md.chalmers.se/~rjmh/Papers/whyfp.pdf should explain quite nicely :)
18:41:22 <Cale> you'd*
18:41:22 <sproingie> Saulzar: i'd prefer a little more ad hoc.  i.e. ad hoc polymorphism.
18:41:24 <icb> Just:: forall a. a-> Maybe a .
18:41:27 <Korollary> Cale: I don't remember whyfp talking about lazyness...
18:41:30 <Saulzar> sproingie, Hehe.
18:41:35 <icb> what does the first "." and second "." mean?
18:41:51 <syntaxfree> infiniteness seems like a cool feature to write cool-looking code, but I haven't found a real use for it yet.
18:41:59 <sproingie> syntaxfree: you will
18:42:18 <Cale> icb: the first is just a separator, the last looks like an ordinary period, ending a sentence or something. It's not part of Haskell syntax.
18:42:20 <syntaxfree> I've been using Haskell for common everyday stuff and it has empowered me to do a lot I couldn't do before.
18:42:35 <Cale> Korollary: read it again, that was the whole point :)
18:42:36 <syntaxfree> I do feel that pure-ness gets in the way when dealing with random numbers.
18:42:43 <sproingie> syntaxfree: you can think of all lists as being generators.  this means you do not need while loops, you just calculate all the values that are possible, and take what you need
18:42:47 <syntaxfree> I haven't started to mess with type and data constructors, though.
18:42:49 <icb> ahh, thanks
18:43:08 <Cale> Korollary: well, at least a major point
18:43:08 <syntaxfree> sproingie : in other words, you can have closer-to-declarative programming.
18:43:15 * icb returns to the monad panic paper
18:43:25 <sproingie> syntaxfree: exactly
18:43:31 <syntaxfree> > let odds = [1,3..] in let evens = map (2*) odds in take 3 evens
18:43:32 <lambdabot> [2,6,10]
18:43:42 <syntaxfree> Why do I want that?
18:43:49 <sproingie> syntaxfree: granted, the examples like fibonacci don't make any of it very interesting
18:44:12 <syntaxfree> Fibs have a closed form.
18:44:13 <Cale> syntaxfree: read whyfp
18:44:14 <sproingie> the dialog pattern is a pretty nifty example of laziness
18:44:18 <syntaxfree> I did read whyfp.
18:44:23 <Cale> syntaxfree: I just posted the link above
18:44:42 <syntaxfree> .I'm kinda playing devil's advocate.
18:44:50 <syntaxfree> to get my mind off widerExistentialCrisis.
18:44:51 <Cale> syntaxfree: and you didn't see the million examples of laziness being useful?
18:45:26 <sproingie> syntaxfree: it's like database cursors, but you don't have to iterate explicitly.  you issue a "query" that could return a billion results, but if you only need the first 10, you only pay for those
18:45:27 <Korollary> sproingie: those slides state that purity is independent of monads and seems to cross off lazyness from the list of what matters.
18:45:40 <Korollary> err, independent of lazyness
18:45:48 <sproingie> syntaxfree: mind you i wouldn't try that with a real rdbms interface, but a big data structure in haskell behaves the same way
18:46:12 <Cale> Laziness is one of the most important kinds of glue available in functional programming. It increases the potential for modularity quite a lot.
18:46:36 <sproingie> Korollary: oh yah, heh, i forgot.  he does still explain laziness as a useful property that "keeps you honest"
18:46:43 <syntaxfree> what drew me to functional programming is higher-order functions.
18:46:48 <sproingie> Korollary: which made sense at the time i read it
18:46:50 <syntaxfree> maybe it's because I haven't been able to grok OO.
18:47:07 <sproingie> syntaxfree: OO is a lot of things and can usually be expressed as HOFs
18:47:24 <Korollary> sproingie: His question makes more sense: how to add lazyness to a strict language.
18:47:45 <syntaxfree> yes. from what I've read OO is a different way to abstract away stuff.
18:47:52 <sproingie> syntaxfree: all programming is
18:48:16 <Cale> Laziness is important because it lets you break problems up in a variety of ways that you couldn't otherwise.
18:48:38 <syntaxfree> I do feel FP allows for a more mathematically tractable way though.
18:48:53 <syntaxfree> I'm able to write programs with pen & paper when away from a computer in Scheme or Haskel.
18:48:55 <syntaxfree> s/ll.
18:49:17 <sproingie> i wish i could write interesting haskell without my brain going boom
18:49:19 <Korollary> Cale: Those problems are solvable in strict languages, yes? 
18:49:35 <sproingie> i looked at the source for Frag and just sort of stared incoherently
18:49:36 <syntaxfree> my bar for "interesting" is lower.
18:49:41 <syntaxfree> I feel empowered by FP.
18:49:45 <resiak> Is not any problem that is solvable by a computer solvable by any turing-complete language?
18:49:52 <Cale> > let isSubstringOf x y = any (isPrefixOf x) (tails y) in "orl" `isSubstringOf` "Hello, World!"
18:49:53 <lambdabot> True
18:50:10 <Cale> Would you write that in a strict language?
18:50:11 <sproingie> syntaxfree: my lack of education makes me feel really dumb when i start doing any FP.  but at least that means there's something I can learn
18:50:25 <syntaxfree> I think my lack of education draws me to FP.
18:50:30 <syntaxfree> I never learned proper data structures.
18:50:38 <syntaxfree> or memory management in C.
18:50:44 <syntaxfree> or the basics of OO.
18:50:55 <Cale> The problem being solved isn't important here, it's the way in which the solution is expressed.
18:50:59 <sproingie> syntaxfree: well, it's category theory and denotational semantics that are going to be your bogeymen in FP-land
18:51:07 <syntaxfree> I can grok cat theory.
18:51:07 <Korollary> Cale: It's a means to an end. The question is not how I would write it. It's if I'd write it.
18:51:10 <syntaxfree> to some point.
18:51:14 <syntaxfree> I can't grok OO.
18:51:18 <Cale> Korollary: hm?
18:51:32 <sproingie> syntaxfree: that makes you pretty unique.  OO is simple.  category theory is phenomenally abstruse stuff
18:51:47 <syntaxfree> well, I didn't go far in my cat theory studies.
18:51:55 <Korollary> Cale: If you want to know if a string is a substring of another, you can get it done in many strict languages.
18:52:03 <int-e> > let isSubstringOf x y = any (isPrefixOf x) (tails y) in "orl" `isSubstringOf` $ cycle "Hello, World!"
18:52:04 <lambdabot>  parse error on input `$'
18:52:04 <sproingie> syntaxfree: you had studies ... i can never find a decent place to begin
18:52:05 <Cale> No, you're missing the point here
18:52:15 <int-e> > let isSubstringOf x y = any (isPrefixOf x) (tails y) in "orl" `isSubstringOf` (cycle "Hello, World!")
18:52:16 <lambdabot> True
18:52:21 <syntaxfree> sproingie: Michael Arbib's "Arrows, structures, functors".
18:52:21 <Cale> I could give any number of other examples
18:52:28 <Cale> the point is how the problem is being solved
18:52:38 <sproingie> syntaxfree: url?  or is it dead tree?
18:52:40 <Cale> How that algorithm is being written
18:52:46 <Cale> How the problem is being decomposed
18:52:51 <syntaxfree> sproingie:  it's, AFAIK, the only cat theory manual that doesn't require previous knowledge of Black Magic Abstract Algebra.
18:52:56 <syntaxfree> dead tree, I'm afraid.
18:53:09 <Cale> You can't decompose that problem in that way in a strict language
18:53:23 <Cale> At least, if you do, you'll run into horrid performance problems.
18:53:24 <sproingie> yah.  i could never make myself buy expensive books when i'm never sure if i can even meet the prereqs
18:53:26 <Korollary> Cale: I don't want to decompose that problem in that particular way.
18:53:29 <syntaxfree> Cale: yes, but I find it harder to relate the code to how it'll be executed.
18:53:46 <syntaxfree> I don't see *algorithms* in Haskell, I see *declarations*.
18:53:56 <Cale> syntaxfree: as you should :)
18:54:00 <sproingie> syntaxfree: that's the point of FP
18:54:01 <Korollary> Cale: As much as you don't try to decompose problems using OO in haskell.
18:54:06 <int-e> Lazy evaluation is great if your algorithm requires calculating values on demand and then cache the result. Or if you need coroutines with producer/consumer relationships.
18:54:06 <syntaxfree> And while on one hand I feel that frees me from the burden of programming and lets me just state what I want, on the other hand it gives me anguish.
18:54:18 <syntaxfree> I don't know when my program is gonna memoize stuff.
18:54:36 <Cale> Korollary: I'm saying that laziness gives you more ways to break up a problem than strict evaluation does.
18:54:59 <Cale> It also increases the compositionality of code.
18:55:25 <syntaxfree> Scheme has given me problems with inadvertently gone infinite, yes.
18:55:45 <Cale> If I have a function which produces the tails of a string, and a have a function which checks if one string is a prefix of another, I have a substring checker. I can reuse my other code.
18:55:58 <int-e> In strict languages, both usually require quite a lot of boilerplate code to work; with lazy evaluation the required machinery is part of the language's runtime system and compiler.
18:56:18 <syntaxfree> The thing is, I don't understand what's going behind the scenes!
18:56:27 <Cale> syntaxfree: that's easy enough to learn
18:56:38 <Cale> It's just that the evaluation order is slightly different.
18:56:39 <Korollary> Cale: I don't see why that is not possible in a strict fashion
18:56:49 <syntaxfree> I feel I'm being alienated from the actual programming, which is liberating on one hand, but scary on the other hand.
18:57:00 <Cale> Korollary: you can do it, but computing all the tails first is a stupid thing to do
18:57:03 <sproingie> Korollary: it's always possible.  it's a matter of what's more expressive and elegant
18:57:12 <Cale> and reasonably efficient
18:57:29 <mjl69_> I am just realizing that there is no storage, just recursive functions simulating storage.
18:57:40 <Cale> in this case, computing all the tails isn't a reasonably efficient thing to do
18:57:51 <SamB> mjl69_: are you certain it is not the reverse?
18:58:02 <syntaxfree> the glimpses of the algorithmic problems I've seen make me scared.
18:58:07 <Cale> In a lazy language, that's only ever going to consume constant space, and linear time.
18:58:22 <mjl69_> SamB: no
18:58:25 <Korollary> Cale: How often is the entire list already computed?
18:58:26 <syntaxfree> I learned the difference between tail recursion and stack-accumulating recursion.
18:58:28 <Cale> In a strict language, quadratic space.
18:58:31 <Saulzar> The easy one to see benefits for me is safety, you can't have a pure language without being lazy
18:58:42 <sproingie> syntaxfree: tail calls are goto in disguise :)
18:58:44 <syntaxfree> It gives me great trouble that such a HUGE algorithm difference is so subtly hidden away.
18:58:44 <Cale> Korollary: hm?
18:58:54 <SamB> Saulzar: well, you could...
18:59:06 <SamB> Saulzar: but it might not be very useful
18:59:21 <Saulzar> True
18:59:22 <sproingie> syntaxfree: tho thinking in terms of goto or control flow in haskell is usually not what you want todo
18:59:23 <syntaxfree> With Haskell, I feel like I'm talking to one of those stupid sci-fi computers that understand human language.
18:59:32 <syntaxfree> I'm just being declarative.
18:59:32 <SamB> hahah
18:59:39 <Cale> isSubstringOf x y = any (isPrefixOf x) (tails y) -- this essentially carries out the nested loops approach to solving the problem in an imperative language
18:59:59 * sproingie isn't sure he'd call it human language
19:00:10 <SamB> you can't say "computer, blah blah blah this"
19:00:11 <Cale> with early breaks
19:00:11 <syntaxfree> @pl isSubstringOf x y = any (isPrefixOf x) (tails y)
19:00:12 <lambdabot> isSubstringOf = (. tails) . any . isPrefixOf
19:00:32 <syntaxfree> I say, computer, give me the integers that are not even and not divisible by three.
19:00:36 <sproingie> human language is typically imperative.  we have a sense of linear time, so we organize our thoughts accordingly
19:00:48 <Cale> sproingie: no, it's higher order
19:00:57 <sproingie> when we don't, we get legalese and specifications ... no surprise those are often very hard to read
19:01:11 <SamB> sproingie: thats only when we are describing happenings
19:01:17 <int-e> we also have the habit of splitting problems into subproblems that can be solved in any order
19:01:18 <syntaxfree> > let nothrees = (let evens = [1,3..] in filter (\x->x `mod` 3 /=0) evens) in take 10 nothrees
19:01:19 <lambdabot> [1,5,7,11,13,17,19,23,25,29]
19:01:30 <Korollary> sproingie: umm, human language is beyond simple classifications imho
19:01:37 <Cale> sproingie: when you go to wash the dishes, you don't number your dishes and iterate over them, you just wash all the dishes.
19:01:43 <SamB> even then, there is a great deal of parralism and associated uncertainty about ordering
19:01:54 <syntaxfree> let nodiv x = let evens = [1,3..] in filter (\x->x `mod` x /=0) evens) in take 10 nothrees 3
19:01:56 <mjl69_> functional programming is solid state, no moving parts.
19:01:59 <Saulzar> Heh, funny that you should say Haskell is like a human language - the biggest complaint seems to be "But humans think imperitively"
19:02:07 <int-e> mjl69_: yes. everything is a constant
19:02:15 <syntaxfree> > let nodiv x = let evens = [1,3..] in filter (\x->x `mod` x /=0) evens) in take 10 nothrees 3
19:02:15 <lambdabot>  parse error on input `)'
19:02:18 <Cale> It's a lot less like for (i = 0; i < numDishes(); i++) { wash(dish[i]) }, and a lot more like map wash dishes
19:02:19 <Korollary> Saulzar: humans express a lot of procedures imperatively
19:02:25 <SamB> 2600 programming is practically the exact opposite
19:02:32 <syntaxfree> let nodiv x = (let evens = [1,3..] in filter (\x->x `mod` x /=0) evens) in take 10 nothrees 3
19:02:41 <sproingie> Cale: no, but i also don't view the dishes as being in an already washed state as a function of the application of dishwashing liquid to the state of the kitchen that includes the dishes
19:02:41 <syntaxfree> grrr..
19:02:47 <syntaxfree> > let nodiv x = (let evens = [1,3..] in filter (\x->x `mod` x /=0) evens) in take 10 nothrees 3
19:02:48 <lambdabot>  Not in scope: `nothrees'
19:02:54 <int-e> mjl69_: the trick is to only evaluate those constants that are actually needed to find (and execute) the 'value' of the main constant. That's all lazy evaluation is about.
19:02:58 <Saulzar> Hmm, I guess that's it - procedures vs. functions
19:03:00 <sproingie> Cale: it's obviously a continuoumm
19:03:08 <sproingie> continuum... however the hell you spell it
19:03:10 <int-e> syntaxfree: 1 is not even!
19:03:17 <Cale> or perhaps mapM wash dishes
19:03:26 <syntaxfree> > let nodiv x set = filter (\x->x `mod` x /=0) set in take 10 nodiv 3 [1..] 
19:03:27 <lambdabot>   The function `take' is applied to four arguments,
19:03:27 <lambdabot>   but its type `Int -> [a] -> [a]' has only two
19:03:27 <lambdabot>   In the definition of `ufb':
19:03:42 <syntaxfree> > let nodiv x set = filter (\y->y `mod` x /=0) set in take 10 nodiv 3 [1..] 
19:03:43 <lambdabot>   The function `take' is applied to four arguments,
19:03:43 <lambdabot>   but its type `Int -> [a] -> [a]' has only two
19:03:43 <lambdabot>   In the definition of `egm':
19:03:46 <Cale> nevertheless, you tend not to think about the mechanics of 'all' in washing 'all' the dishes
19:03:53 <SamB> > let nodiv x set = filter (\y->y `mod` x /=0) set in take 10 $ nodiv 3 [1..] 
19:03:54 <lambdabot> [1,2,4,5,7,8,10,11,13,14]
19:03:54 <mjl69_> the human brain works like the current state-of-the-art technology.
19:04:16 <Cale> Letting an abstraction take care of it for you is closer than writing those mechanics yourself.
19:04:19 <sproingie> actually it's just "wash dishes".  we generalize effectively
19:04:25 <SamB> mjl69_: I'm fairly certain that is not true
19:04:26 <sproingie> we have really good code generators
19:04:55 <syntaxfree> > let nodiv x set = filter (\y->y `mod` x /=0) set; primesUpto x 0 = x; primesUpto x n = nodiv (n-1) x
19:04:56 <lambdabot>  parse error on input `}'
19:04:59 <Korollary> Cale: What you're describing is a fold. However, we do iterate as well. In fact, we generate code on the fly: "ow, I just broke this dish. Outta loop."
19:05:00 <SamB> either that, or the state of the art is a well-kept secret
19:05:05 <int-e> The human brain really lacks introspection, otherwise computers would be much smarter. In my experience we can't really explain how we think.
19:05:09 <sproingie> studying people with brain damage gives good insights into how we think
19:05:18 <Cale> Korollary: I'm not describing a fold
19:05:19 <syntaxfree>  > let nodiv x set = filter (\y->y `mod` x /=0) set; let primesUpto x 0 = x; let primesUpto x n = nodiv (n-1) x in primesUpto 10
19:05:24 <mjl69_> SamB: just look at historical descriptions of how the brain works in relation to the current technology.
19:05:31 <sproingie> you can reverse engineer a lot of things with defect analysis
19:05:36 <Korollary> Cale: or a map.
19:05:43 <SamB> sproingie: kinda like watching damaged movie files gives insight into the compression techniques?
19:06:40 <sproingie> SamB: yep, lot of stuff is cracked by introducing defects.  smart cards for example
19:06:48 <syntaxfree> People think recursively, for one.
19:06:49 <SamB> e.g., its a lot easier to see how delta frames/motion compensation work that way...
19:07:01 <Cale> Korollary: yeah, in general, there are a bunch of common patterns which we carry out without thinking too much. In programming, we obviously have to choose one, but making the choice easy is better than forcing the programmer to think about the mechanics of that pattern every time.
19:07:01 <sproingie> syntaxfree: yes, but we have a pretty short stack
19:07:02 <SamB> syntaxfree: but not terribly recursively
19:07:13 <syntaxfree> sproingie: we use paper.
19:07:20 <syntaxfree> we use paper to carry out recursion.
19:07:27 <SamB> or at least, not in the same kind of thaught
19:07:31 <mjl69_> fp good for distributed computing, right?
19:07:36 <syntaxfree> to store stack, anyway.
19:08:08 <syntaxfree> > let nodiv x set = filter (\y->y `mod` x /=0) set; let primesUpto x 0 = x; let primesUpto x n = nodiv (n-1) x in primesUpto 10 [1..3]
19:08:08 <lambdabot>  parse error (possibly incorrect indentation)
19:08:16 <sproingie> mjl69_: evaluation order doesn't matter in pure functions, so those distribute nicely
19:08:21 <syntaxfree> > let nodiv x set = filter (\y->y `mod` x /=0) set; let primesUpto x 0 = x; let primesUpto x n = nodiv (n-1) x in primesUpto 10 [1..]
19:08:21 <lambdabot>  parse error (possibly incorrect indentation)
19:08:28 <Korollary> Cale: That's right. Iteration is a much powerful pattern than a map, thus it can be more tedius if you use it to emulate a map.
19:08:35 <syntaxfree> help?
19:08:36 <sproingie> mjl69_: once you start sequencing things with monads, it's about the same problem parallizing it as any other
19:09:56 * SamB is trying to figure out how Thrust works (the 2600 version, to be specific)
19:09:59 <syntaxfree> off-the-wall question::
19:10:14 <syntaxfree> why is it said that Java took C programmers half-way to Lisp?
19:10:18 <SamB> and let me tell you, my brain does not work like a 2600 game ;-)
19:10:40 <audreyt> any cabal hackers around? Debolaz ran into some problems on pkgsrc
19:10:54 <SamB> syntaxfree: it has memory management and a lot of restrictions but not a decent object or macro system
19:11:03 <int-e> syntaxfree: that's a strange metric (for distances between languages)
19:11:15 <sproingie> syntaxfree: one of the creators of java said that
19:11:26 <Korollary> syntaxfree: also, lisp folk would say that any other language contains a poor implementation of half of lisp.
19:12:00 <sproingie> Korollary: greenspun.  it's also ridiculously smug nonsense
19:12:05 <Pupeno> Korollary: in fact, he said that any significantly big program written in some other languages contains that implementation of Lisp.
19:12:19 <Korollary> Pupeno: that was it.
19:13:34 <Korollary> sproingie: it's fun hiding behind confusing, exotic languages and talk $hit about others ;)
19:14:31 <Korollary> if anybody listens, we should tell them that any programs written in any language contains a poor implementation of half of the monadic framework of haskell
19:14:38 <sproingie> Korollary: even more fun in #python or #ruby etc where one ordinary and uninteresting language slags off the other
19:16:18 <Korollary> sproingie: there should be a channel named #fad
19:16:37 <sproingie> Korollary: i believe it's called #rubyonrails
19:16:49 <Pupeno> Tring to use NewBinary... do I do data Message = Message { id :: Word16 }; instance Binary Message where put_ bh msg = return (); get bh = return (Message 16) (of course replacing put_ and get by real implementations).
19:16:54 <Korollary> sproingie: there's #ruby and #rubyonrails at the same time?
19:17:06 <syntaxfree> Ruby is massively OO language from what I got of it.
19:17:12 <sproingie> Korollary: yep.  half the folks in #rubyonrails don't even know ruby
19:17:13 <Korollary> damn, there are
19:17:16 <syntaxfree> ruby on rails is a system that automatically generates code.
19:17:27 <Pupeno> Korollary: why not ? there's #c and #apache.
19:17:34 <Korollary> Pupeno: good one.
19:17:48 <sproingie> syntaxfree: it generates really uninteresting crud screens that are useless for real work.  it fails to generate decent security, caching, or i18n
19:18:26 <Pupeno> Now, how do I turn a Ptr into a BinaryHandle ?
19:19:12 <sproingie> syntaxfree: it does have a clever template engine to weave ajax stuff into crud screens.  not sure that constitutes a whole framework.
19:21:17 <sproingie> haskell doesn't quite aim to take over the "agile web dev" space tho
19:21:28 <sproingie> haskelldb and haxml could really be a killer combo tho
19:21:35 <syntaxfree> Scheme has real potential on agiel web dev, I feel.
19:21:54 <syntaxfree> maybe it's just because the parenthetical syntax maps real well on XML syntax.
19:22:25 <tennin> I've wondered how Greenspun's notion could be formalized ;)
19:22:40 <syntaxfree> (html (head (css css.. )) (body (ul (li bullet) (li bullet) (li bullet))))
19:22:43 <sproingie> syntaxfree: long as they don't start and end at "continuation based framework" .  deep linking into an application's running state just isn't interesting in the large
19:22:55 <sproingie> and talk about making the client responsible for somethign it shouldn't
19:23:25 <Korollary> I'd be very surprised if scheme got any more useful after this point.
19:24:06 <sproingie> scheme has always been useful
19:24:17 <Korollary> ''more''
19:24:38 <syntaxfree> if anything, Scheme is the perfect first language to teach
19:24:40 <syntaxfree> .
19:25:11 <Korollary> I'm not so sure about that
19:25:12 <sproingie> r6rs isn't exactly going to rip open new frontiers, no.  but the SRFI's are still quite active
20:43:15 <glasser> ok, so I'm reading the Paterson arrows paper
20:43:21 <glasser> and I get to the following definition:
20:43:32 <glasser> er
20:43:37 <glasser> trace :: ((b,d) -> (g,d)) -> b -> g
20:43:37 <glasser> trace f b = let (c,d) = f (b,d) in c
20:44:37 <glasser> so i'm a little confused... how does this compile? the 'd' in f (b,d) doesn't seem to be "declared" anywhere except as a result of pattern matching against its own result
20:45:58 <Saulzar> It is recursive
20:46:50 <glasser> so what exactly does that "do"?
20:47:01 <glasser> I tried it out against swapInts :: (Int, Int) -> (Int, Int)
20:47:08 <glasser> with swapInts (x,y) = (y,x)
20:47:31 <glasser> and then "trace swapInts 6" -> 6.  But I don't really get why
20:48:32 <Saulzar> Hmm.
20:49:49 <Saulzar> It's because of lazy evaluation
20:49:54 <int-e> glasser: you have a recursive let binding there
20:51:15 <Saulzar> Er
20:51:18 <int-e> glasser: the d is defined, by the (c, d) = f (b, d) binding. computationally, you can imagine that the implementation builds a placeholder for an unevaluated value for d and passes it to f.
20:51:47 * int-e thinks recursive let bindings are something different, let a=b; b=a in ... but couldn't think of a better term.
20:53:04 <glasser> Does this end up "calling" (for some definition of calling) f more than once?
20:53:09 <glasser> like basically searching for a fixpoint?
20:53:13 <int-e> yes
20:53:29 <glasser> ah
20:53:59 <glasser> so the function has to be something where haskell can "figure out" the fixpoint?
20:54:00 <Korollary> you have to read it backwards. is d even needed?
20:54:00 <int-e> fix f = let x = f x in x  is actually very similar.
20:54:08 <Saulzar> I think it would "call" swapInt twice for that example, right?
20:54:21 <glasser> (my next try was: 
20:54:22 <glasser> trace :: ((b,d) -> (g,d)) -> b -> g
20:54:22 <glasser> trace f b = let (c,d) = f (b,d) in c
20:54:25 <glasser> er
20:54:29 <glasser> sorry
20:54:30 <glasser> data M = A | B
20:54:30 <glasser>   deriving Show
20:54:30 <glasser> conditional :: (Int, M) -> (Int, M)
20:54:30 <glasser> conditional (x, A) = (10*x, A)
20:54:32 <glasser> conditional (x, B) = (30*x, B)
20:54:44 <glasser> and then "trace conditional 5" loops
20:55:06 <glasser> Oh, right, there is fix! I guess I never thought about how it owrks
20:55:38 <int-e> for fix it's quite easy: whenever f uses x, it will 'call' itself recursively (by the definition of x)
20:56:13 <glasser> I suppose that the set of functions where trace or fix can "figure out" the answer is much less than the set of functions that actually have a fix or whatever
20:56:35 <int-e> of course.
20:56:36 <glasser> int-e: ok... and then as soon as it finds x = f x it bottoms out? no, that doesn't make sense
20:57:16 <int-e> > let { fix f = let x = f x in x } in fix (*1)
20:57:17 <lambdabot> Exception: <<loop>>
20:57:28 <glasser> What is *1?
20:57:32 <glasser> oh, never mind :)
20:57:36 <int-e> a function that multiplies by 1 :)
20:58:02 <int-e> > let { fix f = let x = f x in x } in take 10 $ fix (1:)
20:58:04 <lambdabot> [1,1,1,1,1,1,1,1,1,1]
20:58:06 <glasser> hmm, why can't it figure this one out? I guess that in order to do that it would have to make some sort of "guess"
20:58:33 <int-e> glasser: because you can't find a fixpoint of 1 as a product ...*1*1*1*1
20:58:48 <glasser> is the distinction between those two cases basically that the definition of (:) is more "axiomized" in haskell than arithmetic?
20:58:52 <int-e> glasser: there's no starting point; it's an infinite recursion.
20:59:17 <int-e> glasser: the distinction is that you can figure out the head of (1:x) without knowing x.
20:59:32 <glasser> ah, ok.  So the things that haskell can find the fixpoint for are the data that you can manipulate in a non-strict way?
21:00:18 <Cale> > let within eps (x:y:xs) = if abs (x/y - 1) < eps then y else within eps (y:xs) in within 0.0001 . iterate (*1) $ 1
21:00:19 <lambdabot> 1.0
21:01:22 <glasser> Cale: hmm, but you don't even need the approximation stuff there, right?  The part that makes that work is the 1 at the end I think
21:01:25 <Cale> > let within eps (x:y:xs) = if abs (x/y - 1) < eps then y else within eps (y:xs) in within 0.0001 . iterate cos $ 0.5
21:01:26 <lambdabot> 0.7391090814205267
21:02:10 <glasser> > let done (x:y:xs) = if x == y then y else done (y:xs) in done . iterate (*1) $ 1
21:02:11 <lambdabot> 1
21:02:18 <Cale> well, yeah
21:02:23 <glasser> > let done (x:y:xs) = if x == y then y else done (y:xs) in done . iterate cos $ 0.5
21:02:24 <lambdabot> 0.7390851332151607
21:02:37 <glasser> OK, I was expecting the second one to fail :)
21:02:43 <SamB> @pl snd (until (\x -> fst x > n) (\(i,m) -> (i+1, i*m)) (1,1))
21:02:43 <lambdabot> snd (until ((> n) . fst) (uncurry (ap ((.) . (,) . (1 +)) (*))) (1, 1))
21:02:53 <Cale> well, Doubles are really discrete, so it'll eventually converge :)
21:02:58 <SamB> @pl snd (until ((>n) . fst) (\(i,m) -> (i+1, i*m)) (1,1))
21:02:59 <lambdabot> snd (until ((> n) . fst) (uncurry (ap ((.) . (,) . (1 +)) (*))) (1, 1))
21:03:11 <Cale> (but not necessarily, if you hit numerical problems)
21:04:32 <int-e> glasser: nonstrictness plays an essential part in making fix and trace work, right.
21:04:34 <Cale> > let y f = f (y f) in y (\fac x -> if x == 0 then 1 else x * fac (x-1)) 12
21:04:34 <lambdabot> 479001600
21:05:17 <Cale> > product [1..12]
21:05:18 <lambdabot> 479001600
21:08:24 <int-e> @djinn ((b,d) -> (g,d)) -> b -> g
21:08:25 <lambdabot> -- f cannot be realized.
21:08:50 <Cale> representing iterative numerical algorithms with lazy lists is really effective
21:09:23 <Cale> You can add all sorts of early termination conditions and improvements after the fact.
21:11:10 <Cale> and keep them out of the initial code which just produces an infinite list of progressively 'better' results
21:12:40 <shapr> Good morning #haskell!
21:12:52 <Saulzar> The y f function makes my head hurt
21:12:57 <Cale> morning :)
21:13:31 <shapr> Y is that?
21:14:13 <Cale> Saulzar: well, staring at the definition: y f = f (y f), treat this as an equation. It says that applying f to (y f) results in y f
21:14:30 <Cale> that is, y f is a fixed point of f
21:14:35 <Saulzar> I imagine that it is because I can't think what the type of y is
21:14:35 <SamB> hmm.
21:14:44 <SamB> aren't you supposed to say
21:14:59 <Cale> y :: (t -> t) -> t
21:15:15 <SamB> y f = x where x = f x
21:15:21 <int-e> Saulzar: y is really just fix in disguise. if you give y f a name (say, x), you get:  y f = let x = f (y f) in x   or,  y f = let x = f x in x.
21:15:22 <Cale> SamB: why?
21:15:38 <SamB> in case the compiler is an idiot?
21:15:46 <Cale> meh
21:16:13 <SamB> also, to emphasize that you are passing in the result of the function?
21:16:15 <Cale> it works well enough :)
21:16:28 <int-e> if the compiler is bad, optimizing y won't help you much :)
21:17:10 <Cale> but that's also a good way to write it, because the equation defining 'fixed point' is right there
21:17:12 <SamB> int-e: its not just an optimization, afaik
21:17:36 <Cale> > let y f = f (y f) in y (\fac x -> if x == 0 then 1 else x * fac (x-1)) 12
21:17:37 <lambdabot> 479001600
21:17:40 <Cale> it works :)
21:17:49 <Cale> > let y f = f (y f) in y (\fac x -> if x == 0 then 1 else x * fac (x-1)) 100
21:17:50 <lambdabot> 933262154439441526816992388562667004907159682643816214685929638952175999932
21:17:50 <lambdabot> 299156089414639761565182862536979208272237582511852109168640000000000000000
21:17:50 <lambdabot> 00000000
21:18:01 <int-e> SamB: yes it is, you can get your definition from Cale's by common subexpression elimination.
21:18:14 <SamB> int-e: I suppose so...
21:18:29 <SamB> but I still think mine brings you immediately to the point
21:18:48 <SamB> rather than leave your brain running around in circles
21:18:48 <int-e> I'd expect the resulting code to be the same.
21:19:22 <Cale> I just read y f = f (y f) equationally, and expect it to work :)
21:19:29 <Cale> (and look, it does :)
21:19:36 * SamB waits for lame pun involving alternative definitions of the word "point"
21:19:59 * int-e is too dizzy from running around in circles.
21:20:10 <SamB> int-e: see!
21:20:37 <SamB> my code prevents that, because it does not require CSE but only redex preservation
21:20:51 <int-e> But anyone who has seen lambda calculus and its definition of a fixed point combinator will find Cale's definition quite natural.
21:28:28 <Saulzar> Hmm, I must be missing something fundimental, if y :: (t -> t) -> t  and f :: (Integer -> Integer) -> Integer -> Integer,  why is it possible that y f :: Integer -> Integer    -- f does not look like it should match t -> t ?!
21:29:06 * shapr hops
21:29:08 <int-e> -> is right-associative.
21:29:29 <int-e> i.e. (a -> a) -> a -> a is actually ((a -> a) -> (a -> a))
21:29:56 <int-e> or, t -> t  with t = a -> a.
21:33:38 <Saulzar> Oh crikey, I'm a moron. One can only take so much currying it seems :)
21:37:54 <shapr> gih, I want the Yi version of Gnus.
21:38:37 <shapr> @seen ndm
21:38:38 <lambdabot> I saw ndm leaving #haskell-overflow, #haskell-blah and #haskell 8 days,
21:38:38 <lambdabot> 11 hours, 45 minutes and 10 seconds ago, and I have missed 7 days, 8
21:38:38 <lambdabot> hours, 10 minutes and 42 seconds since then.
21:43:13 <palomer> gah
21:43:18 <palomer> could someone paste the last thing someone said to me
21:43:33 <palomer> I keep missing messages from this channel
21:46:26 <shapr> hiya dominicand, learning Haskell?
21:47:28 <dominicand> well
21:47:32 <dominicand> reaeding about it on wp
21:47:42 <shapr> Do you have any questions?
21:47:56 <shapr> I could give you a short intro if you like.
21:48:05 <dominicand> hehe i am bad at programming
21:48:14 <shapr> We all start at the beginning :-)
21:48:21 <palomer> yeah
21:48:28 <palomer> even the great Bill Gates
21:48:46 <palomer> may his glory be only eclipsed by his intelligence
21:49:05 <dominicand> hehe
21:49:37 <dominicand> i need to go to sleep.. but what is the advantage of haskell?
21:50:03 <Korollary> endless discussions
21:50:48 <shapr> dominicand: It's a simple powerful language with few moving pieces and big ideas.
21:51:08 <shapr> > map (+1) [1..5]
21:51:10 <lambdabot> [2,3,4,5,6]
21:51:19 <shapr> foldr1 (+) [1..5]
21:51:23 <shapr> > foldr1 (+) [1..5]
21:51:25 <lambdabot> 15
21:51:32 <shapr> > scanr1 (+) [1..5]
21:51:32 <Korollary> the motivation seems to be being able to think about what a program does without having to deal with how it does it.
21:51:33 <lambdabot> [15,14,12,9,5]
21:51:43 <shapr> Korollary: I think that's an excellent summary.
21:52:08 <dominicand> shapr: i will be back tomorow, thanks 
21:52:18 <shapr> Haskellers and functional programmers want to be able to analyse a whole program without executing it.
21:52:23 <shapr> dominicand: Ok, have a nice day.
21:52:45 <shapr> Korollary: I have many frustrations with Plone/Python because it doesn't lend itself to that.
21:52:53 <Korollary> I drew UML diagrams for hours today, bah.
21:53:23 <shapr> Can you generate UML from your code?
21:53:34 <shapr> Or can you generate UML from combinators?
21:53:38 <tennin> frustration with Python over that issue was what drove me to learn Haskell
21:53:41 <Korollary> no, I did it manually. I am reverse engineering our own C++ code heh
21:54:35 <shapr> tennin: How do you like Haskell so far?
21:54:35 <Korollary> tennin: well, python doesn't have types, so I have to read either the code or the comments :)
21:56:03 <tennin> I like it very much, but it's taking a very long time for me to digest
21:56:34 <Korollary> tennin: do you have an actual book?
21:56:42 <Korollary> on haskell, that is
21:57:40 <Korollary> ah, I was going to read Wadler tearing apart SICP...
21:57:58 <tennin> no --- I started with the Gentle Introduction and then read the nomaware monads tutorial
21:59:06 <shapr> Wow
21:59:17 <shapr> Korollary: Wadler makes some excellent points.
21:59:31 <Korollary> shapr: yeah, I read 6-7 pages today.
22:00:26 <tennin> I'm also reading introductory books on category theory and denotational semantics
22:00:50 <Korollary> tennin: welcome to overload country
22:01:06 <Korollary> tennin: do you know operational semantics?
22:01:28 <tennin> only vaguely
22:01:48 <glasser> Korollary: where is that Wadler paper?
22:02:03 <Korollary> glasser: http://www.cs.kent.ac.uk/people/staff/dat/miranda/wadler87.pdf
22:03:26 <tennin> but, I started all this several months ago, have been studying fairly intensively but still feel like I've barely made any headway
22:04:33 <shapr> Headway is deceptive, because you see how much is left to learn.
22:05:10 <shapr> I make notes in the margins of the books I read, with questions, thoughts, etc. Yesterday I found a bunch of notes from a few months ago that showed how much less I understood then.
22:05:21 <Korollary> tennin: maybe you're too parallelized. try one subject at a time.
22:06:18 <shapr> Or just keep notes so your progress is a first class value too :-)
22:06:36 <shapr> I try to keep some progress values in my blog for later perusal.
22:06:40 <Korollary> I'd know nothing without my notes
22:11:44 <tennin> well, I like to at least study theory in one thread and work on some (quasi-)practical project in another
22:16:09 <Pupeno> Hello. How do I turn a Ptr into a BinaryHandle ?
22:18:14 <Korollary> @index BinaryHandle
22:18:14 <lambdabot> bzzt
22:18:29 <Korollary> where's BinaryHandle from ?
22:19:13 <Pupeno> well, actuaally, it is BinHandle, it is a type of NewBinary.
22:19:58 <Pupeno> @type BinHandle
22:19:59 <lambdabot> Not in scope: data constructor `BinHandle'
22:20:14 <Pupeno> @type BinMem
22:20:15 <lambdabot> Not in scope: data constructor `BinMem'
22:20:16 <Lemmih> Where did you get the 'Ptr CChar'?
22:20:46 <Pupeno> Lemmih: from a socket (UDP).
22:21:25 <Pupeno> I am not sure Ptr of what type, but I can re-cast it if necesarly, I know the total size.
22:32:27 <palomer> quick, someone say something to me
22:36:50 <Korollary> palomer: Hello
22:37:08 <palomer> gah, doesn't work
22:38:56 <Pupeno> what ?
22:39:27 <palomer> nothing
22:40:19 <Cale> hm?
22:40:26 <palomer> hm!
22:41:38 <Pupeno> boo.
22:48:27 <Pupeno> hurra! I have just unserialized the firt word16 of a DNS packet properly using NewBinary! :)
22:49:20 <palomer> yay!
22:49:27 <palomer> who let the dogs out?
22:50:28 <Pupeno> @type BinPtr
22:50:29 <lambdabot> Not in scope: data constructor `BinPtr'
22:50:49 <palomer> cool, it DOES work
22:50:51 <palomer> only very slowly
22:51:23 <Cale> palomer: what are you working on?
22:55:47 <palomer> oh
22:55:55 <palomer> I loaded a script that sends highligts to a separate tab
22:56:00 <palomer> that way I won't miss any more highlights
22:56:04 <palomer> yeah baby
22:56:07 <Cale> XChat?
22:56:11 <palomer> yeah
22:56:18 <Cale> cool, what script?
22:56:37 <palomer> http://scripts.xchat.org/cgi-bin/search?str=highlight&cat=0&Submit=Search
22:56:46 <palomer> Highlight Logger
22:56:55 <Korollary> I wrote a custom python script for xchat once. I remember how the api sucked.
22:57:15 <Korollary> well, undocumented
22:57:42 <Cale> well, it's more reasonable than writing an mIRC Script :)
22:57:49 <Cale> (then again, most things are)
22:57:50 <Korollary> probably
22:58:30 <shapr> Maybe lambdabot needs a gtk2hs plugin?
22:58:55 <Korollary> to display stuff on a computer in .au?
22:59:14 <shapr> Oh hey, I need to poke people about TMR.
22:59:20 <Korollary> @draw figure 8
22:59:20 <lambdabot> Unknown command, try @listcommands.
22:59:27 <shapr> Korollary: The idea being that you could use lambdabot as a client :-P
22:59:31 <Cale> nah, just grab the person's IP and assume their X server is running on the machine they're IRCing from
22:59:41 <Lemmih> SamB: Ping.
22:59:47 <Korollary> shapr: but lambdabot is more than a client.
22:59:48 <Cale> (and is insecure)
22:59:52 <Cale> hehe
22:59:55 <shapr> Korollary: and so are most irc clients.
23:00:07 <Korollary> ugh
23:00:21 <shapr> We can call it... lambdamacs.
23:00:31 <Korollary> oh geez. that reminds me of...
23:01:36 <Korollary> http://www.xwem.org
23:01:46 <Korollary> sheer madness
23:02:27 <Cale> haha, that's awesome
23:05:28 <Pupeno> having a Word16, how do I get from bit 1 to 4 ? oring to a mask and right shifting 11 places ?
23:05:37 <Pupeno> s/oring/anding/
23:05:43 <palomer> emacs is evil
23:05:54 <palomer> when are we going to come out with haskell's answer to squeak?
23:06:09 <shapr> What about pivotal?
23:06:41 <palomer> is that really our answer to squeak?
23:06:46 <shapr> Have you seen it?
23:06:50 <syntaxfree> I thought Squeak = new version of Smalltalk.
23:07:56 <palomer> in squeak I can click on objects, change them, have them interact with other objects, etc...
23:08:01 <palomer> pieces of code were objects, etc...
23:08:19 <Cale> palomer: have you seen vital?
23:08:29 <Cale> oh
23:08:47 <Cale> well, it's a start anyway :)
23:08:52 <syntaxfree> Pivotal looks like Haskell's answer to Maple and Mathematica.
23:08:52 <syntaxfree> sp. Maple..
23:09:17 <syntaxfree> later Maple is a combination of document publishing system and computer algebra system.
23:09:44 <Cale> Yeah, without the math part :)
23:10:10 <syntaxfree> well, it's to Haskell what  Maple is to Macsyma or Yacas.
23:10:15 * palomer wishes that maple could produce code for any language I want 
23:10:40 <Cale> I suppose it's to Haskell what xmaple is to maple :)
23:11:03 <Cale> palomer: well, it can produce C and then you can call that code via FFI
23:11:18 <Pupeno> is there a syntax to write number is radix 2 (aka binary) like 0xNNN for hexadecimal ?
23:11:32 <palomer> I mean, if I have a polynomial and I want the corresponding C representation
23:11:34 <Cale> > 0b0111010
23:11:35 <lambdabot>  Not in scope: `b0111010'
23:11:39 <Cale> > 0o0111010
23:11:40 <lambdabot> 37384
23:11:45 <Cale> that's octal
23:12:07 * Cale looks at the Haskell syntax reference
23:12:14 <Cale> nope
23:12:21 * palomer can't wait for sablecc 4.0
23:12:21 <Pupeno> outch, ok.
23:12:21 <Cale> just octal and hex
23:12:26 <palomer> finally a parser that's usable!
23:12:48 <syntaxfree> > 0xFF
23:12:49 <lambdabot> 255
23:13:17 <sieni> >0xDEADBEEF
23:13:26 <sieni> > 0xDEADBEEF
23:13:27 <lambdabot> 3735928559
23:14:13 <syntaxfree> > 0xABCDEF
23:14:14 <lambdabot> 11259375
23:20:24 <palomer> @hoogle String -> Num
23:20:25 <lambdabot> Prelude.error :: String -> a
23:20:25 <lambdabot> Graphics.UI.ObjectIO.CommonDef.dummy :: String -> x
23:20:35 <palomer> what, no parseString?
23:20:38 <palomer> @hoogle String -> Int
23:20:39 <lambdabot> Text.Regex.Posix.regExtended :: Int
23:20:39 <lambdabot> Text.Regex.Posix.regIgnoreCase :: Int
23:20:39 <lambdabot> Text.Regex.Posix.regNewline :: Int
23:20:48 <palomer> @hoogle String -> Integer
23:20:49 <lambdabot> System.CPUTime.cpuTimePrecision :: Integer
23:20:49 <lambdabot> Prelude.error :: String -> a
23:20:49 <lambdabot> Prelude.read :: Read a => String -> a
23:21:35 <palomer> hoogle should returns all functions which are sub types and super types of the one asked
23:21:58 <palomer> @hoogle String -> Double
23:21:58 <lambdabot> Prelude.error :: String -> a
23:21:58 <lambdabot> Prelude.read :: Read a => String -> a
23:21:58 <lambdabot> Text.Read.read :: Read a => String -> a
23:22:16 <Cale> Num isn't a type, by the way
23:22:26 <palomer> no?
23:22:30 <Cale> It's a class
23:22:36 <palomer> is there really a difference?
23:22:39 <Cale> yes
23:22:53 <palomer> > 2 :: Num
23:22:53 <lambdabot>   Class `Num' used as a type
23:22:53 <lambdabot>   In the type `Num'
23:22:53 <lambdabot>   In an expression type signature: Num
23:22:56 <Cale> Classes are like predicates which types can satisfy
23:23:07 <Cale> > 2 :: (Num a => a)
23:23:08 <lambdabot> 2
23:23:32 <palomer> Num a => b -> a looks really similar to b -> Num to me
23:23:34 <Cale> @type 2
23:23:35 <lambdabot> forall t. (Num t) => t
23:24:00 <Cale> How would you write  (C a b) => a -> b
23:24:01 <Cale> ?
23:24:13 <palomer> good point
23:24:19 <palomer> very good point
23:24:38 <palomer> btw, can all these things be sorted out at compile time?
23:24:44 <Cale> yeah, they are
23:24:46 <palomer> is there ever any runtime type checking?
23:24:48 <Cale> no
23:24:51 <palomer> nice
23:26:06 <Cale> Well, there's Data.Typeable, which is a way to reify types into data structures. class Typeable a where typeOf :: a -> TypeRep
23:26:21 <palomer> whoa
23:26:26 <palomer> is that part of haskell98?
23:26:28 <Cale> no
23:26:50 <Cale> That's a GHC thing, possibly hugs too, but I haven't tried it
23:26:51 <Lemmih> It could easily be. There's no magic involved.
23:27:01 <palomer> btw, why do you dislike fail as part of Monad so much?
23:27:14 <Cale> palomer: because it's not part of the definition of a monad
23:27:16 <Cale> and it
23:27:25 <Cale> 's sort of ugly
23:27:28 <palomer> from a programming point of view
23:27:40 <palomer> have you ever thought "man I don't feel like implementing fail"
23:27:48 <Cale> From a programming point of view, it just doesn't make sense for most monads.
23:28:01 <Cale> Most of the time, it's impossible to implement fail
23:28:15 <palomer> do you think they'll remove it in the next standard?
23:28:16 <Cale> You just write fail = undefined
23:28:20 <Cale> I hope so
23:28:31 <Cale> I'd like to see MonadZero brought back
23:28:45 <Cale> which is like MonadPlus, without the plus :)
23:29:33 <Cale> and MonadPlus should be split into MonadPlus and MonadElse, which would be the same, except that instances could be expected to satisfy different laws.
23:29:59 <Cale> Maybe is a MonadElse, List is a MonadPlus
23:30:13 <palomer> does everyone in the community agree?
23:30:29 <Cale> no, but my views seem somewhat popular
23:31:03 <Cale> I'd also like to have monad comprehensions back, if only to reinforce the monads as containers view.
23:31:35 <palomer> why were they removed?
23:31:54 <Cale> removing them makes error messages simpler for newbies
23:32:10 <eivuokko> Huh?
23:32:13 <palomer> we should have "baby haskell"
23:32:17 <Cale> yeah, we should
23:32:29 <Cale> or leave that to Helium
23:33:10 <palomer> whoa, helium has readable type errors
23:33:15 <palomer> ghc should soooo get that!
23:34:11 <tennin> why were monad comprehensions removed?
23:34:39 <Cale> tennin: because new haskell users would get errors about monads when using them
23:34:52 <Cale> (as list comprehensions)
23:35:00 <palomer> gotta go!
23:35:01 <palomer> night!
23:35:05 <Cale> night
23:35:41 <Cale> Restricting them to lists makes the error messages simpler, but it seems like a sort of extreme thing to do.
23:36:46 <Cale> There's do-notation, which, together with guard, is pretty much isomorphic, but the comprehension syntax emphasizes the idea that monads are containers, which is good.
23:36:50 <Cale> (sometimes good)
23:41:11 <Cale> fail was added to the Monad class in order to handle pattern match failures in do-notation, but really, I think a better solution would be to make do-notation recognise MonadZero and MonadError and fail more gracefully just when a stronger class is guaranteed.
23:41:40 <eivuokko> uhm
23:41:48 <eivuokko> That makes little sense imo
23:41:53 <Cale> oh?
23:41:58 <eivuokko> (having fail for pattern match failures)
23:42:03 <Cale> yeah
23:42:17 <Cale> but that's the reason it's there
23:45:17 <eivuokko> Hmh.  You mean you'd it should use throwError or mzero instead?
23:45:34 <Cale> yes, just when those are guaranteed to be available
23:46:00 <Cale> otherwise, it would be a runtime pattern match error just like with the equivalent lambda
23:46:20 <eivuokko> Is there some good reason beyond laziness to not make pattern match an io-level exception?
23:46:46 <Pupeno> good night.
23:47:09 <Cale> well, sometimes it's nice, in a MonadZero, to have pattern matches be a funny sort of guard
23:47:44 <MarcWeber> I want to create a tuple (value, state) using the State Monad (return function). Using :m +Control.Monad.State and typing return 2 3::(Int, Int) should return  a tuple (a, State), shouldn't it? Can you show me the right way?
23:48:57 <Lemmih> 'return (2,3) :: State a (Int,Int)'
23:49:07 <Cale> runState (return 2) 3
23:49:07 <Cale> (2,3)
23:49:28 <Cale> @type Control.Monad.State.runState
23:49:29 <lambdabot> forall s a. Control.Monad.State.State s a -> s -> (a, s)
23:49:44 <Cale> > do { (x:xs) <- [[1,2,3],[],[4,5]]; return x }
23:49:45 <lambdabot> [1,4]
23:49:59 <sieni> argh, how does pattern matching work in a lambda expression
23:50:01 <sieni> ?
23:50:17 <Cale> > (\(x:xs) -> x * 10) [1,2,3,4,5]
23:50:18 <lambdabot> 10
23:50:23 <Cale> like that
23:50:37 <Cale> > (\(x:xs) -> x * 10) []
23:50:38 <lambdabot>  Non-exhaustive patterns in lambda
23:51:05 <Cale> > (\xs -> case xs of [] -> 0; (y:ys) -> y * 10) []
23:51:06 <lambdabot> 0
23:51:09 <Cale> > (\xs -> case xs of [] -> 0; (y:ys) -> y * 10) [1,2,3,4,5]
23:51:10 <lambdabot> 10
23:51:19 <sieni> ahh case
23:51:48 <sieni> thanks
23:51:53 <Cale> no problem
23:55:45 <C-Keen> moin.
23:55:58 <eivuokko> Cale, does that view about pattern match failures imply that it's a good idiom to use pattern match failures as a way to bail out, or did I understand something wrong?  It seems like bad idiom to me...
23:56:26 <Cale> eivuokko: well, it's handy sometimes
23:56:52 <Cale> I think in the case of lists and maybe, that is, proper MonadZero monads, it's fine.
23:56:59 <sieni> pattern matching failures sound eerily close to run time errors in dynamically typed languages
23:57:14 <Cale> They are runtime failures.
23:57:56 <Cale> But in the case of monads which naturally support a notion of failure, it seems more natural to let the monad deal with them.
23:58:09 <eivuokko> Maybe I just don't use monads enough, but in my code pattern match failures have been my (ie programmer) errors so far.
23:58:46 <Cale> eivuokko: well, in the case of do-notation, it's hard not to notice the possibility of a pattern match failure
23:58:59 <eivuokko> Yeah, I guee.
23:59:02 <Cale> since the way that they occur is by placing patterns on the left side of an  <-
23:59:08 <Cale> which is a strange thing to do
