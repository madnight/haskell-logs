00:34:01 <prologic> hi guys :)
00:44:27 <root> hello #haskell!
00:44:41 <root> omg! i am back.. give me a sec
00:45:28 <prologic> lol
00:45:35 <harsha> hi shapr 
00:45:37 <prologic> didn't people tell you not to irc as root :)
00:45:52 <harsha> prologic that was a mistake.. 
00:45:56 <prologic> yeh
00:47:41 <shapr> hi harsha 
00:48:29 <harsha> shapr sup?
00:49:10 <harsha> shapr i took a few days off from work.. had this really bad allergy in my eye :( .. feels much better now..
00:54:18 <harsha> user
01:00:58 <harsha> q
01:04:11 <harsha123> finally.. hi everyone.. (a formal one)
01:05:11 <harsha123> Pseudonym: Hello
01:09:25 <prologic> heh
01:09:29 <prologic> hello :)
01:10:21 <harsha123> prologic:  :) hi
01:57:25 <crazney> with IO, any function that does any IO (or calls any function that does IO) has to return some kind of IO () type yeah?
01:57:34 <ejt> y
01:57:49 <ejt> IO <something>
01:57:50 <crazney> ta
01:57:54 <crazney> yeah
02:58:57 <prologic> how would I write a function that returns all the elements in a list that occur more than once ?
03:04:39 <ejt> accumulate a set of seen members ?
03:04:40 <mudboy> prologic: I would sort the list, and have a recursive function that has an argument with the previous value, and the list.  If the previous value equals the head of the list, it is a duplicate.
03:05:39 <ejt> but what about 3 in a row ?
03:06:15 <mudboy> I would accumulate the answer in another list that discards duplicates.
03:06:56 <mudboy> of course there are probably a thousand other ways to do it.
04:44:05 <prologic> .
04:44:37 <prologic> hrmm well mudboy I'm actually not allowed to use primitive recursion or re-invent the wheel
04:44:41 <prologic> so I'm a little stuck :)
04:45:09 <clausen> prologic: what are you trying to do?
04:45:59 <prologic> write a function that returns all the elements in a list that occur more than once.
04:46:09 <prologic> it's just a tute exercise :)
04:46:22 <Lor> So what are you allowed to use?
04:46:33 <clausen> prologic: is the list ordered?
04:46:39 <clausen> (i.e. of type Ord a => [a])
04:46:39 <prologic> well dot lists, list comprehension, and Prelude and List functions of course
04:46:46 <prologic> it doesn't say
04:46:52 <clausen> that means "no"
04:46:56 <prologic> doesn't say in any of them wether the lists are ordered or not
04:47:06 <prologic> done the first two ok :)
04:47:16 <prologic> first three rather
04:48:03 <prologic> it's up on http://daisy.ods.org/~prologic/10.pdf if any of you want to take a look
04:48:24 <Lor> So you want a solution with no explicit recursion?
04:48:30 <prologic> yeah :)
04:48:48 <Lor> Does it need to be efficient.
04:48:50 <Lor> ?
04:48:55 <prologic> probably not
04:49:21 <Lor> Then there is a very easy solution.
04:49:34 <Lor> Well, maybe not easy, since you are having problems with it, but at least simple.
04:49:41 <prologic> hehe
04:50:09 <prologic> I obviously don't know even Haskell to know it yet :) but go on...
04:50:19 <Lor> Do the elements need to be in the same order as in the input?
04:50:28 <prologic> not necessarily
04:50:44 <prologic> there's really no restrictions imposed except how the function is written
04:50:56 <Lor> It's just a fold.
04:51:05 <Lor> _Everything_ you do with lists is a fold. :)
04:51:14 <prologic> hrmm i see
04:51:47 <prologic> see I don't understand how to use fold though but I'll believe you
04:52:07 <clausen> foldl1 (+) [1..5]
04:52:15 <clausen> will get you started with fold :)
04:52:23 <Lor> Fold just means iterating through the list and doing something with each element, and accumulating a value.
04:52:31 <prologic> k
04:52:33 <prologic> ta clausen
04:52:49 <prologic> that's the same as: sum [1..5]
04:53:05 <clausen> prologic: right
04:53:19 <prologic> obviously fold can do something more complicatd than just sum :)
04:53:42 <o3> like product :D
04:56:20 <prologic> hrmm
05:03:22 <prologic> Lor: I don't get how I can use fold to solve this
05:08:57 <Lor> You should accumulate a list that doesn't contain duplicates.
05:11:45 <Lor> Remember that you can use List.elem to check whether a list contains a specific value.
05:11:53 <prologic> yep
05:12:16 <prologic> how do I accumulate a list ?
05:12:39 <YveDeLarc> look at the type of the first argument of foldl1
05:13:03 <Lor> The function that you give to fold as a parameter gets as arguments the previous accumulated value and the next element of the list, and returns the new accumulated value.
05:13:20 <prologic> which could be a list ?
05:13:24 <YveDeLarc> xsure
05:13:51 <prologic> ok :)
05:13:58 <Lor> prologic, can you write a reverse function with foldl?
05:14:22 <prologic> that reverses a list ?
05:14:25 <Lor> Yep.
05:14:29 <prologic> I'll try :)
05:14:31 <prologic> wait there
05:14:39 <Lor> You'd better do that first. Then the rest will be easy.
05:14:42 <prologic> ok
05:27:34 <prologic> geez I can't even do that :(
05:27:37 <prologic> (yet)
05:30:18 <Lor> How far did you get?
05:30:43 <prologic> errors with unification would give infinite type
05:30:54 <YveDeLarc> foldl (\l e -> [e] : (map (e:) l)) [] [1..5]
05:30:57 <YveDeLarc> another example
05:31:41 <Lor> prologic, show the code.
05:31:54 <prologic> r as = foldl1 r' as
05:31:54 <prologic> r' as' e = e:[] ++ as'
05:31:58 <prologic> I have no idea what I"m doing :)
05:32:27 <Lor> Not foldl1, foldl.
05:33:05 <prologic> ahh
05:33:23 <prologic> oh I see now
05:34:28 <Lor> Did you get it working?
05:36:07 <prologic> not yet
05:36:16 <Lor> You have the right idea.
05:36:21 <prologic> k
05:36:29 <Lor> You just need to supply the initial value for the accumulator.
05:36:47 <prologic> excellent :)
05:36:53 <prologic> r as = foldl r' [] as
05:36:53 <prologic> r' as' e = e:as'
05:37:16 <Lor> Very good. :)
05:37:44 <prologic> ok
05:37:45 <prologic> ty :)
05:37:56 <prologic> I understand foldl a bit better now
05:38:03 <Lor> Now then, you need to modify this so that the returned list only contains each value once.
05:38:58 <prologic> well actually I have to write a function that returns all elements that occor more than once in a list
05:40:17 <Lor> All right, then you need to modify it a bit more.
05:40:24 <prologic> yeh
05:41:47 <Lor> But it's still very simple, you only need to change the r' -function.
05:41:55 <prologic> yup
05:42:35 <Lor> Hum, actually, there are several ways to do it here, none of them quite as neat as I'd like.
05:44:04 <Lor> prologic, before trying to code anything, have you figured out an algorithm for finding all the elements that occur many times?
05:44:56 <prologic> i guess just go through each element building up a list of tuples would be one way ?
05:45:39 <Lor> List of tuples? I was thinking of a tuple of lists. :)
05:45:49 <Lor> What would each tuple in your list contain?
05:46:04 <prologic> the element and the number of times it's occured
05:46:10 <Lor> Ah, yes, that's one way.
05:46:22 <ejt> and I was thinking: sort it, split into a list of lists of identical elements, knock one off the front of all the lists, add to a set
05:48:25 <Lor> There are of course several ways to do this with high-level list operations that are found in the List module.
05:48:33 <phubuh> map hd . group . sort?
05:49:04 <Lor> But I get the impression that this is meant to be an exercise in folds.
05:49:17 <ejt> or comprehensions ?
05:49:21 <prologic> well not really
05:49:33 <prologic> look at http://daisy.ods.org/~prologic/10.pdf
05:50:56 <Lor> Well, all right.
05:51:11 <Lor> You can do the entire thing using only two functions in List.
05:51:33 <prologic> and they are ?
05:52:11 <Lor> I think the point of the exercise is for you to figure them out. :)
05:52:18 <prologic> ok
05:52:19 <prologic> :)
05:52:33 <prologic> I will sooner or later
05:52:41 <prologic> neway still working on what you said earlier
05:53:30 <Lor> Well, apparently folds are not the point of this exercise, so better not worry about them right now. The high-level solution is much neater.
05:53:43 <prologic> k
05:53:45 <Lor> It _is_ essential to understand how folds work, though.
05:53:49 <prologic> I may as well learn folds too though
05:54:09 <Lor> That's the attitude. :)
05:54:15 <prologic> lol
05:56:45 <prologic> oneormore as = foldl oneormore' [] as
05:56:45 <prologic> oneormore' as' e
05:56:45 <prologic>    | null as' = e:as'
05:56:45 <prologic>    | otherwise = if elem e as' then e:as' else as'
05:56:51 <prologic> I just wrote a rather strange function
05:56:54 <prologic> that does half the job :)
05:56:57 <prologic> lol
05:57:39 <Lor> Yep, that was what I first thought you were after (since I read carelessly).
05:57:51 <prologic> it's strange
05:57:53 <prologic> but I understand it
05:58:06 <prologic> it will return a list of duplicates of the first element
05:58:20 <prologic> Tute10> oneormore [1, 2, 3, 4, 1, 1, 1] 
05:58:20 <prologic> [1,1,1,1]
05:58:28 <prologic> which is kinda alright
05:59:07 <Lor> Waitasec... that's not quite what I first thought you were after, after all...
05:59:50 <Lor> Hint: try to reverse the branches in the if, and see what happens.
06:00:18 <prologic> it removes duplicates :)
06:01:43 <prologic> hrmm usefull :)
06:02:37 <Lor> Now, how could you use this to aid you in finding those elements which _have_ duplicates?
06:04:42 <prologic> hehe
06:05:10 <prologic> not sure yet lemme think about it
06:08:05 <Lor> Consider what happens when you remove one of each distinct value from the original list.
06:16:36 <Lor> Are you still there?
06:20:19 <prologic> yeh sorry
06:20:24 <prologic> we're cooking our dinners
06:20:25 <prologic> hehe
06:21:53 <Lor> Ah. Anyway, I think I have almost given out the answer already.
06:22:02 <prologic> probably :)
06:22:17 <prologic> ty Lor, sorry I'm a bit lacking on the thinking side, bloody flu :(
06:22:46 <Lor> Just have a look at the List module and see if you'll find something useful.
06:22:51 <prologic> yup
06:23:06 <prologic> we have a nice Haskell "cheat sheet"
06:23:12 <prologic> really 5 pages of usefull functions
06:23:22 <prologic> comes in handy :)
06:26:24 <Lor> Several people have already privmsgd me with their solutions which, though correct, are horribly messy. Seems they haven't read their List module thoroughly enough either. :)
06:26:42 <Lor> As I said, there's an answer that uses _nothing_ except two functions from List.
06:26:48 <Lor> (One of them twice)
06:27:03 <prologic> *nods*
06:28:36 <Lor> I'll hint even more: you already implemented one of them. :)
06:36:00 * isomer is away: actually doing something
07:26:20 * earthy looks at 10.pdf/1d and notes it is trivially implemented using 1c
07:32:23 <Lor> Hm. I must be slow, but I don't figure out how. Though of course it depends on what you mean by "trivially".
07:32:43 <Lor> Anyway, I'm still quite positive that my solution is the neatest. :)
07:33:04 <BlitzNL> The term 'first class' is something I seem to run into quite often, but the meaning is not fully clear to me. I am reading some work on program transformation at the moment and it states that pattern matching in Haskell is not 'first class', because if a pattern match failes it crashes. (?)
07:33:25 <Lor> Uh.
07:33:30 <Lor> Patterns are not first class, that is true.
07:34:01 <Lor> But I don't see what that has to do with the behavior when there are no matches.
07:34:05 <BlitzNL> Why aren't they, what properties should hold for something to be first-class..
07:34:31 <Lor> First-class means that the concept is represented as a value which can be processed at run-time like any other value.
07:34:49 <Lor> For example, in Haskell IO computations are first-class, since you can put them into lists and whatnot.
07:35:29 <Lor> In most other languages, you can only perform actions.
07:36:08 <earthy> owh, dang, stupid me
07:36:13 <BlitzNL> Lor: that makes sense, thnx
07:36:15 <Lor> Actually, that's not a very good explanation.
07:37:36 <Lor> Hm, if we were talking about ML I could use first-class references as an example...
07:40:40 <BlitzNL> Mm, there should be a library bot or something running in the channel we could ask for haskell/functional terminology ;)
07:54:27 <Lunar^_> Damn
07:54:34 <Lunar^_>     Overlapping instance declarations:
07:54:34 <Lunar^_>       PDFLib.hsc:782: Opt [a]
07:54:34 <Lunar^_>       PDFLib.hsc:795: Opt String
08:10:14 <earthy> lor: okay, not totally trivial. :)
08:16:52 <shapr> ahem
08:17:05 <shapr> ppmm: !help
08:17:08 <ppmm> shapr: I don't know
08:17:12 <shapr> ppmm: help
08:17:42 <shapr> ppmm: lart lambdabot 
08:17:44 * ppmm decapitates lambdabot conan the destroyer style
08:17:58 <ski> :)
08:18:10 <shapr> who runs ppmm ?
08:23:34 <Lor> I have now received three solutions to prologic's problem, all of them fairly verbose. Anyone still want to try? :)
08:23:51 <prologic> lol
08:23:58 <prologic> I haven't even solved it myself yet :(
08:23:58 <ski> prologic : a new one ?
08:23:59 * earthy still working ;)
08:24:00 * prologic sigh
08:24:13 <prologic> ski, just some exercises for this week's tutorial :)
08:24:18 <ski> k
08:24:19 <earthy> and zip xs (tails xs) will not do it ;)
08:24:34 <prologic> I should go to sleep
08:24:37 <prologic> bloody flu :(
08:24:37 <earthy> even if it is a Big Hint ;)
08:25:15 <prologic> @type tails
08:25:21 <shapr> @type heads
08:25:27 <prologic> tails is not a function ?
08:25:27 <shapr> @type faster damn you
08:25:36 <earthy> tails is in module List
08:25:39 <prologic> nor it heads
08:25:41 <earthy> not in the Prelude
08:25:42 <prologic> is it now
08:25:43 <prologic> hrmm
08:26:01 <prologic> ahh god damn it
08:26:08 <prologic> I've been looking at the list functions of the Prelude
08:26:13 * prologic smacks his head in
08:27:02 <prologic> Lor, why didn't you make that distiction earlier :P lol
08:28:13 <Lor> I did specificly and explicitly speak of the List module all the time.
08:28:24 <prologic> lol
08:28:27 <prologic> I know you did
08:28:33 <prologic> I just didn't know it
08:28:46 <prologic> thought you were referring to the list functions of the Prelude module :(
08:28:51 <Lor> You yourself mentioned that you are allowed to use Prelude and List functions, so I thought the distinction was clear to you.
08:28:57 <prologic> blah
08:29:04 <prologic> how embarassed I am
08:29:17 * earthy ponders
08:30:13 <earthy> so close, and yet so far
08:32:09 <prologic> I'm so close but so far with this function
08:32:23 <prologic> I'm determined to write a function that'll return unique elements of a list
08:32:40 <Lor> You wrote that already.
08:32:45 <prologic> did i ?
08:32:48 <Lor> Oh, sorry, no.
08:32:52 <prologic> heh
08:32:59 <prologic> I wrote one to remove duplicate elements
08:33:11 * shapr chortles merrily
08:33:22 <prologic> Tute10> removedups [1, 2, 2, 4]
08:33:23 <prologic> [4,2,1]
08:33:29 <ski> prologic : which of the ex.s are you on ?
08:33:35 <prologic> umm
08:33:39 <prologic> 1. (c)
08:33:47 <prologic> err
08:33:49 <prologic> sorry
08:33:49 <earthy> ski: 1. d, I hope.
08:33:50 <prologic> 1. (d)
08:33:55 <prologic> yeh
08:33:58 <prologic> I already done the first 3
08:34:25 <prologic> (c) is a nice solution --> isin as bs = filter (\n -> elem n bs) as
08:34:27 <ski> in no particular order, i assume
08:34:37 <prologic> I did them sequentially
08:34:37 <prologic> :)
08:35:07 <ski> i meant : return the multi-occurings in no part. order ..
08:35:21 <prologic> oh
08:35:26 <prologic> yeh doesn't matter about the order
08:35:35 <prologic> not according to the tute sheet
08:39:28 <ejt> (c) can be simplified even more :)
08:39:40 <shapr> hi Joe
08:39:41 <shapr> wassup?
08:39:43 <prologic> do show me :)
08:40:03 <ejt> hi shapr
08:40:15 <ejt> well it looks to me that: isin = intersect
08:41:17 <prologic> lol it is too
08:41:30 <prologic> gawd you've almost gotta be a mathematician to understand the names of some haskell functions
08:41:32 <ejt> I'm not sure that's the answer they were looking for though
08:41:45 <Lor> prologic, do you read the assignment as requiring that in the returned list each value should occur as many times as it occurred in the input, or only once?
08:42:00 <prologic> for (d) ?
08:42:04 <Lor> Yeah.
08:42:10 <prologic> umm
08:42:18 <prologic> I guess just a list of elements that occur more than once
08:42:22 <Lor> Should f [3,4,4] return [4] or [4,4]?
08:42:27 <prologic> eg: [1, 2, 2, 4] --> [2]
08:42:35 <Lor> Right, that's how I understood it too.
08:42:39 <prologic> me too :)
08:42:49 * earthy understood it to return [2,2] and [4,4]
08:42:59 <prologic> if I can write a function that'll return [1, 4] for [1, 2, 2, 4]
08:43:02 <prologic> then I'll be close ;)
08:43:13 <prologic> perhaps the List module already has one ?
08:43:23 <earthy> nope. it doesn't. :)
08:43:35 <prologic> ok that's good to know :)
08:43:44 <prologic> not wasting me time then
08:44:26 <Lor> As long as you are aware that there _is_ a solution which doesn't require you to do anything except call some List functions... :)
08:44:34 <prologic> yeh
08:45:05 <Lor> But use whatever you think is the most straightforward approach for you.
08:45:34 <Smerdyakov> Lor, just call list functions without creating any of his own functions?
08:45:45 <prologic> yeah
08:45:51 <ski> no own prim. rec.
08:45:52 <prologic> ie: dont' reinvent the wheel
08:45:56 <prologic> and that too
08:46:31 <Lor> Smerdyakov, besides the top-level one, no.
08:46:35 <prologic> I think this week's tute is all about trying to understand the libraries a bit more and how we can use them without writing too much recursion (primitive)
08:46:54 <Smerdyakov> Hm. There must be some very similar functions in the prelude or earlier in the assignment. :)
08:46:57 <Lor> prologic, how come you use the term "primitive recursion" so much?
08:47:00 <earthy> 1e is very simple
08:47:13 <prologic> Lor, why ? :)
08:47:22 <Lor> What do you mean with it?
08:47:37 <Smerdyakov> Ah. We have a "traditional mathematician" here.
08:47:42 <prologic> umm
08:47:44 <prologic> hrmm
08:47:48 <prologic> how the heck do I explain it :P
08:47:50 <Smerdyakov> Doesn't know about higher order logic OR recursion theory. :D
08:47:54 <prologic> you outta know what primitive recursion is
08:47:55 <ejt> because his question sheet says not to use it ?
08:47:57 * earthy grins
08:47:58 <Lor> Yes, I know.
08:48:04 <prologic> lol
08:48:11 <Lor> I asked what you mean with it.
08:48:20 <Smerdyakov> Lor, so you know the meaning in recursion theory?
08:48:20 <prologic> the same as what the sheet means
08:48:42 <prologic> ie: instead of re-writing a function that does the same job as "takeWhile" or "repeat" etc, use them instead.
08:48:50 <prologic> that's what's meant by it
08:49:09 <Lor> Because it's not something _directly_ applicable to Haskell since there are no inbuilt constructs for primitive recursion.
08:49:17 <prologic> there are many functions in the Prelude and List module which use recursion, so you don't have to re-write functions that do the same thing
08:49:32 <prologic> ok well I am not aware of the mathematical side of primitive recursion :)
08:49:38 <Smerdyakov> Lor, nonetheless, it is possible to rule on whether a given function is primitive recursive in the mathematical sense.
08:49:51 <ski> hmm, does that mean prologic can use non-primitive recursion ?  :)
08:49:57 <prologic> although I did discrete maths, I'm not that good with maths anyway
08:49:59 <Lor> Yes, but it doesn't seem likely that that is what is meant here.
08:50:08 <prologic> Lor, stop confusing me :)
08:50:08 <prologic> lol
08:50:32 <earthy> wasn't primitive recursion something like the primitive operators plus one transitive closure level of recursion?
08:50:42 * earthy is a bit hazy on his recursion theory
08:50:57 <Smerdyakov> earthy, yeah. You get a fold over the naturals. :)
08:51:07 <ejt> is the other Bird book a good intro to this stuff ?
08:51:08 <Lor> prologic, I just want to understand why that term is used here, since it seems fairly technical and not quite to the point.
08:51:13 <earthy> yeah, so no ordinals, right? :)
08:51:27 <Smerdyakov> earthy, I hate ordinals. So right.
08:51:28 <prologic> Lor, seems so, but in this case I think he means something entirely different.
08:51:40 * esap likes ordinals.
08:51:45 <Lor> Is it just a fancy name for "recursion" in general here?
08:51:51 <prologic> yup
08:51:53 <ejt> Bird/de Moor I mean
08:52:02 <Lor> All right, but it's very confusing.
08:52:26 * earthy would think that prologic is allowed to recurse over function arguments...
08:52:28 <Smerdyakov> prologic, you should ask the professor in class if he means the primitive recursion from recursion theory, and see if he gets flustered!
08:52:32 <ejt> anything else you'd reccommend ?
08:52:42 <prologic> lol
08:52:43 <Lor> There is a sense in which things like folds are "primitive recursive" (though structural recursion would be a better name), but the general recursive constructs in Haskell are not those.
08:52:47 <prologic> but I don't even know recursion theory :(
08:52:50 <prologic> so I'd look stupid
08:53:04 <Smerdyakov> prologic, nah, just say you've heard of it and hope you don't have to learn it.
08:53:09 <prologic> haha
08:53:14 <prologic> we're nearing the end of our semester
08:53:18 <prologic> I doubt we'll be learning it
08:53:27 <Smerdyakov> It's just a ruse to embarrass the professor, you know?
08:53:34 <prologic> we'll be doing monads and IO shortly, and looking at CGI and implementing a Prolog intepreter
08:53:35 <earthy> prologic: usually it's a different course. :)
08:53:48 <earthy> anyway, time to go home
08:53:49 <Lor> Uh, if there's a _professor_ who doesn't know what primitive recursion is, s/he earns to be embarrassed.
08:54:00 <prologic> hehe
08:54:12 <prologic> I'm sure he'd know, he's a very intellectual profressor
08:54:25 <prologic> cya earthy :)
08:54:31 <Smerdyakov> Lor, why's that? It's a pretty esoteric subject to most of the CS/math world.
08:55:16 <earthy> no it isn't
08:55:19 <Lor> I think recursion theory is pretty essential for understanding the basics of computation.
08:55:29 <Smerdyakov> earthy, it is.
08:55:31 * earthy agrees with lor
08:55:56 <Lor> The relationship between computations and the mathematical functions which they compute.
08:56:04 <earthy> anyone with a slightly more theoretical interest in computing should know recursion theory
08:56:11 <earthy> especially due to the results that stem from it
08:56:18 <earthy> but, I'm gone
08:56:51 <Smerdyakov> And a small fraction of the aforementioned community has a slightly more theoretical interest in computing.
08:56:54 <Lor> I would like to know an exact mathematical characterization for the functions computable in F2.
08:57:18 <Lor> It's a superset of PR functions since eg. Ackermann's can be computed in F2, but it's not the full general recursive functions, since everything in F2 terminates.
08:58:03 <Smerdyakov> I've yet to take a CS class that gives primitive recursion more than a single mention, I think.
08:58:05 <Lor> smerdyakov, uh, CS and math pretty much _mean_ theory, at least to me.
08:58:12 <Lor> If it's not theoretical, it's not CS or math. :)
08:58:27 <Smerdyakov> Lor, then you're at odds with the world!
08:58:35 <Lor> Tough luck for the world. :)
08:59:28 <Lor> Applied CS is called "software engineering", and applied math is called "physics" or "engineering". If it's called just cs or math, it's not applied.
08:59:31 <Lor> That's how I see it. :)
08:59:39 <Smerdyakov> Especially in the USA, where traditional mathematical logic is not accepted as a foundation of CS in most places.
08:59:44 <Smerdyakov> places = universities
08:59:59 <prologic> well I'm doing software engineering
09:00:09 <prologic> I'll probably end up learning recursion theory one day
09:00:11 <Lor> prologic, and you're using Haskell? Wow.
09:00:21 <prologic> why wow ?
09:00:38 <Lor> Because Haskell has a reputation for being solely of theoretical interest.
09:00:52 <prologic> it's actually fairly practical ):
09:00:53 <prologic> :)
09:00:56 <prologic> stupid keyboard
09:01:06 * esap is using Haskell for practical purposes, that is, building compilers.
09:01:11 <Lor> Of course, but few people know it.
09:01:18 <prologic> true
09:01:19 <ski> Lor : F2 ?
09:01:36 <Lor> System F².
09:01:39 <prologic> what does the 'nub' function do ?
09:01:41 <ski> ok
09:02:26 <Lor> prologic, it removes duplicates from a list. :)
09:02:34 <prologic> does it now
09:02:35 <prologic> lol
09:02:38 <prologic> I'm an idiot :)
09:02:50 <ski> Prelude> multis [1,2,2,4]
09:02:51 <ski> [2]
09:02:55 <Lor> No you're not.
09:03:05 <prologic> ski you're brilliant :)
09:03:13 <prologic> but lemme figure it out :)
09:03:22 <Lor> I told you that you had already implemented something you need. But it was still good practice.
09:03:27 <prologic> yeap
09:03:43 <ski> prologic : not so readable code, though.
09:03:49 <prologic> heh
09:03:52 <Lor> You know, you have spent so much time on this that I'll tell you the solution if you happen to feel like giving up.
09:03:55 <prologic> still it works
09:04:02 <prologic> ok ta Lor :)
09:04:12 <prologic> I'll ask for it before I go to bed and sleep on it if I can't figure it out
09:04:42 <ski> Lor : can one use some of the other ex.es as help for d) ?
09:04:52 <prologic> why not :P
09:04:54 <prologic> I did
09:05:20 <Lor> ski, maybe, but the solution is so utterly simple just using standard libraries, that it's hardly useful.
09:05:53 <ski> then, maybe i'm using the wrong function..
09:07:18 <prologic> what's the union function do ?
09:07:19 <ski> ca 130 chars
09:07:26 <prologic> combine two lists ?
09:07:32 <prologic> in some way
09:07:56 <ski> Prelude> List.union [1,2,3,4] [3,4,5,6]
09:07:59 <ski> [1,2,3,4,5,6]
09:08:13 <Lor> prologic: http://haskell.org/onlinereport/list.html
09:08:30 <prologic> I feel it's usefull in some way for (d) :P
09:08:31 <prologic> ta Lor
09:08:37 <ski> Prelude> List.union [1,1,2,3,3,4,4,4] [3,3,3,4,5,6,6]
09:08:38 <ski> [1,1,2,3,3,4,4,4,5,6]
09:10:31 <prologic> Lor, am I on the right track with set operations ?
09:10:36 <prologic> intersect, union etc ?
09:10:46 <Lor> You could say so, yes.
09:11:10 <esap> lor: about that System F thing, were you saying that Curry-Howard correspondence doesn't solve the problem of how expressive the polymorphic lambda calculus is?
09:11:14 <Lor> Maybe "bag operations" would be better, since we're here treating lists as "bags" or "multisets" which can contain multiple occurrences of a value.
09:11:33 <Lor> esap, uh, what? About what's computable in F2?
09:11:53 <esap> lor: yes.
09:12:36 <Lor> How does C-H relate to this?
09:15:23 <ski> 24 chars,  much better :)
09:16:57 <ski> e,f,g seems easy
09:17:09 <Lor> ski, that sounds like mine.
09:18:31 <ski> ow
09:18:37 <ski> @prelude \\
09:18:38 <lambdabot> DictModule.hs:113: Irrefutable pattern failed for pattern (w' GHC.Base.: rest)
09:18:53 <Smerdyakov> @prelude confuseme
09:18:54 <lambdabot> No match for "confuseme".
09:19:29 <esap> lor: Well I guess you can use C-H to map the terms of the SystemF into proofs, and then you can map this to different theories. I would expect those mathematical theories have the same expressive power than the polymorphic lambda calculus, if you can do this mapping [and I think first-order theories are quite well known in their expressive power]
09:20:22 <Lor> Uh. That sounds like confusing syntax and semantics.
09:21:33 <esap> heh maybe :-) I'm trying to think only about the semantics.
09:22:59 <esap> I mean, you are asking about what is the expressive power of the polymorphic lambda calculus, I guess.
09:23:59 <Lor> Let us say "computational".
09:24:16 <Lor> "Expressive power" can mean so many things (see eg. Felleisen's paper on that).
09:25:32 * esap hasn't read that. Thanks for the link.
09:25:53 <prologic> Tute10> multis [1,2,2,4]
09:25:54 <prologic> [2]
09:26:01 <prologic> I think I've done it :)
09:26:16 <Smerdyakov> Lor, and what kind of lambda calculus do you mean? Do you include any uncomputable primitives?
09:26:45 <ski> prologic : code ?
09:26:52 <ski> is it simple ?
09:26:58 <prologic> multis :: Eq a => [a] -> [a]
09:26:58 <prologic> multis xs = nub $ concat $ filter (\x -> if length x > 1 then True else False) (group xs)
09:27:09 <prologic> I understand it :)
09:27:11 <Lor> Smerdyakov, just pure F2.
09:27:26 <prologic> I'm assuming it can be done much simpler though :(
09:27:34 <Lor> prologic, do you also understand why "if something then True else False" is redundant?
09:27:42 <prologic> not yet :)
09:28:04 <prologic> give me some hints to make it simpler
09:28:08 <prologic> don't show me your solutions yet :)
09:28:11 <prologic> I wanna learn
09:28:21 <Smerdyakov> Lor, sorry, I don't know what that is.
09:28:39 <sandos> prologic: uhm.
09:28:42 * shapr boings
09:28:43 <prologic> afik group will groups duplicate elements into lists. right ?
09:29:09 <ski> List> group "aabcba"
09:29:10 <sandos> (\x -> if length x > 1 then True else False) -> (length > 1)
09:29:10 <ski> ["aa","b","c","b","a"]
09:29:12 <sandos> right?
09:29:17 <Smerdyakov> sandos, no
09:29:23 <Smerdyakov> sandos, different types
09:29:24 <sandos> sorry
09:29:26 <prologic> hrmm
09:29:31 <esap> smerdyakov: Lor means System F I guess.
09:29:33 <prologic> ty ski :P back to the drawing board I go
09:29:42 <Lor> Smerdyakov, pure polymorphic lambda calculus.
09:29:53 <sandos> would \x -> length x > 1 work?
09:30:05 <prologic> :e
09:30:22 <Smerdyakov> Lor, and I don't know what that means. Are there uncomputable primitives?
09:30:29 <Lor> No, there are no primitives.
09:30:40 <Lor> Just like pure lambda calculus: there are only functions.
09:30:43 <ski> prologic : otherwise, it was a good idea
09:30:48 <prologic> lol
09:30:51 <Lor> Primitives and ground types are just syntactic sugar. :)
09:31:16 <Smerdyakov> Lor, then isn't it equivalent to, say, Turing machines?
09:31:17 <Lor> Well, not sugar, but just a convenience, nothing really essential. :)
09:31:25 <ski> prologic : perhaps, you can fix it
09:31:41 <Lor> Untyped LC is. But F2 is strongly confluent, ie. it always terminates.
09:31:53 <Lor> So it cannot express all general recursive functions.
09:32:04 <Lor> (You cannot type the fixpoint operator in F2)
09:32:46 <Lor> And I say "F2" here to make a distinction from the F_omega with kinds and type operators.
09:32:59 <prologic> Tute10> multis [1, 2, 2, 4, 2, 1]
09:32:59 <prologic> [1,2]
09:33:01 <prologic> there :)
09:33:08 <prologic> multis xs = nub $ concat $ filter (\x -> length x > 1) (group (sort xs))
09:33:20 <ski> though you'd do it like that :)
09:33:27 <prologic> why not :)
09:33:37 <prologic> now I'm sure there is a simpler way
09:33:43 <Lor> Yes, that seems to work.
09:33:52 <ski> you can ".":ize it if you like
09:34:10 <prologic> eh?
09:34:19 <ski> foo . bar . baz
09:34:23 <Lor> People on this channel are obsessed with point-free style.
09:34:29 <prologic> k
09:34:31 <Lor> Which paradoxically seems to imply writing lots of points. :)
09:34:40 <prologic> I meant there has to be another way of doing this though
09:34:40 <ski> or change parens to $ for consistency
09:34:58 <Lor> prologic, yes. Want to hear?
09:35:12 <prologic> yes please ;)
09:35:25 <ski> multis2 as = as \\ nub as
09:35:36 <Lor> I was going to explain it first.
09:35:44 <ski> oh
09:35:45 <prologic> aye shit ski!
09:35:46 <prologic> lol
09:35:49 <prologic> please explain that
09:36:05 <Lor> You need another nub, though: nub (as \\ nub as)
09:36:22 <ski> to get only one of all, yes
09:36:27 <Smerdyakov> Y'all do realize you are writing someone's homework problem answer for him before the assignment is due, right?
09:36:41 <Lor> Simply enough, you remove from the list one of each value that occurs there.
09:36:55 <prologic> Smerdyakov, no they're not :)
09:36:58 <Lor> And then remove the duplicates from the returned list.
09:37:07 <Smerdyakov> prologic, why not?
09:37:10 <prologic> Smerdyakov, just trying to get ahead :)
09:37:13 <prologic> not assignment
09:37:13 <Lor> Smerdyakov, he already solved it himself.
09:37:23 <Smerdyakov> prologic, oh, OK.
09:37:27 <prologic> I'm obsessed with Haskell :P
09:37:28 <Lor> With _quite_ sufficient effort.
09:38:06 <Smerdyakov> I'm just saying that I've been in classes where help even after a solution has been produced would require a formal statement about who helped you and how to be submitted with the assignment.
09:38:29 <Lor> That's silly.
09:38:40 <Smerdyakov> It would be if everyone in university were there to learn.
09:38:48 <Smerdyakov> But they're just there to get diplomas and go to high-paying jobs.
09:38:58 <Smerdyakov> Have you ever been a teaching assistant for a programming class?
09:38:59 <Lor> It's impossible to control cheating anyway.
09:39:02 <Lor> Yes.
09:39:21 <Smerdyakov> How many cheaters were uncovered in a representative semester/other appropriate time unit?
09:39:35 <Smerdyakov> I found 6 personally when I was a TA./
09:39:37 <Lor> Our exercise sessions essentially consist of _forcing_ students to share solutions.
09:39:57 <prologic> Tute10> multis "hello how are you?"
09:39:57 <prologic> " ehlo"
09:39:57 <prologic> Tute10> multis2 "hello how are you?"
09:39:57 <prologic> "lho e"
09:39:59 <prologic> interesting :)
09:40:07 <prologic> I guess because my version, sorts the list first
09:40:08 <Lor> I have seen one bigger programming task where two people submitted curiously similar programs.
09:40:25 <prologic> often my own colleages and i can produce almost identical programs
09:40:30 <Smerdyakov> Lor, catching cheaters for programming assignments is generally very, very easy.
09:40:31 <prologic> because we work together a lot
09:40:58 <Smerdyakov> Lor, http://www.cs.berkeley.edu/~aiken/moss.html
09:41:02 <Lor> Smerdyakov, then probably we don't have them, maybe exactly for that reason.
09:41:04 <prologic> but as part of software engineering, (in what we learn), it's mostly about reproducability
09:42:17 <Smerdyakov> prologic, I think you learn less through "working together" on programming assignments to an extent that makes your code almost identical. High level approaches, sure, but not code details.
09:42:33 <prologic> I understand now, from you (Lor) and ski's solution how \\ is used now :) ta
09:42:49 <Lor> At our place, the general consensus seems to be that trying to control cheating is mostly too much trouble, and exercises only give you so few points that if you don't know anything at the test then you're going to flunk anyway.
09:43:12 <Smerdyakov> Lor, cheating detection is very easy if you use software to detect it.
09:43:12 <Lor> Really the point of it all is just to get people to _come_ to the exercise sessions so they could learn something there.
09:43:27 <prologic> Smerdyakov, we work together on projects, software engineering ones. though out of my colleages I'm the better programmer so I try to let them learn it for themselves
09:43:31 <Smerdyakov> Lor, and then you can make "exercises" worth about half of the final grade.
09:43:45 <Smerdyakov> prologic, "colleagues"
09:43:49 <Lor> That's a different approach, and only a couple of our courses use them.
09:43:57 <prologic> Lor, well that's what it is, what I've been doing tonight. A lab (tutorial) on Thursday morning. just exercises :)
09:44:09 <prologic> Smerdyakov, yeh yeh I can't spell :P it's 2.44am :)
09:44:19 <prologic> I'm gonna do a couple more questions
09:44:31 <Smerdyakov> Lor, I always prefer to have as much of a grade as possible come from assessments I have a lot of time to work on, as a student. I think that's also the best from the teacher's perspective.
09:45:05 <Lor> Depends on how you view the exercises.
09:45:35 <Lor> Our weekly exercises are very informal, and their point is mainly to get people to discuss trouble points and to ask questions from the TA's.
09:45:50 <Lor> Some courses have bigger projects, then, and those are of course given more weight.
09:46:56 <Lor> But it's pretty hard to "cheat" since you have to be able to explain your solution verbally.
09:47:21 <Smerdyakov> I'm talking about longish-term (2 weeks) programming assignments.
09:47:31 <Lor> Yeah, that's different.
09:47:35 <Smerdyakov> Something you can't count towards a grade in good faith without controlling for cheating.
09:47:41 <prologic> will --> equal as bs = length $ as `intersect` bs <-- work for 1. (e) ?
09:48:22 <Smerdyakov> Lor, I don't think programming courses that never grade students on programs of significant size written on their own can have very accurate assessments.
09:48:40 <ski> if i understand (e) correctly, then no
09:48:46 <prologic> hrmm
09:48:54 <prologic> how do you interpret e ?
09:48:59 <Lor> Smerdyakov, the programming projects are usually so uniquely worded that it's unlikely that a ready solution can be found on the internet. And of course if one person reviews all the projects, it's easy to spot similarities.
09:49:18 <ski> corresponding, i take to mean : on same index
09:49:21 <benja_> hi
09:49:23 <ski> benja_ : hi
09:49:33 <Smerdyakov> Lor, the students share code with each other.
09:49:45 <prologic> ski: ahh k ta
09:49:46 <Lor> Smerdyakov, we do have some courses that include bigger projects. And we do have separate courses that specifically consist _only_ of a bigger project.
09:49:48 <Smerdyakov> Lor, and one person reviewing 100+ projects can have a hard time remembering.
09:50:37 <Smerdyakov> Well, I have to go vote now.
09:50:44 <Lor> We often give several alternative project assignments, and different people review different assignments.
09:51:04 <benja_> wondering if somebody here knows an idiom for \fn list -> nub (concat (map fn list)) ?
09:51:09 <Lor> So a single person still gets to see all the programs where sharing is conceivable.
09:51:32 <Lor> benja, there's concatMap
09:51:45 * benja_ goes looking :)_
09:51:46 <Lor> You still have to nub separately.
09:51:51 <benja_> ok
09:52:27 <Lor> That, by the way is the binding operator for the set monad.
09:53:17 <ski> Lor : set ?
09:53:30 <Lor> Like a list monad, but with at most one occurrence of everything.
09:53:45 <ski> ah, you mean with nub also..
09:53:57 <Lor> Yep. Useful for nondeterminism.
09:54:03 <Lor> To avoid duplicate branches.
09:54:17 <Lor> Though of course one could use a smarter representation than a linear list. :)
09:55:25 <benja_> cool, that sounds like a good way to do one thing I just coded =)
09:55:33 <benja_> (graph traversal where you want to consider each node only once)
09:57:37 <prologic> equal as bs = equal' as bs 0
09:57:38 <prologic> equal' [] [] n = n
09:57:38 <prologic> equal' (a:as) (b:bs) n = if a == b then equal' as bs (n + 1) else equal' as bs n
09:57:43 <prologic> I suspect there's a simpler way :)
09:57:46 <prologic> has to be
09:58:23 <ski> yes :)
09:58:47 <ski> (no own rec, mind)
09:59:08 <prologic> using something similar to elemIndex ?
09:59:35 <ski> um
09:59:51 <ski> not what i thought of
10:00:00 <prologic> just seeing if I'm on track :)
10:00:09 <prologic> it's something to do with element indices
10:00:58 <benja_> hm, equal as bs = sum (map (\(x,y) -> if x==y then 1 else 0) (zip as bs))) ? :)
10:01:53 <ski> seem correct
10:02:43 <benja_> or, equal as bs = length [1 | (x,y) <- (zip as bs), x==y]
10:03:23 <benja_> or equal as bs = length (filter (curry (==)) (zip as bs))  :)
10:03:32 <prologic> yeeesh
10:03:33 <prologic> stop it!
10:03:35 <prologic> lol
10:03:46 <prologic> you're making me hang me head in shame :)
10:03:57 <ski> one can make variations of those. some shorter
10:05:11 <prologic> neway least I understand them though by having done one myself :)
10:30:27 <prologic> equal as bs = length $ filter (uncurry (==)) $ zip as bs
10:30:31 <prologic> ski, is yours shorter ?
10:30:48 <prologic> benja_, you needed uncurry :)
10:32:02 <ski> i was thinking of using zipWith, in benja_'s first version
10:32:54 <prologic> does that allow you to zip with a function ?
10:33:26 <ski> @type zipWith
10:33:27 <lambdabot> zipWith :: (a -> b -> c) -> [a] -> [b] -> [c]
10:39:23 <esap> lor: Now I found it: In "Lectures on the Curry-Howard isomorphism", theorem 12.6.6: "The class of functions definable in (System) F coincides with the class of provably recursive functions of second-order Peano Arithmetic."
10:39:33 <prologic> sum $ zipWith (curry (\(x, y) -> if x==y then 1 else 0)) as bs
10:39:40 <Lor> esap, ah, thanks.
10:39:41 <prologic> is all I can some up with, with zipWith
10:40:00 <ski> that was what i was thinking of, yes
10:40:00 <Lor> Is there any further elaboration as to what this class does or does not include?
10:40:06 <prologic> ski, but it's longer :P
10:40:47 <ski> than benja_'s first ?
10:40:59 <Darius> You don't need to curry the lambda abstraction, just have it take 2 arguments.
10:41:21 <ski> oh :  s/curry (\(x,y) ->/(\x y ->/
10:41:28 <ski> yes
10:42:01 <prologic> ic
10:42:09 <benja_> @type curry
10:42:09 <lambdabot> curry :: ((a,b) -> c) -> a -> b -> c
10:42:25 <benja_> hm, right
10:42:27 <esap> lor: it refers to Girard for proof, hmm... There is quite a bit of material about the System F, including translations of existential and universal quantification etc. for the System F.
10:42:54 <benja_> sounds like my third version should use uncurry :)
10:43:00 <prologic> yup
10:43:02 <prologic> I fixed it up :)
10:43:08 <prologic> length . filter (uncurry (==)) $ zip as bs
10:43:15 <Lor> All right, I'll have a look at it when I both remember and have the time.
10:44:05 <benja_> (right, sorry, wasn't reading backlog, only last few lines =) )
10:50:43 <det> shapr: heya
11:08:51 <shapr> hi det 
11:15:24 * shapr chortles happily
11:16:28 <SyntaxPolice> heya shapr!
11:18:52 <shapr> hi SyntaxPolice 
11:21:16 <det> shapr: I always manage to come across haskell logs of you when searching google
11:21:24 <det> shapr: small internet
11:22:16 <det> 12:58:13 <shapr> Marvin--: I'd like to see a system that is written from S & K combinators on up, and allows for binary reimplementations of supercombinators.
11:22:29 <det> what do you mean by " binary reimplementations of supercombinators"
11:25:41 <benja_> unlambda is written from S & K combinators on up ;-)
11:26:00 <Lor> It's ugly. It has side effects.
11:26:08 <ski> supercombinators can be implemented by the compiler as special-generated code, instead of expressed in terms of the translations of S & K ??
11:27:12 <benja_> Lor: I have been considering an Unlambda Virtual Machine, like JVM (tho probably not for implementing Unlambda): a VM that has only three instructions-- S, K, and apply
11:27:32 <Lor> How is that different from combinatory logic?
11:27:32 <benja_> referentially transparent languages could compile down to it =)
11:27:40 <ski> how about a language with possibility for explicitely managing continuations, but all continuations are linear ?
11:27:49 <Lor> What's a linear continuation?
11:27:53 <Lor> One-shot?
11:28:04 <ski> linear as in linear type-system
11:28:12 <ski> yes
11:28:22 <det> can only be refereneced in one place
11:28:29 <Lor> Referenced or called?
11:28:34 <ski> is such a lang. ref. trans. ? :)
11:28:55 <det> I dont imagine you can call a continuation more than once if it is referenced only once
11:29:20 <ski> depends on what is meant by reference :)
11:29:38 <ski> (e.g. with additive conjunction)
11:29:58 <det> referenced as in linear type systems :)
11:30:37 <ski> can one call it "live reference" ?
11:33:16 <det> ski, explicitly managing continuations ?
11:34:13 <ski> i was thinking of how much "expressive power" linear cont.s give as opposed to no cont.s
11:35:28 <det> what are the periods for ?
11:35:39 <ski> abbrev.
11:35:46 <det> oh
11:36:10 <det> you mean, linear opposed to no continuations at all ?
11:36:17 <ski> yes
11:36:28 <det> exceptions come to mind
11:37:17 <ski> or, perhaps, a linear type system, with support for non-linear continuations if you want them, but with different type
11:37:25 <ski> coroutines, perhaps
11:39:03 <det> do you prefer linear types to monads ?
11:39:19 <ski> i was also thinking of writing coinductive func. definitions in a "cont" style, without accidentally duplicating or destroying cont.s
11:39:42 <ski> i think they solve overlapping, but not same problem areas
11:40:19 <det> I mean, as far as referential transparency goes (IO, mutability, etc)
11:40:26 <ski> 'twould perhaps be interesting to have good support for both, in a lang
11:40:35 <ski> well
11:41:05 <det> I mean side-effects
11:41:21 <Darius> ski: Presumably you've read, "Linear Continuation Passing"?
11:41:26 <ski> linear types seems more "declarative" (i.e. like ordinary side-effect free code), for things as arrays, e.g.
11:42:00 <ski> Darius : yes, some such paper..  author ?  Thielecke ?
11:42:25 <Darius> ski: Among others.  Berndine being listed first.
11:43:41 <ski> det : while with monads, you can have pointers/references again. which avoids finding all places of a data structure to update with a new value
11:44:52 <ski> btw, why doesn't Clean or Mercury have add. conj.s ?  problems with GC ?
11:45:31 <ski> Darius : ok, Thielecke is the one i downloaded a couple of papers from.
11:45:32 <det> ski, well, you should be able to "promote" a linear value to non-linear and lose mutability if you like
11:46:23 <ski> det : but not for all types, say e.g. World, no ?  :)
11:46:41 <det> ski, World?
11:46:56 <ski> the World is passed around in Clean
11:47:13 <det> what depends on the World ?
11:47:19 <ski> ?
11:47:33 <det> what kind of functions take the "World"
11:47:42 <det> presumably returning a new World
11:47:46 <ski> things like openFile
11:47:58 <ski> IIRC
11:48:00 <det> oh, yes
11:48:08 <det> World must always be linear :)
11:48:41 <ski> so, perhaps.. we could have a type class for this..
11:49:52 <det> example ?
11:50:11 <ski> class Promote a
11:50:12 <ski>   where
11:50:21 <ski>   promote :: a -> !a
11:50:27 <ski> pseudo-haskell
11:50:36 <ski> ! is "of course"
11:51:46 <det> oh, you mean for promoting values
11:51:53 <det> perhaps
11:51:55 <ski> yes
11:53:14 <ski> additive conjunction is probably going to be lazy, i think
11:54:02 <det> I dont know what tha tis
11:54:12 <det> but in general I do not like lazy evaluation
11:54:35 <ski> fst :: a & b -> a
11:54:44 <ski> snd :: a & b -> b
11:55:07 <ski> these are the destructors(/selectors) of an additive pair
11:55:52 <Lor> Additive?
11:56:12 <ski> if you have an additive pair, you can get the first field, or the second. it is your choice. but you can't take out both fields at the same time
11:56:30 <det> hmmm
11:56:46 <ski> Lor : i believe Girard called this connective, and the usual disjoint sum, additive ones
11:56:49 <det> how might one be used ?
11:57:18 <Lor> Ah, right, linear logic.
11:57:41 <Lor> Wadler's tutorial is a great introduction to linear logic.
11:57:51 <ski> the thing is : because only one of the fields really can be selected, we can use linear variable duplicatedly !  but only once in both fields
11:58:22 <Lor> http://citeseer.nj.nec.com/wadler93taste.html
11:58:45 <ski> det : if you want to provide two values, and the user gets to choose which one s/he wants.
11:59:46 <ski> det : with the normal (i.e. multiplicative) pair/conjunction in linear logic, one *must* use both fields !
12:00:55 <ski> (there is also a multiplicative sum/disjunction, but it is harder to understand intuitively)
12:01:02 * det thinks
12:03:40 <det> this is impossible without linear types ?
12:04:01 <ski> what ?
12:04:08 <det> <ski> det : if you want to provide two values, and the user gets to choose which one s/he wants.
12:04:30 <Lor> det, how would you do it otherwise?
12:04:46 <Darius> det: that's the wrong part to focus on
12:05:18 <Darius> det:  The point is that the user -must- choose one or the other.  Without a linear type system or equivalent the user could choose neither or both.
12:05:20 <ski> det : it is, if you want the user to be forced to choose exactly one field
12:06:57 <Darius> det: So given f :: a & b -> something, f couldn't be f _ = 10, or f p = fst p + snd p, but it could be f p = fst p + 10
12:07:56 <ski> one can see it as somewhat similar to disjoint sum, in a way. but, instead of you choosing to return (Left ...) or (Right ...), the choice of what to return is, so to speak, given to the user
12:08:15 <ski> but
12:08:21 <det> somewhat related, how could you implement sum values using only functions ?
12:08:36 <Darius> det: Church encodings
12:09:00 <ski> f could be   f p = <snd p * 2,fst p + 8>   if <... , ...> is our way of creating additive pairs
12:09:04 <Lor> You can implement _everything_ using only functions.
12:09:29 <det> not sum types like "type list = Nil | Cons of int * a"
12:09:34 <det> if that is what you mean
12:09:47 * esap thinks you cannot implement everything using functions. Layering is not simple to do with just functions.
12:09:49 <Lor> Yes you can, if you have polymorphism.
12:10:11 <det> Lor, why is polymorphism important ?
12:10:34 <Darius> esap: You -can- implement any computable functions, but that doesn't mean that the result will be fun to use.
12:10:35 <Lor> A disjoint union A + B can be represented by a function of type forall C. (A -> C) -> (B -> C) -> C.
12:11:03 <Darius> @get-definition either
12:11:03 <lambdabot> either = \f g e. e f g
12:11:07 <Darius> @get-definition Right
12:11:07 <lambdabot> Right = \x l r.r x
12:11:29 <esap> darius: Well I think you cannot even express all things. In particular, how do you express a program with two layers, where the upper one uses the lower one?
12:12:13 <Darius> esap: If express == compute, than yes you can.  If express /= compute than what does that have to do with what either Lor or I said.
12:13:08 <Lor> I spoke only of implementations, not of any higher level abstraction mechanisms.
12:13:31 <Lor> But once you have the interface for a datatype, that interface _can_ be implemented by a function.
12:14:26 <esap> lor: ok, but that is not _everything_, I'd say :-)
12:16:20 <Lor> Well, depends maybe on what we mean by "implement".
12:18:49 <det> esap: what do you mean by "a program with two layers
12:18:51 <det>        , where the upper one uses the lower one?"
12:19:45 <esap> det: well I mean the concept used in UML ("is-allowed-to-use"). That is, you have two modules where the upper layer depends on the lower layer, but not vice versa.
12:20:39 <esap> det: in general, to express layers, you need to have some concept of a reference (to refer to "values" stored by the lower layer).
12:20:44 <det> I dont know anything of UML
12:21:55 <esap> det: Layered designs are common in OO languages.
12:22:38 <esap> det: Also, in Haskell, 'import' specifies this kind of dependency.
12:23:00 <det> esap: I still dont understand you but I suspect you mean that it is somehow impossible for the one of the layers to hold some kind of reference?
12:25:06 <esap> det: well, it's more like that lower layers _shouldn't_ depend on the upper layers. That is, one of the modules provides "platform" and the another builds an application based on that platform. The distinction between the platform and the application is the layering.
12:26:00 <esap> det: You wouldn't in that case want that the platform cannot be used for other purposes due to a dependency from the platform to one of the applications (because the platform should be useable for building many different applications).
12:26:04 <det> Yeah, i dont think we are going to come to any kind of understanding
12:26:31 <ski> if the lower level doesn't mention/import the upper level, then it can't possibly depend on it ?
12:27:00 <esap> ski: yes.
12:27:33 <ski> esap : so it is just a matter of not importing the upper into the lower, then ??
12:27:54 <ski> (and making the layer division in a godd way ?)
12:28:29 <esap> ski: yes, basically. The layering expresses the fact that you should not import the upper into the lower.
12:29:59 <ski> esap : when you say references, are you thinking of, e.g. updating the lower level with a new impl., possibly at run-time, and new calls from the upper level automagically calls the new impl. of the lower level ?
12:32:35 <esap> ski: no, I mean that you need to be able to "refer" to entities defined in the lower layer. Those references can be used to indicate some resources and/or definitions defined in the lower layer.
12:33:35 <ski> esap : so you just refer to exported things from the lower layer (?)
12:34:35 <esap> ski: yes, that's the simple case. A more complicated occurs in the OO, where object references can be used to refer to objects storing some state. I guess you could think of IORefs like that.
12:35:03 <esap> ski: in both case, you have an upper layer ("the user"
12:35:14 <ski> esap : mutable references or mutable objects ?
12:35:16 <esap> ski: in both case, you have an upper layer ("the user") using facilities provided by the lower layer ("implementation")
12:35:39 <ski> objects, i presume
12:35:41 <esap> ski: mutable objects.
12:37:29 <esap> ski: and the point was, this kind of structuring is not easy to implement using just functions.
12:37:49 <ski> esap : records ?
12:38:29 <det> esap: not easy or not possible
12:38:34 <esap> ski: Well it depends. Records and universal quantification might actually allow you to express that. But I'm not entirely sure.
12:39:18 <det> hrmm, functions can express anything a record can
12:39:22 <det> just less efficiently
12:39:45 <ski> esap : what are the problems you envisage ?  any small examples ?  do you need records that can contain types, similar to Cayenne ?
12:41:25 <esap> ski: no, I guess nothing that fancy. I guess the most important problem is how to refer to things in upper layers without causing a dependency.
12:41:59 <ski> ?
12:42:23 <ski> i though you not wanted lower levels to refer to things in upper levels ??
12:43:13 <esap> ski: no, the idea is to prevent _dependency_, that is, one part of the system (the lower layer) must not require the existence of a lower layer to work.
12:43:39 <esap> ski: the existence of the upper layer, I mean. You can refer to the entities of upper layers, as long as no dependency is generated.
12:44:03 <ski> not getting it
12:44:19 <ski> what do you mean by dependency, here ?
12:44:33 <esap> ski: For example, id :: a -> a   does not depend on f = id 10, but f does depend on 'id'.
12:44:34 <ski> it seems not to be just a use/refer
12:45:10 <ski> "You can refer to the entities of upper layers, as long as no dependency is generated." ?
12:45:45 <ski> how can lower refer to upper, w/o dep. ?
12:46:21 <esap> ski: Well in OO, callback functions are the most common example of referring to upper layers. The upper layer registers a callback function, which is then called by the lower layer when some event occurs (but the lower layer doesn't know what function it is calling).
12:47:06 <ski> ah, so a dep. is a named use of an entity in another subsystem (?)
12:47:31 <ski> *named*, being the important bit, no ?
12:47:57 <esap> ski: yes. I guess if you need 'import M', then you have a dependency to M.
12:48:11 <ski> and then a refer is just a use of an entity created in another subsystem, right ?
12:48:33 <esap> ski: but actually, the important relation is "is-allowed-to-use", that is, whether you are _allowed_ to make that dependency or not.
12:48:41 <ski> (under whatever variable-name you have as a handle to it)
12:49:24 <esap> ski: yes.
12:49:35 <ski> (i was trying to understand how your usage of refer and depends differed, and what, specifially, you meant by each)
12:49:48 <ski> esap : ok
12:50:40 <ski> esap : so, for starters, the allowed-to-use should not be circular ?
12:50:48 <esap> ski: yes.
12:51:01 <esap> ski: it cannot be circular.
12:51:18 <esap> ski: that is why you can do "upper" vs. "lower" layers.
12:51:21 <Riastradh> ...how do you write recursive functions, then?
12:51:30 <ski> esap : i.e. if you want circular, you instead make one impl. depend on the others spec. and vice versa, right ?
12:51:52 <esap> If you have mutual dependency, then those both modules are at the same layer.
12:51:55 <ski> esap : or is that disallowed, also ?
12:52:06 <ski> ok
12:52:26 <ski> (even if mut. dep. through spec. only ?)
12:53:14 <ski> (specification/interface i.e.)
12:53:44 <esap> ski: well I guess you can have a dependency to an _interface_, but there has to be a possibility to have more than one implementation of that interface, obviously.
12:54:03 <ski> esap : most probably
12:54:42 <esap> ski: Basically, interfaces are a way of decoupling different implementations, so that they do not depend on each other that much. Actually type classes do this quite well, I guess.
12:55:17 <ski> esap : (though, at the class level in most OO langs, each class interface can have only one (direct) implementation (all others are (code-)inherited from it))
12:55:48 <ski> esap : does this has to do with DIP ?
12:55:52 <esap> ski: well you can have multiple implementations of interfaces in most OO languages.
12:56:20 <esap> ski: yes, the dependency inversion principle is the way to use interfaces to decouple the layers.
12:56:57 <ski> esap : Java,C# and O'Caml   any more ?  (of the statically typed ones, i.e.)
12:57:37 <esap> ski: Well in C++, you can also do interfaces (classes with only abstract methods).
12:57:45 <ski> esap : it feels somewhat cleaner in O'Caml
12:58:58 <ski> esap : umm ..  hm  well that perhaps also does it  (i'm not so at home with OO)
12:59:58 <ski> esap : so to implement an interface, you subclass the fully abstract class (?)
13:00:38 <esap> ski: yes. [but I guess even C++ people make a difference between subtyping and subclassing].
13:01:12 <ski> (i haven't thought so much of this, in connection to (code) inheritence and abstract methods)
13:01:53 <esap> ski: implementation inheritance is not usually recommended [it has its uses, but subtyping is the more important relationship]
13:02:14 <ski> i have heard so :)
13:02:42 <esap> ski: and 'implementing an abstract class' is often called (e.g. in UML) the 'realizes' relationship.
13:05:00 <ski> esap : i think i (for some reason) find it cleaner to have a separate concept interface (as in Jave), i.e. separate from the class concept which has implementation.  actually i am thinking along making "classes" (i.e. implementations) not define any implicit interface, at all.
13:06:01 <ski> esap : i.e. the interface is a type of a class, or something like that
13:07:20 <esap> ski: well not quite. Each class is a type. What might be a good idea would be that _references_ could only be made via interfaces, not directly to concrete classes.
13:08:02 <ski> esap : that sounds like what i (think i) mean :)
13:08:16 <ski> (terminology)
13:09:09 <esap> ski: but I think Java does this badly as well, that is, it's very easy to have references to objects by their concrete type.
13:09:26 <ski> esap : yes, but better than C++
13:09:45 <esap> ski: Well I don't really see much difference in C++ and Java in this respect.
13:10:05 <ski> esap : i'm not quite sure of the details of how O'Camls does this
13:10:36 <esap> ski: ML has module signatures.
13:11:15 <ski> esap : well, in Java (and C#) they have created a separate concept, interface, instead of getting it as one extreme special case of more and more abstract classes
13:11:41 <ski> esap : thinking re. the OO system in O'Caml
13:11:54 <ski> esap : not the module system
13:12:13 <det> people actually use the OO system of O'caml ?
13:12:41 <ski> i dunno, i have just played with O'Caml some.
13:12:56 * esap hasn't really used that much. I guess it does have interfaces (signatures of class types).
13:13:03 <phubuh> det, i do.
13:13:25 <ski> i tried to implement (infinite) streams, e.g.
13:13:31 <esap> http://caml.inria.fr/ocaml/htmlman/manual005.html section 3.6 has description how it works.
13:13:41 <ski> (as objects of a stream class)
13:13:58 <phubuh> det, not using it gets really inconvenient since there aren't any type classes
13:15:36 <ski> (re: type classes, i, for some time ago, made up a weird way to implement inductive and coinductive datatypes, with the help of type classes)
13:15:47 <det> phubuh: I decided I prefered modules + closures
13:16:21 <esap> ski: "weird"? Well I have a good interface for both. What kind of interface?
13:16:23 <phubuh> my only objects are stream objects, actually :-)
13:17:00 <phubuh> (well, actually i use them for some other stuff, but i've planned to refactor those instances into closures)
13:17:24 <ski> phubuh : i also tried to make an infinite "sheet", for example for computing pascals triangle
13:17:36 <phubuh> heh
13:17:42 <det> phubuh: http://caml.inria.fr/archives/200308/msg00268.html
13:18:26 <ski> but, i haven't played much with inheritance (or subtyping either), just using it as a way to define a coinductive type.
13:22:29 <esap> The really important paper about inductive and coinductive types is "Poll:Subtyping and inheritance for categorical datatypes" (http://www.cs.kun.nl/~erikpoll/publications/kyoto97.html).
13:22:38 <ski> yes !!! :)
13:22:43 <ski> have read
13:27:44 <ski> also "Poll:Subtyping and Inheritance for Inductive Types" (http://www.cs.kun.nl/~erikpoll/publications/durham97.html) is nice
13:27:50 <esap> yes.
13:29:45 <ski> i especially like the notation for defining coinductive functions and data
13:30:28 <esap> yes. I think it's a bit more verbose than e.g. the notation in C++, but it's nice nonetheless.
13:31:28 <ski> it would fit good into a language with pattern-matching like ML,Haskell
13:31:51 <ski> also, i'm wondering about multi-methods
13:32:52 <ski> the .-syntax doesn't fit as well with that
13:37:07 <ski> it seems like, if we define a multi-method func. then we must fix the numbers of producers (i.e. implementations), to have a chance of covering all cases
13:37:42 <esap> ski: hmm.. why?
13:38:38 <ski> (at least, fix the impl.s accepted by that multi-method)
13:39:30 <ski> esap : well, um, say we have a coinductive type
13:39:58 <ski> codata Gesture = Beats :: Gesture -> Ordering
13:40:25 <ski> perhaps, some other destructors, as well
13:41:47 <ski> scissor,stone,bag :: Gesture
13:42:10 <ski> Beats scissor scissor = EQ
13:42:38 <ski> Beats scissor stone   = LT
13:42:50 <ski> Beats scissor bag     = GT
13:42:51 <ski> ...
13:43:18 <esap> but if you can find structure out of your implementation, then you can avoid this explosion of alternatives. e.g. Beats x y = f x + g y
13:43:27 <ski> (defining Beats on all possible configurations)
13:44:08 <ski> esap : i wasn't currently thinking about the explosion of alternatives ..
13:45:22 <esap> I guess subtyping can anyway be used to limit the number of implementations you need to write.
13:45:35 <ski> the problem is : what if two clients, separately from each other, make their own, new constructor, making sure to define Beats on all known constructors to them.
13:46:04 <ski> and then a third client, downloads these two clients programs, and try to merge them into one
13:46:51 <ski> who's the responsibility to definie what Beats should return over client1,clent2 and vice versa
13:46:56 <esap> Then that third one gets a type error and has to provide the missing implementation.
13:47:35 <ski> yes, we have to make a type error, if we want to avoid this kind of (run-time) error
13:48:34 <ski> i.e. the original implementation of Beats can not just be reused, after it has been extended to handle new cases
13:48:34 <esap> I think that error would be a type error anyway.
13:48:57 <ski> yes, it should, IMO also
13:49:45 <ski> but adding yet another implementation/constructor doesn't (at least naively) make a new interface type
13:50:21 <ski> it looks like just making another implementation of an already existing interface
13:51:20 <ski> (just as making a new function that pattern-matches over the list type does not make a new List type somehow)
13:52:36 <ski> w/o multi-matching (and not using any sub-/super-typing ATM), we seem to have this :
13:53:01 <esap> ski: A multimethod's type should indicate all type combinations that it's applicable. It's very hard to build a multi-method that is applicable to every combination of subtypes.
13:53:37 <ski> each inductive type is characterised by it's fixed set of constructors. you can add how many consumers you like which pattern-matches on it. it doesn't change the type
13:53:47 <esap> ski: unless you have a "default branch" that can be used to implement the function in case none of the cases match.
13:54:42 <ski> each coinductive type is characterised by it's fixed set of destructors. you can add how many producers defined by messege-dispatching you want. it doesn't change the type
13:55:37 <ski> esap : yes. soething similar to multi-methods doesn't occur normally with inductive datatypes
13:56:14 <esap> ski: oh but it does. You can pattern match on two arguments.
13:56:21 <ski> esap : (we'd probably need multi. disj. or coexponentials for seeing the dual problem, i suspect)
13:56:45 <ski> esap : not the same
13:57:10 <ski> esap : for inductive datatypes, the set of constructors is already fixed
13:57:15 <esap> ski: I mean, you need supertyping to actually make realize the same problem.
13:57:44 <ski> the problem manifests itself w/o any sub- or super-typing
13:58:05 <ski> (for multi-methods on coinductive datatypes i.e.)
13:59:41 <ski> esap : for inductive datatypes, we have a closed set of constructors. with coinductive datatypes, we have an open set of producers
14:00:06 <esap> ski: for inductive data types, we have an open set of destructors.
14:00:10 <ski> yes
14:00:33 <ski> but to make the same problem, i suspect, we need mult. disj. or coexponentials :)
14:00:42 <ski> s/same/dual/
14:01:19 <esap> ski: hmm... actually, I guess you might be right, you might need to be able to return more than one result from a destructor or something.
14:02:14 <esap> ski: no, actually, using two different destructors in the body of the function might actually do the same, wouldn't it+
14:03:15 <ski> something like    foo ! (bar ! Hey) = ...     where Hey :: (X >- Foo) -> Bar    and ! is back-application of continuation on function
14:03:35 <ski> i think
14:04:06 <ski> esap : um, how do you mean ?
14:05:07 <esap> I thought of something like   (length,toString) Hey = (10,"hey")
14:06:07 <ski> yeah, that would prolly work too.  then (..,..) is formation of a continuation of mult. disj. type.,no ?
14:06:27 <ski> (the (length,toString) part, i mean)
14:06:27 <esap> well maybe :-)
14:06:50 <esap> It combines two destructors.
14:07:01 <ski> and mult. disj. types seem to be quite rare :(
14:07:15 <ski> (in ordinary programming, i mean)
14:07:56 <esap> I guess it's not that common. But I think it should be as useful as multimethods....
14:08:05 <ski> hmm, i guess :  lawOfExcludedMiddle :: (Not a) # a    where # is mult. disj.
14:08:08 <ski> :)
14:08:26 <ski> also
14:08:46 <ski> foo :: (a -> b) <-> (Not a) # b)
14:09:09 <ski> foo :: (a -> b) <-> ((Not a) # b)        should be
14:09:50 <ski> i was thinking that some iteration could possibly be done with a function returning a # type
14:10:45 <ski> (mind, in a lin. lang., if you case on a 'a # b' typed value, *both* case branches will always get executed)
14:11:14 <esap> I guess case operation is quite a bad thing for a mult. disj. [unless it means forking :-)]
14:11:30 <ski> why ?
14:11:50 <ski> it is the way to destruct it
14:12:14 <esap> what would be the result type of the case operation?
14:12:39 <ski> whatever both the case branches return, i assume
14:12:57 <esap> But you get two values as result.
14:13:00 <esap> Not one.
14:13:42 <esap> Or should case also produce a mult. disj. type as result in that case?
14:14:06 <esap> Or a pair of the results or something?
14:14:06 <ski> the examples i've been able to come up with so far all involve, sooner or later, invoking a continuation that would, i due time, cause the other branch to be entered
14:14:29 <ski> e.g. in lawOfExcludedMiddle
14:15:02 <ski> when we case on this, if first looks like ca "Left ka"
14:15:47 <ski> then, sometimes later, perhaps still in the Left-branch, perhaps after the case-expr, we invoke this ka on something of type a
14:15:49 <ski> the 
14:16:06 <ski> then this a is passed as "Right a" to the second branch
14:16:07 <ski> :)
14:16:46 <ski> that's how it works when it try that in SML/NJ
14:17:25 <ski> (except SML hasn't a linear type-system so i guess (?) that it should be mult.)
14:17:42 <ski> nice ?  :)
14:17:49 <esap> Ok, I agree, but the mult. disjunction stuff was a bit new, I thought you could use normal products for it. Hmmm...
14:17:59 <ski> for what ?
14:18:10 <esap> For dual of multimethods.
14:18:38 <esap> But I guess I was wrong [I didn't much think about the multimethod case].
14:18:50 <ski> if we un-co-curry a nested coexponential, we get an exponential with "result" of mult. disj. type, right ?
14:18:56 <esap> My focus was on thinking about the semantics of coimplication.
14:20:01 <ski> unCocurry :: (a >- b) >- c  |-  a >- (b # c)
14:20:10 <esap> ski: well I thought it like this:   _ \\ A  -|  A \/ _   would be dual to  A /\ _ -| A => _
14:21:23 <ski> (A /\ B) => C  -||-  A => (B => C)     right ?
14:21:23 <esap> ski: and A \\ (B \/ C) == (A\\B)\\C
14:21:39 <ski> yes
14:22:06 <esap> ski: yes.
14:22:07 <ski> so your \\ is my >-
14:22:21 <esap> ski: it's subtraction, I guess it's your >-.
14:22:28 <ski> yes
14:22:42 <esap> The naming is from Crolard:Subtractive logic.
14:23:30 <ski> I don't remember right now if i invented the >- symbol, or if i've seen it somewhere  (i think the previous is true, but not sure)
14:24:24 <ski> Gamma , A |- B ; Delta
14:24:30 <ski> ----------------------
14:24:45 <ski> Gamma |- (A -> B) ; Delta
14:24:50 <ski>  
14:25:07 <ski> Gamma , A |- B ; Delta
14:25:11 <ski> ----------------------
14:25:21 <ski> Gamma , (A >- B) |- Delta
14:25:32 <ski>  
14:25:38 <ski> i used these rules
14:25:44 <esap> http://www.kotiposti.net/epulkkin/summary.dvi
14:26:05 <ski> , is usual meta-mult.disj.bag
14:26:15 <ski> no
14:26:22 <ski> , is usual meta-mult.conj.bag
14:26:29 <ski> : is meta-mult.disj.bag
14:26:47 <ski> symmetric sequent style system, more or less
14:27:48 <ski> i thought >- looked like a good (converse) dual to -> :)
14:28:05 <ski> especially if we write |-  as >-> instead
14:28:54 <esap> ski: that's an interesting approach. I have to think about that.
14:29:04 <ski> e.g.
14:29:09 <ski> A >-> B
14:29:12 <ski> -----------
14:29:21 <ski> 1 >-> (A -> B)
14:29:29 <ski> and
14:29:33 <ski> A >-> B
14:29:36 <ski> -------------
14:29:43 <ski> (A >- B) >-> 0
14:30:01 <ski> 1 is a final object, and 0 is an initial object
14:30:27 <esap> I'm wondering whether your >- and -> are duals or inverses.
14:30:40 <ski> converse dual :)
14:31:15 <ski> -> is dual to -< which is converse to >- which is dual to <- which is converse to ->  ...
14:31:50 <ski> thinka -> b = ~(a * ~b)   a >- b = a * ~ b
14:31:54 <ski> think  a -> b = ~(a * ~b)   a >- b = a * ~ b
14:31:57 <esap> oh ok, then my \\ is not the same, because my \\ is dual of ->.
14:32:25 <ski> sure ?
14:32:31 <esap> I think so.
14:32:58 <esap> Because of the adjunction   _ \\ A -| A \/ _ 
14:33:51 <ski> your \\-E rule in summary.dvi looks like my >- rule about, i think
14:34:34 <ski> esap : i still haven't 'got' adjuctions, just have a vague feeling of what they are yet..
14:35:48 <ski> try removing my Gamma above, then rename my A to Gamma, then rename my B to A, then rename my Delta to B
14:36:05 <ski> Gamma |- A ; B
14:36:11 <ski> --------------
14:36:13 <esap> ski: basically you can model adjunctions with a haskell type class: class (Functor p, Functor s) => Adjunction p s | p -> s, s -> p where { leftAdjunct :: (p t -> b) -> t -> s b ; rightAdjunct :: (t -> s b) -> p t -> b }
14:36:22 <ski> Gamma >- A |- B
14:36:25 <ski>  
14:36:32 <ski> Gamma |- A ; B
14:36:35 <ski> --------------
14:36:36 <ski> Gamma >- A |- B
14:37:21 <ski> and remove your proof terms, and rename my >- to \\ and my ; to \/, innit the same, then ?
14:37:57 <esap> yes, I think \\-E is the same (but it's inverse to the control operation rule, e.g. it's the rule for control application!)
14:39:15 <esap> The adjunction generates both \\-I and \\-E.
14:39:24 <ski> esap : i've seen such diagram about adjuctions, but i haven't developed enough intuitions, seen (and worked through and understood) enough examples and haven't come to the point where i have it memorised, yet :(
14:39:44 <ski> Joopdod : hi
14:39:52 <Joopdod> hi
14:40:40 <esap> ski: Have you read Fokkinga:Adjunctions? it explains adjunctions very well, I'd say.
14:40:46 <ski> esap : you are not sure the \\-I is the same (as mine corresponding) ?
14:41:27 <Joopdod> Yesterday, I had asked something about e-mailfunctions in Haskell, but the only thing that came out of it, was WasMail.
14:41:37 <Joopdod> But that didn't have complete modules.
14:41:41 <Flaggle> anyone know of any good tutorials or websites that discuss parselib and monads?
14:41:56 <ski> esap : i even printed it out on dead-wood some couple of weeks ago, but i didn't come that far..  i'l; have to understand natural transformations, first, i think :)
14:42:39 <Joopdod> No, but there's a tutorial with a module @ http://www.cs.uu.nl/~daan/parsec.html
14:42:48 <Flaggle> thanks
14:43:01 <esap> ski: I think \\-E seems to correspond with your Gamma |- A;B =====> Gamma >- A |- B rule.
14:43:11 <esap> ski: but I'm not sure about the other rule.
14:43:35 <esap> ski: I think that is the rule =>-I.
14:43:58 <ski> esap : yes (1st)
14:44:13 <ski> esap : yes (3rd)
14:44:44 <Joopdod> I forgot how I ever became a member of the freenode network, does anyone knows how to become a member? Although, I am already somehow a member :?
14:44:48 <ski> it is indeed
14:45:37 <esap> ski: hold on, did I miss two rules? :-)
14:45:52 <ski> esap : ?
14:46:11 <ski> you mean the left rule for -> and the right rule for >-  ?  :-)
14:46:26 <Xcalibor> which formats does the image create photo command support without the Img extension?
14:46:31 <Xcalibor> oops
14:46:37 <Xcalibor> :)
14:46:40 <ski> :)
14:46:43 <esap> ski: heh I guess :-)
14:46:54 <Xcalibor> sorry, Tcl question, wrong channel :-P
14:47:10 <ski> esap : well, i hadn't mentioned those yet, because they are harder to remember :)
14:47:17 <ski> letsee
14:47:20 <ski> hm
14:49:54 <Joopdod> Well, I see it meself when I have some time to spare..
14:50:23 <ski> Gamma , B |- A ; Delta
14:50:26 <ski> ------------------------
14:50:35 <Joopdod> Is there a working MySQL databasebinding for Haskell?
14:50:38 <ski> Gamma , (A -> B) |- Delta
14:50:43 <ski> i think
14:51:36 <ski> Joopdod : http://www.volker-wysk.de/mysql-hs/  ???
14:52:41 <ski> hmm
14:53:06 <esap> ski: hmm. what is the semantics of that rule?
14:53:23 <ski> Gamma0 , B |- Delta0   Gamma1 |- A ; Delta1
14:53:23 <Joopdod> it's kind of old, undocumented, dead end... I already knew this one... 
14:53:27 <ski> -----------------------------------------------
14:53:44 <ski> Gamma0,Gamma1 , (A -> B) |- Delta0;Delta1
14:53:50 <ski>  
14:53:56 <ski> rewriting
14:53:57 <ski>  
14:54:05 <ski> Gamma , B |- A ; Delta
14:54:10 <ski> ------------------------
14:54:11 <ski> Gamma , (A -> B) |- Delta
14:54:17 <ski> that was wrong i think
14:54:22 <ski>  
14:54:26 <ski> Gamma0 , B |- Delta0   Gamma1 |- A ; Delta1
14:54:29 <ski> -----------------------------------------------
14:54:32 <ski> Gamma0,Gamma1 , (A -> B) |- Delta0;Delta1
14:54:32 <ski>  
14:54:42 <ski> i think that is the correct one.
14:55:50 <ski> esap : it is easier to think of proof-search a la Prlog or LambdaProlog or Lolli, when trying to get this one right, i think
14:55:55 <esap> ski: yea that looks like function application.
14:55:55 <ski> Prolog
14:56:09 <ski> it is not modus ponens !
14:57:18 <ski> think e.g. on the case where Delta1 = {}
14:57:30 <esap> ski: no not quite, but I think it's basically function composition (think the sequence Gamma0, Gamma1, A, A -> B, B, Delta0, Delta1)
14:58:24 <ski> esap : yes, i serves more or less the purpose of MP, though not doing all things that MP does
14:58:59 <esap> ski: if you look at my rule =>-E, that is also restricted.
14:59:23 <ski> as i said, say Delta1 = {} = Delta and we say that we have sets and not bags, so we also assume Gamma0 = Gamma1 = Gamma
14:59:32 <Joopdod> http://sourceforge.net/projects/htoolkit/
14:59:36 <Joopdod> Database binding...
14:59:40 <ski> then, in proof search, we have the following
15:00:03 <ski> we want to prove (deduce)  Gamma , (A -> B) |- Delta
15:00:16 <ski> or in more prolog syntax
15:00:28 <Joopdod> I didn't test it, but they even have a release and it's an active project. Sorry, for interrupting.
15:00:31 <Joopdod> Bye
15:00:40 <ski> Gamma , (B :- A) |- D                  (we say Delta is just one form. D)
15:01:36 <ski> so, in a prolog-like proof-search, we consider that from the clause (B :- A), we might deduce D
15:02:11 <ski> (e.g. B might be = D  or B might be (D :- A')  or B might be (D & D'))
15:02:32 <ski> so what we must prove then is
15:02:40 <ski> Gamma |- A
15:02:48 <ski> (i.e. the body of the clause)
15:03:04 <ski> and, then, if that succeeded, we try to prove
15:03:11 <ski> Gamma , B |- D
15:03:20 <ski> esap : does that help ?
15:03:35 <ski>  
15:03:36 <esap> ski: Yes, I think I understood.
15:04:49 <ski> esap : re your =>-E, yes it seems restricted in a suitable/sufficient way to more or less mimic my left intro of ->
15:05:28 <ski> esap : though, you have all your proof terms as values
15:06:00 <ski> esap : (and you have both left-intro,right-intro,left-elim,right-elim :)
15:09:48 <ski> esap : your system is somewhat confusing for me to understand because of my last two remarks, and that you don't seem to have Gamma and Delta as sets or bags of formulas, but instead as a single monolithic formula each  :)
15:10:29 <esap> ski: heh, well I've assumed the category-theory convention that contexts and types are not that much different...
15:11:59 <ski> esap : (one could perhaps say that our ->Left / =>-E is related to ordinary MP / =>-E in a similar way as your or-E is related to the usual one (in Natural deduction))
15:13:06 <esap> ski: yes. I might have misused some of the notation :-) I do it all the time...
15:13:11 <ski> esap : yes, i think i understand that. 'tis just that i haven't developed much intuitions/experience about that approach. so it takes longer to understand  :)
15:13:42 <esap> ski: like I guess natural deduction doesn't really like my or-I rule.
15:14:39 <esap> ski: but I wrote it like that for symmetry reasons (it's very nicely symmetric with and-I, and-E and or-E rules.
15:14:47 <ski> esap : re: or-I, i for some time thought that the two judgements/arrows were composed by meta-add.disj., but i now understand that it must be meta-add.conj. in your rule, to make sensne
15:15:24 <ski> esap : well, natural deduction isn't totally symmetrical :)
15:15:44 <ski> esap : sequent calculus is symmetrical, though
15:16:07 * esap always thought sequent calculus has confusing syntax :-)
15:16:15 <ski> esap : natural deduction uses right-intro and right-elim
15:16:54 <ski> esap : sequent calculus uses left-intro and right-intro   (plus *one* symmetric elim. rule)
15:17:20 <esap> right, cut elim.
15:17:49 <ski> esap : what is confusing ?  the logic judgements ?  or symmetrical proof terms ? (i.e. both value and cont ones)
15:17:56 <ski> yes
15:18:17 <esap> the confusing part for me is that the same notation is used for two different things in different side of the |-.
15:18:30 <ski> what ?
15:18:38 <ski> elaborate
15:18:51 <ski> do you mean , ?
15:19:00 <esap> yes.
15:19:32 <ski> oh, that. well, i used ; to the right above.  i don't think it is a very big deal
15:19:45 <ski> both ways have simple consistant interpretations
15:19:54 <esap> yea, I agree, it's not a big thing.
15:20:22 <ski> if we have , on both sides, then , means set (or bag) union
15:20:23 <andersca> ski: are you still at mdstud?
15:20:31 <ski> andersca : yes
15:20:37 <andersca> neat :)
15:20:48 <ski> andersca : what about it ? :)
15:21:18 <andersca> just wondering
15:21:53 <ski> esap : so it is the side of the |- which determines if the set (bag) should be interpreted conjuntively or disjunctively
15:22:41 <esap> ski: yea, the confusing part is, what is the syntax if I want conjunctively interpreted bag on the right side?
15:23:05 <ski> there is none, in standard sequent calculus
15:23:31 <ski> only one meta-connective (plus neutral element) on each side, respectively
15:24:02 <ski> i guess one could try display logic or something similar, if one wants that
15:25:16 <ski> esap : http://citeseer.nj.nec.com/curien00duality.html  is quite good, i think
15:25:22 <esap> ok, I guess you could do that, but then you'd want a similarly symmetric notation.
15:25:35 <ski> esap : "The Duality of Computation", is the title
15:25:43 <ski> for what ?
15:25:52 <esap> ski: yes, I agree (I've read it). Also have you read the Wadler's paper on duality?
15:26:48 <ski> no, but i have printed it out
15:26:55 <ski> :/
15:27:01 <esap> ski: I mean a symmetric notation for the logic display on wrong side of the |-.
15:27:11 <Maddas> If printing were just enough, that would be nice :)
15:27:17 <Maddas> err, if just printing were enough
15:27:19 <ski> :)
15:27:53 <ski> esap : youv'e heard of display logic ?
15:28:21 <esap> oh :-)
15:28:59 <ski> esap "Relevant and Substructural Logics (2001)",Greg Restall,http://citeseer.nj.nec.com/restall01relevant.html , talks some about it, and about relevant and linear and other sub-structural logics  :)
15:30:18 <esap> ski: I haven't seen that paper. Thanks for the link.
15:31:07 <ski> no prob :)
15:32:10 <ski> esap : i've actively been thinking of using two meta-connectives on the left for a linear type system. i think this would solve a small problem i thought of re lin. type-systems :)
15:33:01 <esap> ski: I haven't yet got to the point where I could start thinking about linear logic. :-)
15:33:41 <ski> The "Relevant anf ..." paper is quite large, so i haven't actually read it all. just mostly looked through the first half about syntax, and a little in the second part about semantics (models)
15:33:53 <ski> :)
15:34:17 <esap> ski: My point of view is I'm trying to model message passing, OO and functional programming in the same language.
15:34:44 <ski> this paper explains some about why many connectives split into two, when going from classic/intuitionistic logic to a sub-structural one
15:35:56 <ski> message passing, how ?  concurrent between processes ?   message/methods in objects/coinductive data ?
15:36:58 <esap> ski: there is a programming paradigm where you have state machines that send messages to each other.
15:37:00 <ski> (i saw some article somewhere where they applied mult. and add. conj. and disj. to concurrency)
15:37:06 <esap> ski: each state machine is modelled as an object, but represents a collection of objects.
15:37:16 <ski> hmm
15:37:44 <ski> how, represents a collection of objects ?
15:38:01 <ski> is the messages asynch. or synch. ?
15:38:06 <esap> ski: I mean, the state machine determines what messages are allowed. This is contrast to the OO ideology where you have each method of an interface can be independently invoked.
15:38:27 <esap> ski: ... as long as you have a reference to that interface.
15:38:34 <esap> ski: asynchronous messages.
15:40:31 <esap> ski: so actually, when you combine with OO, each _state_ of the state machine has to be mapped to one (OO-like) interface
15:41:11 <ski> hmm, didn't some erlang guy (Ulf Wiger ? Joe Armstrong ? Viding ?) talk about something like that on c.l.f for some time ago ?
15:41:23 <esap> ski: Yes, that was very relevant for that.
15:41:35 <ski> i can't find it ATM, though
15:41:56 <esap> ski: erlang does actually implement this kind of message passing.
15:42:42 <esap> ski: My current problem is, how do I integrate layering (from OO-design) into functional programming.
15:43:41 <esap> ski: because that will, I think, solve one of the problems with message-passing based programning.
15:44:27 <esap> ski: which has limited mechanisms for controlling dependencies.
15:45:49 <esap> ski: I actually do have a partial solution, but I think it's not the whole story.
15:46:09 <ski> back from google groups archive :)
15:47:11 <ski> esap : but you still send a message to a State Machine, no ?
15:47:22 <esap> ski: yes.
15:47:42 <ski> esap : and it handles it, if it is in an appropriate state (possibly changing state as a result)
15:48:13 <esap> ski: yes. But one message may also have multiple actions that it requests the receiver to perform.
15:48:27 <ski> ?
15:49:00 <esap> ski: messages are fundamentally larger things than methods.
15:50:05 <ski> in normal OO/coinductive data message is interface of (possibly many) methods, right ?
15:50:13 <esap> ski: a message describes a whole sequence of operations. The receiver can split that into parts and divide the responsibility to perform those operations.
15:50:18 <ski> esap : do you mean in another sense here ?
15:50:30 <ski> hmm
15:51:08 <esap> ski: message is a piece of data that is sent to the state machine.
15:51:33 <ski> yes, i'm with you so far
15:51:46 <ski> how is the message structured
15:51:57 <ski> is it just isomorphic to a tuple of args ?
15:52:21 <ski> (plus one message name/descriptor/id)
15:52:32 <esap> ski: well almost, but as part of the message, you can have 'requests for changes'. And there can be many of those in each message.
15:53:19 <esap> ski: so even if messages are named, they are not named according to the operation that should be performed.
15:53:27 <ski> so message ~= [(MessageId,Args)]   ??
15:54:02 <ski> esap : they are named according to what has already happened ? :)  (H.S.Lahman)
15:54:03 <esap> yes, that's quite close. Also args for different MessageId may overlap.
15:54:44 <esap> ski: preconditions and postconditions are I guess one way of naming them [I've seen many conventions though]
15:55:39 <ski> (H.S.Lahman on comp.objects seemed to advocate a pre-condition naming)
15:56:56 <ski> how is this ~List~ handled ? can some parts of it be executed while others wait until FSM is in right state ?  or must all execute at (roughtly) the same time ?
15:57:36 <ski> and if the first case, are they tried in order, or non-deterministically ?
15:57:54 <esap> ski: well usually it first goes to one piece of code, then that does something, and forwards the message onwards, which then becomes responsible for doing the rest.
15:58:34 <esap> ski: and this way you can split different operations to different locations.
15:59:06 <ski> um, hmm
15:59:46 <esap> ski: of course, some principle should be used to determine what is done at each place.
16:01:04 <esap> ski: Each of those places has a separate state machine.
16:01:34 <esap> ski: if a message has an instruction that the state machine doesn't allow, then the whole message is rejected.
16:02:05 <ski> hmm, not sure i get it
16:03:01 <esap> ski: The message structure is more like "Bag (MessageId,Args)", and each part takes those parts from the bag that they can perform.
16:03:18 <ski> i'm not sure what a part is
16:03:40 <esap> ski: part is one instance of (MessageId,Args).
16:03:55 <ski> doesn't allow = doesn't allow in current state  or doesn't allow in any state ?
16:04:08 <esap> ski: in current state.
16:04:12 <ski> ok
16:05:06 <ski> is these parts executed sequentially or parallell somehow or a non-deterministic sequentialized version of concurrent
16:05:08 <ski> hmm
16:05:52 <esap> ski: well usually things in one message is executed sequentially, but there can be lots of such messages in the system at any given time.
16:06:17 <esap> ski: although there are cases where the operation can be optimized by performing things in parallel.
16:06:51 <esap> ski: And I feel somehow that this kind of thing is best when the parallelism is utilised.
16:10:00 <ski> ok, hm
16:10:09 <esap> ski: When I talked about layering, this message passing thing is what happens _in_ one layer.
16:10:19 <ski> ok
16:10:40 <esap> ski: but between layers, things are much different.
16:10:49 <ski> oh ?
16:12:41 <esap> ski: because when you have layers, there is one piece of code that commands what another must do. In the message passing scenario, a message sender doesn't know who will perform the things described in the sent message.
16:13:24 <esap> ski: And my understanding of OO design is that you always have "client-server" like relationships between objects [one object tells what another must do!]
16:13:37 <ski> so the above level sets up the FSMs and connects them ?
16:14:30 <esap> ski: yes. It can control the operation of the whole collection of FSMs.
16:15:03 <esap> ski: well ok, that is one view. You can also put the OO design inside the FSMs.
16:15:45 <esap> ski: e.g. each state in the state machine is one object [at least close to that, I'm not sure about the details there]
16:16:05 <ski> hm
16:19:50 <esap> But as a whole, I think combining these paradigms will produce some very interesting possibilities.
16:22:10 <ski> i have mostly been thinking of a lang with both ind. and coind. datatypes
16:22:57 <ski> i.e. not very much about modules,layers and processes concurrency
16:23:02 <esap> In my thinking, inductive data types are required for describing messages and coinductive data types are required for describing state machines.
16:24:07 <ski> In my thinking messages are destructors on coinductive data (prolly synch. messages)
16:25:08 <ski> i.e. a message don't have to exists as a single first-class entity at all  (c.f. with messages as objects in smalltalk)
16:25:18 <esap> ski: that's true as well, but that relates to how to set up and clean up 'objects'.
16:25:27 <ski> yes
16:25:51 <esap> ski: messages can be used to both create and destroy objects.
16:26:16 <ski> i.e. i'm thinking of the message in the message/method dichotomy that some OO people seem keen to keep separated
16:26:22 <esap> that is, a state machine can create an object in response to receiving a message.
16:27:23 <ski> well, of course as part of the action associated with the message for that particular state in the FSM, we can construct objects ..
16:30:23 * esap has to get some sleep (it's 2:30 am here)
16:30:38 <ski> 1:30 here
16:31:17 <ski> so i'll prolly go home and get some sleep as well ..
16:31:24 <esap> see you later.
16:31:27 <ski> g'd night
17:03:22 <shapr> @yow
17:03:23 <lambdabot> ..  My vaseline is RUNNING...
17:52:40 <blackdog>  @yow
17:52:45 <blackdog> @yow
17:52:45 <lambdabot> You mean now I can SHOOT YOU in the back and further BLUR
17:52:45 <lambdabot>  th' distinction between FANTASY and REALITY?
21:05:20 --- mode: orwell.freenode.net set +b *!23kjhkjf@*
22:03:16 <reffie_> hi..
22:54:30 <Pseudonym> @yow
22:54:31 <lambdabot> While you're chewing, think of STEVEN SPIELBERG'S
22:54:31 <lambdabot>  bank account..  This will have the same effect as
22:54:31 <lambdabot>  two ``STARCH BLOCKERS''!
