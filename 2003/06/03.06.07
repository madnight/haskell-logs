01:32:30 <jlouis> hmmm
03:36:50 <wli> IBM has pissed me off
03:36:54 <wli> er
03:43:58 <jlouis> ehm?
03:45:14 <wli> ignore that
03:53:46 * seth is away: Back soon
04:05:44 <jlouis>  /kick seth learn to use /away ;)
06:36:20 <shapr> has anyone here tried HSHGL 3.00 ?
06:51:11 <Igloo> Is it GHC 6.0.0 or just 6.0?
06:54:19 <Igloo> Looks like the latter - they'd changed it more than I'd realised
06:55:28 <d33p> what the hell does GHC mean?
06:55:45 <Igloo> Glorious Haskell Compiler
06:56:03 <d33p> how lame
06:57:09 <Riastradh> I thought it was the Glorious Glasgow Haskell Compilation System.
06:57:35 <Riastradh> ...shortened to the Glasgow Haskell Compiler, acronym'd to GHC.
06:57:44 <d33p> thanks Riastradh :)
06:58:10 <Igloo> Hmmm, the webpage doesn't agree with me, but I'm sure it was meant to have been renamed a while ago
07:02:03 <Riastradh> Where can one read about type lambdas in GHC?
07:12:27 <Riastradh> @fact twistedhaskell
07:12:28 <lambdabot> Nothing
07:12:33 <Riastradh> @fact TwistedHaskell
07:12:33 <lambdabot> Nothing
07:12:36 <Riastradh> Curses.
07:12:38 * Riastradh pokes shapr.
07:13:48 <shapr> eh?
07:13:56 <shapr> oh
07:14:14 <Riastradh> Where is it?
07:14:29 <shapr> it's in the twisted cvs repo, top level, checkout TwistedHaskell
07:15:48 <shapr> I haven't looked at it lately.
07:40:53 <shapr> hey Arnia 
07:41:14 <shapr> how would you do automatic persistence ZODB-style?
07:41:15 <Arnia> heya
07:41:30 <Arnia> shapr: Recursive arrows holding a property sheet
07:44:11 <shapr> hm
07:45:11 <Arnia> *shrugs* :)
07:46:23 * shapr thinks about that
07:53:20 <Arnia> The main problem would be the cycles eaten up reducing these expressions when nothing has changed. We could encode them as SFConsts until a change event arrives at which point we swap in a new SFConst (a misuse of SFConst I know, but a useful misuse)
08:00:01 <shapr> Arnia: could you include a static changed property?
08:00:45 <Arnia> shapr: As part of the property sheet datatype? I think so, yes
08:34:34 <shapr> is there such thing as a denotational semantics tutorial?
08:35:54 <Arnia> If there is, let me know where I might find it :)
08:36:01 <shapr> :-)
08:36:09 <Arnia> There are a couple of good books online. I'll try and dig them out for you
08:36:19 <shapr> cool, I'd appreciate that.
08:39:51 <shapr> I have a strange idea..
08:40:05 <Arnia> go on :) I like strange ideas
08:40:21 <shapr> I'd like to put a lot of these academic papers I've been reading onto the wiki, and allow people to attach notes to them.
08:40:43 <Arnia> That's very cool...
08:41:06 <shapr> I'd like to be able to put questions at certain points in a paper
08:42:16 <shapr> or answer questions from other people
08:42:38 <Arnia> XMLise the papers? ;)
08:43:00 <shapr> I guess I'd need permission from the authors
08:43:31 <shapr> when I print out a paper I make lots of notes in the margins
08:43:50 <shapr> when I go back and reread that paper I often have answers to the questions I've scribbled in the margins.
08:45:47 <Arnia> That's a good technique... I think I'll start doing that :)
08:45:57 <shapr> it's really handy
08:46:21 <shapr> sometimes I find really brilliant notes I've made that didn't make much sense to me at the time.
08:46:56 <shapr> I'd like to wikify that process, since that would also make it multi-user
08:47:20 <shapr> though it's more difficult to doodle drawings in a wiki
08:49:03 <Arnia> shapr: Isn't there a document annotation system around already?
08:49:23 <shapr> PDF has some annotation features
08:50:18 <shapr> I'd prefer to upload PDF and PostScript to a Wiki page so the original content could be separate and the annotations hidden.
08:51:13 <Arnia> Yeah... but wasn't there a through-the-web one... I'm sure the W3C had something about it
08:58:10 <Arnia> anyway... have to dash now. I'll speak soon
09:40:52 * d33p is away: Zzzz
10:11:51 <shapr> so, in relation the Module Monad Interpreters paper by Liang, Hudak, and Jones...
10:12:07 <shapr> were constructor classes added to Haskell after this paper was written?
10:30:13 <Janni> hi
10:30:41 <shapr> hi Janni, what's up?
10:31:05 <Janni> Trying to find a good solution for some standard function
10:31:51 <Janni> I don't know if the terminology is correct when speaking of the "profile" of two sets
10:32:13 <wli> boundary?
10:33:22 <Janni> I mean roughly the same of the result of "intersect" but with two Lists
10:33:53 <wli> Janni: List.intersect did that already AFAIK
10:34:05 <shapr> the intersection of two sets?
10:34:29 <wli> He said he wanted it on lists (or did he misspeak?)
10:34:52 <shapr> probably no
10:34:54 <shapr> t
10:34:54 <Janni> What I want is [ x | x <- l1, y <- l2, x == y ] but I want, that with l1=[1,2,3,3,3] and l2=[1,2,3,3] results in [1,2,3,3]
10:35:12 <Janni> I want it on lists
10:35:15 <wli> oh easy
10:35:33 <Janni> I did it like this
10:35:33 <Janni> profile1 [] _ = []
10:35:33 <Janni> profile1 _ [] = []
10:35:33 <Janni> profile1 (x:xs) ys
10:35:33 <Janni>         | x `elem` ys = x : profile1 xs (ys \\ [x])
10:35:33 <Janni>         | otherwise   = profile1 xs ys
10:35:35 <wli> you basically want the minimum multiset count
10:35:57 <Janni> (profile is the word I found in a dictionary for "Querschnitt", that's why)
10:36:11 <Janni> But I don't like that solution...
10:37:10 <wli> mkCounts = map (\xs -> (head xs, length xs) . group
10:37:47 <wli> minCounts [] ys = []
10:38:39 <wli> minCounts (x:xs) ys = case lookup $ fst x in { Nothing -> minCounts xs ys ; Just n -> (fst x, min n $ snd x) : minCounts xs ys }
10:39:17 <wli> profile xs ys = mkCounts xs `minCount` mkCounts ys
10:40:14 <Janni> Ah, yes I see.
10:40:24 <Janni> Should be faster than mine...
10:40:29 <wli> oh
10:40:34 <wli> I forgot to re-expand
10:41:28 <wli> countExpand = map (\(x,n) -> take n $ repeat x)
10:42:36 <wli> minCounts (x:xs) ys = case lookup $ fst x in { Nothing -> [] ; Just n -> (fst x, snd x `min` n) } : minCounts xs ys
10:43:04 <wli> profile xs ys = countExpand $ mkCounts xs `minCount` mkCounts ys
10:43:14 <wli> ergh!
10:43:23 <wli> countExpand = concatMap (\(x,n) -> take n $ repeat x)
10:48:11 <wli> countExpand = concatMap (uncurry $ flip replicate)
10:48:40 * wli yearns for the day -ffunction-sections works
10:48:50 <shapr> ?
10:48:51 <Janni> Hmm. Your's doesn't preserve the order...
10:49:03 <wli> Janni: it follows the order of the first
10:49:33 <Janni> Ah, yes I see.
10:58:22 <Janni> wli: Your program doesn't work correctly
10:58:24 <Janni> profile [1,3,3,2,3] [1,2,3,3,4] => [1,3,3,2,3]
11:04:38 <Janni> You can't do it with a group on an unsorted list.
11:05:55 <wli> Janni: oic
11:06:37 <wli> well that's easily fixed
11:07:53 <Janni> Organization of Islamic conferences, Office of the Information Commissioner, Office Innovation Center, Open Interchange Consortium
11:08:03 <Janni> What of those above could "oic" mean?...
11:08:11 <wli> Oh, I See
11:08:43 <Janni> Ah, nearly...
11:10:23 <Janni> I wonder if my solution is really as inefficient as I thought...
11:13:08 <wli> Now I have:
11:13:31 <wli> mkCount :: (Eq t, Ord t) => [t] -> FiniteMap t Int
11:13:31 <wli> mkCount = foldr (\x fm -> addToFM_C (+) fm x 1) emptyFM
11:13:34 <wli> minCount :: (Eq t, Ord t) => FiniteMap t Int -> FiniteMap t Int -> FiniteMap t I
11:13:35 <wli> nt
11:13:35 <wli> minCount = plusFM_C min
11:13:38 <wli> countExpand :: [(t,Int)] -> [t]
11:13:38 <wli> countExpand = concatMap (uncurry $ flip replicate)
11:13:41 <wli> profile :: (Eq t, Ord t) => [t] -> [t] -> [t]
11:13:42 <wli> xs `profile` ys = countExpand . fmToList $ mkCount xs `minCount` mkCount ys
11:14:54 <wli> Janni: looks to be immune to order problems now
11:15:15 <Janni> Hmm. Is it much more efficient than my solution? 'Cause it doesn't look so nice...
11:15:26 <wli> Janni: try it
11:15:35 <wli> except I think there's a bug for mincounts of 0
11:17:48 <Janni> Mine is "profile", your's "profile'":
11:18:00 <Janni> moment...
11:18:14 <Janni> Main> length $ profile [1..1000] [1..1000]
11:18:15 <Janni> 1000
11:18:15 <Janni> (62030 reductions, 92058 cells)
11:18:15 <Janni> Main> length $ profile' [1..1000] [1..1000]
11:18:15 <Janni> 1000
11:18:15 <Janni> (1697659 reductions, 3109269 cells, 14 garbage collections)
11:18:17 <Janni> Main> length $ profile [1..1000] [1000..1]
11:18:19 <Janni> 0
11:18:21 <Janni> (51 reductions, 80 cells)
11:18:23 <Janni> Main> length $ profile' [1..1000] [1000..1]
11:18:25 <Janni> 1000
11:18:27 <Janni> (837604 reductions, 1530159 cells, 6 garbage collections)
11:18:35 * Janni is wondering
11:18:50 <wli> yes that's the bug
11:19:43 <Janni> Sorry, I can't debug it, because I don't know that FiniteMap stuff...
11:20:07 <wli> I can debug it
11:24:33 <Janni> Hmm, could it be that my solution is O(n^2)? (I thought I'd be worse at first)
11:25:00 <wli> Janni: okay, try this:
11:25:06 <wli> module Profile where
11:25:06 <wli> import List
11:25:06 <wli> import FiniteMap
11:25:10 <wli> mkCount :: (Eq t, Ord t) => [t] -> FiniteMap t Int
11:25:10 <wli> mkCount = foldr (\x fm -> addToFM_C (+) fm x 1) emptyFM
11:25:26 <wli> minCount :: (Eq t, Ord t) => FiniteMap t Int -> FiniteMap t Int -> FiniteMap t Int
11:25:26 <wli> fm1 `minCount` fm2 = foldFM (\k e r -> case fm2 `lookupFM` k of { Nothing -> r ; Just e' -> addToFM r k (e `min` e') }) emptyFM fm1
11:25:30 <wli> countExpand :: [(t,Int)] -> [t]
11:25:31 <wli> countExpand = concatMap (uncurry $ flip replicate)
11:25:34 <wli> profile :: (Eq t, Ord t) => [t] -> [t] -> [t]
11:25:35 <wli> xs `profile` ys = countExpand . fmToList $ mkCount xs `minCount` mkCount ys
11:25:38 <wli> zs :: [Char]
11:25:38 <wli> zs = ['c','b','a','c','c'] `profile` ['c','b','a','c','d']
11:26:06 <wli> Janni: probably screws up order
11:26:52 <wli> Janni: I'd reassemble order by tagging them with their position
11:28:12 <Janni> But still, my solution seems to be very efficient (much more than yours). I'm beginning to doubt that my program is correct...
11:28:20 <cale|afk> Janni: profile1 looks about O(n^2) to me.
11:29:18 <Janni> Yes, but an O(n^2) which rather tends to Theta(n log n) in most cases...
11:30:09 <cale> Yeah.
11:30:41 <Janni> So, not bad at all. But I'm trying to understand why wli's solution is so inefficient...
11:31:56 <Janni> Btw: Is "profile" a appropriate function name for this? I guess not, so you could give me a better one...
11:32:23 * Janni doesn't know English math terminology
11:33:34 <cale> Hmm... the first thing I think of (definitely nonstandard, and probably a bad name) is listgcd.
11:33:57 <Janni> ... where GCD stands for ...
11:34:12 <Riastradh> Greatest Common Denominator.
11:34:16 <cale> yeah
11:34:47 <wli> Janni: how is it supposed to handle the unsorted case?
11:34:52 <wli> e.g.
11:35:03 <wli> zs = ['c','b','a','c','c'] `profile` ['c','b','a','c','d']
11:35:16 <cale> If you put two prime factorisations in, if it's doing what I think it's doing, it should give the prime factorisation of the GCD.
11:35:18 <Janni> "cbac"
11:37:30 <cale> Profile isn't a taken name, afaik anyway, and seems decent for this purpose.
11:41:10 <Janni> So it will go into my personal GeneralFunctions module like this:
11:41:12 <Janni> profile [] _ = []
11:41:12 <Janni> profile (x:xs) ys
11:41:12 <Janni>         | x `elem` ys = x : profile xs (delete x ys)
11:41:12 <Janni>         | otherwise   = profile xs ys
11:42:41 <Janni> Of course I appreciate your effort, wli...
11:43:45 <wli> Looks like the unsorted case's handling mandates O(n^2) or worse
11:44:00 <wli> I'd personally rather just have the counts
11:44:05 <wli> frequency counts that is
11:44:32 <wli> foldr (\x -> addToFM_C (+) fm x 1) emptyFM
11:44:32 <wli> and
11:44:55 <wli> plusFM_C (+)
11:45:20 <wli> possibly tagging with list names
11:45:43 <Janni> Hmm. The worst case would be "profile [a..x] [x..a]", I guess. And it should be O(n^2)...
11:47:01 <wli> I guess I can extend my stuff to go even further and keep the set of all positions the stuff was found at
11:47:05 <cale> heh - here's the stupid way: convert the multisets to numbers and a mapping from primes to list elements, then take the numeric gcd, and map back. :)
11:47:16 <wli> and make some arbitrary decisions
11:47:42 <wli> well I wasted a little time
11:47:54 <wli> looks like I should get back to hacking up a kernel
11:48:19 <Janni> cale: I must admit, that I can't really follow the prime factorisation parallel...
11:48:25 <Janni> *thinking*
11:48:40 <wli> each element corresponds to a prime
11:48:51 <wli> the number of occurrences corresponds to its power
11:49:07 <wli> a factor of p^n == element corresponding to p occurring n times
11:49:14 <wli> it loses order also
11:49:56 <wli> I went as far as this:
11:50:04 <wli> mkCount :: (Eq t, Ord t) => [t] -> FiniteMap t (Int,Int)
11:50:15 <wli> mkCount [] = emptyFM
11:50:15 <wli> mkCount xs@(_:_) = mkCount' xs 0
11:50:15 <wli>         where
11:50:15 <wli>                 mkCount' :: (Eq t, Ord t) => [t] -> Int -> FiniteMap t (Int,Int)
11:50:15 <wli>                 mkCount' [] _ = emptyFM
11:50:16 <wli>                 mkCount' (x:xs) n = addToFM_C (\(k,p) (k',p') -> (k + k', p `min` p')) (mkCount' xs (n+1)) x (1,n)
11:50:20 <wli> minCount :: (Eq t, Ord t) => FiniteMap t (Int,Int) -> FiniteMap t (Int,Int) -> FiniteMap t (Int,Int)
11:50:21 <wli> fm1 `minCount` fm2 = foldFM (\k (e,p) r -> case fm2 `lookupFM` k of { Nothing -> r ; Just (e',p') -> addToFM r k (e `min` e', p) }) emptyFM fm1
11:50:21 <cale> i.e. [2, 2, 3, 3] <-> 36 ; [2, 3, 7] <-> 42, gcd 36 42 = 6 <-> [2, 3]
11:50:27 <wli> countExpand :: [(t,(Int,Int))] -> [t]
11:50:28 <wli> countExpand = concatMap (uncurry $ flip replicate) . map (\(x,(n,_)) -> (x,n)) . sortBy (\(_,(_,k)) (_,(_,k')) -> k `compare` k')
11:50:32 <wli> profile :: (Eq t, Ord t) => [t] -> [t] -> [t]
11:50:32 <wli> xs `profile` ys = countExpand . fmToList $ mkCount xs `minCount` mkCount ys
11:50:42 <wli> that's still not good enough
11:50:46 <Janni> whew!
11:50:58 <wli> it needs to keep the set of all positions the things were found at and reassemble the list according to that
11:51:48 <cale> The simple solution is often better still simply because you can come back to it and understand what it's doing in 2 seconds.
11:52:25 <Janni> But can be the O(n^2) barrier be undercut anyway?
11:52:31 <Janni> s/anyway/at all
11:52:52 <wli> well
11:52:54 <wli> one can do:
11:53:14 <Janni> On unsorted lists of course...
11:53:49 <wli> I'm banging out the position-recording crud
11:53:54 * Janni wonders if he made correct use of the word undecut...
11:54:00 <cale> yeah
11:54:49 <cale> Unless you want to assume certain things about the lists, I don't see an algorithm doing better than O(n^2).
11:55:11 <Smerdyakov> Algorithm for what?
11:55:54 <cale> Smerdyakov: essentially the intersection operation on multisets implemented as unsorted lists.
11:56:04 <Janni> I sent him the code...
11:56:58 <cale> If they were integer lists, there are algorithms that can sort integer lists very quickly iirc.
11:57:03 <Smerdyakov> Looks like it gives a set as the output.
11:57:03 <wli> mkCount :: (Eq t, Ord t) => [t] -> FiniteMap t (Int,[Int])
11:57:09 <wli> mkCount [] = emptyFM
11:57:10 <wli> mkCount xs@(_:_) = mkCount' xs 0
11:57:11 <Smerdyakov> Not a multiset.
11:57:18 <wli>         where
11:57:18 <wli>                 mkCount' :: (Eq t, Ord t) => [t] -> Int -> FiniteMap t (Int,[Int])
11:57:18 <wli>                 mkCount' [] _ = emptyFM
11:57:18 <wli>                 mkCount' (x:xs) n = addToFM_C (\(k,ps) (k',ps') -> (k + k', ps ++ ps')) (mkCount' xs (n+1)) x (1,[n])
11:57:24 <wli> minCount :: (Eq t, Ord t) => FiniteMap t (Int,[Int]) -> FiniteMap t (Int,[Int]) -> FiniteMap t (Int,[Int])
11:57:24 <wli> fm1 `minCount` fm2 = foldFM (\k (e,ps) r -> case fm2 `lookupFM` k of { Nothing -> r ; Just (e',ps') -> addToFM r k (e `min` e', ps ++ ps') }) emptyFM fm1
11:57:30 <wli> countExpand :: [(t,(Int,[Int]))] -> [t]
11:57:35 <cale> Smerdyakov: delete x ys only removes the first instance of x from ys.
11:57:35 <wli> countExpand = map fst . sortBy (\(_,p) (_,p') -> p `compare` p') . concatMap (\(x,(n,ps)) -> zip (replicate n x) $ sort ps)
11:57:42 <wli> profile :: (Eq t, Ord t) => [t] -> [t] -> [t]
11:57:42 <wli> xs `profile` ys = countExpand . fmToList $ mkCount xs `minCount` mkCount ys
11:57:54 <wli> Janni: how's that do?
11:58:30 <wli> cale: sort, group, deleteBy
11:59:17 <cale> wli: huh?
11:59:20 <Smerdyakov> wli... how about paste a URL to the code instead next time??
12:00:13 <wli> cale: deleteBy (\y -> snd y == x) . group . sort
12:00:44 <cale> wli: I was explaining why Janni's code should return a multiset.
12:02:55 <wli> Well I banged out the bits to make it preserve position I guess
12:04:21 <wli> Profile> profile [1..10] $ reverse [1..10]
12:04:21 <wli> [1,10,2,9,3,8,4,7,5,6]
12:04:30 <wli> Janni: Is that what you expect?
12:05:09 <wli> hmm
12:05:23 <wli> I could probably do it for an arbitrary number of lists in O(n*lg(n))
12:05:56 <wli> profile :: (Eq t, Ord t) -> [[t]] -> [t]
12:06:00 <wli> or something
12:06:17 <cale> wli: Janni's implementation gives [1,2,3,4,5,6,7,8,9,10]
12:06:30 <Janni> Yes, that's right.
12:06:31 <wli> crap
12:09:35 <wli> okay that's a one liner
12:09:42 <Janni> For my special purpose it could have any order, and it could be slow, but this discussion has gone far beneath my origin intentions...
12:09:42 <Janni> s/origin/original
12:09:43 <wli> minCount :: (Eq t, Ord t) => FiniteMap t (Int,[Int]) -> FiniteMap t (Int,[Int])
12:09:43 <wli> -> FiniteMap t (Int,[Int])
12:09:43 <wli> fm1 `minCount` fm2 = foldFM (\k (e,ps) r -> case fm2 `lookupFM` k of { Nothing -
12:09:43 <wli> > r ; Just (e',ps') -> addToFM r k (e `min` e', ps ++ (map (+(foldr1 max ps)) ps
12:09:43 <wli> ')) }) emptyFM fm1
12:09:58 <wli> GRRR linewrap
12:11:21 <Janni> wli: _Plain_ ugly, but nice :)
12:11:24 <wli> I'm not sure if that breaks n*lg(n)
12:11:30 <wli> I think it doesn't
12:11:41 <wli> actually it's no worse than before
12:11:50 <wli> ++ is already pretty bad
12:12:04 <wli> one would need a rank-ordered tree to do better
12:12:26 <wli> there isn't one handy in the libs
12:12:27 <cale> I always hear people complaining about ++, but is it really that bad with lazy lists?
12:13:26 <wli> Janni: incoming DCC
12:14:07 <wli> Janni: if that doesn't DTRT I can twiddle some more
12:14:26 <wli> but I think it's already complex enough you may not be interested in further updates if that screws up
12:14:38 <wli> (though if you are do say so)
12:15:37 <Janni> Interested... well, I don't _need_ a function like this (right now :)
12:15:49 <wli> Janni: is it faster/slower?
12:16:28 <Janni> For profile [1..1000] [1000..1] it's still much slower
12:16:47 <Janni> (26055 reductions, 37092 cells) vs. (792612 reductions, 1485176 cells, 6 garbage collections)
12:17:31 <wli> Janni: you realize the second list is empty?
12:17:49 <Janni> nope :)
12:18:00 <Janni> fuckit! :)
12:18:04 <wli> Profile> [1000..1]
12:18:04 <wli> []
12:18:19 <wli> Janni: I see here:
12:18:20 <wli> Profile> profile' [1..1000] [1000..1]
12:18:21 <wli> []
12:18:21 <wli> it :: [Integer]
12:18:21 <wli> (0.08 secs, 1425476 bytes)
12:18:21 <wli> Profile> profile [1..1000] [1000..1]
12:18:22 <wli> []
12:18:24 <wli> it :: [Integer]
12:18:26 <wli> (0.09 secs, 1705808 bytes)
12:18:36 <steveh> ++ is bad if you use foldl ...
12:18:42 <steveh> IIRC
12:18:50 <wli> (1.74 secs, 9368600 bytes)
12:18:51 <cale> @prelude ++
12:18:53 <lambdabot> *** "++" prelude "Haskell Standard Prelude Dictionary": text follows
12:18:53 <lambdabot> ++
12:18:53 <lambdabot>   See (++)
12:18:58 <cale> @prelude (++)
12:19:00 <lambdabot> *** "(++)" prelude "Haskell Standard Prelude Dictionary": text follows
12:19:00 <lambdabot> (++)
12:19:02 <lambdabot>   infixr 5 ++
12:19:04 <wli> that's mine on profile [1..1000] $ reverse [1..1000]
12:19:04 <lambdabot>   (++) ::  [a] -> [a] -> [a]
12:19:06 <lambdabot>   []     ++ ys      = ys
12:19:08 <lambdabot>   (x:xs) ++ ys      = x : (xs ++ ys)
12:19:16 <Janni> Yes, now yours is slightly faster, but needs a bit more memory
12:19:18 <wli> (1.79 secs, 9367120 bytes)
12:19:19 <steveh> @prelude concat
12:19:20 <lambdabot> *** "concat" prelude "Haskell Standard Prelude Dictionary": text follows
12:19:20 <lambdabot> concat
12:19:20 <lambdabot>   concat ::  [[a]] -> [a]
12:19:22 <lambdabot>   concat            = foldr (++) []
12:19:24 <wli> Janni: that's yours on the same
12:19:33 <steveh> compare concat to foldl (++) []
12:19:39 <Janni> OK, I'l try to understand it...
12:24:08 <cale> steveh: yeah, I see what you mean.
12:26:33 <wli> length $ profile [1..2000] [2000 - n | n <- [1..2000]]
12:26:45 <wli> simple: (6.17 secs, 30045292 bytes)
12:26:54 <wli> complex: (5.80 secs, 30615720 bytes)
12:27:02 <cale> steveh: what exactly ends up being evaluated if you take the head of the foldl version, or try to get the nth element? Does it eval more than necessary, or is it just traversing the tree that costs all the time?
12:29:24 <wli> signs appear to be the complex one is scaling asymptotically about the same
12:29:31 <wli> I must have screwed something up
12:30:05 <wli> ah
12:30:16 <wli> the difference only matters when one has heavy repetition
12:30:36 <Smerdyakov> You just used "one" in That Way on _IRC_.
12:30:39 <Smerdyakov> What is wrong with you?!
12:30:57 <cale> Smerdyakov: huh?
12:31:05 <Janni> Smerdyakov: huh?
12:31:18 <Riastradh> Smerdyakov: huh?
12:31:23 <Smerdyakov> <wli> the difference only matters when _one_ has heavy repetition
12:31:46 <Riastradh> ...?
12:31:56 <cale> I think we know which instance of 'one' you were referring to - it's the rest of your comment that made no sense.
12:32:48 <Smerdyakov> Was "one" there meant to mean one of the lists or "you"?
12:32:55 <wli> one of the lists
12:32:58 <Smerdyakov> Oh
12:33:00 <Smerdyakov> Never mind then :D
12:33:25 <Riastradh> What is wrong with one using 'one' in such a context?
12:33:36 <steveh> cale ack, sorry, missed your question...
12:33:37 <Riastradh> (*ptbthpth!*)
12:33:45 <wli> okay I need a structure which can be concatenated quickly
12:33:47 <cale> And what's wrong with the use of "one" to avoid use of personal pronouns anyway?
12:33:59 <Riastradh> I just asked that, foo, cale.
12:34:16 <cale> oop
12:34:24 <Smerdyakov> Because it's only used in formal writing or by pedants
12:34:25 <cale> didn't watch while typing.
12:34:33 <jlouis> wli: hum, what
12:35:00 <Riastradh> Smerdyakov, uh, no.
12:35:04 <cale> Smerdyakov: I don't see any reason that would be the case. It's a perfectly correct use of the word.
12:35:37 <steveh> cale: the issue is that (++) complexity is the length of the first list
12:35:38 <Smerdyakov> I claim that it's formal enough that it is strange to use it on IRC.
12:35:57 <steveh> cale: since once you reach the end of the first list you just chuck a pointer to the second
12:35:59 <Riastradh> You claim lots of other absurd things, Smerdyakov.
12:36:14 <jlouis> are we talking list concatenation?
12:36:19 <steveh> cale: if you foldl (++) the length of the first list keeps growing and growing
12:37:22 <cale> Yeah, I see.
12:40:02 <steveh> hum
12:40:49 <cale> It would be nice if there was a way to indicate to the evaluator that ++ was concatenative, so that to get the head of the list, it would just traverse the tree to the bottom in O(n) where n is the number of (++) applications, rather than having to iterate over the while thing.
12:41:01 <Janni> Bye, I'm off (and probably back later)...
12:41:03 <cale> s/while/whole/
12:41:30 <steveh> head (...(l1 ++ l2) ++ l3)...) ... I think it'd have to evaluate the whole concatenation before finding the head ... not sure though
12:41:44 <steveh> but head (l1 ++ (l2 ++ l3)... ) it wouldn't
12:41:50 <cale> yeah
12:42:10 <cale> I can see why that is now - it's because it doesn't have any idea what ++ is doing to the lists.
12:42:17 <steveh> right
12:42:30 <steveh> until it evaluates all the subexpressions it doesn't know where the head is
12:42:39 <steveh> and if it goes into the subexpressions it has to evaluate them
12:42:59 <cale> A very nice optimisation would be to prevent that.
12:43:19 <steveh> hm
12:43:25 <steveh> it might make things harder to reason about?
12:43:43 <steveh> it wouldn't really be applying functions "in normal order" anymore then
12:44:18 <steveh> better might be just to remember that you should right-associate your ++ usage
12:44:34 <steveh> or
12:44:45 <steveh> have a compiler do it for you :)
12:45:03 <steveh> without parens it right-associates
12:45:52 <cale> I think the real issue here is when you have many functions that are being used to successively build up lists, and it's awkward to build them in reverse.
12:46:05 <d-bug> isn't that also a case where a function made to be tail-call optimized is unsuitable?
12:46:17 <cale> (such as the whole issue with shows)
12:50:24 <cale> There are ways around it, the optimisation would be nice if one could prove it's strictly better, and doesn't adversely affect reasoning about the programs (i.e. functions will return the same thing, in all cases).
12:51:10 <cale> s/it, the/it, though the/
12:51:43 * Riastradh cheers for CPS.
13:03:10 <Riastradh> You people are referring to, e.g., how map is defined non-tail-recursively, but if it were defined with an iterator function one would be forced to call 'reverse' on the resulting list, right?
13:04:08 <steveh> @prelude map
13:04:09 <lambdabot> *** "map" prelude "Haskell Standard Prelude Dictionary": text follows
13:04:09 <lambdabot> map
13:04:09 <lambdabot>   map ::  (a -> b) -> [a] -> [b]
13:04:09 <lambdabot>   map f xs          = [ f x | x <- xs ]
13:04:15 <Riastradh> Er, well.
13:04:20 <Riastradh> I was referring to without list comprehensions.
13:04:22 <Riastradh> i.e.:
13:04:25 <Riastradh> map f [] = []
13:04:31 <Riastradh> map f (x:xs) = f x : map f xs
13:04:40 <cale> I was more talking about how foldl (++) is inefficient.
13:05:07 <cale> and how if the evaluator knew something about what ++ did, it could be more efficient.
13:05:32 <Riastradh> Why would you do foldl (++)?
13:07:45 <cale> well, it's more the structure of the result there than the actual call
13:08:13 <cale> like, any process that left-associatively builds lists with (++) will be inefficient.
13:11:26 <cale> this is because the evaluator can't rely on ++ being order preserving (or on any behaviour of ++) - a possible implementation would take this into account and rather than evaluating the entire application tree in normal order, traversed depth-first until it found the bits it needed.
13:12:08 <cale> The only problem would be whether this obscures reasoning.
13:12:25 <Riastradh> Or just don't be a fool and don't use foldl (++) or any left-associative (++).
13:12:53 <cale> Convenience is always nice though.
13:16:21 <cale> I think that what people should generally be striving towards is a nice system where so long as you have a reasonable description of what is needed, reasonably efficient code results, so that you don't have to worry as much about low (medium?) level details like the exact implementation of prelude functions.
13:18:21 <cale> Of course, as soon as we start doing optimisations that change the runtime order of functions, it may start to get hard to reason about such things.
13:19:34 <wli> cale: e.g. using rank-ordered trees for lists?
13:20:16 <wli> (well, that's a wee bit space-expensive but you get the idea)
13:21:23 <wli> a library function implementing rank-ordered trees would be useful
13:22:03 <cale> wli: I think that's somewhat in line with what I'm thinking.
13:22:38 <wli> I could probably use B-trees too
13:23:04 <wli> they don't fit well with Haskell though
13:23:14 <wli> Haskell is relatively space-transparent
13:23:39 <wli> I guess certain kinds could be brewed up though the bounds would be inconvenient
13:24:05 <cale> I seem to remember seeing a nice reduction in a book about Haskell when I was just starting to learn the language that reduced an O(n^3) algorithm to one which was O(n), and the thing was purely mechanical, and I remember thinking, "wouldn't it be nice if the compiler could do that".
13:24:31 <wli> Was it the Algebra of Programming?
13:24:35 <wli> (the book that is)
13:25:33 <cale> Probably not, I don't recall seeing that book. Could have been Craft of Expression, something like that?
13:25:41 <cale> I had three books out at the time.
13:26:17 <wli> the algebra of programming is dedicated entirely to that topic it seems
13:26:20 <cale> Oh yeah, I recall now - it was the minimal substring sum problem.
13:26:39 <cale> I'll have to pick that book up - it sounds cool.
14:34:48 * shapr boings
14:54:46 <pesco> yay
14:55:30 <shapr> hi pesco 
14:55:34 <pesco> hey shapr
14:55:42 <shapr> wassup?
14:56:07 <pesco> loom and me just proved the correctness of our implementation of transpose!
14:56:15 <shapr> cool
14:56:53 <pesco> yah, considering that a) it's already in module List and b) we'd like to end up not needing it after all
14:57:19 <pesco> but the proofing itself was pretty cool
14:57:31 <pesco> now we can be _sure_ that it works.
14:57:39 <shapr> did you use QuickCheck?
14:57:43 <pesco> BAH!
14:57:46 * shapr grins
14:58:03 <pesco> Proof! We needed PROOF! *G*
15:02:10 * shapr boings
15:15:09 <shapr> hey Darius, I've been reading the ModMonTerpreters paper
15:15:13 <shapr> did you track down the Gofer sources?
15:16:11 <Darius> No.  I guess that means the link they gave is broken (or my memory is broken and I'm thinking of another paper)
15:16:40 <shapr> I couldn't find it, and I'm pretty good at finding stuff.
15:18:09 <shapr> if I rolled back to the earliest LangPack, would that be close to the code in the paper?
15:20:30 <Darius> It'd be closer, most of the structure is still the same (in the other modules), but the actual content is different.
15:21:42 <Darius> Also it's a bit of mix and match between the two papers, though mostly the Modular Interpreters paper.
15:22:07 <Darius> Or the other one?  I'd have to reread it.
15:29:47 <Igloo> Does anyone know the status of gcc3.3 with ghc 6.0?
15:33:52 <shapr> Darius: I'm trying to divine the essence of monad transformers, since I think I understand monads.
15:33:59 <shapr> I'll read the other paper and see if that helps, thanks.
15:41:37 <Darius> Monad transformers are just a way to combine the features of two monads.  Using them is pretty straightforward, making them is also pretty straightforward, figuring out how they do and should interact is not in some cases.
15:47:00 <shapr> how they interact is part of using them, isn't it?
15:50:04 <shapr> ah, now I know what labragayo means
15:54:08 <Darius> Yes, but figuring it out from scratch and reading it in the documentation (ha) are two different things.
15:54:43 <shapr> oh
16:08:10 <wli> are you sure that's the real world?
16:08:13 <wli> ergh
16:10:03 * shapr isn't sure
16:42:44 <Janni> g'evening.
16:47:39 <shapr> hi Janni 
17:53:40 <Riastradh> Hi!
17:54:00 <cleverdra> hello, Riastradh =)
17:55:34 <shapr> hi cleverdra 
17:55:40 <shapr> how're the armed forces treating you?
17:56:17 <cleverdra> shapr - fairly well.
17:57:06 * cleverdra just got a Sharp Zaurus SL-5500, and is trying to find a comfortable programming environoment for it.  It has a hugs port, for instance.
17:57:18 <shapr> cool
17:58:44 <shapr> can you plug in an external keyboard?
17:59:08 <cleverdra> I've an IR keyboard that it supposedly works with.
17:59:26 <cleverdra> What I'd really like is a fabric keyboard for it, but that'll be a while.
17:59:59 <Igloo> How big a fabric keyboard?
18:01:19 <cleverdra> http://www.man-machine.com/FX100.htm -- but I'm not clear on how large it is.
18:02:31 <Igloo> Looks like 2-3 palms from the picture
18:04:07 * Janni wonders why there isn't a "randomElem :: Eq a => [a] -> IO a ; randomElem xs = randomRIO (0, length xs - 1) >>= \i -> return (xs !! i)" defined in the Random module...
18:04:51 <Smerdyakov> Why Eq a?
18:05:10 <shapr> I wrote a randomChoice for lambdabot
18:05:11 <Janni> Oh, that's of course unnecessary...
18:15:04 <guivrl> hey there. I need to compile readline in order to use "-package util" in a mac os X using ghc. do you know where I can find the file and how to build it? thanks.
18:18:43 <Janni> I'm off for bed. G'night...
18:26:35 * Igloo growls as ghc 6 build fails at the linking of stage 2
18:28:22 <Darius> Yah, had that happen.
18:28:43 <Igloo> Did you find out how to fix it?  :-)
18:29:40 <Darius> Not really.  It was likely a configuration error by me that caused different gcc's to be used at different times.
18:29:55 <Darius> That (and) or the changes to the build system fixed the problem.
18:29:59 <Igloo> Ah, hmm, I don't think that should be happening here
18:30:39 <Darius> Does stage1 build any executables?
18:31:25 <Igloo> It makes ghc/compiler/stage1/ghc-inplace
18:31:56 <Darius> I meant, is your stage1 compiler capable of building executables at all.
18:32:23 <Igloo> Oh, I see
18:33:46 <Igloo> No - good thought
18:35:00 <Darius> I've thought about adding a make rule to test the stage1 compiler before attempting to compile stage2.  It's rather a pain to find out it doesn't work after it's rebuilt itself.
18:35:47 <Darius> I don't know if the files it builds are useable (probably).  I always make clean and rebuild in those cases.
18:36:41 <Igloo> *nod*
18:44:55 <Igloo> Aha, looks like GHCziBase_unpackCStringzh_closure gets lost during unsplitting
18:45:11 * Igloo scratches his head - that's the wrong way round isn't it?
18:46:07 <Igloo> Ah, no. Right.
18:51:09 * Igloo bets the 'l' in objdump output means it's a local symbol that ld -x removes
18:52:00 <Igloo> And it's a 'g' in my working build
19:02:37 * Igloo finishes grepping list archives fruitlessly
19:03:04 <Igloo> Just to check it's not me doing something stupid, has anyone here built anything >= 6.0 with gcc 3.3 successfully?
19:05:09 <shapr> I built just barely pre-6 with gcc 3.3
19:05:21 <seth> I built the 6 release with 3.2.2
19:05:42 <Igloo> shapr: Hmmm, and it worked?
19:06:06 <shapr> yup
19:06:23 <shapr> I got a bunch of warnings, but no other problems.
19:46:56 * Igloo urgles at 15k of perl object splitter
19:49:17 * wli vomits
19:50:12 <seth> Perl?  Oh, yes, a mixture of cobol and basic.
19:50:44 <Igloo> The fact it uses local where I assume it means my isn't a great confidence booster either  :-)
19:53:56 * Igloo splutters at (the first 3 words in particular of) # [perl made uglier to work around the perl 5.7/5.8 bug documented at [...]
19:56:12 <seth> Igloo: I might argue that something that is already infinitely ugly can't be made uglier.
20:02:19 * ddarius is away watching Fight Club.
20:03:06 * wli is trying not to vomit while hacking around code with obsolete workarounds for deranged x86 archaisms all over it.
20:03:26 <wli> the x86 is the most disgusting architecture ever made
20:03:56 <seth> wli:  the 6502 was pretty bad also.
20:04:00 <wli> even m88k is cleaner and that needs the kernel to do instruction emulation to restart instructions
20:06:23 <seth> x86 is made even uglier by the stuff they put in there for backward compatibility.  Not that it wouldn't be ugly anyway.
20:06:33 <wli> the 6502 doesn't have a high enough gate count to have as much crap as x86
20:06:52 <seth> that's true.  :)
20:23:33 <lament> perl isn't a mixture of cobol and basic.
20:23:54 <wli> it's pure crap =)
20:24:17 <seth> lament: I know, I wasn't being serious.
20:24:33 <seth> lament: but a lot of it does resemble certain things in basic.
20:25:58 <lament> basic is, well, basic
20:26:27 <lament> cobol tries being basic (sorta like an elephant trying to dance)
20:26:44 <lament> Not perl.
20:27:05 <lament> the other two were intended as languages for idiots
20:27:12 <lament> perl just happened that way :)
20:29:46 * cleverdra notes with disinterest that he no longer feels the need to spout nonsense about languages that he hasn't learned.
20:30:15 * wli can write perl.
20:30:40 <seth> O
20:31:00 <seth> I'm just in a bad mood tonight.  I have to deliver something that will run on Mac OS9, and I can't use Haskell there.
20:52:05 <kip> what line of work seth?
20:52:48 <seth> kip: right now I'm doing some forms processing software.
20:53:01 <seth> Haskell works great in OSX, but I have this OS9 problem.
20:53:14 <kip> i'm glad to hear you can use haskell most of the time
23:25:01 <stepcut> i don't suppose there is a haskell compiler for arm?
23:34:12 <cleverdra> step - there's a port of Hugs to the ARM (or, specifically, to the Zaurus)
