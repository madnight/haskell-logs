00:04:50 * jemfinch might want a new language.
00:06:52 <emu> how about japanese
00:07:05 <jemfinch> I mean a computer language :P
00:07:47 <jemfinch> I'd love Haskell if it wasn't lazily evaluated or purely functional.
00:08:03 <emu> then it wouldn't be haskell
00:08:08 <jemfinch> I know :)
00:08:38 <jemfinch> I'd be satisfied with SML with typeclasses and a few things fixed.
00:10:04 <emu> simple pleasures
00:15:57 <jemfinch> it's too bad there's no simple syntactical extension that could give me typeclasses in SML right now.
00:20:52 <emu> what does it have to do with syntax
00:23:19 * d33p is back (gone 12:03:32)
05:29:15 <shapr> hiya
05:29:19 * shapr bounces
05:29:19 <jmalicki-work> sup
05:29:24 * jmalicki-work licks shapr
05:29:36 <shapr> um
05:29:38 <shapr> ewww
05:29:52 <shapr> I probably taste like swedish snow.
05:29:57 <jmalicki-work> mmmmmm
05:30:11 <shapr> :-P
05:40:02 * shapr bounces
05:40:15 <shapr> hi infot3ch, do I know you?
05:40:26 <infot3ch> shapr: doesn't seem so
05:40:38 <shapr> infot3ch: hi, I'm shae. nice to meet you.
05:40:41 <shapr> ok, now I know you.
05:40:53 <infot3ch> hehe nice to meet you too.
05:40:59 <shapr> are you learning Haskell?
05:41:02 <shapr> maybe a long time user?
05:41:03 <infot3ch> wow alot of dead links in the topic
05:41:07 <shapr> really?
05:41:42 <infot3ch> yea I'm supsecting it's the " " around the urls
05:42:00 <shapr> they all work for me
05:42:02 <shapr> just tried them.
05:42:16 <infot3ch> hmmm weird
05:42:37 <infot3ch> You were looking for the file
05:42:37 <infot3ch> which could not be located by this webserver. 
05:42:41 <shapr> ?
05:42:53 <shapr> is that a haiku error message?
05:43:16 <infot3ch> It's an error that the webserver spits out
05:43:48 <shapr> sounds very poetic
05:43:50 <infot3ch> your pasting the links into you browser im supposing?
05:43:55 <shapr> nope
05:44:02 <infot3ch> hmmm
05:44:08 <shapr> I'm using ffap in emacs
05:44:17 <shapr> which I've bound to H-f
05:44:28 * infot3ch nods
05:44:48 <shapr> I'm using ERC Version 3.0 $Revision: 1.405 $ with XEmacs 21.4 (patch 11) "Native Windows TTY Support" XEmacs Lucid!
05:44:53 <shapr> my irc client is emacs ya see
05:45:07 <infot3ch> yea
05:45:28 * shapr jams to chemical brothers
05:45:39 <shapr> infot3ch: are you learning Haskell?
05:45:46 <infot3ch> hmmm do you have a screenshot? (never knew it was available)
05:45:54 <infot3ch> shapr: yes
05:47:32 <shapr> screenshot of ERC?
05:47:42 <shapr> probably several on the sf.net site
05:47:46 <infot3ch> ok
05:47:52 <shapr> I can make a quick one if you want
05:48:13 <infot3ch> that's ok I'll check out the ones on sf.net
05:48:18 <shapr> http://kungens.kemi.fi/~shae/screenie.png
05:48:21 <shapr> that's one from last week
05:48:38 <shapr> yes, I'm aware that my emacs color theme is eye searing for most people :-)
05:48:54 <shapr> http://sf.net/projects/erc
05:49:23 <infot3ch> nice
05:49:25 <shapr> notice the haskell code in the top window :)
05:49:37 <shapr> erc shows modified channels in the modeline
05:49:47 <shapr> one buffer per channel
05:50:01 * infot3ch nods
05:50:10 <shapr> nick highlighting, notify when people are online, etc etc
05:50:50 <shapr> nearly everything you can find in other irc clients
05:50:58 <shapr> plus everything emacs does
05:51:39 <infot3ch> nice
05:52:18 <shapr> only drawbacks are that you have to have gnu emacs 21 to get dcc
05:52:28 <shapr> and emacs isn't multithreaded, so sometimes that gets irritating
05:52:46 <shapr> infot3ch: are you learning Haskell at school?
05:52:51 <shapr> or for fun like me?
05:52:54 <shapr> :)
05:53:37 <infot3ch> shapr: for fun
05:53:39 <infot3ch> :)
05:55:27 <jmalicki-work> someone needs to make a haskell-macs
05:57:58 <shapr> jmalicki-work: I totally agree
05:58:01 <shapr> there's already PyMacs
05:58:08 <shapr> could be based on something like that
05:58:49 <shapr> http://emacswiki.org/cgi-bin/wiki.pl?PyMacs
05:58:58 <shapr> infot3ch: emacs has an excellent haskell-mode
05:59:13 <infot3ch> shapr: cool
06:03:00 <shapr> hej Marvin--, vad gjörde du?
06:03:12 <Marvin--> heh, almost ;)
06:03:19 <shapr> which I think means "whatcha doin?"
06:03:24 <Marvin--> in present tense it should be "gör"
06:03:29 <shapr> oh, thanks!
06:03:30 <Marvin--> past tense, "gjorde"
06:03:45 <shapr> oh, I said "what have you done"
06:03:45 <Marvin--> close but no cigar :)
06:03:57 <shapr> bah,I'm improving :-)
06:03:58 <Marvin--> no, but s/ö/o/ and you would have :)
06:04:23 <shapr> ah
06:06:00 <Marvin--> what I do, well, I just filled in a last-minute application for the course on compiler construction
06:06:10 <shapr> oh, neat
06:06:15 <shapr> do you get to use Haskell? 
06:06:26 <Marvin--> I think we can use whatever language we like
06:06:30 <shapr> neat!
06:06:41 <jmalicki-work> dude, you should use cobol
06:06:46 <shapr> noooo
06:06:53 <shapr> unlambda
06:06:57 <Marvin--> the course book exists in three different versions, ML, C and Java :P
06:06:59 <jmalicki-work> hells yeah
06:07:03 <jmalicki-work> actually
06:07:05 <shapr> much easier to edit unlambda source.
06:07:09 <jmalicki-work> use the bourne shell
06:07:13 <jmalicki-work> and POSIX.1 utility set
06:07:19 <shapr> right, sed and awk
06:07:30 <shapr> actually, you could do term rewriting with sed and awk
06:07:35 <jmalicki-work> yep
06:07:36 <shapr> it could work :-)
06:07:44 <Marvin--> you people are too sick for me
06:07:51 <shapr> haha
06:08:25 <Marvin--> Maybe I should do it in C+Python+Haskell just in spite
06:08:26 <Marvin--> with corba
06:08:40 <shapr> I've been thinking about strategy combinators
06:08:53 <shapr> I'm trying to come up with a combinator library I'd like to work with
06:09:01 <shapr> that would be a combination of languages
06:09:28 <shapr> hack up an FFI wrapper so that Haskell is just a Python module written in C
06:09:43 <jmalicki-work> hhahaha
06:10:01 <jmalicki-work> Marvin--: no, use .NET
06:10:14 <shapr> eww, that's too much for me
06:10:20 <Marvin--> "Modern Compiler Implementation in ML"
06:10:35 <shapr> I'd like to take that course.
06:10:36 <shapr> oh hey!
06:10:42 <shapr> I met several people who know haskell yesterday
06:10:57 <shapr> well, sort of know it
06:11:00 <shapr> they're all from luth.se
06:11:22 <Marvin--> heh
06:11:31 <shapr> sadly, none of them actually liked Haskell enough to use it after the intro course :-(
06:12:00 <shapr> we need more stuff to tempt in those sorry materialists.
06:12:17 <jmalicki-work> shapr: you mean like hairy wimmin?
06:12:21 <shapr> um.
06:12:22 <shapr> no.
06:12:25 <jmalicki-work> oh
06:12:37 <shapr> I mean like database bindings, and protocol libraries.
06:12:53 <shapr> see? you scared him off just by mentioned hairy wimmin.
06:12:56 <shapr> sheez
06:12:59 * shapr grins
06:13:00 <jmalicki-work> hahaha
06:13:20 <jmalicki-work> haskell doesn't need database bindings
06:13:26 <shapr> I want one.
06:13:27 <jmalicki-work> languages that have database bindings scare me
06:13:32 <shapr> really?
06:13:48 <jmalicki-work> it means that if i learn that language, i might end up getting stuck in a job doing SQL queries all day long
06:13:52 <jmalicki-work> and that would blow chunks
06:14:05 <shapr> with Haskell, you could automate those chunks
06:14:15 <jmalicki-work> true
06:14:19 <jmalicki-work> and i could do it lazily
06:14:28 <jmalicki-work> but it's the principle of it
06:14:48 <shapr> right on
06:14:53 <shapr> have you seen HaskellDB?
06:15:04 <jmalicki-work> i mean, who wants to say, on their deathbed, "I was a J2EE monkey!"
06:15:07 <jmalicki-work> no
06:15:23 <shapr> I've been a J2EE monkey
06:15:26 <shapr> I probably will be again.
06:15:30 <shapr> it pays well
06:15:38 <jmalicki-work> yeah, but not worth it imo
06:15:47 <shapr> when you're hungry, it's worth it.
06:15:51 <jmalicki-work> ill take a paycut to work with people who have blue hair
06:16:03 <shapr> what if that's a paycut to zero?
06:16:05 <jmalicki-work> programming pays well enough you can sacrifice a little, though
06:16:12 <jmalicki-work> oh, true
06:16:17 <jmalicki-work> im talking like a career though
06:16:24 <shapr> yah, that's a good piont.
06:16:25 <shapr> point
06:16:29 <Marvin--> I quote things from here to a friend at times
06:16:34 <shapr> Marvin--: like what?
06:16:39 <Marvin--> he usually replies "On second thought, let's not go there, 'tis a silly place"
06:16:44 <shapr> haha
06:16:48 <jmalicki-work> :)
06:16:50 <shapr> good quote
06:17:52 <shapr> HaskellDB appeals to my sense of elegance
06:21:21 <shapr> hi Radek 
06:23:08 <shapr> jmalicki-work: what do you do at work?
06:23:15 <shapr> jmalicki-work: don't you work with Mr Syntax?
06:23:19 <jmalicki-work> i irc, listen to music, etc
06:23:25 <jmalicki-work> i got this job first
06:23:30 <jmalicki-work> no
06:23:35 <jmalicki-work> i went to school with him
06:23:37 <shapr> oh
06:23:57 <jmalicki-work> i write streaming media server daemons, and associated utilities and miscellany
06:24:03 <shapr> sounds like fun
06:24:09 <jmalicki-work> its not too bad
06:24:09 <shapr> you work for real.com or something?
06:24:16 <jmalicki-work> not the most heady shit, but still pretty decent
06:24:23 <jmalicki-work> starbak communications
06:24:27 <jmalicki-work> its like a startup
06:24:28 <shapr> haven't heard of them
06:24:34 <jmalicki-work> yeah, we're pretty small
06:25:44 <jmalicki-work> shapr: what do you do?
06:26:19 <shapr> I'm a contract programmer. I've done VB, Java, J2EE, etc
06:26:31 <shapr> recently Zope work has been my primary source of income.
06:26:38 <Marvin--> shapr has been a J2EE monkey =)
06:26:38 <jmalicki-work> woah
06:26:42 <jmalicki-work> whats that like?
06:26:42 <jmalicki-work> hehehe
06:26:48 <jmalicki-work> ive never touched zope
06:26:59 <shapr> Zope is very cool
06:27:09 <shapr> I get pissy about the APIs, and the difficulty of unit testing
06:27:18 <shapr> but overall it's the best web dev platform I've ever used.
06:27:23 <jmalicki-work> cool
06:27:26 <lament> shapr: tell it to that guy in #python
06:27:30 <shapr> which guy?
06:27:37 <lament> vextor
06:27:44 <lament> he's looking for a python web platform
06:27:57 <lament> and he already uninstalled zope because it's "too slow" :)
06:28:02 <shapr> hah
06:28:03 <shapr> too funny
06:28:38 <Marvin--> lament: has he looked at twisted?
06:28:57 <Marvin--> come to think of it, I think they can advocate it without me, they're good at that
06:28:58 <lament> Marvin--: not really, it seems. He said he has, and it's 'pleh'
06:29:05 <shapr> pleh?
06:29:06 <lament> there's no one there :(
06:29:10 <Marvin--> weird
06:29:21 <lament> Marvin--: yes, go to #python and advocate twisted NOW!
06:29:28 <shapr> sounds like a man who has his own opinions firmly attached, and does not need facts.
06:29:33 <Marvin--> "twisted is good, because, uh, it uses my ssl module!"
06:30:20 <lament> shapr: well, he uses PHP
06:30:20 <jmalicki-work> shapr: hah
06:30:20 <shapr> wargh
06:30:20 <shapr> netsplit
06:30:20 <jmalicki-work> lament: your mom uses PHP.
06:30:40 <shapr> I spoke to the guys on #php about Python and Haskell a bit
06:30:44 <lament> jmalicki-work: wrong! She doesn't, tee-hee.
06:30:53 <shapr> they don't seem very interested in stuff outside of php.
06:31:13 <shapr> I think php is good at what it does.
06:31:13 <Marvin--> shapr: of course not, php is the 1337:est language in the world, it solves all the problems in the world, including starvation and cancer
06:31:22 <lament> shapr: generally people who only know one or two languages are not interested in learning more
06:31:28 <jmalicki-work> php is a really cool little language
06:31:39 <Marvin--> php is an ugly abomination
06:31:54 <lament> shapr: That's true of most people
06:32:05 <Marvin--> I could dream up a language in a nightmare that is easier to use than php
06:32:08 <shapr> lament: yah, I agree.
06:32:18 <shapr> most people only know what they need to know for their job.
06:32:20 <jmalicki-work> Marvin--: i dunno.. i picked it up in like 2 minutes
06:32:21 <shapr> some people don't even know that.
06:32:24 <lament> I used to be a single-language zealot
06:32:28 <lament> And I don't even have a job :)
06:32:30 <shapr> lament: which language?
06:32:37 <jmalicki-work> i guess if you're trying to cure cancer with it PHP might suck
06:32:39 <lament> shapr: C... *blush*
06:32:45 <jmalicki-work> but for throwing a scriptlet in a webpage its great
06:32:45 <Marvin--> jmalicki-work: yes it's pretty *newbie* friendly, but not particularly *user* friendly
06:32:46 <shapr> heh :-)
06:32:49 <shapr> well, we all start somewhere.
06:33:08 <Marvin--> lament: you could do worse :)
06:33:15 <lament> Marvin--: Well, I started with VB
06:33:18 <jmalicki-work> if your'e going to be a single language elitist, C and Lisp seem to be the good ones to go with
06:33:19 <emu> whoa, lag
06:33:47 <lament> Marvin--: But back then I didn't have Internet, so I wasn't a zealot.
06:33:55 * emu went from C to Lisp
06:34:04 <lament> jmalicki-work: And Python!
06:34:10 <jmalicki-work> lament: i dunno
06:34:14 <Marvin--> lament: heh
06:34:15 <shapr> my first language was BASIC on a sinclair
06:34:20 <jmalicki-work> lament: python has a little too much of a scripty-feel
06:34:35 <lament> jmalicki-work: if you're a single-language zealot, you wouldn't care about that :)
06:34:38 <jmalicki-work> lament: python seems like its everyone's favorite secondary langauge to me :)
06:34:41 <shapr> then I learned assembly from an i386 reference and debug.com
06:34:49 <shapr> I decided asm was extremely painful.
06:34:54 <emu> eh
06:34:59 <Marvin--> I only did 286 asm, and that was painful too
06:35:05 <emu> 16 bit x86 asm was a piece of cake
06:35:06 <jmalicki-work> x86 asm r00lz
06:35:13 <shapr> not with debug.com
06:35:17 <emu> a86
06:35:19 <jmalicki-work> shapr: well yeah :)
06:35:20 <shapr> you could only edit the line you were working on
06:35:25 <jmalicki-work> emu: yeah d00d, thats what it was called :)
06:35:27 <shapr> if you made a mistake, you were scrud
06:35:44 <emu> geez, like editting with ed, but worse
06:35:49 <shapr> I was unable to get anything better
06:36:12 <emu> a86 was shareware
06:36:16 <emu> was/is
06:36:27 <shapr> was it available in 1989?
06:36:35 <emu> not sure
06:36:46 <emu> I think I used it in the early nineties
06:36:58 <shapr> I ran up a massive phone bill calling long distance bulletin boards, trying to find decent compilers or assemblers.
06:37:02 <shapr> I was unsuccessful.
06:37:07 <shapr> then the phone bill arrived.
06:37:16 <jmalicki-work> when did djgpp come out?
06:37:17 <jmalicki-work> hah
06:37:35 <shapr> thus, my introduction to programming was delayed.
06:37:43 <shapr> sort of
06:37:59 <shapr> I wish I'd had linux and gcc then
06:38:07 <emu> I have still have a CD I picked up around 10 years ago, with all sorts of dev tools on it
06:38:09 <lament> not ghci? :)
06:38:20 <shapr> my mind would have exploded 
06:38:48 <lament> heh.
06:40:35 <emu> being exposed to high level programming concepts earlier on would probably have been nice
06:41:37 <jmalicki-work> emu: hmmm i actually liked not having that
06:41:47 <jmalicki-work> emu: its kinda cool to discover the needs for things
06:41:55 <jmalicki-work> emu: then you appreciate why things are the way they are
06:41:59 <jmalicki-work> and understand them much better
06:42:02 <lament> yes, i agree
06:42:04 <jmalicki-work> at least in my experience
06:42:08 <shapr> yah, same here
06:42:19 <shapr> I actually got into FP because of my hatred of globals in Python
06:42:25 <lament> heh.
06:42:28 <emu> like how most programmers don't know anything beyond printf and if statements?
06:42:35 <shapr> I realized that I wanted to find the total opposite of global variables
06:43:01 <lament> Uh, Python is object-oriented
06:43:07 <lament> no one forces you to use globals
06:43:10 <shapr> I started writing all of my functions to communicate via input and return values, nothing else
06:43:11 <jmalicki-work> shapr: OO always seemed like a decent solution to that one to me
06:43:27 <shapr> nah, part of the problem is control flow doesn't follow execution flow
06:43:27 <lament> shapr: then you were misusing python
06:43:34 <shapr> lament: I agree
06:43:38 <jmalicki-work> shapr: true
06:43:40 <shapr> python is not functional programming
06:43:48 <shapr> the thing is, I wanted FP
06:43:53 <lament> shapr: using globals is also mis-use :)
06:43:59 <shapr> I just didn't really know about it yet.
06:44:13 <shapr> I found scheme, but it left a bad taste in my mouth...
06:44:15 <shapr> same for elisp
06:44:22 <lament> aww.
06:44:30 <lament> scheme is pretty.
06:44:41 <shapr> yah, but it's not what I was looking for
06:45:04 <shapr> my first haskell lesson was like "AHA
06:45:04 <jmalicki-work> i learned scheme pretty early on
06:45:08 <emu> what would the total opposite be?
06:45:08 <shapr> "
06:45:09 <jmalicki-work> i think i was like 14
06:45:13 <jmalicki-work> so that was kinda cool
06:45:15 <lament> jmalicki-work: lucky!
06:45:25 <shapr> jmalicki-work: how old are you?
06:45:28 <lament> When I was 14, I only knew VB
06:45:32 <jmalicki-work> shapr: 23
06:45:44 <jmalicki-work> actually maybe i was 15
06:45:50 <shapr> I'm 31
06:45:59 <lament> wow, you're _old_ :)
06:46:04 <shapr> :-P
06:46:12 <emu> lament: what does that mean?
06:46:14 <emu> ..lag
06:46:16 <emu> elisp is in bad taste, no surprise
06:46:20 <shapr> all the better to code over your head, little red riding hood :-P
06:46:33 <lament> emu: what does what mean?
06:46:41 <lament> heh
06:46:46 <emu> lament: I was referring to your comment about Python being OO
06:46:50 <emu> but got lagged to hell
06:46:56 <lament> Well, Python is OO
06:47:18 <shapr> see, I want greater simplicity in the system I use to program
06:47:33 <emu> lament: which says very little =)
06:47:36 <lament> shapr: you mean like Python? :)
06:47:43 <shapr> lament: python has some of that
06:47:50 <shapr> I really like python 1.5
06:47:50 <lament> emu: it means you should have lots of stateful objects, and not globals
06:47:56 <lament> shapr: how about smalltalk?
06:48:03 <lament> Ew, ancient Python
06:48:04 <jmalicki-work> hmmm i kinda like the simplicity of C
06:48:11 <shapr> I haven't had the honor to work with smalltalk yet.
06:48:14 <jmalicki-work> like, there aren't any gotchas
06:48:15 <lament> jmalicki-work: the simplicity of C99, I hope? :)
06:48:17 <emu> lament: what do global variables have in conflict with stateful objects?
06:48:25 <jmalicki-work> lament: C99 is noticably simpler than C89?
06:48:26 <emu> jmalicki-work: haha
06:48:38 <lament> jmalicki-work: yes, it's almost as simple as C++!
06:48:51 <lament> emu: they don't, but shapr doesn't like globals
06:48:53 <jmalicki-work> like, since C isnt very abstracted, there realy arent any misabstracted things
06:48:56 <jmalicki-work> lament: hahahahaha
06:49:10 <jmalicki-work> lament: man, i use some C99 stuff... what did they fuck up?
06:49:11 <lament> shapr: Ancient Python (like 1.5) is not consistent enough for my taste
06:49:14 <emu> ``C sucks, that's why it's good''
06:49:26 <shapr> imho, python 1.5 is the most orthogonal and consistent
06:49:32 <jmalicki-work> lament: i do like that % is now defined as remainder, and not implementation-dependent behavior
06:49:55 <lament> shapr: e.g. you can't subclass from builtin types
06:50:02 <lament> you don't have metaclasses
06:50:16 <lament> the scoping rules are idiotic
06:50:29 <shapr> lament: that's true, I like that...
06:50:33 <lament> scoping-wise, python 1.5 is a toy
06:50:35 <shapr> also, you can now subclass from C classes
06:50:40 <shapr> I agree with that
06:50:46 <shapr> but that was one of its good points I think
06:50:50 <shapr> I started with python 1.4
06:50:51 <lament> heh :)
06:51:00 <shapr> and I used to fight about "the crappy scoping"
06:51:04 <emu> well, most of the world doesn't know what a lexical closure is anyway
06:51:17 <shapr> but one day we had a huge discussion about it, and I realized that in python1.5, you don't need better scoping.
06:51:20 <emu> if you don't know what you're missing, you don't miss it
06:51:21 <shapr> it just works the way it is.
06:51:25 <jmalicki-work> i dunno what a lexical closure is
06:51:26 <jmalicki-work> :)
06:51:40 <emu> so says the Haskell programmer
06:51:44 <emu> ahem
06:51:45 <lament> heh.
06:51:50 <Heffalump> It just works (TM)
06:51:51 <jmalicki-work> how does it differ from a non-lexical closure?
06:51:59 <lament> jmalicki-work: consider C.
06:52:07 <lament> jmalicki-work: C doesn't have any scoping whatsoever :)
06:52:12 <Heffalump> you don't get non-lexical closures
06:52:19 <jmalicki-work> sure, C has scoping
06:52:19 <Heffalump> if you have dynamic scoping you don't need closures
06:52:31 <emu> lexical closure meaning that the captured bindings are determined by lexical scope
06:52:42 <jmalicki-work> oh ok
06:52:55 <emu> { int a = 1; { a++; } }
06:53:13 <lament> ooh, lexical scoping! amazing!
06:53:13 <emu> admittedly, it's not very helpful, but it's sorta there
06:53:14 <lament> heh.
06:53:24 <emu> since you can only have dynamic extent
06:53:34 <Heffalump> You need a more complicated example for it to be interesting
06:53:36 <lament> {} doesn't add new scope
06:53:57 <lament> i mean new environment
06:54:04 <Heffalump> int a=1; void f() { a++; }
06:54:12 <Heffalump> { int a=2; f(); }
06:54:22 <Heffalump> the question is which a will be incremented by the call to f()
06:54:35 <emu> ooh fun
06:54:47 <Heffalump> well, that's the difference between lexical and dynamic scoping.
06:54:51 <emu> I presume since C doesn't have special variables, the outer A will be
06:54:55 <lament> Heffalump: that's still not complex enough
06:54:57 <Heffalump> lexical scoping it'll be the first a, dynamic it'll be the second.
06:55:17 <lament> Heffalump: you need three separate variables with the same name -- for lexical, dynamic and global
06:55:21 <jmalicki-work> Heffalump: any sane implementation should just coredump with "Programmer being inconsistent, and shouldn't name two variables the same thing just to be confusing"
06:55:32 <Heffalump> jmalicki-work: huh?
06:55:41 <emu> jmalicki-work: no, it would core dump with "Segmentation fault"
06:55:41 <Heffalump> The rules of lexical scoping are clear.
06:55:57 <Heffalump> dynamically scoped languages are generally a bad idea, on the other hand
06:56:03 <jmalicki-work> Heffalump: yes. but you're talking as an academic, not a programmer
06:56:03 <emu> jmalicki-work: but mostly because the compiler writer was too fucking lazy to think up the proper semantics, or even to write an error message and handling code
06:56:11 <emu> jmalicki-work: I mean, we're talking about _C programmers_ here
06:56:14 <jmalicki-work> Heffalump: one should program such that is does not matter
06:56:21 <emu> Heffalump: you can have both ;)
06:56:27 <Heffalump> jmalicki-work: if you want the compiler to complain, you're basically saying one can never reuse variable names in code.
06:57:06 <jmalicki-work> Heffalump: the compiler should require some sort of explicit namespace in that instance
06:57:08 <jmalicki-work> well, sure :)
06:57:19 <emu> I think it's perfectly sane to rely on lexical scoping where needed, and indefinite wherever needed as well
06:57:20 <Heffalump> jmalicki-work: that's damages code reuse, then
06:57:23 <Heffalump> s/that's/that/
06:57:26 <jmalicki-work> or even if the compiler doesnt complain, the code reviewer should
06:57:51 <Heffalump> It's an encapsulation issue.
06:57:53 <emu> Heffalump: when the language doesn't well state the semantics for such a situation, I think that jmalicki-work is right.  just don't use C =)
06:58:04 <jmalicki-work> Heffalump: closures should not inherit scope imo
06:58:07 <Heffalump> emu: but C does state them well.
06:58:14 <jmalicki-work> Heffalump: any programmer who does that deserves a foot up their ass :)
06:58:15 <Heffalump> It's a lexical scoped language, like all sane languages are.
06:58:22 <lament> gah
06:58:24 <jmalicki-work> Heffalump: if you're going to use a variable, pass it in :)
06:58:26 <lament> C is not lexically scoped
06:58:41 <lament> Nor dynamically
06:58:49 <Heffalump> jmalicki-work: but that fragment of code (in C) isn't trying to use the local variable a.
06:58:52 <emu> C is not lexically scoped in the sense that Haskell or Lisp is
06:58:55 <Heffalump> lament: go on then, what is it?
06:58:57 <lament> There're function variables, and global variables
06:59:06 <lament> err, s/function/local
06:59:06 <Heffalump> oh, yeah, ok.
06:59:09 <emu> both of those have lexical scope and indefinite extent bindings
06:59:11 * Heffalump looks apologetic
06:59:12 <lament> that's it
06:59:16 <lament> there's no "scope" to speak of
06:59:22 <Heffalump> oh, hangon, yes there is
06:59:33 <lament> since you can't nest functions
06:59:40 <emu> lament: you can nest { }'s though
06:59:42 <jmalicki-work> lament: C does have file scope
06:59:47 <jmalicki-work> lament: among globals
06:59:50 <lament> emu: {} don't add a new environment
07:00:08 <emu> I don't think { { int a = 1; } a++; } is valid, but I may be wrong
07:00:22 <lament> emu: you can't do this: { int a = 1;  { int a = 2;}}
07:00:29 <emu> so, no shadowing
07:00:30 <lament> emu: therefore, {} simply doesn't qualify
07:00:36 <jmalicki-work> lament: and what about void foo(int a) { while (--a >= 0) { int a = 1; return a; } }
07:00:41 <jmalicki-work> lament: that looks like scope to me?
07:00:50 <lament> jmalicki-work: eh, that's legal?
07:00:54 <jmalicki-work> lament: totally
07:00:57 <jmalicki-work> lament: it returns 1
07:01:05 <lament> hm
07:01:17 <jmalicki-work> lament: perhaps you should know a language before you comment on it like that :)
07:01:24 <lament> that's disgusting :)
07:01:25 <jmalicki-work> lament: its retarded as fuck, but legal :)
07:01:39 <Heffalump> it's not that disgusting.
07:01:42 <jmalicki-work> lament: relying on scoping behavior, period, is disgusting
07:01:44 <jmalicki-work> Heffalump: yes, it is
07:02:01 <Heffalump> Why?
07:02:06 * emu is confused why relying on scoping is wrong
07:02:09 <lament> jmalicki-work: no, it's not
07:02:11 <jmalicki-work> Heffalump: its clear to you because you are more worried about scoping rule masturbation than doing actual work :)
07:02:16 <emu> how else are you supposed to do anything?
07:02:16 <lament> jmalicki-work: scoping is your friend
07:02:37 <lament> jmalicki-work: scoping is not limited to shadowing variables
07:02:39 <Heffalump> jmalicki-work: no, it's clear because you look for the closest-nested declaration
07:02:47 <emu> does that mean I can't write: f x y = x + y because that relies on lexical scoping?
07:02:57 <Heffalump> it means that you can cut and paste a big block of code from somewhere else and not have to worry about variable collisions
07:03:13 <jmalicki-work> Heffalump: if the author of code makes me search for closest-nested declarations when reading his code, he gets his foot up my ass
07:03:14 <Heffalump> which is good for encapsulation (albeit cut-and-paste is not exactly the ideal way of doing that kind of thing...)
07:03:34 <jmalicki-work> Heffalump: it should be obvious that i dont need to worry about it :)
07:03:41 <emu> jmalicki-work: that's a different argument than ``don't rely on scoping''
07:03:51 <jmalicki-work> emu: yes , and no
07:03:55 <emu> jmalicki-work: that's ``don't playing shadowing games''
07:03:59 <jmalicki-work> emu: well yeah
07:04:07 <jmalicki-work> emu: how is that really different?
07:04:08 <Heffalump> There are rarely good reasons for the same programmer to use the same variable in that way (in the above example)
07:04:12 <emu> it's hugely different
07:04:15 <jmalicki-work> Heffalump: no, there aren't.
07:04:18 <Heffalump> But there are plenty of good reasons for /different/ programmers to do it.
07:04:26 <emu> jmalicki-work: you can't write a line of Haskell without invoking some kind of scoping, just about
07:04:31 <Heffalump> Or indeed the same programmer at significantly different times.
07:04:39 * Heffalump --> do something, biab
07:04:54 <jmalicki-work> emu: depends how you view it
07:05:02 <jmalicki-work> emu: i presume you're mostly talking about like closures?
07:05:32 <jmalicki-work> i always thought of closures as contructing a new function
07:05:48 <emu> closures are functions + the free variable bindings
07:06:13 <jmalicki-work> some people see it that way
07:06:17 <emu> um
07:06:20 <jmalicki-work> it seems like a crappy way to view them to me.
07:06:20 <emu> hehe
07:06:26 <emu> okay, you can have another interpretation of them, in fact
07:06:33 <lament> um
07:06:34 <emu> the substitution intepretation
07:06:38 <jmalicki-work> yeah
07:06:38 <lament> that's not how people see closures
07:06:47 <lament> that's what closures are :)
07:07:02 <emu> essentially, whenever you evaluate a variable binding, you substitute that binding for every instance of the variable in the scoped code
07:07:04 <jmalicki-work> lament: thats how they are often implemented :)
07:07:14 <emu> this can be proved to be equivalent to the other definitio nof closures
07:07:21 <emu> er, lexical scoping
07:07:29 <emu> where you maintain an environment
07:07:33 <jmalicki-work> lament: once you have graph machines and shit like in haskell, the substitution version makes a lot of sense to implement
07:07:42 <emu> and this happens to be much easier to implement than substitution, efficiently
07:07:51 <emu> usually
07:08:03 <emu> since hardware tends to not be so great at the substitution bit...
07:08:07 <jmalicki-work> yeah, but lazy execution makes everything bizzare :)
07:08:16 <shapr> but lots more fun
07:08:49 <jmalicki-work> the substitution bit and the laziness of haskell are more or less analagous
07:08:55 <jmalicki-work> err
07:08:59 <jmalicki-work> like
07:09:03 <jmalicki-work> kind of reduce to each other
07:09:13 <jmalicki-work> i'm a few years out of academia-speak :)
07:09:13 <shapr> hm, call-by-need has something to do with that
07:09:18 <shapr> but I can't remember the connection
07:09:21 <emu> you need to memoize results
07:09:33 <emu> to do it efficiently
07:09:39 <jmalicki-work> oh sure
07:09:44 * Heffalump back
07:09:48 <emu> and substitution doesn't do that well..
07:10:38 <Heffalump> but graph reduction does
07:10:51 <jmalicki-work> emu: well, i guess the "traditional" defintion of closures is just a lazy substitution ;)
07:11:01 <jmalicki-work> emu: substitution is graph reduction
07:12:35 <jmalicki-work> like, sure, they may be funciton + environment
07:12:56 <jmalicki-work> but its like, if you can prove it to be equivalent to the subsritution
07:13:11 <jmalicki-work> you can think about it a lot easier without getting trapped in scoping rules and shit in your head
07:13:35 <Heffalump> scoping rules are far easier to think about than the effects of substitution...
07:16:22 <emu> uh
07:16:22 <emu> where do you get your substitutions from? =)
07:16:22 <emu> if you're looking at code, you may substitute values in for variables mentally, but then you're performing the ``environment'' bit in your head anyway =)
07:16:22 <emu> lexical scoping is really simple, I don't see what the problem is 
07:16:42 <emu> you don't even have special variables to consider, hehe
07:16:43 <Heffalump> yeah
07:17:09 <Heffalump> You have to find a variable declaration /somehow/, whatever
07:17:39 <Heffalump> and looking from inside to outside is just as good as (if not better than) any other way
07:30:34 <Logan> Woah, a shapr appearance in #python. :P
07:30:44 <shapr> hah
07:30:49 <shapr> special request :-P
07:32:01 <Logan> A lot of #python people have this arrogant asshole thing going for them, yet most are really smart.  It's kind of annoying. :P
07:32:50 <shapr> sometimes I'm an arrogant asshole, but usually it lasts about an hour... and then someone much smarter than me comes along and unwittingly bursts my bubble.
07:33:00 <Logan> I've never seen you play the asshole.
07:33:07 <shapr> it happens, trust me :-)
07:33:18 <mgoetze> shapr: we want proof :P
07:33:23 <shapr> so I try to look at the arrogant people and say "is there some way I can let them down easy?"
07:33:31 <emu> I thought Python was the touchy-feely language
07:33:42 <shapr> mgoetze: it's embarassing afterwards :-)
07:33:45 <shapr> I try to forget about those times.
07:35:00 <shapr> and try to find ways of not repeating them
07:35:49 <lament> but being an arrogant asshole is fun!
07:35:55 <shapr> whenever I start to get a big head, I go look at Simon PJ's list of published papers.
07:36:02 <lament> _especially_ when you know you're wrong/biased.
07:36:21 * mgoetze is configuring his new server :)
07:36:26 <shapr> http://research.microsoft.com/Users/simonpj/Papers/papers.html
07:37:02 * emu forces lament to program in Pascal
07:37:02 <lament> shapr: see, he's so smart, and yet he works for microsoft :)
07:37:52 * lament dies
07:38:10 <Heffalump> M$ pay well :-)
07:38:28 <emu> MS Research: Where they look into new ideas, and rip them off from their creators =)
07:39:06 <lament> Heffalump: yes, but being an arrogant asshole, I can just employ ad hominem and say that all of Simon PJ's papers simply don't matter
07:39:11 <lament> since he works for microsoft :)
07:39:17 <Logan> emu: It's more of a "we're right and you're really stupid" language.
07:39:18 <Marvin--> and what's the deal with not putting In-Reply-To headers in mails??? Stupid MUA!
07:39:45 <lament> Logan: well, it's often true :)
07:39:53 <emu> and when they're not doing that, they're busy creating new themes for Windows XP
07:40:12 <Logan> lament: Not as often as some people would like to think. :P
07:40:47 <shapr> John Hughes is another head shrinker.
07:41:17 <shapr> I mentioned on the pragmatic programmer's wiki that QuickCheck had sucky documentation
07:41:28 <shapr> so he tracked down my email address and asked me for suggested improvements.
07:41:31 <Heffalump> Marvin--: yeah, that's really annoying
07:41:39 <Heffalump> shapr: wow, cool
07:41:47 <shapr> yah, I was highly impressed.
07:42:04 <lament> hm, what's cool about that?
07:42:17 <shapr> he's the guy who wrote "generalizing monads to arrows"
07:42:33 <shapr> he and Koen Claessen wrote QuickCheck
07:42:49 <shapr> he's done a lot of very impressive stuff
07:42:49 <andersca> funny you should mention that
07:42:55 <emu> sounds like a check printing program
07:43:01 <andersca> Koen Claessen is teaching at a course I'm taking
07:43:04 <shapr> emu: heh, cute
07:43:26 <shapr> I'd very much like to take classes from Hughes
07:43:50 <shapr> http://www.math.chalmers.se/~rjmh/
07:44:10 <shapr> he sent me a prerelease of the new monadic quickcheck when I asked for an update :-)
07:44:12 * emu would much rather prefer to sleep through classes, as it turns out
07:44:46 <shapr> emu: boring class?
07:44:56 <emu> redundancy
07:45:49 <emu> actually I just value my sleep highly
07:46:18 <shapr> see, with people like Hughes and Simon PJ out there
07:46:31 <shapr> it's hard to feel arrogant
07:46:56 <emu> I'm sure someone will find a way
07:46:57 <Marvin--> andersca: what, datalogins grunder?
07:47:02 <andersca> Marvin--: yeah
07:47:07 <shapr> emu: truly
07:47:08 <Marvin--> ok
07:47:12 * Marvin-- has to go
07:47:12 <andersca> Marvin--: he seems good
07:47:18 <shapr> cya Marvin-- 
07:47:23 <Marvin--> andersca: he is
07:47:30 <shapr> I wonder if I could find some paying Haskell work in Lulea
07:47:35 <shapr> that would totally rock
07:47:46 <Heffalump> I wouldn't hold your breath
07:47:59 * shapr looks up the web address of the arbetsformudlingen(sp?)
07:48:22 <shapr> Heffalump: yah, I know, but I gotta look :)
08:47:20 <Janni> hellas
08:49:37 * shapr bounces
08:53:21 <Igloo> How's it going, shapr?
08:59:12 <shapr> Igloo: life is good, how are you?
08:59:26 <shapr> oh, I've been reading those three files, I have some questions
08:59:57 <shapr> Igloo: do you have a few minutes?
09:01:01 <Igloo> I'm fine, and yes, certainly do
09:01:20 * shapr looks at his notes
09:02:15 <shapr> ok, it looks like the InfM monad in after is just a straight replacement for the explicit call
09:02:43 <shapr> in that case, the only advantage is that you don't have to worry about remembering that extra argument each time, is that correct?
09:03:28 <Igloo> Explicit pair of arguments you mean?
09:03:29 <Igloo> Yup
09:03:29 <shapr> as for the implementation of bindM with those two nested case statements
09:03:35 <Igloo> And the code become smaller because you aren't passing things around all over the place
09:03:38 <shapr> I think function application is happening in there, but I'm not really sure
09:04:11 <shapr> is v being applied to bs? or is that pattern matching?
09:04:13 <Igloo> I think that's a standard pattern used by any monad that carries state around
09:04:19 * Igloo goes to find the code
09:04:21 <shapr> I think f is being applied to x
09:05:10 <Igloo> both are applications, yes
09:05:14 <shapr> oh, ok
09:05:36 <shapr> I get the impression that it's happening in reverse, f is applied to x before v is applied to bs
09:06:00 <Igloo> So you take the current state bs, apply it to v to get the intermediate value x and new state bs'
09:06:22 <Heffalump> am I the only one that keeps netsplitting?
09:06:32 <shapr> big netsplits
09:06:41 <listener> I've got an ocaml problem, but I think it is also a Haskell problem because it involves srong typing.
09:06:51 <shapr> hello thaddeus
09:06:58 <listener> let fac n=
09:07:00 <listener>   let rec int_fac n h=
09:07:02 <listener>     if(n<2) then 1
09:07:04 <listener>     else n * ( h (n-1) h) in
09:07:05 <listener>   int_fac int_fac n;;
09:07:11 <listener> Hi. shapr.
09:07:14 <Heffalump> grr, freenode hide all their network topology
09:07:32 <mgoetze> Heffalump: what do you need to know? :)
09:07:34 <listener> The problem is the second h.
09:07:39 <Igloo> What is h for?
09:07:43 <andersca> fac n = product [1..n] , right? :)
09:07:46 <Heffalump> mgoetze: where the splits are happening and which side I should move to
09:07:50 <Heffalump> (if at all)
09:08:08 <shapr> I always join irc.se.freenode.net
09:08:15 <shapr> cuz it's close
09:08:17 <Heffalump> I think the code is wrong
09:08:25 <Heffalump> after in, it should be "int_fac n int_fac;;"
09:08:29 <mgoetze> Heffalump: they should be over soon
09:08:39 <shapr> Igloo: so there are three inputs here?
09:08:45 <shapr> where does bs come from?
09:09:12 <Heffalump> igloo: h is so that int_fac isn't explicitly recursive, I guess (so I dunno why it uses let rec)
09:09:14 <shapr> I get the feeling I should go through the state monad tutorial again.
09:09:52 <Igloo> Oh, I see, h and n are the wrong way round
09:10:00 <listener> Heffalump: it uses let rec because I was trying anything I could think of to make it work ;). It really shouldn't be there.
09:10:19 <Heffalump> right :-)
09:10:38 <listener> Also the order on int_fac n int_fac, I changed the order of the functions.
09:11:52 <listener> It's supposed to be factorial without being recursive. ( Gee guess what paper of Richard Gabriel's and what chapter of Little Schemer I'm reading.) Hint: They both have the same name.
09:12:16 <Igloo> shapr: bs for one monadic value comes from one on the left of a >>= or, for the first one, from set_bits. I have to pass an undefined in at the very left in extract, which I'm not sure is the standard thing to do, but it works so I didn't go looking to see what others did
09:12:52 <Igloo> listener: scheme has iterative loops
09:12:53 <shapr> oh!
09:12:55 <shapr> oh I see!
09:13:02 <shapr> set_bits constructs the first monad
09:13:04 <Igloo> Haskell doesn't
09:13:13 <shapr> er, actually it stuffs the bits into the monad
09:13:15 <shapr> sort of
09:14:11 * Igloo thinks this might be simpler to understand if it wasn't recursive  :-)
09:14:26 <shapr> monads are just hard.
09:14:32 <shapr> somedays I really do understand them.
09:14:55 <shapr> but it's easy to lose that understanding since I haven't used it much.
09:20:04 <Igloo> Have you got some code you could try to monadise?
09:20:17 <shapr> I'm not sure
09:20:47 <shapr> v is the entire monad coming in the from the left?
09:20:51 <shapr> %-O
09:20:53 * Heffalump still wants to use TH to automatically monadise code.
09:21:23 <shapr> you apply the left hand monad to bs ...
09:21:28 <shapr> but bs comes from the left hand monad...
09:21:29 <shapr> huh?
09:22:26 <Igloo> Oege keeps sending me more things to do with TH
09:22:27 <shapr> what code gets called when you apply v to bs?
09:23:02 <Igloo> shapr: This example is a bit nasty because the result is one of the parameters
09:23:37 <shapr> is that why you have two case statements rather than one?
09:23:43 <kunphuzil> Smerdyakov: hi
09:23:45 <Igloo> Or are you confused by the >>= definition?
09:23:46 <shapr> two applications?
09:23:52 <shapr> I really don't know :-)
09:24:04 <Igloo> More parentheses might help
09:24:09 <shapr> it looks like Harry Potty couldn't understand this ;-)
09:24:19 <Igloo> It's InfM (\bs -> (...))
09:24:20 <shapr> er, Potter
09:24:31 <shapr> ohhh
09:24:35 * shapr adds parens
09:24:57 <shapr> ohhh
09:25:01 <shapr> that makes a difference.
09:26:40 <shapr> argh
09:26:55 <shapr> staring at it is not helping.
09:27:09 <shapr> oh well
09:27:24 <shapr> Igloo: I'll study it more and get back to you, thanks for writing this :-)
09:30:02 <listener> Getting back to my question. The caml group showed me a way to do it. But I'm still
09:30:15 <listener> curious about how a part can be handled in ML.
09:30:20 <Smerdyakov> Hm, the logs for this channel are stored on a Tunes server? =D
09:30:35 <Smerdyakov> "A part"?
09:30:40 <listener> The problem is that the h is an ambiguous type.
09:31:08 <Smerdyakov> What was your question, listener? (I wasn't here.)
09:31:14 <listener> First it was used as int-> 'a.
09:31:34 <listener> THen int->int->'a. Then ...
09:32:24 <Heffalump> listener: that's not ambiguous, that's just illegal
09:32:35 <listener> What it really is is  rec_fun=int->'a|int->rec_fun
09:32:35 <Smerdyakov> Can someone tell me listener's question?
09:33:07 <shapr> Smerdyakov: you could look at the logs :)
09:34:03 <Smerdyakov> shapr, I could if they updated in realtime.....
09:34:20 <Smerdyakov> Oh, apparently they do
09:34:24 <Smerdyakov> And it was just taking a while to load =)
09:34:53 <listener> Smerdyakov: let fac n=
09:34:59 <listener>   let int_fac n h= if(n<2) then 1
09:35:01 <listener>     else n * ( h (n-1) h) in
09:35:03 <listener>   int_fac n int_fac;;
09:35:25 <Heffalump> doesn't it work like that?
09:35:42 <Igloo> No
09:35:46 <listener> It's really a ocaml question, but I was curious about how Haskell gets around the typing of h.
09:35:49 <Heffalump> I don't see any type ambiguity in that.
09:35:51 <Igloo> Oh, wait, int_fac
09:36:02 <smkl> listener: it's type is: val int_fac : int -> (int -> 'a -> int as 'a) -> int
09:36:42 <Heffalump> oh, duh
09:37:02 <Smerdyakov> Why would you want to "get around it"? Why would you want to use this code?
09:37:27 <Heffalump> You can't do things like that in a strongly type language.
09:37:30 <Heffalump> Haskell just uses recursion
09:37:38 <listener> It's an example in Richard Gabriel's Why of Y.
09:37:53 <Heffalump> right, and you can't write Y in Haskell or O'Caml
09:38:05 <Smerdyakov> Maybe someone can explain to me why anybody cares about the Y combinator in real programming? =)
09:38:24 <listener> Because of a paper "That About Wraps it up." by Bruse McAdam.
09:38:34 <Smerdyakov> Heffalump, I've seen an ML implementation that compiles, but takes as input a value of an uninhabited datatype =)
09:38:57 <Smerdyakov> listener, what is that paper's thesis that gives the Y combinator relevance to practical programming?
09:39:08 <listener> http://www.lfcs.informatics.ed.ac.uk/reports/97/ECS-LFCS-97-375/
09:40:00 <listener> It claims it present a programming technique combining Y combinator and "wrappers" which
09:40:36 <Smerdyakov> I shall read it.
09:40:37 <listener> "give programmaers a level ofcontrol over the internal workigns of functions not otherwise possible without rewriting and recompiling code".
09:40:53 <Smerdyakov> Possibly first class continuations give the same control.... we shall see.
09:41:10 <listener> Haskel has continuations?
09:41:57 <Smerdyakov> I don't know. SML/NJ does.
09:42:08 <Smerdyakov> [BRB]
09:47:19 <Janni> and back again...
09:56:14 <Smerdyakov> listener, this paper makes it clear that the technique used does not use the Y combinator in full generality, but only for a special case.
09:57:49 <Smerdyakov> listener, do you have any other reason why anyone should care about the Y combinator in real programming? =)
09:59:34 <listener> Nope. I was interested in Y combinator because of the paper. I feal if I learn something like Y combinator, I should do a thorough job.
10:00:18 <listener> I'm really more interested in debugging techniques in FP languages. 
10:00:54 <Smerdyakov> OK
10:01:04 <Smerdyakov> Do you prefer other programming paradigms?
10:02:03 <listener> I don't know what you mean by prefer. I ( like most programmers ) am much more familiar with other programming paradigms.
10:03:03 <listener> But in terms of prefer, does a carpenter prefer to use samd paper instead of a file? Both are just tools.
10:03:27 <Smerdyakov> If you have to write an application, daemon, etc., for x86 Linux, say, what is the first language that pops into your head to use?
10:04:00 <jmalicki-work> smkl: haskell
10:04:02 <Heffalump> depends on the application
10:04:30 <Heffalump> and in particular on what languages have good library support for the problem domain
10:04:49 <Smerdyakov> I'm asking listener. :P
10:05:19 <listener> Definitely not Smalltalk.
10:05:20 <Smerdyakov> And perhaps we can go ahead and assume perfect library support for all languages for the purpose of this question.
10:05:50 <smkl> jmalicki-work: ??
10:06:02 <Smerdyakov> listener, and a _positive_ answer? =)
10:06:02 <listener> OR perl, python or ruby.
10:06:19 <listener> The language the boss tells me to use.
10:07:13 * Smerdyakov rolls his eyes.
10:08:34 <Heffalump> Whether or not you asked me, I'm going to answer :-p - Given perfect library support I would use Haskell unless performance was critical or I needed some algorithm or data structure which was unpleasant without mutable references.
10:09:44 <Smerdyakov> I think I'm with you, Heffalump, but I'll use ML for the time being, given compilation speed and interfacing-with-nasty-code issues =)
10:10:03 <Smerdyakov> I mean execution speed, not compilation speed
10:10:08 <Heffalump> heh
10:10:34 <Smerdyakov> Hm... and I wonder how easy it is to produce certified Haskell executables versus certified ML executables.
10:11:34 <Heffalump> certified?
10:12:22 <Smerdyakov> Containing proof of interesting properties, like type safety
10:12:39 <Heffalump> ah
10:13:03 <Heffalump> Is there any practical (i.e. not just a research prototype) implementation of those things?
10:13:06 <listener> I would use the programming language that I am learning at the time, if the problem was simple enough ( and most are ) that I fealt I could manage it easily ( to make up for my clumsiness with the language ). If I expect the problem to get hairy then I use the language that I'm strongest in at the time.
10:15:25 <Smerdyakov> Heffalump, yes. I know of at least one dead company that sold a certifying native code Java compiler =)
10:16:33 <Smerdyakov> listener, interesting viewpoint. I think most of us here would say that languages like Haskell and ML are just inherently better suited to putting together working solutions in small amounts of time, so you would be in trouble if you were learning something else when you found a problem. =)
10:18:17 <Heffalump> certifying in what sense?
10:18:32 <Smerdyakov> Type safety: can't crash
10:18:49 <Heffalump> ok, but who checks the certificate?
10:18:53 <Smerdyakov> Where "crash" is anything it would do that would cause an operating system spanking
10:19:09 <Heffalump> and what is the certificate
10:19:11 <Smerdyakov> A certificate checking program
10:19:20 <Heffalump> who, not what
10:19:26 <Heffalump> i.e. which computer
10:19:29 <listener> And is the OS certified?
10:19:34 <Smerdyakov> The computer running the program
10:19:39 <Heffalump> that's not relevant, you trust your OS
10:19:43 <Smerdyakov> listener, that's another issue altogether
10:19:52 <Heffalump> why not just download the bytecode and use a JIT?
10:20:06 <listener> You can do everything right and the program can crash the OS, because the OS is buggy.
10:20:17 <Smerdyakov> I think JIT's today will insert extra checks at runtime to avoid possible nastyness.
10:20:32 <Smerdyakov> The compiler of which I speak can perform smart optimizations and include proofs that they're OK.
10:20:33 <listener> Go to go. Thanks. Bye.
10:20:39 <Heffalump> right
10:20:44 <Heffalump> Got a link for it?
10:20:48 <Smerdyakov> No need for the client computer to perform costly optimization processing itself.
10:21:28 <Smerdyakov> Outdated page by one of the co-inventors of the idea and co-people-in-charge of the dead company: http://www-2.cs.cmu.edu/~petel/papers/pcc/pcc.html
10:22:35 <Smerdyakov> Outdated especially because many people accomplish the same goals using an approach less like an explicit logical proof: we use typed assembly language instead.
10:23:18 <Heffalump> the assembly language directly translates to machine code for a particular CPU? (Alpha?) 
10:24:08 <Smerdyakov> It IS regular assembly language, with type annotations.
10:24:18 <Heffalump> right
10:28:42 <Janni> and back once more...
11:11:27 <Janni> re
11:11:44 <kunphuzil> what does re mean?
11:13:04 <Janni> a short form for "rehi", which is used if you return to a channel after having parted a short time ago
11:13:31 <kunphuzil> and what is rehi then?
11:14:00 <Janni> knuphuzil: a short form of s.th. like "hi again"
11:14:14 <Janni> knuphuzil: so "re" is a double short form
11:14:15 <kunphuzil> oh, ok
11:14:19 <kunphuzil> heh
11:33:06 * shapr bounces
11:33:19 <shapr> rockin to Chemical Brothers
11:33:24 <shapr> Janni: which Aphex Twin?
15:01:13 <johs> Good evening.
18:28:51 * Igloo bounces
18:34:38 <Heffalump> hmm?
18:35:33 <Igloo> I have a simple, incorrect, slow, but working Haskell evaluator
18:36:23 <Smerdyakov> With type classes? =)
18:36:54 <Heffalump> in TH?
18:36:58 <Igloo> No  :-)
18:37:02 <Igloo> No, in normal Haskell
18:37:06 <Heffalump> why?
18:37:10 <Heffalump> oh, semantics?
18:37:24 <Igloo> Yeah - the code is reasonably close to the semantic rules I have
18:37:40 <Igloo> Unfortunately I might need to implement the type classes and inference stuff
18:58:28 <Pseudonym> Igloo: Does it support strong laziness?
19:01:32 <Igloo> What is strong laziness?
19:01:48 <Igloo> If it's the same as full laziness then no
19:02:21 <Igloo> In fact it does the opposite of full laziness (while still being lazy)
19:05:46 <Pseudonym> Sorry, yes, full laziness.
19:39:11 <Smerdyakov> What is full laziness?
19:39:51 <Smerdyakov> Never mind, I asked the Internet.
19:41:08 <lament> what is partial laziness?..
19:41:49 <whee> lament: probably providing some language construct for lazy features, instead of it being automatic
19:42:20 <Smerdyakov> Just look up what ful laziness is, like I did :P
19:43:22 <Pseudonym> :-)
19:43:36 <Pseudonym> Generally speaking: Laziness means that if it's not needed, it won't be evaluated.
19:44:00 <Pseudonym> Full laziness is the added restriction that it will be evaluated at most once.
19:44:11 <lament> oh.
19:44:21 <lament> so, laziness vs. call-by-name?
19:44:26 <Pseudonym> Er... no.
19:44:39 <Smerdyakov> I think that is a bad/wrong way of putting it, Pseudonym.
19:44:50 <Pseudonym> How would you put it?
19:44:53 <whee> Smerdyakov: that's how it is, though
19:45:07 <Smerdyakov> The way the Internet puts it: http://burks.brighton.ac.uk/burks/foldoc/14/46.htm
19:45:27 <Pseudonym> Well the Internet is always right, of course. :-)
19:45:32 * Pseudonym looks it up
19:45:42 <Smerdyakov> Though I have just now read this, it sounds like a very specific optimization, as opposed to something as overarching as "nothing will be evaluated more than once."
19:45:53 <Pseudonym> Oh, no.  That's not what I said.
19:46:03 <Pseudonym> Not "nothing will be evaluated more than once".
19:46:47 <Smerdyakov> I think I have failed to resolve a cross-sentence "it" as you intended, then :P
19:47:18 <Smerdyakov> <Pseudonym> Full laziness is the added restriction that it will be evaluated at most once. <-- "it"?
19:47:34 <tez> "It" being shared constant expressions.
19:47:45 <Pseudonym> "Generally speaking: Laziness means that if it's not needed, it won't be evaluated." <- same "it"
19:48:36 <Smerdyakov> What is "it" there?
19:49:18 <Pseudonym> To be more precise, it's subexpressions that do not depend on any bindings in the enclosing lambda.
19:49:49 <Smerdyakov> Is that an emendment of your definition of full laziness?
19:50:02 <Pseudonym> It's a less "generally speaking" version.
19:51:19 <Smerdyakov> You still have a free "it" :P
19:51:39 <lament> "it"s are evil.
19:52:26 <Pseudonym> \ it -> previousStatement it
19:52:40 <Smerdyakov> "It" is free in the previous statement as well.
19:55:28 <Pseudonym> Actually, no, now that I think of it.
19:55:36 <Pseudonym> I said "If it's x, then it's y".
19:55:46 <Pseudonym> In a logic language, that's:
19:56:13 <Pseudonym> all [X] ( not needed(X) => not evaluated(X) )
19:56:55 <Pseudonym> So in total:
19:57:06 <Smerdyakov> In one way of interpreting your English, yes. In others, no. :P
19:57:39 <Pseudonym> full_laziness :- all [X] ( not needed(X) => not evaluated(X) ), all [X] ( evaluated(X) => evaluated_once(X) ).
19:58:05 <Pseudonym> Where "evaluated_once" means something specific.
19:58:27 <Smerdyakov> I think I'd like to know what evaluated means, myself.
19:58:53 <lament> 'normalized' :)
19:59:04 <Pseudonym> Yes, converted to weak head normal form. :-)
19:59:08 <Smerdyakov> And what is X?
19:59:16 <Pseudonym> X is a subexpression.
19:59:25 <Smerdyakov> A textual subexpression in the code?
19:59:39 <Pseudonym> To a first approximation, yes.
19:59:52 <Pseudonym> Modulo syntactic sugar.
19:59:53 <Smerdyakov> I'm not asking about approximations :P
20:00:36 <Smerdyakov> Then even your definition of lazyness is faulty... since an expression may need to be evaluated twice, with different free variable values.
20:00:43 <lament> then X is a lambda form to which subexpressions of our language get converted
20:00:51 <Pseudonym> Ah, but that's okay.
20:00:55 <lament> i mean, X is one of the lambda forms
20:01:08 <Pseudonym> If there are different free variable values, they have different weak head normal forms.
20:01:16 <Pseudonym> Either way, the lambda body will only be normalised once.
20:01:29 <Pseudonym> Probably at compile time.
20:02:17 <Pseudonym> Take, for example: \x -> sqrt x
20:02:20 <Pseudonym> It's already in WHNF.
20:02:32 <Pseudonym> So it's fully evaluated.
20:02:42 * Smerdyakov jumps up and down in glee!
20:03:14 <Smerdyakov> What about a case expression with multiple possible branches to be taken?
20:03:36 <lament> what about it? Cases can be converted to lambdas
20:03:52 <Pseudonym> lament is right
20:04:06 <Pseudonym> Algebraic data types are just syntactic sugare.
20:04:21 <Pseudonym> data Maybe a = Just a | Nothing
20:04:28 <Pseudonym> That's syntactic sugar for:
20:04:39 <Pseudonym> Just a = \ j n -> j a
20:04:42 <Pseudonym> Nothing = \j n -> n
20:05:16 <Smerdyakov> Ya, I know. Nonetheless, I remain convinced for whatever reason that your textual expression-based definition of lazyness is unsatisfactory. Let me ponder why =)
20:05:17 <Pseudonym> case x of { Just a -> y a ; Nothing -> z } = x y z
20:07:19 <Smerdyakov> What about recursion?
20:07:26 <Pseudonym> Fixpoint operator.
20:08:25 <Pseudonym> fac x = if x == 0 then 1 else x * fac (n-1)
20:08:28 <Pseudonym> is the same as:
20:08:39 <Pseudonym> fac x = y (\fac' -> if x == 0 then 1 else x * fac' (n-1)f)
20:08:45 <Pseudonym> where y is the fixpoint operator
20:09:41 <Smerdyakov> I think I heard just earlier today that you can't implement y in Haskell.
20:09:49 <Pseudonym> Yes you can.
20:09:49 <lament> eh
20:09:59 <Pseudonym> The classic version is untypeable.
20:10:08 <Pseudonym> But you can type: y f = f (y f)
20:10:45 <Pseudonym> But that doesn't matter.
20:10:59 <Pseudonym> Once your program is proven type correct, you're allowed to do what you like internally.
20:11:12 <Pseudonym> So long as it doesn't make a type correct program non-type-correct.
20:11:36 <Smerdyakov> Yeah, so back up a little more.... if a named function is called in more than one place, you would inline expand it with each call?
20:12:05 <Pseudonym> Er... no.
20:12:23 <Pseudonym> Suppose you have: sqrt x = STUFF
20:12:30 <Pseudonym> Then: f x y = sqrt x + sqrt y
20:12:34 <Pseudonym> That would be:
20:12:52 <Pseudonym> f x y = (\sqrt -> sqrt x + sqrt y) STUFF
20:13:16 <Pseudonym> Sorry: sqrt = STUFF
20:13:19 <Pseudonym> But you get the idea.
20:14:46 <Smerdyakov> So we have
20:14:52 <Smerdyakov> addOne x = x + 1
20:14:58 <Pseudonym> OK.
20:15:02 <Smerdyakov> answer = addOne 1 + addOne 2
20:15:10 <Pseudonym> That's equivalent to:
20:15:29 <Pseudonym> answer = (\addOne -> addOne 1 + addOne 2) (\x -> x + 1)
20:16:19 <Smerdyakov> And how do we evaluate that without evaluating a subexpression more than once, with a model true to how a Haskell compiler will work (i.e., no fair substituting function bodies for let-bound variables!)
20:17:17 <Pseudonym> You have to understand that "evaluating" means converting to WHNF.
20:17:47 <Smerdyakov> So you go about doing that and now (\x -> x + 1) has been copied to two places.
20:17:53 <Pseudonym> No.
20:17:55 <Pseudonym> It's shared.
20:18:19 <Smerdyakov> At some point, I would say that x + 1 is evaluated twice.
20:18:34 <lament> with different values of x
20:18:39 <Pseudonym> But it's a different x + 1.
20:18:59 <Smerdyakov> It comes from the same textual subexpression of the code translated to Mr. Fancy Pants Form. :P
20:19:05 <Pseudonym> You can't evaluate x + 1 because it has a free variable.
20:19:37 <Pseudonym> Note that I said: IF it's evaluated, it's evaluated only once.
20:19:38 <Pseudonym> x + 1 isn't evaluated.
20:20:07 <Pseudonym> 1 + 1 is evaluated and 2 + 1 is evaluated.
20:21:04 <Pseudonym> \x -> x + 1 can be evaluated (only once, mind you) but it's already in WHNF.
20:21:05 <Smerdyakov> Nonetheless, I asked you if the expressions whose evaluation or lack thereof your definition of laziness concerns are identified entirely by textual parts of the source code. You said yes, modulo a linear-size transformation. Correct?
20:21:35 <Pseudonym> I said : To a first approximation, yes, modulo syntactic sugar.
20:22:36 <Pseudonym> Obviously it has to be something that _can_ be evaluated.
20:23:21 <Pseudonym> x + 1 can't be evaluated.  to prove it:
20:23:22 <Smerdyakov> So "syntactic sugar" is not restricted to being a single linear-size transformation?
20:23:29 <Pseudonym> Prelude> x + 1
20:23:35 <Pseudonym> <interactive>:1: Variable not in scope: `x'
20:24:20 <Pseudonym> No, it need not be linear-size.
20:24:34 <Smerdyakov> Then it could be pre-evaluation of the whole program :P
20:24:44 <Pseudonym> Converting to SK combinators, for example, requires quadratic size (unless you introduce some redundant combinators).
20:25:45 <Pseudonym> Modulo I/O, there's nothing stopping a Haskell implementation from executing everyting at compile time.
20:26:00 <Pseudonym> There isn't even an explicit requirement in the spec that the compiler must terminate. :-)
20:26:24 <Pseudonym> As has been pointed out to me on the mailing list, Haskell overloading is actually DEXPTIME-hard.
20:26:43 <Pseudonym> So even static analysis is going to be more than linear.
20:26:48 <Smerdyakov> OK... so how is my example above transformed to an expression, such that everything about whose evaluation we are interested is a separate textual subexpression?
20:27:12 <Pseudonym> Will SK combinators do?
20:27:33 <Smerdyakov> I guess. Not that I know what they are ;)
20:28:01 <lament> Pseudonym: use typed lambda-calculus, it's much more convenient :)
20:28:31 <lament> and everything is expressed in terms of lambdas in the haskell standard, anyway
20:28:56 <Pseudonym> True.
20:29:12 <Pseudonym> SK combinators are convenient, I think, because it shows that the variables are all artificial.
20:29:43 <lament> they aren't convenient to someone who doesn't know them :)
20:30:04 <lament> hm, writing a haskell->lazy k compiler would be cool.
20:30:10 <Pseudonym> S (B (+) (C I 1) (C I 2)) (C (+) 1)
20:30:16 <Pseudonym> There you go.
20:30:18 <Pseudonym> Where:
20:30:25 <Pseudonym> S f g x = f x (g x)
20:30:30 <Pseudonym> B f g x = f (g x)
20:30:39 <Pseudonym> C f g x = f x g
20:30:42 <Pseudonym> I x = x
20:31:06 <Pseudonym> Hang on, that's not quite right.
20:31:34 <Smerdyakov> OK. And which part corresponds to adding together 1 and 1?
20:33:13 <Pseudonym> S (B (+) (C I 1)) (C I 2) (C (+) 1)
20:33:15 <Pseudonym> That's it.
20:33:21 <Smerdyakov> ^ question remains =)
20:33:30 <Pseudonym> OK, perform one reduction, you get:
20:33:38 <Pseudonym> let x = C (+) 1
20:34:09 <Pseudonym> in B (+) (C I 1) x (C I 2 x)
20:34:14 <Pseudonym> Note the x is shared.
20:34:26 <Pseudonym> Expand again:
20:34:51 <Pseudonym> let x = C (+) 1 in (+) (C I 1 x) (C I 2 x)
20:35:16 <Pseudonym> Expand the first argument:
20:35:18 <Smerdyakov> So your definition referred to textual subexpressions during any step of reduction, not in the original code?
20:35:28 <Pseudonym> let x = C (+) 1 in (+) (I x 1) (C I 2 x)
20:35:30 <Pseudonym> let x = C (+) 1 in (+) (x 1) (C I 2 x)
20:36:22 <Pseudonym> Uhm... without thinking about it too hard, I believe so.
20:36:58 <Smerdyakov> Well, the "in the original code" version can't be correct if the "syntactic" sugar has to avoid exponential size blowup :P
20:37:37 <Pseudonym> This is true.
20:37:55 <Smerdyakov> And so now I understand your original definition, but I think it wasn't stated very clearly. =)
20:38:03 <Pseudonym> Oh, I know that.
20:38:15 * Pseudonym said at the time it was in "general terms"
20:38:59 <Pseudonym> If you see the SK example, C (+) 1 is indeed evaluated only once.
20:39:12 <Pseudonym> It just happens to already be in WHNF.
20:39:36 <Smerdyakov> Yes, I no longer care about the answer to my example question given your clarification.
20:39:44 <Pseudonym> :-)
22:29:01 <Pseudonym> Night everyone.
