00:07:23 * earthy ponders
00:07:32 * andersca boings
00:07:47 * earthy thinks back to the time he did some work on formally proving deadlock-free behaviour of automata
00:07:55 <earthy> this is Not Trivial by any means
00:07:58 * Pseudonym laughs
00:08:05 <Pseudonym> Yeah, I can imagine.
00:08:22 <Pseudonym> I'm thinking more engineering than science, though.
00:08:29 <earthy> but, yeah, most deadlocks are of the `obviously wrong' type.
00:08:47 <earthy> there's more than enough well-known locking algorithms
00:09:05 <earthy> that don't deadlock
00:09:39 <Pseudonym> Well that's true, but not all operating systems/environments provide them.
00:09:57 <Pseudonym> For example, priority ceiling protocol only makes sense if you have fixed-priority threads.
00:10:06 <Pseudonym> Unix time-scheduled threads don't count.
00:10:35 <earthy> no, but what I learned from doing BeOS programming was that there is *very* simple methods of ensuring moderately safe behaviour: always lock and unlock in the same order
00:10:45 <Pseudonym> Yes, there are.
00:11:00 <Pseudonym> And, IMO, most threading problems are like this.
00:11:06 <earthy> precisely
00:11:26 <earthy> it's the complex algorithms that you don't need too often that tend to be hard to get deadlock-free
00:11:39 <Pseudonym> Either there's an obvious order, or the programmer is careful to ensure that data is encapsulated appropriately (e.g. a resource is controlled by a _thread_, not by a mutex).
00:11:45 <Pseudonym> Right.
00:11:54 <Pseudonym> And you know in advance that you're in one of those situations.
00:12:00 <earthy> exactly
00:12:08 <Pseudonym> That's exactly what I figure.
00:12:12 <earthy> but, this needs to have been taught.
00:12:24 <Pseudonym> Incidentally, I'm convinced that most synchronisation problems are really encapsulation problems in disguise.
00:12:38 * earthy ponders that a bit\
00:12:45 <Pseudonym> Concurrent Haskell programmers, for example, don't seem to have these problems.
00:12:49 <Pseudonym> But there's no global state!
00:13:03 <Pseudonym> Erlang programmers tend not to have these problems either.
00:13:27 * earthy agrees, *most* syncrhonisation problems are not due to synchronisation at all
00:13:35 <Pseudonym> Right.
00:13:39 <Pseudonym> Emphasis on the _most_.
00:14:13 <Pseudonym> However, threads are there to solve _hard_ synchronisation problems.
00:14:22 <earthy> that 5k-line multithreaded thing I wrote in python really really benefited from encapsulating the threads and the involved synchronisation into a small set of objects.
00:14:24 <Pseudonym> But most problems are not "hard".
00:14:31 <Pseudonym> Right.
00:15:08 <Pseudonym> I'm actually writing an essay in an attempt to respond to things like this: http://www.catb.org/~esr/writings/taoup/html/ch07s03.html#id2924224
00:18:16 <Pseudonym> My thesis is that programming with threads is hard, but when used appropriately, it's often no harder than the alternatives.
00:18:32 <earthy> hm. yeah. well.
00:18:44 <Pseudonym> Threads aren't the most appropriate solution to many problems, but it's an important tool in the toolbox.
00:18:47 <earthy> there is a point at which you *need* coroutines
00:19:08 <earthy> the question of them running in parallel or not is often irrelevant
00:20:09 <earthy> I fully agree with the `useful tool in the toolbox' adage
00:20:37 <earthy> however, separate processes, in a sense, can already be seen as threads. it's the IPC mechanism that needs to be appropriate.
00:20:42 <Pseudonym> A classic piece of stupidity is "fine-grained locking can be more expensive than a big lock".
00:20:50 <Pseudonym> Well... DON'T DO THAT.
00:21:10 <Pseudonym> Putting in fine-grained locking when you don't need it is just like any other premature optimisation.
00:21:21 <earthy> yeah. fine-grained locking should, as a rule, be designed to allow *removal* of locks.
00:21:27 <Pseudonym> Right.
00:21:49 <Pseudonym> And, of course, if your code is appropriately encapsulated, you should be able to move to fine-grained locking and not touch code which uses it.
00:22:21 <Pseudonym> Oh, I do think there's a continuum between multi-processing, event loops and threads.
00:22:49 <Pseudonym> It largely depends on how much shared state there is and how separable it is.
00:23:08 <Pseudonym> Also, it depends how responsive your program has to be.
00:23:20 <earthy> the question is `should there be such a continuum?'
00:23:35 <Pseudonym> Well there's a continuum in the problem space.
00:23:41 <Pseudonym> Inevitably there is.
00:24:00 <earthy> no. there is such a continuum in the current `solution' space.
00:24:05 <Pseudonym> True.
00:24:20 <earthy> which implies that no really satisfying solutions have been found and/or implemented.
00:24:25 <Pseudonym> Multiple processes with SysV shared memory and SysV semaphores are one step away from threads with thread local storage.
00:24:37 <earthy> true enough.
00:25:08 <Pseudonym> Well, I think there are satisfying solutions, but they're language-level abstractions.
00:25:14 <Pseudonym> Encapsulation isn't always the OSes problem.
00:25:20 <Pseudonym> The programmer has to take some of the blame.
00:25:30 <earthy> ah, yes, *some*. :)
00:25:34 <Pseudonym> :-)
00:25:38 * earthy remembers the KeyKOS stuff
00:25:54 <earthy> in which a `thread' was just a capability to actually run on the processor
00:26:05 <Pseudonym> Well, even Java allows multiple tasks to run in the same OS process and not step on each other.
00:26:19 <earthy> and the capability was parametrised... so threads could be stopped from the OS at will
00:26:30 * Pseudonym nods
00:26:44 <earthy> there *is*, therefore, a way to do it cleanly at the OS level
00:26:53 <earthy> it's never been popular, though. ;)
00:26:58 <Pseudonym> I guess my point is that threads, even as currently implemented, are not inherently a "problem".
00:27:05 <earthy> owh, no
00:27:10 <earthy> I fully agree.
00:27:13 <Pseudonym> You can misuse them, but then you can misuse file I/O.
00:27:27 <Pseudonym> Hell, threads are somewhat unportable, but then so is file I/O.
00:27:40 <earthy> on the other hand, warning against overuse of threads is a good thing to do
00:27:47 <Pseudonym> Anyone who has tried to get code which manages file permissions running under both Unix and Win32 can tell you that.
00:27:53 <Pseudonym> Well, yes.
00:28:03 <Pseudonym> So is overuse of signals, or overuse of fork().
00:28:34 <Pseudonym> There's a happy medium, and I'm trying to find it.
00:29:02 <earthy> good luck. :)
00:29:18 <Pseudonym> Well, at first, I'm just trying to say that there is indeed a happy medium.
00:29:43 <Pseudonym> And provide enough anecdotal evidence to counteract stuff like that section by ESR which I just pasted a link to.
00:29:58 <earthy> ah, yes. true.
00:31:33 <Darius> Pseudonym: ever look at Mozart/Oz?
00:32:31 <Pseudonym> Not really.  I know something about it, but I've never used it.
00:32:50 <Pseudonym> The MErcury people used to talk to Peter Van Roy a lot.  Probably still do.
00:32:55 <earthy> I have roughly 2 datapoints on it. :)
00:33:25 <earthy> one is that it is reckoned to be cool, the other is that it is reckoned to be slow...
00:33:44 <Pseudonym> Yup.  Cool, fast.  Pick any one.
00:34:28 <Darius> It does look rather cool... and like it would be slow.
00:34:53 <earthy> well, the datapoint of it being slow I've got out of a PhD thesis... :)
00:35:19 <Darius> The combination of dataflow variables and concurrency does seem like a very good idea, but I haven't used it either.
00:36:39 <Darius> I believe Curry does the same thing, but I'm less aware of Curry's concurrency features.
00:51:58 <Pseudonym> Night all.
03:05:01 <d33p> hey clausen 
03:05:10 <clausen> gday d33p
03:05:41 <d33p> could i get some number theory help :)
03:09:44 <clausen> d33p: yep
03:11:11 <d33p> ok, iam to show Z/pZ forms a group (under modulo p) 
03:11:27 <d33p> and i looked through the properties of a group
03:11:37 <clausen> under multiplication, I take it ;)
03:11:45 <d33p> yes 
03:11:58 <d33p> iam not sure which operation actually forms the group :)
03:12:15 <clausen> well, it is very easy to show it is a group under addition
03:12:22 <clausen> it is also a group under multiplication
03:12:26 <clausen> but this is a little more tricky
03:12:32 <d33p> but the thing is, you showed me what n/nZ looks like the other day, i dont know how you can pick two elements from the set and show closure (since they are sets)
03:12:35 <clausen> (and is probably what the question is about)
03:12:59 <clausen> n/nZ ?
03:13:03 <d33p> so A, B from Z/pZ, and show say A.B is in Z/nP (they are sets)
03:13:04 <clausen> I think you mean Z/nZ
03:13:10 <d33p> sorry, yes, Z/nZ 
03:13:23 <clausen> you mean pZ (in the last case) ?
03:13:32 <clausen> (well Z/pZ)
03:13:34 <d33p> yes sorry :\
03:13:50 <clausen> I think you're getting a bit confused
03:14:01 <d33p> i meant A, B from Z/pZ and show A.B is in Z/pZ
03:14:06 <d33p> iam very much confused :)
03:14:06 <clausen> Z/pZ is different (but isomorphic) to (mod p) arithmetic
03:14:17 <clausen> Z/pZ is a set of p sets
03:14:22 <d33p> yup
03:14:39 <clausen> let I = {..., -p, 0, p, 2p, ... }
03:14:44 <d33p> there are different things with the same name?
03:15:03 <d33p> k
03:15:05 <clausen> then Z/pZ = {I, 1 + i, 2 + i, ..., p - 1 + I}
03:15:23 <d33p> yup, agreed
03:15:32 <clausen> where n + I = {..., n - p, n + 0, n + p, n + 2p, ... }
03:15:43 * d33p nods
03:16:11 <d33p> i get that much
03:16:47 <clausen> so, the elements of Z/pZ are sets
03:16:58 <clausen> we can define addition on these sets
03:17:14 <clausen> (a + I) + (b + I) = (a + b) + I
03:17:26 <d33p> oh
03:17:30 <d33p> wait
03:17:31 <d33p> you mean
03:17:36 <d33p> you can add a set and a set?
03:17:48 <clausen> perhaps I should be a bit clearer
03:17:49 <Cale> well, we're defining this addition.
03:17:52 <clausen> addition is merely a function
03:17:57 <clausen> yes, we are defining a function
03:18:00 <clausen> and calling it addition
03:18:05 <d33p> k
03:18:09 <clausen> because it looks like addition on the real numbers, that we are familiar with
03:18:17 <d33p> yup
03:18:49 <clausen> I should elaborate on that definition
03:18:54 <d33p> please do
03:18:58 <clausen> any element M of Z/pZ can be written in the form:
03:19:02 <clausen> M = m + I
03:19:10 <clausen> where m is an element of Z
03:19:23 <clausen> so, if we take M + N
03:19:24 <d33p> hm
03:19:26 <clausen> we can define it as:
03:19:39 <d33p> well we have  0<=m <p-1
03:19:41 <clausen> M + N = (m + I) + (n + I) = (m + n) + I
03:19:49 <clausen> that is not necessary
03:20:03 <clausen> it is possible to show that addition is well defined regardless
03:20:21 <d33p> hmm, but M is an element of Z/pZ
03:20:24 <clausen> that is, that the result of addition of M and N both exists and is unique
03:20:36 <clausen> regardless of the choice of representatives m and n
03:20:44 <d33p> you lost me
03:20:48 <clausen> (provided that m is an element of M)
03:20:56 <clausen> I'll give you an example
03:20:59 <d33p> ok
03:21:05 <clausen> pick Z/5Z, say
03:21:12 <Cale> It is possible to always *choose* an m in the range 0<= m <p-1 as a representative of M
03:21:19 <clausen> and I = 5Z
03:21:34 <clausen> now, say M = 3 + I
03:21:40 <clausen> I claim that M = 8 + I also
03:21:49 <d33p> oh!
03:21:59 <d33p> because after a while the elements repear
03:21:59 <clausen> in fact, if you take any element m of M
03:22:02 <clausen> M = m + I
03:22:05 <clausen> right
03:22:10 <d33p> so they are gonna already exist in Z/pZ
03:22:11 <d33p> i got you
03:22:12 <d33p> ok
03:22:30 <d33p> nice
03:22:52 <clausen> so, you can check that this satisfies the group axioms:
03:23:11 <clausen> (1) (A + B) + C = A + (B + C)
03:23:21 <clausen> (follows trivially from addition in the integers)
03:23:30 <clausen> (2) there is an identity element such that:
03:23:54 <clausen> A + I = I + A = A for all A in Z/pZ
03:24:19 <clausen> (3) there is an inverse: (-A) + A = I
03:24:50 <d33p> hmm, zero is an identity?
03:24:53 <clausen> yes
03:24:57 <d33p> ok
03:24:59 <clausen> well, I is the identity
03:25:03 <clausen> 0 + I
03:25:06 <clausen> (if that helps ;)
03:25:10 <d33p> alright
03:25:10 <Darius> (0 + I)
03:25:18 <clausen> I = 0 + I
03:25:36 <clausen> you also need to check the group operation is well defined
03:25:46 <clausen> (i.e. it always exists and is closed, and is unique)
03:25:48 <d33p> division and so on
03:25:53 <clausen> division?
03:25:59 <d33p> well i mean say division by zero
03:26:07 <d33p> thats the only undefined thing i've run into so far
03:26:09 <clausen> that doesn't come into it
03:26:14 <d33p> yeah
03:26:22 <clausen> there are other ways things can go wrong
03:26:35 <clausen> for example, you need to prove that 3 + I = 8 + I in that above example
03:26:38 <d33p> iam going to show (Z/pZ)^Mod p 
03:26:43 <clausen> ?
03:26:44 <d33p> so it will come into it, then i think
03:26:57 <clausen> anyway, this group is (Z/pZ, +)
03:27:04 <d33p> yea sorry, go on
03:27:05 <clausen> under addition
03:27:22 <clausen> you can also show that you can form a multiplicative group as well!
03:27:33 <d33p> hmm?
03:27:38 <d33p> on this group you mean
03:27:41 <clausen> where (m + I) * (n + I) = (mn + I)
03:27:41 <d33p> er set
03:27:41 <clausen> !
03:27:45 <clausen> yes
03:27:48 <d33p> k
03:28:07 <clausen> this time, you take the set to be {1 + I, ..., p - 1 + I}
03:28:11 <clausen> (we don't include I)
03:28:41 <d33p> so writing the elements of Z/nZ as i + nZ really helps to show we have a group 
03:28:51 <clausen> no
03:29:05 <clausen> it is merely the definition of Z/nZ
03:29:12 <d33p> oh
03:29:29 <clausen> numbers modulo a prime should be written a different way
03:29:30 <d33p> but you just used it to show closure under addition
03:29:37 <d33p> ah.
03:29:43 <d33p> thats true
03:29:44 <clausen> they are a different animal
03:29:50 <d33p> okay :)
03:29:52 <clausen> (although, very closely related)
03:30:00 <clausen> (that's why they are often written the same way)
03:30:06 <clausen> I suggest Z mod p
03:30:24 <clausen> or just Zp
03:30:28 <d33p> that congruence stuff?
03:30:32 <clausen> yep
03:30:37 <d33p> ok
03:30:44 <clausen> I'm going to watch TV now... back in an hour :)
03:30:49 <clausen> BTW, you might find the wikipedia helpful
03:30:51 <d33p> thanks so much
03:30:52 <clausen> wikipedia.org
03:30:57 <d33p> i'll check it out
03:30:58 <clausen> look for group theory, and number thoery
03:31:06 <d33p> have fun :)
03:32:14 <Cale> Once you get used to it enough, you just mention that you're working in Z/pZ or whatever, and leave the +I or mod p off of the numbers you're working with.
03:33:19 <d33p> Cale: yeah, its confusing for a newbie, people just leave it out and i didnt know what was going on
03:33:46 <Cale> Yeah, it can be.
03:36:03 <Cale> It's simply because in that case, we're using the symbol "0" to represent I, "1" to represent 1+I, and so on.
03:36:41 <d33p> is that when its written [n] 
03:36:51 <d33p> like [1], [2], ... 
03:37:03 <Cale> Our prof for the first couple of weeks recommended that we write the numbers of Z/nZ in brackets with a subscripted n.
03:37:29 <Cale> then we dropped them eventually, because it becomes tiring to write.
03:37:59 <Cale> [1]_4 + [3]_4 = [0]_4 
03:38:11 <Cale> 1 + 3 = 0 mod 4
03:38:29 <d33p> hmm :\
03:38:34 <Cale> or just 1 + 3 = 0 (with the implicit notion that you're working mod 4)
03:38:53 <d33p> oh, i see
03:40:05 <d33p> [a]_n + [b]_n = [(a+b) Mod n]_n 
03:40:48 <Cale> well, when mathematicians use mod, it's not generally as an operator
03:41:08 <d33p> right
03:41:15 <d33p> its an operator when you have mod k 
03:41:16 <Cale> [a]_n + [b]_n means the same thing as a + b (mod n)
03:41:52 <Cale> it's more of a notation, reminding that we're working modulo some integer.
03:41:59 <d33p> gotcha
03:42:23 <Cale> If it's obvious, it can be left out.
03:42:38 <d33p> okay
03:42:56 <Cale> (usually, which particular n is important, and it gets left in though)
03:43:28 <d33p> is the set {0} ever an element of Z_n ?
03:44:06 <Cale> No. All the elements of Z_n are countably infinite sets.
03:44:26 <Cale> They're just shiftings of nZ
03:44:32 <d33p> right
03:44:41 <d33p> so if its not an element... 
03:44:48 <d33p> how do i show such an element exists?
03:45:02 <Cale> Oh, have a look at nZ
03:45:12 <Cale> nZ contains 0
03:45:12 <d33p> k
03:45:30 <d33p> yeah, it does
03:46:00 <d33p> nZ= {... -n, 0, n, ...}
03:46:08 <d33p> oh
03:48:24 <d33p> Cale: it just contains one zero :\
03:48:29 <Rafterman> is Z/nZ { {..., -n, 0, n, ... }, {..., -n+1, 1, n + 1, ...}, {..., -n + 2, 2, n+2, }, ... {..., -1, n-1, 2n - 1, ...}}?
03:48:46 <d33p> Rafterman: yeah
03:48:49 * Rafterman doesn't know much group theory but is trying to follow the conversation :)
03:48:53 <Rafterman> ok cool
03:49:59 <d33p> Cale: can you go back to defining an operation on a set, specifically the operation we perform on the elements of that set 
03:50:35 <d33p> how is it done in general for an operation say *
03:50:44 <Cale> Okay, so an operation is a function of two parameters, that is closed
03:50:48 <Cale> say
03:50:58 <d33p> a binary operation, yeah
03:51:02 <Cale> *: A x A -> A
03:51:02 <Cale> yeah
03:51:21 <d33p> AxA -> A
03:51:29 <d33p> thats the cross product?
03:51:38 <Cale> the Cartesian product
03:51:42 <d33p> ok, sorry
03:51:49 <d33p> i should go read up on it, first
03:52:14 <Cale> the Cartesian product is the following, (you're probably familiar with it, actually)
03:52:42 <Cale> it's the set of pairs of elements taking one element form the first and one from the second set
03:53:08 <Cale> i.e. {a,b,c} x {1,2} = {(a,1),(a,2),(b,1),(b,2),(c,1),(c,2)}
03:53:20 <d33p> those are ordered tuples, or what?
03:53:24 <Cale> yeah
03:53:25 <d33p> oh
03:53:25 <d33p> wait
03:53:28 <d33p> you do the operation on them
03:53:29 <d33p> right?
03:53:57 <d33p> hold on
03:54:32 <Cale> Yeah, so it's like * is a 2 parameter function of an A and an A, or a 1 parameter function of the pairs. The two ways of looking at it are isomorphic.
03:54:59 <d33p> um, so just so we are clear, * is a binary operation, not necessarily multiply, right?
03:55:06 <Cale> Yeah
03:55:09 <d33p> cool
03:55:12 <d33p> okay
03:55:23 <d33p> so we have AxA gives an A 
03:55:25 <d33p> nice
03:56:23 <d33p> you said, an operation is a function of two parameters that is closed, *: AxA->A, and that its closed 
03:56:25 <d33p> what do you mean by its closed
03:56:26 <Cale> so when we define such an operation, we want to know what kind of structure it has... this is where we get into looking at whether we have a semigroup, a monoid, a group, or what.
03:56:37 <d33p> that it gives back an element thats in A, right?
03:56:43 <Cale> as in, it never fails to yield something for a given pair.
03:56:47 <d33p> ok
03:57:02 <Cale> and that something is always in A
03:57:12 <d33p> got it
03:57:49 <d33p> so depending on the operations we can use, we get either a semigroup, monoid a group or ... 
03:58:01 <d33p> what operations do we require for a "group"
03:58:08 <Cale> Just the one.
03:58:13 <d33p> i see
03:58:21 <d33p> and it is, product?
03:58:28 <Cale> All of those I mentioned are one operation structures
03:58:37 <d33p> oh
03:58:37 <Cale> you can call it product if you like
03:58:44 <d33p> :\
03:58:57 <d33p> but if they all satisfy that um, property, arent they the same thing?
03:59:00 <Cale> sometimes we call it sum, it's just "the group operation"
03:59:19 <Cale> each of those imposes conditions on the workings of that operation
03:59:34 <d33p> oh.
03:59:55 <d33p> please go on
04:00:30 <Cale> To say that (A, *) is a semigroup is to say that * has the associative property.
04:00:50 <d33p> thats just notation?
04:00:51 <Cale> i.e. if a,b, and c are in A, (a * b) * c = a * (b * c)
04:01:01 <Jerub> man, that topic-quote has got to be the longest running one I've ever seem/
04:01:05 <Cale> (A, *) is just notation
04:01:06 <Jerub> seen.
04:01:11 <d33p> cool
04:02:01 <Cale> Another way to put it is that A is a semigroup under the operation *.
04:02:13 <d33p> hold on
04:02:27 <Jerub> I've been reading up on parsers in haskell, and I'm wondering whats the 'best' parser library to use. I've read the monadic parser combinator (95) paper and the parsec (01) paper.
04:02:53 <Cale> Most people seem to like parsec. I do. I don't know about many others though.
04:03:04 <Cale> http://planetmath.org/encyclopedia/Semigroup.html
04:03:33 <Cale> that might make it a little clearer, I'm not sure.
04:03:52 <Jerub> Cale: coolish.
04:03:55 <d33p> nice
04:04:06 <d33p> Cale: and more on a, b, c 
04:04:09 <d33p> they are just sets 
04:04:18 <d33p> and they go through the *:AxA-> A thing
04:04:22 <Jerub> I'm looking into writing a php->parrot compiler and I'd really like to use a nice implementation language (not yacc)
04:04:29 <Cale> no, a, b, and c are elements of A
04:04:49 <d33p> so we form the set axb, and apply *, to get c?
04:05:04 <d33p> elements, but they are sets :\
04:05:19 <Cale> They're sets in the case of Z/nZ
04:05:26 <d33p> cool, btw
04:05:31 <Cale> I'm speaking in more general terms here
04:05:52 <d33p> nevermind, i got it 
04:05:55 <d33p> okay
04:05:58 <Cale> If you want to get really formal, we're forming the pair (a,b) and applying the function *
04:06:11 <Cale> to get c, say
04:06:20 <d33p> yeah, thats fine
04:06:31 <d33p> cale you are a really good teacher.
04:06:35 <Cale> thanks!
04:06:40 <d33p> i cant believe i get this stuff.. :\
04:06:45 <d33p> your welcome
04:07:14 <d33p> so, the reason i was looking at groups
04:07:21 <d33p> i wanted to prove fermats little theorem
04:07:25 <Cale> Ah.
04:07:26 <d33p> and i was told to look at groups
04:07:29 <d33p> yeah
04:07:45 <d33p> and, i was told to show that Z/pZ forms a group (p is prime)
04:07:50 <d33p> but the person didnt tell me what operation
04:08:03 <d33p> although i think one is supposed to make sense :\
04:08:51 <mandrill> Z/pZ is even a field (if i'm not mistaken with english words), any Z/nZ is a group
04:09:05 <Cale> Fermat's Little theorem says if a and p are integers, and p is prime, and p does not divide a, then a^(p-1) = 1 (mod p)
04:09:22 <d33p> yes, or a^p = a mod p 
04:09:31 <d33p> or p|a^p-a
04:09:34 <d33p> same thing
04:09:38 <Cale> yeah
04:10:37 * Jerub whistles while hugs downloads.
04:11:21 <Cale> Okay, I did this in first year, I'm just reading a bit to refresh my memory.
04:11:34 <Cale> (I'm a Pure Math student at Waterloo)
04:11:49 <d33p> nice :)
04:11:58 <Jerub> I did maths/CS at UQ :)
04:14:39 <Cale> so Fermat's Little Theorem (in that form) is seen to be a multiplicative property of the integers modulo some prime p.
04:15:32 <d33p> multiplicative property of integers modulo prime p
04:15:51 <Cale> as in the property is one of how multiplication works
04:16:28 <d33p> k
04:17:19 <Cale> Okay, so another thing you might like to see is that Z/pZ can be considered as a field.
04:17:56 <d33p> whats special about a field?
04:19:28 <Cale> It has pretty much all the nice basic properties we could ask for. Calculations in a field occur like calculations over the rationals - division is possible (exept by zero), multiplication and addition work nicely together, etc.
04:19:43 <Cale> Here are the field axioms...
04:20:45 <d33p_> sorry, got d/c'd
04:20:49 <d33p_> <d33p> whats special about a field?
04:20:59 <Cale> <Cale> It has pretty much all the nice basic properties we could ask for. Calculations in a field occur like calculations over the rationals - division is possible (exept by zero), multiplication and addition work nicely together, etc.
04:21:20 <Cale> Okay, so what is a field exactly?
04:22:04 <Cale> A field is a set F, together with two binary operations (call them +: F x F -> F, and *: F x F -> F) such that
04:22:31 <Cale> 1. + is associative. (a + b) + c = a + (b + c) for a, b, and c in F.
04:22:56 <Cale> 2. + is commutative. a + b = b + a for a, b in F
04:23:27 <Cale> 3. There is an element 0 of F such that a + 0 = a for all a in F.
04:24:11 <Cale> 4. There exist additive inverses, i.e. there is a number b in F for each a in F such that a + b = 0
04:25:20 <Cale> 5. Multiplication and addition distribute: a * (b + c) = (a * b) + (a * c) and (a + b) * c = (a * c) + (b * c)
04:25:20 <d33p_> is a field a group over <operations>?
04:25:37 <d33p_> <+, *> at least
04:25:39 <Cale> (F, +) is a group
04:25:45 <d33p_> nice
04:26:15 <Cale> okay the five I've given so far make F a ring
04:26:20 <Cale> there are a couple more
04:26:33 <d33p_> are all fields, rings?
04:26:36 <Cale> 6. * is commutative.
04:26:39 <Cale> yes
04:26:43 <d33p_> okay
04:26:50 <Cale> 7. 1 is not 0
04:27:04 <Cale> ah
04:27:09 <Cale> I should replace that
04:27:36 <Cale> 7. There is an element 1 of F such that a * 1 = a for all a in F, and 1 is not 0.
04:27:47 <Rafterman> do you have to have 1 != 0?
04:28:03 <Cale> There are some strange people that don't.
04:28:24 <Rafterman> hmm
04:28:38 <Rafterman> I was in a tutorial on this exact topic today and 1 != 0 wasn't mentioned
04:28:54 <Cale> well, it only throws away one potential field
04:29:06 <Rafterman> having said that, it's not the most rigorous course going round...
04:29:08 <Cale> if 1 = 0, then there's only one element in the field
04:29:12 <Rafterman> yeah
04:29:20 <Rafterman> proving that was one of our tute exercises :)
04:29:37 <d33p_> what is that element, 0 or 1?
04:29:43 <Rafterman> both
04:29:47 <Rafterman> they're both the same thing
04:29:48 <Cale> both :) they're equal
04:29:50 <Cale> yeah
04:29:51 <d33p_> oh.. :\
04:30:09 <Cale> oh there's one more
04:30:28 <Rafterman> multiplicative inverses?
04:30:43 <d33p_> yea
04:30:56 <d33p_> but hm..
04:30:58 <Cale> 8. Multiplicative inverses. If some a not equal to 0 is in F, then there exists b in F with a * b = 1
04:31:22 <d33p_> Cale: if we are working on integers, how does that work
04:31:30 <d33p_> we dont have rationals
04:31:30 <Cale> so if we have all these 8 things, it's called a field.
04:31:31 <Rafterman> true
04:31:56 <Rafterman> integers are a commutative ring with identity
04:32:00 <d33p_> unless ofcourse either a =1 or b =1 
04:32:04 <Cale> The integers alone are not a field, they're only a commutative ring. The integers mod p *do* form a field though
04:32:12 <d33p_> ok, sec
04:32:20 <Cale> and it is possible to define division on them
04:32:52 <d33p_> i get it now
04:33:03 <d33p_> thats cool, because once we have the remainder, division makes sense
04:33:17 <Rafterman> that's not how it's done though
04:33:29 <d33p_> a divide b = b*gcd(a,b) + a mod b 
04:33:56 <d33p_> divide RHS by b 
04:33:57 <Cale> you're using mod as an operator there?
04:34:04 <d33p_> yeah
04:34:14 <d33p_> i dunno.. nm
04:34:46 <d33p_> but, hmm
04:34:50 <d33p_> how does division work?
04:35:09 <Cale> okay
04:35:20 <Rafterman> a^{-1} is whatever number makes a^{-1}*a == 1 (mod p)
04:35:21 <steveh> aaah
04:35:29 <Rafterman> where == means congruence
04:35:30 <steveh> abstract algebra and number theory
04:35:32 <steveh> aaah
04:35:36 <Cale> heh
04:35:59 <Cale> If you don't like it too bad. It's not that abstract anyway :)
04:36:09 <d33p_> rafter hm, you are using ^ as the operator, i see
04:36:13 <Cale> (Not by today's standards)
04:36:20 <d33p_> how do you write write that anyway :)
04:36:22 <d33p_> its invisible
04:36:39 <d33p_> G(F, )
04:36:40 <Cale> yeah, ^ is used as exponentiation in text.
04:36:45 <Rafterman> let me rephrase, the inverse of a is the number b such that a*b == 1 (mod p)
04:36:48 <d33p_> okay
04:37:13 <d33p_> see i knew there was something wrong with those ** using fools :) 
04:37:43 <Cale> ** is also sometimes used :)
04:37:49 <d33p_> Rafterman: um, lets see
04:38:01 <Cale> So what is division?
04:38:05 <d33p> yes.
04:38:21 <Cale> It's multiplication of one number against another number's multiplicative inverse.
04:38:25 <Rafterman> "division" is simply multiplying by the inverse
04:38:58 <Cale> so 3/4 is 3 * (the inverse of 4)
04:39:04 <Rafterman> calling it devision when working in Zp is often confusing
04:39:15 <d33p> multiplicative inverse then
04:39:27 <Cale> the inverse of 4 is guaranteed to exist by the field axioms.
04:39:32 <Rafterman> writing it as 1/a is Bad IMHO
04:39:37 <Cale> (if we're working in a field)
04:39:55 <d33p> Cale: but we dont have a field yet, do we?
04:40:02 <Rafterman> as the "division" in the traditional sense has little to do with the multiplicative inverse in Zp
04:40:02 <d33p> we are defining it 
04:40:06 <Cale> well, we need to prove it's a field
04:40:10 <d33p> right
04:40:14 <Cale> I've defined what a field is.
04:40:21 <d33p> um
04:40:28 <d33p> <Cale> 8. Multiplicative inverses. If some a not equal to 0 is in F, then there exists b in F with a * b = 1
04:40:31 * Rafterman pops off for coffee
04:40:36 <d33p> thats what we are doing, right?
04:40:42 <Cale> okay
04:41:03 <Cale> We didn't rigorously go through and do the others, but sure.
04:41:14 <Cale> Most of the others are boring or ovious.
04:41:18 <d33p> *nod*
04:41:18 <Cale> obvious*
04:41:48 <d33p> hmm, but to show 8. we are assuming its a field, but the whole reason we are showing 8 is to show its a field :\
04:41:55 <Cale> no
04:42:08 <Cale> I wasn't acutally trying to show 8 when I said that
04:42:20 <d33p> okay
04:42:49 <Darius> He was saying something must have that property to be considered a field.
04:43:04 <d33p> Darius: ok
04:43:43 <d33p> so given a field F, we can assume a multiplicative inverse exists for each element in F
04:43:56 <Cale> Yeah, that's what 8 says.
04:44:00 <d33p> cool
04:44:06 <Cale> well, every element but 0
04:44:17 <d33p> but dont we define 0/0 as 0
04:44:21 <d33p> or something :\
04:44:23 <Heffalump> ugh, no
04:44:31 <Cale> no, 0/0 is undefined
04:44:39 <Cale> as is anything / 0
04:44:40 <d33p> i read it somewhere today.. nevermind
04:45:05 <d33p> maybe it was 0 = 0 mod p 
04:45:13 <d33p> which is different, sorry.. i got it
04:45:17 <Cale> well, that makes sense :)
04:45:21 <d33p> yup :)
04:48:08 <Cale> Note now that Fermat's Little Theorem gives us a way to calculate the inverse. If a^(p-1) = 1, then a * a^(p-2) = 1, so we see a^(p-2) as a's inverse.
04:48:55 <d33p> yeah
04:49:00 <d33p> how very cool 
04:49:30 <Cale> Fermat's Little Theorem is actually way more important than his Last Theorem :)
04:50:01 <d33p> hehe
04:51:21 <d33p> but a*a^(p-2) = a^(p-1) 
04:51:27 <d33p> not a
04:51:36 <d33p> er
04:51:41 <d33p> but its = 1 :\
04:51:46 <Cale> yeah
04:51:48 <d33p> isnt that true for any prime power p though
04:51:49 <Cale> that's the poinr
04:51:51 <Cale> point
04:51:59 <d33p> hm.
04:52:09 <Cale> just the p that we're modding out by
04:52:18 <d33p> right
04:53:13 <d33p> hehe.. wow
04:53:27 <d33p> does fermats theorem define an operator?
04:53:45 <Rafterman> how do you mean?
04:53:48 <Cale> well, you can see it that way if you want.
04:53:58 <d33p> okay
04:53:59 <Cale> It actually is more important still.
04:54:54 <d33p> this is the operator that takes a and p, and gives 1 for prime p?
04:55:14 <Cale> oh
04:55:22 <Cale> no I thought you meant division
04:55:24 <Rafterman> no, it's just exponentiation in hte "normal" sense
04:55:39 <Rafterman> you multiply a by itself p times
04:55:40 <d33p> hmm
04:55:50 <Rafterman> well, p-1 times
04:55:54 <d33p> i see 
04:56:06 <Cale> In a sense, you can use the result to give a calculational definition for multiplicative inverse.
04:58:31 <Cale> Okay, so let's just start by showing that Z/pZ is a field by proving 8.
04:58:40 <d33p> okay
04:58:42 <Rafterman> what was the original question?
04:58:49 <Rafterman> show that Z/pZ is a field?
04:59:03 <Rafterman> or are we trying to do FLT now?
04:59:06 <Cale> well, we're moving towards the proof of FLT
04:59:11 <Rafterman> ok cool
04:59:26 <Cale> The proof of 8 is a good chunk of the proof of FLT
04:59:56 * Rafterman considers buying a textbook on this topic
05:00:15 <d33p> incidentally FLT is the acroynm for the last theorem too :)
05:00:40 <Cale> Okay, so the first thing that we want to look at, is the elements a, 2a,..., (p-1)a of Z/pZ
05:01:21 <d33p> k
05:01:43 <Cale> I claim that in these elements, the numbers 1 through (p-1) modulo p show up. (To be proven)
05:02:28 <Cale> That is to say {a, 2a, 3a,...,(p-1)a} = {1,2,...p-1} (the sets are equal)
05:02:55 <d33p> if a = 1?
05:03:08 <Cale> no, they could be all jumbled up
05:03:21 <Cale> a is the number we're trying to take the inverse of
05:03:29 <d33p> oh, ok
05:03:46 <Cale> note that if what I said is true, that a must have an inverse
05:03:59 <Cale> because we multiplied it by something and got 1
05:04:15 <d33p> yeah
05:04:18 <Cale> (we don't know what that is, but we know it's between 1 and p-1
05:04:40 <Cale> Okay, so now we want to show two things.
05:04:59 <Cale> That each of a, 2a, 3a... is not zero.
05:05:03 <Cale> oops
05:05:09 <Cale> up to (p-1)a
05:05:26 <Cale> i.e. the set I'm talking about does not contain zero
05:05:31 <Cale> and b
05:05:37 <Cale> that they're all distinct
05:05:46 <d33p> it wouldnt because you started from a, and a isnt 0 
05:06:12 <Cale> but can ka be zero for some k in {1, 2, 3,... p-1} ?
05:06:30 <Darius> If p were, 4 say, and a were 2 then 2a would be 0.
05:06:49 <Darius> Of course, we're assuming p is prime.
05:09:18 <Cale> okay, so think about this... if ka were 0, then p would divide ka (everything divides 0)
05:09:36 <Cale> so p would have to divide k or a
05:09:40 <Cale> does it?
05:09:56 <d33p> dont know 
05:10:04 <d33p> it either divides k or a 
05:10:18 <d33p> or maybe both?
05:10:21 <Darius> Well k should be easy.
05:10:23 <d33p> what am i missing
05:10:47 <d33p> well a isnt zero, so 
05:10:59 <Cale> does p divide 1, 2, 3, 4, ... p-1 ?
05:11:10 <d33p> ka=0, k must be zero 
05:11:17 <d33p> and then p must divide k 
05:11:42 <Cale> d33p - the fact that ka = 0 and a is not 0 does not imply that k is 0
05:11:53 <Cale> this is important
05:12:01 <d33p> :|
05:12:15 <Darius> Again using p := 4 and a := 2 the 2a = 0.
05:12:16 <Cale> look at 2*3 = 0 mod 6
05:13:03 <d33p> yeah, sorry
05:14:22 <d33p> can it be zero, yes
05:14:29 <Cale> no
05:14:48 <d33p> :\
05:14:58 <Cale> there's a good reason why it can't be zero (for one, it breaks the whole theorem :)
05:15:10 <d33p> okay, iam lost
05:15:17 <Cale> Okay.
05:15:21 <Cale> Let's start again.
05:15:24 <d33p> we started with z/pZ 
05:15:53 <d33p> which is {0+pZ, 1+pZ, .. (p-1) + pZ}
05:15:55 <Cale> Yeah, we're working with numbers in Z/pZ
05:16:34 <d33p> and whats the operation?
05:16:45 <Darius> Perhaps a restatement of the theorem, then identifying explicit properties it requires?
05:17:31 <Cale> well, I'll oblige you if you'd like me to redefine addition and multiplication over Z/pZ
05:17:47 <d33p> go for it
05:18:14 <Cale> (a + pZ) + (b + pZ) = ((a + b) + pZ)
05:18:47 <Cale> (a + pZ) * (b + pZ) = ((a*b) + pZ)
05:19:45 <Cale> Note that this is equivalent to defining addition and multiplication on sets as follows:
05:19:55 <Jerub> hmm
05:19:59 <Jerub> lots to learn.
05:20:14 <Jerub> writing parsers with parsec isn't trivial ;)
05:20:15 <Cale> A + B = { a + b where a is in A and b is in B }
05:20:24 <Cale> A * B = { a * b where a is in A and b is in B }
05:20:37 <d33p> thats cool
05:21:10 <d33p> but where is a*b ?
05:21:14 <d33p> in what set?
05:21:21 <Cale> a*b as an integer
05:21:24 <d33p> or just some element of Z/pZ ?
05:21:29 <d33p> right but where would we find it 
05:21:59 <Cale> here, I'm reading a*b as in Z and (a*b)+pZ as in Z/pZ - there is no shorthand.
05:22:11 <d33p> oh.
05:22:13 <d33p> so we have
05:22:28 <d33p> *:pZ x pZ -> pZ ?
05:22:50 <Cale> *: (Z/pZ) x (Z/pZ) -> (Z/pZ)
05:23:03 <d33p> cool
05:23:23 <Cale> (a + pZ) * (b + pZ) = ((a*b) + pZ) <-- by this rule
05:23:39 <d33p> okay
05:23:43 <d33p> i got it
05:24:10 <Darius> Jerub, if you want a system that takes a grammar, there is Happy or Pappy.
05:24:26 <Jerub> Darius: LR(1) or LL(1) ?
05:24:41 <Cale> okay, also are you aware of the definition of a prime as being a number p such that if p divides ab then p must divide a or b?
05:25:05 <d33p> um, no.. 
05:25:15 <d33p> but it makes sense
05:25:19 <Cale> That's the actual definition of "prime".
05:25:44 <d33p> yea, i just thought of it as p = ab, either a=1 or b=1
05:26:52 <Cale> The property of being divisible by only one and itself is actually called "irreducible"
05:26:59 <Darius> Happy is LALR(1)
05:27:15 <d33p> another name for prime?
05:27:18 <Cale> but in this case, they're equivalent
05:27:43 <d33p> ok
05:27:52 <Cale> so it gets called "prime" a lot, even though primality in a general ring isn't defined that way
05:28:14 <Rafterman> learn something new every day :)
05:28:28 * d33p nods
05:28:29 <Darius> Pappy isa bit of an oddball, see http://www.pdos.lcs.mit.edu/~baford/packrat/thesis/
05:29:05 <Cale> okay, so back to that proof of 8.
05:29:07 <Jerub> Darius: It would please me no end to find a GLR parser in haskell, but I doubt thats possible.
05:29:17 <d33p> Cale: ok
05:29:56 <Cale> Since p is a prime, if p doesn't divide a and it doesn't divide k, then it won't divide ka.
05:30:09 <Jerub> but L[LR](k) is good enough I guess ;)
05:30:50 <d33p> Cale: yup
05:32:10 <Cale> now, it's true that p divides neither
05:32:20 <Darius> Jerub: I think there are GLR systems in Haskell
05:32:25 <Cale> k is in the set 1 through to p-1
05:32:36 <Cale> and so is a
05:33:12 <Cale> and none of those are divisible by p (all the things divisible by p are in the set represented by 0)
05:33:16 <Darius> Try HASDF
05:33:51 <Cale> Hence, it's true that a, 2a, ... (p-1)a are all nonzero
05:34:09 <Cale> because if one of them was zero, then p would divide it, and so on
05:34:15 <Darius> If you can find the implementation...
05:34:20 <Cale> get that?
05:34:22 <d33p> Cale: yeah
05:34:31 <d33p> Cale: sorry, i didnt see that at first 
05:34:46 <d33p> are you going to show a contradiction?
05:34:58 <Cale> well, I sort of just did.
05:35:05 <d33p> hm
05:35:09 <d33p> true
05:35:31 <d33p> alrighty
05:35:45 <Jerub> now all I need is a haskell implementation in parrot, and I'm set :)
05:35:53 <Cale> now we need to show that a, 2a, 3a, ... (p-1)a are all distinct.
05:36:55 <d33p> how do we do that?
05:36:59 <Cale> Now, suppose that ia = ja for some i and j in {1,...,(p-1)}
05:37:16 <Cale> we want to show that i = j
05:37:19 <Jerub> Darius: I think I might give pappy a go before I try anything else.
05:37:44 <Cale> well, if ia = ja, then (i-j) a = 0
05:37:58 <d33p> well ia=ja, ia -ja = 0, a(i-j) = 0, a!=0 so i-j = 0
05:38:03 <d33p> i = j 
05:38:25 <Cale> yeah
05:38:34 <d33p> ok
05:39:19 <Cale> so we have {a, 2a, 3a, ... (p-1)a} a list of p-1 distinct, nonzero elements of Z/pZ
05:39:51 <Cale> if you're counting, you can see that must be everything but 0.
05:40:20 <Cale> So 1 has to be in there somewhere.
05:40:34 <Cale> Hence, a must have a multiplicative inverse
05:40:58 <d33p> hmm
05:41:13 <Cale> get it?
05:41:22 <d33p> yeah, but it doesnt feel right
05:41:35 <Cale> where is the trouble?
05:42:16 <Darius> Jerub: The Pure Functional Parsing link on http://www.cs.chalmers.se/Cs/Research/Functional/MultiLib/index.html probably has some useful code, and the accompanying paper may be an interesting read.
05:42:28 <Cale> {a, 2a, 3a, ... ,(p-1)a} covers p-1 of the p elements in Z/pZ
05:42:40 <Cale> we know that none of these are equal to zero
05:43:03 <d33p> and if 1 is in there
05:43:04 <Cale> and we know that there really are p-1 of them because we proved that they're distinct
05:43:11 <d33p> that means we have a multiplicative inverse, that part i dont get
05:43:36 <Cale> well, that means that we multiplied a by some k in 1...(p-1) and got 1
05:43:55 <Jerub> Darius: thankyou :)
05:43:55 <Cale> because it's a, 2a, 3a, and so on
05:44:09 <d33p> you mean the 1a ?
05:44:35 <Cale> hmm...
05:44:50 <Cale> I'm not sure what you're missing here...
05:45:14 <Cale> The multiplicative inverse of a is the number such that you multiply it against a and get 1.
05:45:45 <Cale> We tried all the nonzero elements of Z/pZ, and saw that we got 1 somewhere along the line.
05:46:01 <Cale> Hence, the multiplicative inverse has to exist.
05:46:26 <d33p> where is the 1 though :\
05:46:40 <Cale> That's what FLT tells us
05:46:56 <d33p> damn, i didnt even think about FLT
05:47:04 <Cale> we're only trying to show it exists
05:47:10 <Cale> not which one it is
05:47:31 <d33p> eek..
05:47:36 <Cale> It's just enough to know it's there. Knowing which one is extra.
05:47:43 <d33p> how do we know it exists?
05:47:55 <d33p> a^(p-1) 1 mod p 
05:48:00 <d33p> a^(p-1) = 1 mod p 
05:48:45 <Cale> well, that's the statement that tells us which one it is (it'll be whichever k is equal to a^(p-2))
05:49:01 <Cale> but we were proving 8
05:49:06 <Cale> and all that 8 says
05:49:25 <Cale> is that there is some b for each a not 0 so that a * b = 1
05:49:28 <Jerub> does shapr still hang around here?
05:49:37 <Cale> Jerub: I hope so.
05:50:18 <o3> Jerub: he does
05:50:34 <Cale> And in all that discussion I gave just now, we constructed the set of multiples of a, and saw that 1 was in there.
05:50:45 <Cale> So we know that it has to exist.
05:50:54 <Jerub> o3: thanks. haven't seen him in ages, was hoping to catch up.
05:51:49 <d33p> Cale: yeah, but you look at the p-1 elements, right?
05:52:14 <Cale> yeah, there were p-1 multiples that we looked at
05:52:33 <Cale> and we found one of them was equal to 1
05:52:52 <Cale> (actually, we found one of them for each of the nonzero elements of Z/pZ)
05:53:37 <Cale> FLT is not much farther from here
05:54:19 <d33p> i thought we just used FLT, it told us 1 exists
05:54:24 <Cale> no
05:54:54 <Cale> we used the fact that a, 2a, 3a, ... (p-1)a are distinct and nonzero
05:55:26 <d33p> is a an integer?
05:55:47 <Cale> and since there are p-1 of those, and since none of them are zero, and since there are only p elements of Z/pZ *including* zero, we must be looking at everything else.
05:55:54 <Cale> a is an element of Z/pZ
05:56:07 <d33p> heh
05:56:08 <Cale> for which we're proving that there is a multiplicative inverse
05:56:09 <d33p> damn
05:56:43 <d33p> so when you write a, you mean ka you mean ka +nZ ?
05:56:59 <d33p> so when you write ka, you mean ka +nZ ?
05:57:02 <Cale> when I write a here, I mean a + pZ
05:57:14 <Cale> and when I write ka, I mean ka + pZ
05:57:25 <d33p> damn.
05:57:56 <d33p> why doesnt zero exist again?
05:58:06 <Cale> in the list
05:58:08 <Cale> ?
05:58:15 <d33p> yep
05:58:24 <d33p> 0 is 0 + nZ 
05:58:27 <d33p> er pZ
05:58:32 <d33p> which is just pZ 
05:58:34 <Cale> right
05:58:42 <d33p> but those elements are divisible by p 
05:58:47 <Cale> and pZ is all the things that are divisible by p
05:58:51 <Cale> yeah
05:58:52 <d33p> ok
05:59:17 <Cale> and so a and k are not divisible by p
05:59:34 <Cale> and since p is *prime* ka isn't either
05:59:47 <Cale> so ka is not 0
06:00:22 <d33p> a and k, these are elements of Z/pZ?
06:00:28 <Cale> yeah
06:00:42 <Cale> and so is 0
06:00:48 <Cale> in that statement
06:01:06 <d33p> yup
06:01:22 <Cale> 0 = pZ
06:01:34 <d33p> hehe
06:01:35 <Cale> okay, so do you think you get it now?
06:01:36 <d33p> this is nuts
06:01:40 <d33p> yea, thats cool
06:01:48 <d33p> :)
06:01:53 <Cale> :)
06:02:30 <Cale> Okay, so we have multiplicative inverses...
06:02:43 <d33p> right, how does that help with FLT?
06:02:50 <Cale> let's go back to our list
06:02:56 <d33p> ok
06:03:01 <Cale> {a, 2a, 3a, 4a, ... (p-1)a}
06:03:19 <Cale> we know they're all different and nonzero
06:03:28 <d33p> *nod*
06:03:47 <Cale> now look at what happens when we multiply them all together
06:04:06 <d33p> there is a factorial there
06:04:12 <d33p> and a^(p-1)
06:04:13 <d33p> oh
06:04:14 <Cale> a*2a*3a*4a*...*(p-1)a = (p-1)! a^(p-1)
06:04:28 <d33p> hm
06:04:36 <d33p> (p-1)! is divisible by p
06:04:47 <d33p> no wait
06:04:50 <Cale> no
06:04:51 <d33p> nm
06:04:59 <d33p> p! is divisble by p
06:05:20 <Cale> also a*2a*3a*...*(p-1)a = (p-1)!
06:05:46 <Cale> because a, 2a, 3a,... (p-1)a are the nonzero numbers up to p-1
06:06:02 <d33p> did you miss the a^(p-1)?
06:06:24 <Cale> no, I'm saying that (p-1)! a^(p-1) = (p-1)!
06:06:54 <d33p> why are you saying that
06:07:02 <Cale> well, look at it
06:07:09 <d33p> ok
06:07:13 <Cale> you get the first part, right?
06:07:16 <Cale> a*2a*3a*4a*...*(p-1)a = (p-1)! a^(p-1)
06:07:19 <Cale> ?
06:07:20 <d33p> yeap
06:07:29 <Cale> a*2a*3a*...*(p-1)a = (p-1)! because...
06:07:45 <d33p> something is 1
06:08:00 <Cale> {a,2a,3a,...,(p-1)a} = {1,2,3,...,p-1}
06:08:06 <Cale> (the sets are equal)
06:08:19 <d33p> ahh
06:08:47 <Cale> so now we have that (p-1)! a^(p-1) = (p-1)!
06:09:24 <d33p> mm 
06:09:40 <Cale> and since the gcd of (p-1)! and p is 1, we're allowed to cancel (p-1)! from both sides
06:10:21 <Cale> this leaves a^(p-1) = 1
06:13:38 <d33p> yea, its almost fermats theorem
06:13:55 <Cale> it is Fermat's Theorem
06:14:05 <d33p> (mod p) ?
06:14:10 <d33p> oh.
06:14:17 <d33p> nZ is mod p?
06:14:20 <d33p> :\
06:14:22 <Cale> all the calculations here are mod p
06:14:27 <d33p> nice
06:14:30 <Cale> we're working in Z/pZ
06:14:52 <d33p> ok.. lemme write this all donw
06:15:07 <Cale> (mod p) at the end of a line means "I did this in Z/pZ"
06:15:35 <d33p> okay
06:18:32 <d33p> Cale: thanks for your patience, i really appreciate your help
06:18:56 <Cale> You're quite welcome.
06:19:05 <d33p> its still striking that a^p is divisible by p
06:19:19 <d33p> proving it doesnt take away from that
06:19:35 <Cale> Yeah.
06:19:48 <meep> can i ask a quick question about state transformers?
06:19:57 <Cale> It's a pretty proof, too. Things hang together nicely.
06:20:17 <Cale> meep: sure, ask away. I'm sure there's someone around who can answer :)
06:20:23 <meep> hehe, ok...
06:21:09 <Cale> (Maybe even me, but I haven't paid too much attention to learning Haskell this month, though I was learning it before.)
06:21:57 <meep> well, usually when ppl talk about them, they give their type as (a -> (a, b)), but in the chapter on Arrows in The Fun of Programming, they talk about a state transformer of type ((a,b)->(a, c))
06:22:04 <meep> i can't work out whgat the second type means
06:23:25 <Cale> Is this a state transformer on arrows rather than on monads?
06:23:48 <meep> well it's kind of neither, it's before it's magically generalised into an arrow
06:24:02 <meep> so it's just a function of a particular type
06:24:41 <Cale> is there any detail restricting, or providing meaning for the types a, b, and c?
06:24:49 <Darius> how 'bout writing it as (state,inp) -> (state,out)
06:25:02 <Cale> ah
06:25:11 <Cale> yeah, that's what I was looking for :)
06:25:13 <meep> oh that could be it
06:25:26 <meep> rather that just state -> (state, out)
06:25:35 <meep> ok, thanks :)
06:31:39 <Darius> So... I've downloaded Squeak and I have nothing to do with it...
06:33:25 <Darius> it has made me want to make an embeddable Haskell interpreter/compiler
06:33:53 <Heffalump> meep: yeah, the point of arrows is that they generalise functions, rather than generalising values as monads do
06:34:00 <Heffalump> so you need to have an input and an output
06:34:19 <meep> yeah, it actually makes more sense (the second type) - ypou an see a state being threaded through the whole function
06:34:50 <meep> (well it makes more sense now...)
06:35:38 <Darius> heya shapr
06:35:44 <shapr> hi Darius
06:35:46 * shapr boings
06:35:52 <shapr> what's up?
06:36:45 <o3> shapr: Jerub was looken' for you
06:36:53 <shapr> y0 Jerub
06:40:58 <reffie> so i wrote this small markov chain thing in ruby
06:41:08 <reffie> and it uses massive amounts of memory :/
06:41:17 <shapr> big chain, eh?
06:41:42 <reffie> refugee@towel:~$ wc -w all_irssi_log
06:41:42 <reffie> 1301001 all_irssi_log
06:48:36 <Cale> not that I'm an expert on this, but what matrix representation are you using?
06:50:17 * Marvin-- is happily using gnome 2 with metacity
06:50:27 <Marvin--> this actually seems to suck less than windowmaker, even though I'm used to wmaker's suckage
06:50:33 <shapr> I like ion
06:50:47 * Cale is happily using just Enlightenment, with gtk2 software.
06:51:53 <shapr> I haven't tried Enlightenment in a loong time, I should check it out again.
06:52:00 <Marvin--> shapr: I like overlapping windows :)
06:52:07 <Cale> It's really very nice.
06:52:20 <shapr> I think I used 0.13
06:52:38 <Cale> Oh, it's been rewritten completely at least once since then.
06:52:41 <Cale> Maybe twice
06:53:29 <Cale> Also, they're currently working on e17 (well, the libraries for it) and I have a fair level of confidence in it being incredibly cool.
06:53:55 <Cale> Evas is really cool.
06:54:13 <Cale> (the canvas library that they wrote in preparation for e17)
06:58:02 <meep> ooh, it can render using OpenGL
06:58:20 <reffie> well Cale.. i'm using an hash of arrays..
06:58:39 <reffie> there must be better ways.
06:58:48 <shapr> I want a Haskell window manager :-)
06:59:32 <Cale> I want a window manager that's just like enlightenment, except that it's programmable on the fly in Haskell :)
06:59:41 <shapr> though I haven't thought of a good name for a HaskellWM yet.
06:59:53 <shapr> Cale: runtime loaded Enlightenment?
06:59:57 <shapr> that would be spiffy
07:00:03 <Marvin--> shapr: as if that's the most important thing
07:00:17 <shapr> good names are motivational
07:00:23 <shapr> look at lambdabot 
07:00:29 <meep> maybe you could use evas as the backend to a haskell wm
07:00:48 <shapr> Alistair Reid wrote the Xlib bindings for Haskell
07:01:04 <Cale> Yeah, that would be cool - doing evas and ecore bindings in Haskell would be sweet.
07:01:10 <shapr> I've been reading some Xlib docs online
07:01:22 <meep> i can imagine programming using raw xlib in haskell would be pretty unpleasent
07:01:37 <shapr> well, abstract it away
07:02:03 <Cale> ecore is an abstraction library that takes care of most of the X cruft for you
07:02:19 <meep> i mean if ur going to abstract it a little bit, you may as well go the whole way
07:02:37 <Cale> Yeah
07:02:41 <Darius> Cale make an embeddable Haskell interpreter so I don't have to then.
07:02:55 <Cale> Heh, we really need one of those.
07:03:00 <meep> can't you embed hugs at all?
07:03:13 * shapr gets a stick
07:03:20 <meep> lol
07:03:23 * meep hides
07:03:28 <earthy> didn't someone on one of the haskell mailinglists mention that ghci is relatively easily embeddable? >:)
07:03:37 <shapr> if you hit something hard enough, you can embed it somewhere...
07:03:47 <meep> yeah, they did before the stick came along i suppose ;)
07:03:51 * shapr grins
07:06:09 <Darius> I don't believe either Hugs or GHCi have a language level interface.  Also GHCi may be somewhat heavy-weight, though that's not a serious problem for most apps I guess.
07:07:04 <meep> maybe haskell wouldn't be so great as an embedded language for most apps, it might be nice to have dynamic typing
07:07:33 <shapr> I've never had Haskell as an embedded language, so I don't know. I would like to try it once.
07:07:45 <Cale> It would be nice if there was a haskell interpreter that you could call from haskell with a tree of bindings, and it would make those bindings appear as if they were in preincluded modules.
07:09:41 <Darius> It would be nice if it were easier to make Haskell development tools.
07:09:55 <shapr> how could it be easier?
07:11:50 <Darius> If you separated the parser/typechecker/interpreter/compiler into more independent chunks then tool development wouldn't have to recreate these things.
07:11:55 <Heffalump> darius: what do you mean by "a language level interface"
07:11:56 <Heffalump> ?
07:12:54 <Darius> As far as I know the only way to use Hugs or GHCi is by passing strings or loading files, at least without getting somewhat dirty.  However, I haven't looked into this aspect much, so I could be wrong.
07:13:09 <meep> i think at the moment you have to use a C language interface
07:13:20 <meep> which is just silly if you're programming in haskell
07:13:42 <meep> and the interface is always changing with new releases and such
07:15:15 <meep> i dunno though that's just a very vague memory
07:17:19 <shapr> IonWM loads everything at startup, but allows you to edit config files and restart the WM
07:17:40 <shapr> maybe that would be a good way to start a HaskellWM
07:17:59 <Cale> yeah, E actually does the same.
07:19:24 <d33p> as does blackbox
07:20:01 <Cale> Hmm...
07:20:49 <Cale> Still, we don't really want to have to recompile the hswm binary every time we change themes, so it would need dynamic loading to some extent.
07:21:18 <shapr> dynamic loadin at that level wouldn't be hard (I think)
07:21:32 <shapr> look at the way lambdabot does it.
07:22:17 <Heffalump> darius: I think you can interface properly with hugs without too much pain, though I could be wrong
07:22:52 <meep> i suppose the only real answer to the embedding problem is Haskell World Domination
07:23:05 <meep> then we wouldn't have to worry about other languages
07:23:08 <shapr> yes, I agree.
07:23:10 <shapr> let's take over.
07:23:13 <Darius> I think that even if it doesn't support it now, it would be a rather easy task to get Hugs to be nicely embeddable.
07:23:24 <meep> easier than starting from scratch anyway
07:23:25 <Heffalump> it has a proper embedding interface already
07:23:39 <Heffalump> I just can't quite remember how you call into it, and how much knowledge of its internals are required
07:23:44 <shapr> http://www.haskell.org/hawiki/HaskellWorldDomination
07:23:53 * Heffalump --> look at the code I wrote using it ages ago
07:23:59 <Darius> I thought that it might.
07:25:09 <Heffalump> oh, that interfaces via strings
07:25:22 <Heffalump> but then that was the input I actually wanted
07:25:50 <Lunar^> shapr: Are you talking about a window manager in Haskell ?
07:26:16 <Cale> I think that bindings to the e17 libraries for Haskell as they are completed would be really cool: http://www.enlightenment.org/pages/components.html
07:26:17 <Heffalump> ah, yes, you can also build expressions on the stack directly
07:29:32 <meep> Cale: i see they're even doing a widget library
07:29:37 <Cale> Yeah
07:29:37 <Darius> Yes, there's a HugsAPI.h that looks like it's meant to be a C level interface.
07:29:59 <Cale> for the configuration dialogs and related programs
07:30:41 <shapr> Lunar^: yes, I am.
07:30:48 <shapr> Lunar^: have you already written one?
07:30:59 <Cale> (like possibly a rewrite of etcher, which is what is used to make windowborders and widgets and such)
07:31:44 <Lunar^> shapr: No ! And I don't want to.. X is such a mess, I don't understand a reason to work on this environnement more that working on a new environement
07:32:21 <Cale> Lunar^: all the programs that use X?
07:32:43 <Cale> I mean, Fresco looks nice, but there are only 4 or so programs that run on it.
07:32:54 <shapr> does Fresco work?
07:33:05 <Cale> Yeah, supposedly
07:33:11 <Cale> I haven't tried it.
07:33:15 <meep> i tried building fresco once :S
07:33:24 <meep> never again until it's a 1.0 release, hehe
07:33:58 <meep> it looks cool though
07:34:28 <meep> i suppose if someone ported gtk to Fresco a lot more programs would run on it
07:34:47 <Cale> Yeah, that's true
07:35:17 <Cale> Looks like there's only 12 things for them to do before M3 :)
07:36:18 <Lunar^> Cale: I'm talking about writing the low level stuff also
07:36:45 <Lunar^> Cale: Once you got a graphical environment, it's not hard to have a nested X (like Quartz or DirectFB have)
07:37:23 <Lunar^> Ok to have a nice nested X you need a window manager
07:52:07 <shapr> hi hal
07:53:19 <hdaume> morning shae
07:53:45 <shapr> what's up?
07:54:35 <hdaume> not a whole lot
07:55:10 <shapr> sounds like a good summer day.
07:55:25 <shapr> Lunar^: Fresco seems like the only alternative to X, are there others?
07:55:55 <meep> someone mentioned DirectFB
08:08:00 <shapr> fresco sounds great from reading the website
08:08:11 <shapr> but I'd like to see if it works :-)
08:08:47 <meep> lol, i wonder how many ppl have read the haskell website and thought the ssame thing
08:08:50 <Lunar^> shapr: To do what actually ?
08:09:19 <shapr> if it works to run emacs and xterm, that would be enough for me
08:09:44 <Lunar^> shapr: keep X then
08:10:24 <shapr> if something else will run Emacs and a terminal emulator, I'd try it.
08:10:45 <shapr> I like the resolution independent idea of Fresco
08:16:04 <Lunar^> shapr: I'd love to see something else, something new, something started from scratch with influence of functionnal programming on original ideas
08:26:19 <shapr> do you know of any projects like that?
08:36:19 <Lunar^> shapr: ... I'm such a dreamer ...
08:43:38 <phubuh> wow, hOp is coming along quickly
09:14:07 <shapr> yah, I saw the hOp progress from the weekend
09:14:08 <shapr> looks good
09:14:27 <shapr> Lunar^: I'm a dreamer too, otherwise I'd be using C++ and making windows programs.
09:32:37 <shapr> hi flippo_
09:37:56 <shapr> hi cl1dev
10:54:38 --- topic: '["We put the Funk in Funktion","See logs @ http://tunes.org/~nef/logs/haskell/","Learning Haskell - http://www.haskell.org/learning.html","lots of cool debs in the haskell-experimental archive - see http://haskell.org/hawiki/DebianUsers","Haddock 0.5 - http://www.haskell.org/haddock/","wxHaskell 0.1 - http://wxHaskell.sf.net/","#haskell meetup @ ICFP, HaskellIrcChannel on the wiki","ghc rpms - http://haskell.org/~petersen/rpms/ghc/"]'
10:54:38 --- topic: set by Pseudonym on [Mon Aug 04 17:08:04 2003]
10:54:38 --- names: list (clog Arnia xkb gdsx bob2_nothome reffie jak wax kaol Verbed skylan tic polli opet norpan mgoetze lambdabot Heffalump Igloo Fractal ibid el_diego phubuh)
11:25:46 --- topic: '["We put the Funk in Funktion","See logs @ http://tunes.org/~nef/logs/haskell/","Learning Haskell - http://www.haskell.org/learning.html","lots of cool debs in the haskell-experimental archive - see http://haskell.org/hawiki/DebianUsers","Haddock 0.5 - http://www.haskell.org/haddock/","wxHaskell 0.1 - http://wxHaskell.sf.net/","#haskell meetup @ ICFP, HaskellIrcChannel on the wiki","ghc rpms - http://haskell.org/~petersen/rpms/ghc/"]'
11:25:46 --- topic: set by Pseudonym on [Mon Aug 04 17:08:04 2003]
11:25:46 --- names: list (clog Strike cl1dev hdaume Rafterman Cale Jerub dalor Darius flippo_ meep mandrill galority Smerdyakov themus kunphuzil o3 sjj_ mattam SyntaxPolice shapr d33p buggs|afk dennisb isomer eivuokko earthy vegai emu keverets Lunar^)
11:26:31 <shapr> @yow
11:26:32 <lambdabot> Put FIVE DOZEN red GIRDLES in each CIRCULAR OPENING!!
11:26:37 <shapr> scary bot
11:27:02 <andersca> hej shapr
11:27:39 <shapr> hej andersca 
11:27:50 <shapr> vad hnder?
11:28:03 <andersca> inte mycket
11:30:58 <shapr> hej el_diego 
11:37:08 <xkb> Hello
11:37:20 <xkb> I have a question about cpo's and lubs
11:37:37 <shapr> what are they?
11:37:40 <xkb> say we have a function fi(n) 
11:37:43 <xkb> with
11:37:51 <xkb> f=n iff n>=i
11:37:54 <xkb> euh
11:37:59 <xkb>  <=
11:38:11 <xkb> or bottom 
11:38:30 <xkb> what is the lub of chain produced by that function
11:38:59 <xkb> Hmm.. this is not really clear in IRC layout
11:39:39 <xkb> so f index i equals the input n iff n <= index or else f index i is bottom element
11:40:05 <xkb> the chain is <fi>, i>=0
11:40:22 <xkb> I think it either has no lub, because there is always a bigger i
11:40:33 <xkb> or the lub is n
11:41:49 <xkb> Any ideas?
11:43:46 <Heffalump> what do you mean by <fi> ?
11:44:04 <xkb> The chain of f indexed by i
11:44:16 <xkb> the function is defined as f subscript i
11:44:28 <Heffalump> where chain is what?
11:45:01 <xkb> chain is a sequence of values with an increasing amount of information
11:45:22 <xkb> with bottom as "least information"element
11:45:54 <xkb> All of this is for finding fixed points in denotational semantics btw
11:46:40 <Heffalump> ok, so bottom, f bottom, f (f bottom) etc?
11:47:02 <xkb> hmm.. no
11:47:05 <xkb> more like 
11:47:31 <xkb> bottom, 1,1,2,3,4,4,4,5 etc etc
11:48:08 <xkb> a sequence of values generated by fi(n), i>=0, n from Naturals and n<=i
11:48:15 <Heffalump> oh
11:48:20 <Heffalump> clearly there's no lub then.
11:48:26 <xkb> my thoughts to
11:48:29 <xkb> too
11:48:37 <xkb> as the sequence is always increasing
11:49:16 <xkb> However the question is : "Define the lub of fi, and proof your answer is correct
11:49:54 <xkb> So that makes me a bit unsure
11:50:03 <Heffalump> I think you're considering the wrong chain, then.
11:50:21 <Heffalump> the lub of fi is i
11:50:52 <xkb> hmm.. can you explain that?
11:51:00 <Heffalump> well, suppose i=3
11:51:08 <xkb> the value of fi is either the input n, or bottom
11:51:14 <xkb> and n is almost <= then i
11:51:16 <Heffalump> fi(0) = 0, fi(1) = 1, fi(2) = 2, fi(3) = 3, fi(4) = bottom, etc
11:52:10 <xkb> s/almost/at most/
11:52:38 <xkb> hmmm.. I will try to prove that lub fi = i 
11:52:42 <xkb> thanks for the help
12:57:27 <shapr> hej Marvin-- 
12:57:45 <Marvin--> hejhej
12:58:13 <andersca> hej
12:58:31 <Marvin--> andersca: I'm running gnome2 and metacity and stuff instead of wmaker now :)
12:58:46 <andersca> uhoh
12:58:52 * andersca expects complaints
13:00:35 <Marvin--> just a few ;-)
13:00:46 <Marvin--> but actually, I'm taking them to #gnome
13:01:03 <andersca> take them to #swedesex
13:26:03 --- topic: '["We put the Funk in Funktion","See logs @ http://tunes.org/~nef/logs/haskell/","Learning Haskell - http://www.haskell.org/learning.html","lots of cool debs in the haskell-experimental archive - see http://haskell.org/hawiki/DebianUsers","Haddock 0.5 - http://www.haskell.org/haddock/","wxHaskell 0.1 - http://wxHaskell.sf.net/","#haskell meetup @ ICFP, HaskellIrcChannel on the wiki","ghc rpms - http://haskell.org/~petersen/rpms/ghc/"]'
13:26:03 --- topic: set by Pseudonym on [Mon Aug 04 17:08:04 2003]
13:26:03 --- names: list (clog_ Igloo Jerub hdaume cl1dev Strike opet polli skylan Verbed kaol wax jak reffie bob2_nothome gdsx xkb phubuh Fractal Cale Rafterman steveh Riastradh lambdabot Marvin-- andersca galority)
14:00:15 <andersca> hi dennisb
14:10:14 <Lunar^> Does anyone already used HToolkit MySQL bindings ?
15:00:35 <Jerub> shapr: *ping*
15:47:49 <shapr> oy
15:48:01 <shapr> Lunar^: lambdabot is using the HToolkit postgresql bindings
16:22:44 <Igloo> Any MacOS X people about?
16:28:37 <Riastradh> Igloo - Me!
16:30:18 <Igloo> Ria: Can you compile "int main(void) { return 0; }", run gdb on the result, "b main", "r", "info registers" and e-mail the output to igloo@earth.li please?
16:30:48 <Igloo> Is there anything more than "cc foo.c -o foo" you need to do to compile C programs incidentally?
16:30:52 <Riastradh> ...er...hmm, no, I'm a Linux person, really!  I was just pretending...
16:31:24 <Riastradh> I don't think there is.
16:31:24 <Heffalump> igloo: hasn't wol gone to bed?
16:31:30 <Igloo> Ah, err, OK  :-(
16:31:45 <Igloo> Heff: She hasn't got a cc, gdb etc
16:31:50 <Riastradh> How do I run GDB on it?
16:31:59 <Igloo> gdb foo
16:32:00 <Heffalump> can't she acquire one without noticing? :-)
16:32:06 <Riastradh> (I may be an OS X person (my sarcasm seems to have not been apparent), but I'm not a C person)
16:32:26 <Heffalump> it wasn't clear to me either that you were being sarcastic :-)
16:32:26 <Igloo> Ria: OK, I got sort of confused  :-)
16:33:22 <Igloo> Heff: Well, also I asked Art and he got link errors so I thought compilation might be slightly less simple
16:33:28 <Heffalump> ah
16:33:39 <Riastradh> OK, sent.
16:33:45 <Igloo> Cool, ta
16:34:24 <Igloo> Bah
16:35:01 <Heffalump> hmm?
16:35:17 <Igloo> Oohhh, I bet fn are floating point registers
16:35:48 <Igloo> Aha, yes, "info registers all" shows them
16:37:26 <Igloo> Compiling ghc on a PPC is producing "error: invalid register name for `F1'" and I'm trying to work out why
16:44:31 <hdaume> anyone have a type parser for in haskell for haskell types (yes, i know i can rip one out ot Language.Haskell.Parser, but i'd rather a non-Happy one)
16:44:41 <hdaume> s/for in/in/
