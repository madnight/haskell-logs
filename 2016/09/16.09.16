00:36:52 <quchen> Would ST be a candidate for region-based memory management, or do I misunderstand its semantics?
00:37:17 <quchen> Ignoring the fact that ST is pretty much IO and this would mean a whole different implementation
01:08:06 * hackagebot spy 0.12 - A compact file system watcher for Mac OS X, Linux and Windows  https://hackage.haskell.org/package/spy-0.12 (StefanSaasen)
01:08:08 * hackagebot spy 0.13 - A compact file system watcher for Mac OS X, Linux and Windows  https://hackage.haskell.org/package/spy-0.13 (StefanSaasen)
01:24:45 <jle`> quchen: i think that's fair
01:46:42 <tsahyt> what do you pronounce -< as from arrow notation?
01:50:03 <pavonia> "arrow application" according to https://wiki.haskell.org/Pronunciation
01:50:18 <tsahyt> that's a useful page. thanks!
02:10:05 <maybefbi> how could 1000000100000::Int64 become 4294967306::Int64?
02:11:29 <pavonia> > 1000000100000 :: Int
02:11:32 <lambdabot>  1000000100000
02:12:33 <maybefbi> i wrote 1000000100000 to a postgresql column which is of the bigint type
02:12:43 <maybefbi> and read it back as an 8 byte integer
02:12:55 <maybefbi> im guessing bigint is not same as 8 byte integer
02:13:40 <maybefbi> i used hasql to convert bigint field to 8 byte integer
02:14:05 <pavonia> > (showHex 1000000100000 "", showHex 4294967306 "")
02:14:08 <lambdabot>  ("e8d4a696a0","10000000a")
02:14:34 <pavonia> Hhm, the second one is interesting
02:15:40 <maybefbi> yes indeed
02:17:27 <merijn> not really, that's still different from 1000000100000
02:18:34 <pavonia> But it's much more "even" than the input number
02:19:36 <merijn> > logBase 2 4294967306
02:19:39 <lambdabot>  32.000000003359034
02:19:58 <merijn> I'm betting on your postgres library fucking up
02:20:11 <merijn> And roundtripping through 32bit int
02:20:17 <merijn> Which library?
02:20:20 <maybefbi> hasql
02:21:17 <maybefbi> im using HD.int8 to read it back
02:21:26 <maybefbi> HD = Hasql.Decoder
02:22:40 <maybefbi> when i write it to the db, 1000000100000 is stored as 1000000100000
02:23:39 <maybefbi> i write them as two seperate writes though and the SUM them 1000000000000 and 100000
02:23:49 <maybefbi> but the column type is bigint
02:24:04 <maybefbi> im changing column type to int8 in sql db to match up with haskell code
02:24:19 <maybefbi> just to see if it is some sort of impedance mismatch
02:25:12 <aukeroorda> Hello. I am new to haskell, so my terminology might not be best, but here I go: I am trying to print a tuple of float lists ([Float], [Float]). When I do this, I get an error: 'no instance for show...' I found this answer: http://stackoverflow.com/a/23505621 but I have no clue how I should modify that answer so that it works for my case. My program
02:25:12 <aukeroorda>  is pasted here: https://dpaste.de/pVFy#L1,2,30,36 with the error pasted beneath it. Can someone explain to me how I should modify that SO answer, so that I can fix the instance for Show?
02:25:17 <merijn> bigint = int8 according to postgres docs
02:25:44 <merijn> aukeroorda: That answer is not actually a solution
02:25:48 <maybefbi> merijn, i see
02:26:00 <merijn> aukeroorda: The problem is that you're trying to print a function
02:26:03 <merijn> aukeroorda: But you can't
02:26:18 <aukeroorda> hmm, I see
02:26:30 <merijn> aukeroorda: You haven't pasted the right code
02:26:34 <merijn> aukeroorda: I don't see a print in there
02:26:45 <merijn> Except in the error
02:26:48 <aukeroorda> I am using GHCI, sorry.
02:27:41 <Boomerang> aukeroorda: you most likely forgot to supply the last argument to the polynomallondiv function
02:28:12 <aukeroorda> Hmm, that could be. I'll look there then.
02:28:17 <merijn> aukeroorda: "No instance for (Show ([Float] -> ([Float], [Float])))" <- this is saying that a function isn't printable. Which means you are passing a function to print. Probably, as Boomerang points out by missing an argument
02:29:01 <aukeroorda> Oke, thanks. Ill look into it and come back with a result in a few
02:32:29 <siwica> Are you aware of a good stack integration into emacs? Espeacially for things like stack ghci?
02:32:45 <maybefbi> intero
02:33:17 <maybefbi> https://commercialhaskell.github.io/intero/
02:33:31 <maybefbi> siwica, ^
02:34:42 <pavonia> If BigInt = Int8, what is a normal Int then? :o
02:35:02 <siwica> maybefbi:I will have a look. Thanks!
02:35:37 <maybefbi> pavonia, "Int" is the more common 32 or 64 bit integer. Implementations vary, although it is guaranteed to be at least 30 bits.
02:36:20 <maybefbi> Int is the type of machine integers, with guaranteed range at least -2^29 to 2^29 - 1
02:36:27 <pavonia> Yeah, but that would be bigger than a BigInt
02:36:36 <intothemountain> Some thread A only appends to a shared list, while the rest of the threads just get elements from the list at some position. What kind of concurrency mechanism should I use? MVars seem a bit waistful
02:37:22 <merijn> intothemountain: Do you really need fetching items at specific indices or just a queue?
02:37:53 <maybefbi> pavonia, bigint is 8bytes wide, haskell Int is only guranteed to be 30 bits wide. 8 bytes > 30 bits
02:38:04 <merijn> pavonia: I think your mixing things
02:38:17 <merijn> pavonia: Haskell's Int8 is very different from postgres int8
02:38:23 <intothemountain> merijn: every reading thread has to be able to get the last n messages it hasn't already downloaded
02:38:28 <pavonia> Oh, never mind then
02:38:38 <merijn> pavonia: Int8 is an 8bit integer. Postgres int8 is an 8 *byte* int
02:38:41 <maybefbi> merijn, i agree. thats why i use Data.Int.Int64
02:38:54 <maybefbi> to handle postgresql int8
02:39:00 <maybefbi> int8/bigint
02:39:07 <merijn> intothemountain: That's going to require some tricky design, tbh
02:39:12 <merijn> intothemountain: To get efficient anyway
02:39:29 <intothemountain> Efficiency is not a requirement
02:39:45 <merijn> intothemountain: So suppose I have more than N messages between reads. Do you want to drop the old ones?
02:39:46 <intothemountain> So just MVars with readMVar and switchMVar?
02:40:01 <merijn> intothemountain: There's an STM based queue package somewhere. Forget the name
02:40:25 <intothemountain> They could be dropped, provided *all* the threads have downloaded them
02:41:08 <aukeroorda> merijn: Boomerang: Yeah, I was indeed forgetting an argument. My recursion was fine, but I just forgot to initialise it with an empty list. Thanks
02:41:28 <aukeroorda> I was looking in the wrong place
02:41:38 <intothemountain> Though I think it would be an easier design (even if it's far less efficient) to just maintain it all in memory and serve indices
02:41:58 <intothemountain> That way you don't have to keep track of what thread has read
02:42:08 <intothemountain> Just serve them what they ask for
02:42:08 <merijn> intothemountain: "could be dropped provided all threads have downloaded them" <- this implies you can never drop them
02:42:35 <merijn> intothemountain: So basically, one thread posts messages and all other threads should be able to read ALL messages independently
02:42:42 <intothemountain> Yes
02:42:52 <merijn> intothemountain: That's a much simpler problem :)
02:43:04 <intothemountain> Good hehe
02:43:10 <merijn> intothemountain: Just use Chan with dupChan
02:43:21 * intothemountain is looking it up
02:43:32 <merijn> "Duplicate a Chan: the duplicate channel begins empty, but data written to either channel from then on will be available from both. Hence this creates a kind of broadcast channel, anyone is seen by everyone else."
02:44:00 <deception1> Hi, I am currently stumbling upon property based testing framework quickcheck. Its inventor John himself said, 70% of bugs are caught by unit testing, but it's that other 30% which causes your program to crash. QuickCheck is testing those last 30%. does quick check exclusively cover only that 30% or includes the original 70% too (at-least to some reasonable extent). Simply saying can I ditch my other testing frameworks in favor of quickcheck ?. Assuming I
02:44:02 <merijn> intothemountain: If you duplicate 1 channel per thread and then start writing to the original one, then basically you end up doing a broadcast to every thread and they can read those at their own pace
02:44:10 <intothemountain> Just what I need!
02:44:15 <intothemountain> Thank you merijn!
02:45:36 <quchen> deception1: QuickCheck will not show whether your program is correct, it will probabilistically show you your program is incorrect if it is.
02:46:11 <quchen> deception1: QuickCheck is a library to just “test stuff”, hoping it finds the cases you forgot thinking about.
02:46:19 <quchen> HUnit is for when you have specific cases you want to check.
02:46:30 <quchen> SmallCheck is for when you want to do exhaustive testing.
02:47:13 <quchen> If you have a complex data structure you want to test, and you generate 1000 test cases with QuickCheck, chances are you’re missing some corner cases. So don’t rely on it exclusively.
02:47:55 <deception1> quchen: oh got that
02:48:03 <deception1> quchen: I understand that it might ignore some valid failures which is the same case when I manually do example based testing right if so, is property based testing a superset of example based testing
02:48:46 <quchen> All tests are testing whether some property holds. QuickCheck’s purpose is randomly generating data to test the property against.
02:49:08 <merijn> Not entirely randomly
02:49:12 <merijn> It's biased to edge-conditions
02:49:13 <quchen> FSVO random
02:49:28 <quchen> Not uniformly random, yes.
02:49:55 <quchen> Smallcheck is a generalization of example based testing, since it is exhaustive, and given large enough parameters, will find test all possible inputs. (Takes forever of course.)
02:50:13 <quchen> QuickCheck is neither a sub- nor a superset of example based testing.
02:50:56 <deception1> Thanks that make sense :)
02:51:06 <deception1> *makes
02:51:19 <lyxia> with unit tests you can compare an input with an exact output
02:51:55 <lyxia> with property based testing you can do that only if you have two implementations of the same function
02:52:26 <lyxia> but in general you use looser properties as an approximation of correctness
02:52:54 <quchen> deception1: My advice is to write unit tests for weird corner cases you think of, and then adding QuickCheck tests for general properties you expect to hold.
02:53:46 <quchen> In other languages, you stop after speficying weird corner cases. In Haskell, you have QuickCheck testing out strange combinations of parameters, enriching your thought-of corner cases.
02:54:12 <deception1> lyxia: So even in example based testing we are checking equivalence property
02:56:14 <quchen> An example that QuickCheck found for me was when I generated a syntax tree and printed it to test my parser, I forgot handling the case where variable names could alias built-in keywords.
02:56:41 <quchen> Needless to say, the chance that a random string is "case" is pretty small. But over many runs of the testsuite, even that case was discovered.
02:56:46 <deception1> quchen: basically I am from python's property based testing framework Hypothesis which is based on Haskell's quickcheck with some upgrades for stateful property based testing. I allows me to upgrade my example based testing to property based ones so I got the impression of it may be a super set of example based testing
02:56:58 <quchen> I would never have thought about this otherwise, although in hindsight it was an obvious mistake.
02:58:23 <quchen> The example test case "True == True" *will* test whether True equals itself. Testing "\x -> x == x" with QuickCheck will not necessarily check that case.
03:00:34 <mauke> @check \x -> abs x >= (0 :: Int)
03:00:38 <lambdabot>  +++ OK, passed 100 tests.
03:00:45 <mauke> not very edgy
03:03:02 <lyxia> @check \x -> (x :: Int) /= 1000
03:03:05 <lambdabot>  +++ OK, passed 100 tests.
03:03:12 <lyxia> the number 1000 does not exist
03:03:46 <sshine> do you mean that the number 1000 is not generated? :P
03:03:56 <sshine> surely it exists! I can prove it!
03:03:57 <lyxia> sssh
03:04:14 <dememorized> sshine: Formal proof left to the reader.
03:05:03 <mauke> @check \x -> x /= (minBound :: Int)
03:05:05 <lambdabot>  +++ OK, passed 100 tests.
03:06:28 <bjs> QuickCheck is guaranteed* to give same answer as a formal proof
03:06:31 <bjs> *given long enough
03:06:37 <dcoutts> monochrom: :-) ?
03:06:44 <quchen> :t GHC.TypeLits.natVal (Proxy :: Proxy 1000)
03:06:46 <lambdabot> Integer
03:06:57 <quchen> 1000 exists! But natVal is not in scope for Lambdabot :-(
03:07:36 <quchen> bjs: No, it is not.
03:07:39 <bjs> quchen: :p
03:10:26 <bjs> quchen: well given long enough it'll exhaust anything that could be generated by the Arbitrary instances, which is close enough
03:11:07 <quchen> But many Arbitrary instances are not exhaustive. Int for example.
03:11:29 <bjs> quchen: that's true, you just need to have better instances :)
03:11:41 <bjs> or use SmallCheck which i think is exhaustive
03:12:20 <mauke> how can it not return minBound? that's just terrible
03:13:38 <quchen> For Int?
03:13:47 <mauke> yeah
03:14:13 <Axman6> yeah, the instance should totally start with [0,1,-1,minBound,maxBound...]
03:14:23 <quchen> That would often be undesirable, consider functions taking ints as arguments, such as replicate. The test \n -> length (replicate n ()) == n is useful, but would take forever.
03:15:42 <lyxia> bjs: there are only finitely many seeds :P
03:16:18 <bjs> lyxia: hence use smallcheck! no seeds required
03:17:57 <lyxia> mauke: https://github.com/nick8325/quickcheck/issues/98 Generate edge cases
03:24:57 <maybefbi> merijn, im going to use NUMERIC(1000,12) and Data.Scientific to store money instead of 8 byte ints. hasql decoder for 8 byte ints is not reliable.
03:25:11 <JonReed> Hi, guys. What do you think would be a good names for three variations of cast functions, f1 throws an error if value is invalid. f2 returs nothing if value is invalid, f3 does no checks on invalid value and casts it, assuming you know what you're going. So, far I came up with `toPositiveInt :: Int -> Positive Int`, `toPositiveIntUnsafe :: Int -> Positive Int`, `toMaybePositiveInt :: Int -> Maybe PositiveInt`.
03:25:42 <JonReed> But then `toPositiveInt` can sneak on somebody and throw an error they don't expect
03:26:23 <JonReed> Ah, perhaps I need to just make it in the error monad or something, then it will be obvious... hm
03:26:36 <Cale> Well, that's what Maybe is for...
03:27:16 <JonReed> Yeah, I just want to expose several ways to cast to the type, some of which are less safe than others
03:27:37 <lyxia> what's the difference between the first two
03:27:49 <lyxia> ah no check
03:28:35 <joncol> Hi, just out of curiousity, what's a shorter way of writing: map f [g x, h x, i x] ? One, where I don't have to repeat the x?
03:28:48 <lyxia> you can call the first and second one _, enforcing that nobody uses them
03:30:15 <joncol> Oh, realised it: map (f . ($x) [...]
03:51:55 <quchen> joncol: [f (s x) | s <- [g,h,i]]
03:52:28 <buttbutter> Is there any way to make this work :D? https://0bin.net/paste/n2BaQ2WefVOPHDfz#54nZZhJMkoK+NSD6-TKZG8JFmmXjGN4+2WYoWSkwsRr
03:52:59 <buttbutter> (aside from just replacing env with fst c, obviously)
03:53:06 <lyxia> buttbutter: let env = fst c in case ...
03:53:15 <buttbutter> Ah, I thought as much.
03:53:24 <buttbutter> So you can't use where for lambda expressions I guess?
03:53:35 <buttbutter> Or inline functions or whatever they're called :)
03:53:55 <lyxia> indeed
03:54:08 <ski> `where' attaches to definitions and to `case' branches
03:54:27 <quchen> buttbutter: http://lpaste.net/198045
03:54:48 <buttbutter> quchen: I like that more than the let :D
03:54:51 <buttbutter> Good idea. Thanks!
03:55:02 <ski> @type \case Nothing -> False; Just b -> c where c = not b
03:55:07 <lambdabot> Maybe Bool -> Bool
03:55:50 <buttbutter> ski: And I didn't quite understand what you said.
03:56:07 <ski> the "`where' attaches to definitions and to `case' branches" bit ?
03:56:26 <buttbutter> Yes.
03:57:24 <ski> you can write
03:57:30 <ski>   foo x = (x,y)
03:57:36 <ski>     where
03:57:39 <ski>     y = x + 1
03:57:46 <ski> but you can't write
03:57:52 <ski>   foo x = (x,y where y = x + 1)
03:58:25 <ski> since the `where y = x + 1' part attaches to the defining equation `foo ... = ...' here
03:58:36 <ski> similarly, you couldn't write
03:58:46 <ski>   foo = \x -> (x,y where y = x + 1)
03:58:47 <ski> nor
03:58:53 <ski>   foo = \x -> (x,y)
03:58:54 <ski>     where
03:58:58 <ski>     y = x + 1
03:59:00 <buttbutter> Ah I see :D
03:59:11 <buttbutter> Basically it attaches a "level too high"
03:59:16 <buttbutter> If that makes any sense.
03:59:28 <ski> yes
03:59:37 <buttbutter> Great. Thanks for the helpful explanation :)
03:59:39 <ski> there are no `where'-expressions
03:59:47 <ski> but there are `let'-`in'-expressions
03:59:55 <buttbutter> Right!
04:00:16 <ski> and, if you use a `case'-expression, *then* you can also attach a `where' to any branch of it
04:00:33 <buttbutter> Right, of course.
04:00:50 <buttbutter> Okay, gotta get lunch. Thanks!
04:00:53 <ski> i showed above that with a `\case', you could attach a `where'
04:01:20 <ski> and of course, you could use a `\case' with a single branch, attaching a `where', instead of a plain `\'-expression, to which a `where' can't be attached
04:01:39 <ski> so using a `\case' instead of a `\' would be a workaround to not being able to attach a `where' to a `\'-expression
04:01:48 <ski> (`\case' is an extension)
04:03:02 <joncol> quchen: OK, that's nice !
04:09:28 <[k-> TIL lambdacase is literally \case
04:12:12 <Cale> heh, yeah
04:12:35 <Cale> I kinda wanted the syntax to just be case of ...  (i.e. leave out the scrutinee)
04:12:48 <Cale> Sort of like the syntax of operator sections
04:13:04 <Cale> But they didn't go with that
04:16:01 <ybit> https://www.youtube.com/watch?v=buQNgW-voAg LambdaConf 2015 - The Next Great Functional Programming Language John A De Goes
04:16:23 <quchen> Cale: What happened to \if?
04:16:51 <tsahyt> why does wires skip the Monad instance even though it's an Applicative and an instance of Bind, hence having everything that makes up a monad?
04:16:57 <tsahyt> for the Wire type that is
04:17:17 <tsahyt> oh wait, the Bind instance was for something else
04:19:01 <tsahyt> looking at the definition for pure for Wire, what's the reason for this let binding? pure x = let x = Wire (\_ -> pure (x, w)) in w
04:19:05 <joncol> I'm trying to use Cassava to convert my record types to CSV. How can I define an instance of the ToRecord typeclass where one of the fields of my record, is in turn another record with a ToRecord instance. I.e. how can I reuse my previous "atomic" toRecord definition in my "composite" record?
04:19:19 <tsahyt> oh, recursion
04:19:29 <tsahyt> I think I'll shut up now after answering my own questions twice in a row...
04:20:32 <ggVGc> Cale: make LambdaCaseTerse
04:21:27 <pavonia> joncol: The toRecord function returns a Vector, so you can use the values of this for building your new result
04:23:14 <joncol> pavonia: Cool, I'll try that.
04:26:58 <joncol> Noob question: How do I import the (++) function from Data.Vector and avoid the warning for name ambiguity with (++) from Prelude?
04:29:01 <pavonia> You can import the module qualified "import qualified Data.Vector as V" and then use the function with that qualifier: "foo V.++ bar"
04:29:02 <Boomerang> joncol, several options: "import qualified Data.Vector as V" and then in code "V.++" or if you're not using (++) from Prelude you can do "import Prelude hiding ((++))"
04:29:18 <joncol> Boomerang: Ty
04:31:42 <Cale> quchen: I'm not sure
04:31:56 <Cale> quchen: I'm okay with using \case there, personally.
04:32:38 <tsahyt> okay reading the code doesn't get me much further, I need to understand the underlying concepts better. Can anyone point me to something that might help me understand arrowized FRP, or particularly the wires library?
04:32:43 <Cale> but yeah, maybe it should exist as part of LambdaCase for consistency
04:33:16 <Cale> tsahyt: The person to talk to about wires would be ertes
04:33:41 <Cale> I can answer general questions about arrowized FRP though
04:33:46 <tsahyt> Cale: I know, but he doesn't seem to be here
04:34:07 <tsahyt> I'd be happy with any paper on the topic that may help me build some intuition for the concepts involved
04:34:12 <Cale> hmm
04:34:12 <tsahyt> or tutorials for that matter
04:34:22 <Cale> the old papers on Yampa might be useful as a start
04:34:55 <tsahyt> was yampa the first AFRP implementation?
04:36:54 <Cale> tsahyt: I *think* so
04:37:12 <Cale> There might have been something else before it, but not by much
04:37:35 <tsahyt> so a Wire m a b is basically the wires version of signal functions I suppose? With an underlying monad included
04:37:37 <Cale> http://haskell.cs.yale.edu/publications/#FunctionalReactiveProgramming -- there are a bunch of other papers here
04:37:40 <Cale> yeah
04:38:09 <Cale> So implicitly, you have this time-varying signal of type a coming into the component, and a time-varying signal of type b going out
04:38:53 <tsahyt> and because those are arrows I can compose them like arrows
04:39:02 <Cale> yeah
04:39:12 <tsahyt> seems simple enough
04:39:36 <Cale> It might be useful to look at the implementation a bit https://hackage.haskell.org/package/wires-0.1.0/docs/Control-Wire-Internal.html
04:39:49 <tsahyt> I have but that's where I started getting confused
04:40:00 <Cale> So internally, a value of type Wire m a b is a function a -> m (b, Wire m a b)
04:40:03 <tsahyt> I haven't seen half of these typeclasses before
04:40:15 <Cale> ignore all the type classes for now
04:40:28 <Cale> Just pay attention to the newtype declaration :)
04:40:57 <tsahyt> so it's a function that gives me a value and a "stepped" version of itself I suppose?
04:41:03 <Cale> yeah
04:41:16 <Cale> to be used for the next value of type a
04:41:38 <tsahyt> where does the first a come from?
04:41:51 <Cale> That's the input to the system
04:42:10 <tsahyt> okay, and how do Events tie into this?
04:43:22 <Cale> In this style of system, Event is typically implemented something like Maybe... and if you look below, yeah
04:43:37 <Cale> He has data Event a = NotNow | Now a
04:44:19 <Cale> Note that this is just the internal representation, so depending on which operations he gives you outside of this module, you may or may not be able to observe that time is discrete
04:44:23 <tsahyt> but it is described as a stream?
04:44:39 <tsahyt> so I thought it'd be more like [(Time, a)] or something
04:44:41 <Cale> Yeah, so typically, the Event will occur as either the input to, or output of a Wire
04:45:50 <Cale> *conceptually* an FRP Event a is not supposed to be the same thing as a Behaviour (Maybe a)
04:46:08 <Cale> but in this style of system, you can implement it like that
04:46:26 <Cale> (the inputs and outputs of Wires are effectively behaviours)
04:47:19 <Cale> He probably doesn't let you tell what an Event is internally and only gives you operations for manipulating Wires whose inputs or outputs involve Events
04:48:20 <Cale> and so you're meant to think of a Wire from a to b as operating on a continuously varying signal of type a to produce a continuously varying signal of type b
04:48:42 <tsahyt> okay
04:49:15 <Cale> and where Event occurs, that's just going to occur at discrete points in time
04:50:00 <Cale> But yeah, internally, the system only computes anything at particular frames, the times when events may be occurring
04:50:23 <tsahyt> that makes sense, otherwise it'd be wildly inefficient I suppose
04:50:50 <Cale> yeah
04:51:54 <tsahyt> I'll read some of the yampa papers. SF looks a bit simpler than Wire because it has no Monad underneath it
04:51:59 <Cale> yeah
04:52:43 <tsahyt> Arrows, Robots, and FRP seems like a good starting point
04:53:04 <Cale> Yeah, that sounds appropriate
04:53:23 <tsahyt> "No knowledge of Arrows is required" appeals to me as I just started learning more about them today. I only ever used them for having a nice API to manipulate tuples
04:53:51 <Cale> Something about Yampa which should be pointed out is that it was very explicit about what times were, compared to many later FRP systems
04:54:12 <Cale> It could integrate signals for example, because it could do algebra with the elapsed time between frames
04:54:48 <Cale> Very convenient in some ways, but limits the applicability of the system in others
04:55:16 <tsahyt> the wires example performs integration as well but uses a tick generator as far as I can tell to get the required time deltas
04:57:01 <Cale> Yeah, it's more typical now to have explicit notions of time be separate from the core of the FRP system, and be treated as input events of some sort.
05:00:37 <tsahyt> Cale: does this not undo the idea of only doing something when events occur? Having explicit time means having events occur quasi-continuously, right?
05:02:26 <Cale> Well, the primitives are still arranged so that you can regard Events as occurring discretely (really it computes the values of behaviours discretely too, it just hides this from you)
05:02:57 <Cale> The Event datatype is abstracted so that you can't tell it's similar to Maybe
05:03:14 <Cale> Even though internally, it looks that way
05:05:07 <Cale> Though, there's another kind of ambiguity present (though the distinction doesn't really begin to matter until you start putting higher order FRP primitives in) -- you can't distinguish between something which is just an Event a, and something which is a Behavior (Event a)
05:05:22 <tsahyt> so a behavior is just a time-varying value? i.e. in this case it's part of the notion of a wire (or signal function)
05:05:42 <Cale> yeah
05:05:46 <tsahyt> seeing as wires doesn't define a Behavior type
05:06:10 <Cale> The idea here is that Wire m a b is something like Behavior a -> m (Behavior b)
05:06:56 <Cale> Except that it probably wants you to think of e.g.  Wire m (Event a) a  as being like  Event a -> m (Behavior a)
05:07:25 <tsahyt> is a Behavior in the classical sense just a Wire with a constant input, i.e. something like Wire m () b ~= Behavior b
05:07:51 <Cale> I suppose you could think of it like that, yeah.
05:08:08 <Cale> (Or perhaps that should be m (Behavior b))
05:08:13 <Cale> (but whatever)
05:08:39 <Cale> actually, the m is a bit subtle
05:09:01 <tsahyt> I suppose it's for allowing things from the real world or some other environment to influence the wire
05:09:10 <tsahyt> like key presses or whatever
05:09:15 <Cale> So the types I've been giving aren't quite right: the effects of the monad can occur on every frame
05:09:30 <Cale> (not just when the network is set up at the beginning)
05:09:43 <Cale> yeah
05:11:02 <Cale> So the m-impurities can show up anywhere, and you'd probably want anything which was actually pure to leave m as a plain variable
05:44:02 <piyush-kurur> question: IO vs ST. Will moving to ST improve performance particularly when the IO action is used for "benign side effects"
05:44:30 <piyush-kurur> I think no because IO is ST RealWord  is it not?
05:45:21 <c_wraith> IO and ST have the same performance.  It is the same structure underneath, but it's not quite that.
05:45:41 <piyush-kurur> c_wraith: yes I understand that ST is better in general
05:45:54 <piyush-kurur> not for the performance but for the intention
05:46:04 <piyush-kurur> of benign effects
05:46:06 <c_wraith> Oh, I just meant that they're both implemented on top of an internal structure
05:46:12 <c_wraith> Rather than one being the other, directly
05:46:28 <c_wraith> But they both are wrappers around the same internal type.
05:49:45 <merijn> piyush-kurur: ST is not going to be more performant than the same implementation using IO, no
05:50:54 <merijn> piyush-kurur: The big advantage is that you can "escape" ST and thus use ST implementation in pure code
05:54:17 <agocorona> I think that behaviours are not general enough
05:54:32 <piyush-kurur> merijn: yes that is wanted to know
05:54:45 <Cale> agocorona: In what way?
05:55:29 <Cale> agocorona: Behaviours on their own certainly are not general enough...
05:57:18 <agocorona> since it should consider space-time. or in the case of a simulation or a game, all the degrees of fredom in the game, understood as all the positions of the objects for example
05:58:39 <agocorona> at last a game is a trajectory in the phase space of the N-dimensional manifold created by all the possible games playable by the program. where N are the degrees of freedom
05:58:55 <agocorona> that is what a physicist would say
05:59:11 <agocorona> where t is only one of the dimensions
05:59:16 <agocorona> t=== time
05:59:42 <merijn> agocorona: But dimensions like user input are already caught by the notion of Behaviour/Events
05:59:49 <agocorona> but I pesonally don´t want to consider this, neither continuous time, neither behaviours, and consider just events
06:07:41 <Cale> agocorona: Well, you can use another continuous representation of space, and have a behaviour of that.
06:07:45 <Cale> But it depends on what you're computing...
06:08:13 * hackagebot ip6addr 0.5.1.3 - Commandline tool to generate IPv6 address text representations  https://hackage.haskell.org/package/ip6addr-0.5.1.3 (MichelBoucey)
06:08:15 * hackagebot traildb 0.1.1.0 - TrailDB bindings for Haskell  https://hackage.haskell.org/package/traildb-0.1.1.0 (Adeon)
06:10:04 <Cale> Also, across different FRP systems, behaviours are subtly different concepts, because the underlying notion of time might be different. Whether you can do things like shift a behaviour by a specific amount of time makes both a big difference in terms of what can be computed and a big difference in terms of the performance characteristics of the system.
06:11:08 <Cale> Being able to offset a behaviour by a given amount of time makes it really easy to write space leaks, but might be really useful to certain applications.
06:12:57 <Cale> (this kind of thing, btw, is why I'm still sort of unsettled by our lack of an algebraic/equational approach to expressing what FRP concepts are -- there are plenty of operations on functions Time -> a which you might nonetheless be prevented from carrying out on values of type Behaviour a
06:13:12 <Cale> )
06:13:17 <agocorona> I could use even quantum mechanics to wait for a button pushed, considering all the superpositions of the user and the PC pressing and not pressing the button, and all the possible worlds that this would produce according with the Everett interpretation
06:13:22 <agocorona> I'm not kidding
06:13:30 <agocorona> but normally, a callback will suffice
06:13:46 <merijn> agocorona: I'm not quite sure what you're trying to argue?
06:13:53 <Cale> Yeah, in fact, such a thing might be possible in Reflex's framework :)
06:15:11 <Cale> Those 't' arguments are meant to refer to a "timeline", and the facility is not yet fully used, but you could imagine a transformer of sorts, like Event (Nondet t) a / Behavior (Nondet t) a, or *maybe* even Quantum t
06:15:40 <agocorona> I'm telling that models are artificially complicated and make programming simple things hard
06:15:53 <agocorona> for practical purposes
06:16:16 <Cale> I don't really understand -- programming simple things with this stuff is typically pretty easy.
06:16:51 <Cale> Well, it might not be quite as easy as it ought to be to write new bindings to external libraries... yet :)
06:17:11 <merijn> agocorona: Are you arguing for/against FRP, having a problem understanding FRP, or...? I'm honestly not really sure what kinda response you're trying to get
06:17:16 <Cale> But once you have one of those, it tends to be pretty nice to use.
06:18:30 <Cale> There are a bunch of pieces of reflex-dom which seem like they don't really have anything to do with the DOM specifically, and would be of use more generally for people creating bindings to similar callback-based libraries.
06:18:56 <Cale> So perhaps they'll end up getting moved out
06:19:32 <agocorona> so FRP is simple?
06:19:40 <Cale> (they're already nicely factored, just haven't moved from that package to the other)
06:20:11 <Cale> agocorona: Simple to use (about as simple to use as could be expected, given the problem domain). Tricky to implement well.
06:20:24 <agocorona> In which problem domain?
06:21:04 <Cale> Situations in which you have many things which need to change over time in a way where each part of the system is reacting to changes in other parts of the system.
06:21:12 <merijn> agocorona: Writing programs that have continuous inputs or outputs
06:21:16 <agocorona> Scala people have reactive distributed streaming. Where is the Haskell FRP equivalent?
06:21:36 <merijn> I'm not sure what "reactive distributed streaming" even menas
06:21:38 <agocorona> merijn: that is a very narrow definition of reactive
06:21:38 <merijn> *means
06:21:49 <merijn> agocorona: Reactive has been hijacked as a term
06:21:51 <Cale> agocorona: You want, like, reflex-distributed-process?
06:22:03 <merijn> agocorona: Functional Reactive Programming is about one thing, programming with continuous time
06:22:05 <agocorona> yes, for example
06:22:10 <bartavelle> merijn, I *think* it's the stuff pushed by typesafe (which changed name)
06:22:11 <merijn> agocorona: Anything else is not FRP
06:22:22 <merijn> bartavelle: That tells me nothing
06:22:38 <merijn> bartavelle: I'm vaguelly aware that typesafe is associated with a Scala compiler
06:22:46 <agocorona> merijn:  I think that the hyaking is th one of haskell.  that narrow domain is so concrete and small that is almost useless
06:22:48 <merijn> But I don't even know if it's a company, organisation, person
06:22:53 <bartavelle> merijn, I *think* it's more like a pipe+untyped actors stuff
06:22:58 <Cale> merijn: I hesitate to say "continuous" these days, since it seems enough to just be non-specific-enough about what time is that it *could* be continuous, without actually supporting operations that let you observe the continuity.
06:23:07 <bartavelle> merijn, distributed too
06:23:39 <Cale> (the operations which let you tell that things are continuously varying in time tend to be the ones which create performance problems in large projects...)
06:23:44 <Forlorn> How can I map a string to a list of integers? "1234" -> [1,2,3,4]
06:24:08 <Cale> > map fromDigit "1234"
06:24:12 <lambdabot>  error:
06:24:12 <lambdabot>      Variable not in scope: fromDigit :: Char -> b
06:24:13 <merijn> agocorona: Conal Elliott invented the term FRP in 1997
06:24:14 <Cale> ah
06:24:24 <merijn> agocorona: In the paper "Functional Reactive Animation"
06:24:30 <Cale> oh, right, digitToInt
06:24:35 <Cale> > map digitToInt "1234"
06:24:35 <agocorona> merijn:   that is FRA, not FRP
06:24:38 <lambdabot>  [1,2,3,4]
06:24:55 <Forlorn> how to write a digitToInt?
06:25:03 <Forlorn> (\x -> read x)?
06:25:23 <Cale> Forlorn: You could use chr/ord, or you could indeed make a singleton list out of the character and apply read to it.
06:25:27 <ski> @index digitToInt
06:25:27 <lambdabot> Data.Char
06:25:34 <merijn> agocorona: Conal has written plenty of papers after that about the term, most of them predating the modern "reactive" frameworks by over a decade
06:25:41 <cocreature> agocorona: papers tend to have other content than the title
06:25:42 <Cale> digitToInt happens to work with 'a' through 'f' as well (for hexadecimal)
06:25:53 <merijn> agocorona: It's also not so limited as you're saying
06:26:00 <Forlorn> Cale, thank you
06:26:42 <merijn> The modern "meaning" of reactive is basically just "event-driven", which is meaningless to the point of uselessness. The original FRP meaning had a wide range of applications and was nicely specific.
06:26:43 <agocorona> So anything reactive that need to be functionally composable that are not "conal FRP" is not worth attention
06:27:14 <merijn> agocorona: I'm not saying that. I'm saying 1) comparing them with FRP is silly, because you're comparing things with very different goals and 2) the naming is wrong
06:27:41 <bartavelle> agocorona, it's like arguing that people are narrow-minded when they don't want to use the word "isomorphic" to mean seamless client/server development
06:28:04 <agocorona> how to call wathever that handles asynchronous events in a composable way?
06:28:10 <merijn> I mean, we had a perfectly good word for "reactive" before idiots started calling it "reactive"
06:28:15 <merijn> agocorona: "event-driven"
06:28:29 <bartavelle> agocorona, well, in the scala world it's done with actors and called "reactive"
06:28:31 <merijn> "composable event-handling"?
06:28:48 <merijn> "functional events", I can think of a million words
06:29:00 <agocorona> reactive is a world of the XVIII century. it was not invented by Conal
06:29:22 <Cale> agocorona: There are a whole bunch of systems in this space which are probably worth attention, but the main properties we're looking for are equational reasoning (it should be possible to reason about replacing one piece of code with another equivalent one, without getting entangled in problems related to how many times effects occur, or in what order), and local reasoning (it should be possible to understand 
06:29:22 <Cale> functions defined in the system without understanding the context in which they will be used)
06:29:46 <Cale> and of course, at the same time as this, we want to discuss things which are changing over time
06:30:20 <bartavelle> agocorona, well, one could argue they are perfectly right with calling this reactive. I won't judge that. The problem is that it's meaningless. In scala it means akka, and sometimes it means RX. In JS it means something else. In Haskell it means conal's flavored FRP.
06:30:23 <Cale> So we either want values which refer to those things which are changing over time, or values which refer to the transformations taking place on those time-varying things
06:30:37 <bartavelle> or some other discrete system
06:30:40 <Cale> But in either case, we really want the equational and local reasoning to hold.
06:30:40 <merijn> agocorona: The entire argument here seems to be that you don't like people here not using your definition of reactive and trying to convince us that we're wrong for that reason
06:30:52 <merijn> agocorona: Which is a pointless discussion
06:31:05 <agocorona> Cale and what if I have a library that does all what Reflex do in the web browser and perform distributed computing and  multithreading in a reactive way?
06:31:30 <agocorona> it is not FRP?
06:31:35 <merijn> Pretty sure reflex already runs in the browser :p
06:31:39 <Cale> I don't know if it's FRP or not.
06:32:08 <merijn> The FRP-ness (FRP-ity?) of that is orthogonal to the distributed computing and multithreading
06:32:18 <Cale> There are two parts: 1) Is it functional? That is, are the time-varying quantities represented by things that behave as mathematically pure values?
06:34:00 <Cale> 2) Is it reactive? That is, are you really discussing time varying quantities in the first place? Is there a sufficiently expressive library of operations from building new time-varying quantities from old?
06:34:43 <agocorona> Lets call my library XXX.  if XXX can do all what FRP can do, and much more, what we can tell about FRP?
06:35:02 <bartavelle> agocorona, C does all the things FRP can do. Is it reactive ?
06:35:04 <Cale> agocorona: Well, the IO monad can do everything FRP can do.
06:35:21 <Cale> That is completely irrelevant to the question
06:35:33 <merijn> Anyway, I don't see the point of this discussion
06:35:49 <Cale> The relevant part is the manner in which we're able to reason about the software that we're constructing.
06:35:54 <agocorona> and assembler. I mean it with less code and in a composable way
06:36:25 <Cale> Not simply what we can accomplish from the ground up, but how easy it is to maintain and refactor and, if one were so inclined, to prove theorems about.
06:36:53 <Cale> On that last one, I think FRP systems could still come a long way -- they're not quite where they ought to be, but there is some hope.
06:38:11 <Cale> I can write down some equational properties involving reflex's combinators for building up Events and Behaviours, but I don't know what is a sufficient basis of axioms to prove everything one might need to.
06:39:05 <Forlorn> Sorry for asking twice, but how is `maximum` presumably constructed? I want to compare multiple elements and find what "I" consider to be the maximum.
06:39:10 * ski . o O ( "reasonability" vs. "expressivity" )
06:39:14 <bartavelle> OTOH I find it debatable that all the FRP machinery is that useful in the common case. As I had technical gripes about ghcjs I went for Elm, and its programming story is a lot simpler and more straightforward than when I wrote reflex-dom stuff ...
06:39:26 <Cale> Forlorn: perhaps you're looking for maximumBy?
06:39:27 <ski> @src maximum
06:39:27 <lambdabot> maximum [] = undefined
06:39:27 <lambdabot> maximum xs = foldl1 max xs
06:39:33 <Cale> @src maximumBy
06:39:33 <lambdabot> Source not found. I don't think I can be your friend on Facebook anymore.
06:39:35 <Cale> aw
06:39:42 <ski> Forlorn : it uses the `Ord' instance
06:39:46 <merijn> :t maximumBy
06:39:48 <lambdabot> Foldable t => (a -> a -> Ordering) -> t a -> a
06:39:58 <Cale> yeah, the type is good enough :)
06:40:11 <Forlorn> I see, thanks again
06:40:45 <agocorona> Cale: mine uses just the laws of monad applicative and alternative. 
06:40:47 <ski> can the argument to `maximumBy' be a lub ?
06:41:01 <Cale> bartavelle: I think it will be -- right now there is not enough of an ecosystem of libraries involving reflex outside of reflex-dom because that's what most of us are doing with our time (because it's what we're paid to do)
06:41:36 <Cale> agocorona: But those laws are not enough to say anything nontrivial, unfortunately. You can't distinguish your monad from the identity monad without saying more than that.
06:41:40 <c_wraith> I just want a ghcjs library that does proper CORS
06:42:03 <Cale> c_wraith: Can you describe what "proper CORS" consists of?
06:42:15 <c_wraith> Cale: works on IE11 and Safari mobile.
06:42:19 <bartavelle> Cale, but to satisfy the laws, I found it was inconvenient to compose stuff (I would have liked an applicative interface for example). That's why they have some TH support to help with that.
06:42:54 <Cale> bartavelle: Are you referring to combineDyn etc?
06:42:58 <bartavelle> yup
06:43:01 <Cale> Because that's all gone now :)
06:43:05 <bartavelle> oh!!!!
06:43:07 <Cale> Dynamic is an Applicative and Monad now
06:43:18 <bartavelle> oh, exactly what I wanted
06:43:24 <bartavelle> I'll take a look again then!
06:43:34 <Cale> (well, in the recent github versions, which you'll get if you grab reflex-platform)
06:43:39 <bartavelle> and stop badmouthing it ;)
06:43:46 <bartavelle> oic
06:43:55 <agocorona> Cale: Sure there are more properties, but it does not invent their own. that is not a good sign of goo composability.. since there is not enoug generatily if it does not agre with the laws of applicative and alternative
06:44:42 <Cale> agocorona: Sure, you want those laws to be satisfied if you're going to support those interfaces, but *only* those laws don't tell you enough about what things mean
06:44:52 <agocorona> does not invent their own operators i mean
06:45:02 <Cale> agocorona: You need laws which relate the actual things you're combining with the Applicative and Monad instances.
06:45:13 <Cale> That's the problem with IO, generally
06:45:47 <Cale> Is that while IO satisfies the Functor, Applicative and Monad laws, once you get beyond that, it's immediately hard to say much
06:46:26 * ski . o O ( `readIORef <=< newIORef = return',&c. )
06:46:50 <Cale> Yeah, there are a few things like that which you can write down
06:47:12 <ski> but as soon as you don't know who has the reference, then some other thread could modify the cell inbetween ..
06:47:17 <Cale> yep
06:47:41 <c_wraith> I'm glad you can't monkey-patch functions in haskell.
06:47:53 <agocorona> Cale: if i can create two distributed applications  with a web interface each one and I can combine them with >>  <|> and <*>  probably we are in front of something with a decent quantity of good properties, Its'nt?
06:48:05 <Cale> agocorona: maybe, maybe not :)
06:48:17 <Cale> agocorona: It could still be something which is really hard to understand
06:48:18 <c_wraith> I was just imagining someone deciding they need that IORef, monkey-patching newIORef to throw all new IORefs into a global structure somewhere, and the chaos that would result.
06:49:22 * ski grins
06:49:37 <agocorona> Cale: hard to understand does not mean anything wrong.  I learn from things hard to understand created by me that Haskell accept as valid and execute in intereting ways. Not the expected ones
06:50:02 <Cale> agocorona: The question is really this: how easy is it to make changes to the system which should maintain the behaviour of a big complicated system, and have some reliable expectation that you really *haven't* subtly screwed things up.
06:50:36 <agocorona> Cale: it is much more easy if it is composable
06:51:18 <agocorona> and the pieces maintain invariants, as is the case
06:52:02 <Cale> agocorona: For example, consider something like f :: Integer -> Integer; f x = x + x in Haskell. We can feel safe in rewriting that as f x = 2 * x
06:52:34 <Cale> Whereas if we were writing a macro in C, that's not a transformation which is valid
06:53:01 <Cale> (e.g. x could be instantiated to something like i++ and then the number of incrementations which occur will vary)
06:54:24 <exio4> Cale: one would be UB and the other would be defined? :P
06:54:39 <agocorona> That is not a serious problem since distributibity does not hold for 99% of the real software and it should not be considered as a valid transformation. The fact is that no compiled does that transformation
06:55:11 <Cale> exio4: right :P
06:55:35 <Cale> agocorona: Well, this is a very simplistic example to give a sense for what kind of thing I mean
06:55:41 <agocorona> what really is interesting are laws not involving commutativity and distributibity
06:55:57 <Cale> I'm not really talking about *numerical* laws
06:56:27 <Cale> I'm talking about laws regarding how your operations for describing time-varying quantities interact with one another.
06:57:49 <Cale> *That's* what FRP is really about -- we don't yet fully have that story fleshed out, but pretty much everything about FRP is aimed toward that goal.
06:58:28 <Forlorn> Can I write multiline string in Haskell without newline?
06:58:43 <Cale> To make it possible to write declarative programs that describe the connections between time-varying quantities in a way that one can imagine reasoning about in an equational fashion (even if we don't know all the equations yet)
06:58:51 <Insanity_> Forlorn: what do you mean?
06:59:11 <agocorona> Cale: that is not problem in discrete time, which is at the end, the real case almost ever
06:59:24 <Cale> No, it's a problem regardless of your notion of time
06:59:25 <ertes> what's a good way to capture structures with two distributive operations in category theory?  like semirings, rings, fields…  in other words:  monoid is to category as semiring is to X; X = ?
06:59:30 <jonored> Forlorn: Like, here-document style?
06:59:32 <Forlorn> Insanity_, ttp://sprunge.us/NKRa
06:59:37 <Forlorn> http://sprunge.us/NKRa
06:59:40 <Cale> For any nontrivial notion of time, it's something that we'd like to be able to do
07:00:02 <Insanity_> that happens to be from a Projecteuler problem? :P
07:00:06 <Cale> and it's almost every bit as difficult for sequential time as for continuous
07:00:18 <Forlorn> and without any spaces and/or newlines
07:00:22 <Forlorn> just numbers
07:00:26 <ski> > "For\  \lorn"
07:00:31 <lambdabot>  "Forlorn"
07:00:46 <Cale> ertes: Additive category?
07:01:42 <lingxiao> hey guys
07:01:44 <lingxiao> I'm using stack and get this error:
07:01:54 <lingxiao> Could not find module ‘Test.HUnit’It is a member of the hidden package ‘HUnit-1.3.1.1@HUnit_7IPmJa5HmrxHl6ny5upchm’.
07:02:03 <lingxiao> even though I put HUnit in build depends 
07:02:10 <agocorona> I don't see the problem of computing a value depending on a variable t at the isntant t= t0 then in the instant t=t1 etc
07:02:14 <ski> Forlorn : or `concat', i suppose
07:02:28 <ertes> Cale: trying to see the ring in there
07:02:41 <Insanity_> ski: what you showed first was correct I think. but you can't put it on multiple lines due to irc :P
07:02:50 <ski> Insanity_ : i know, so i didn't
07:02:52 <Insanity_> Forlorn: you have the text file of that though
07:03:10 <Insanity_> ski: I was clarifying more for Forlorn as to why you didn't use multiline :P I assumed as much
07:03:20 <Cale> agocorona: In fact, I think people get a little too hung up on the continuous time aspect -- admitting a continuous time model in principle seems to be important to defining systems which work well, but actually putting in operations that let the user see that time is continuous (e.g. operations which stretch time or delay it by arbitrarily small quantities) tend not to be central to what we're trying to accomplish in 
07:03:20 <Cale> many cases.
07:03:24 <ski> (ok, good)
07:03:28 <lingxiao> wait nvm
07:03:39 <Insanity_> I'd just read it in from the text file Forlorn instead of copy/pasting it into your code
07:04:08 <Cale> agocorona: For most purposes, if you can do all the things I've described above well enough on discrete time, I'll be pretty happy.
07:04:21 <Cale> i.e. the local and equational reasoning part
07:04:22 <ertes> it's not showing itself to me =/
07:04:51 <Cale> agocorona: Well, and you can get practical performance characteristics in your implementation while you're at it :)
07:05:33 <Cale> agocorona: Behaviours are still important even if time is discrete -- I think people often don't understand this as well.
07:06:53 <Cale> Maybe it really is a bit less obvious then, but when you're describing systems with complex interactions, you need a way to prevent parts of the system which are being permitted to do high-frequency computation from inducing high-frequency computation in all other parts of the system which consume their results.
07:07:22 <Cale> For example, maybe you have some component which is operating at 44kHz, processing audio samples in real time
07:07:23 <agocorona> Cale perhaps because  s= v *  t     is a behaviour made with ordinary variables. It is a pure expression and yet it does not need "behaviours" to express it   
07:07:56 <Cale> and it's constantly computing an FFT
07:08:09 <merijn> agocorona: "it is a pure expression and yet it does not need "behaviours" <- I don't see why the "yet" is in there
07:08:14 * hackagebot progress-reporting 1.0.0 - Functionality for reporting function progress.  https://hackage.haskell.org/package/progress-reporting-1.0.0 (JeroenBransen)
07:08:16 * hackagebot hxt-tagsoup 9.1.4 - TagSoup parser for HXT  https://hackage.haskell.org/package/hxt-tagsoup-9.1.4 (UweSchmidt)
07:08:40 <merijn> You seem to imply that FRP requires everything to be behaviours, which is like saying Haskell requires everything to be IO...
07:08:46 <Cale> Maybe you later want to display that FFT on the screen at 60Hz. You don't want to accidentally be doing 44kHz processing in the graphics component of your system.
07:09:21 <Cale> So in FRP, you would expose that FFT as a Behaviour, which would mean that any other part of the system can observe it, but only at moments in time when an event in scope is firing.
07:09:52 <Cale> and since those other components might not have access to a 44kHz event, you prevent them from being able to drink from the fire hose, as it were
07:09:54 <ski> ertes : for an additive vategory `C', hom classes are abelian groups, iow `Hom_C : C^op * C >---> Ab', with `id_A : |Z >---> Hom_C(A,A)' and `(.) : Hom_C(A1,A2) (*) Hom_C(A0,A1) >---> Hom_C(A0,A2)' being abelian group morphisms, with `(*)' being tensor of abelian groups, iirc
07:11:12 <Cale> Events are push, Behaviours are pull -- it's really not so much about discrete vs. continuous once you get past the initial mental model. When you're engineering complex systems, the distinction matters even if time is really discrete.
07:11:37 <agocorona> Cale: the problem is that 90% of the people want to know how to connect his button with websockets and his database with it
07:11:39 <ski> ertes : iow, `(.)' is bilinear and `id_A' is "nullinear" (maps `0' to `zero_A : A >---> A' where `zero_A a = 0')
07:11:48 <merijn> Cale: Which is why I think the Push-Pull FRP paper is the most readable one :)
07:12:17 <Cale> agocorona: You face all the same problems there too -- the frequency at which it's acceptable to poll the database is very different from the frequency at which you can observe the contents of a text field
07:14:42 <ertes> ski: hmm…  ok, i think i need to study this first before using it…  i'll go with the traditional definition in terms of monoids/groups for now
07:14:50 <ertes> thanks!
07:15:08 <agocorona> Cale: have you heard anyone asking "Hey I have a great problem: the frequency of polling of my database is different of the frequency of my text field"
07:16:09 <MarLinn> agororona: they usually don't call such problems "great" but "f***k"
07:16:42 <bartavelle> freak?
07:16:46 <MarLinn> sry, typed you name wrong, agocorona
07:17:08 <agocorona> MarLinn: don't worry
07:17:13 <Forlorn> whoaaw, introducing reading file to read my long digit string just got complicated
07:17:17 <ertes> MarLinn: you can probably tab-complete nicks
07:17:19 <Forlorn> at least for a beginner like me
07:17:43 <Insanity_> Forlorn: chek pm
07:17:45 <Insanity_> check*
07:18:38 <jonored> I think the other reason you don't hear that much is that you only notice if it's broken or you're working with something that actually expresses intent explicitly, rather than encoding it into some implementation.
07:19:57 <merijn> agocorona: Usually you hear this question instead: "Holy shit! My database is crashing from the insane load of queries, how do I put a cache in front of it?!?"
07:20:16 <merijn> agocorona: That doesn't mean the underlying problem isn't "my text fields updates a lot more than my database"
07:20:25 <MarLinn> ertes, how do you know how my client works? ;) thx
07:21:11 <merijn> MarLinn: Because I don't know a single IRC client that doesn't have tab-completion :p
07:22:15 <agocorona> merijn: yep but that demands its own particular solution
07:22:52 <jonored> merijn: ii doesn't :)
07:24:53 <merijn> agocorona: Does it, though? Why couldn't you find an abstraction that solves every similar problem?
07:32:16 <agocorona> sure there is one. but probably, not behaviours
07:36:43 <pushkinma> .
07:37:15 <shapr> ,
07:37:19 <shapr> really big semicolons!
07:37:36 <ongy> ; and normal ones
07:38:09 <pushkinma> it was just for test:)
07:44:42 <lingxiao> hey all
07:44:56 <lingxiao> anyone familiar with attoparsec that knows a combinator that allows me to express optonal parsr
07:45:02 <lingxiao> so opt p make p optional
07:51:10 <ClaudiusMaximus> lingxiao: maybe something like   option Nothing (Just <$> p)   but see the docs about not consuming input (maybe this is copied from parsec docs without thinking? earlier docs for try say attoparsec always backtracks.. but not used it myself..) http://hackage.haskell.org/package/attoparsec-0.13.1.0/docs/Data-Attoparsec-Combinator.html#v:option
07:52:12 <infandum> I have an infinite list, xs :: [IO a, IO a, ...]. Is it possible to filter this list based on "a" and take a few of them? Whenever I try I hit a brick wall due to the strictness of bind in the IO monad.
07:52:43 <infandum> I suppose it's possible with streaming, but I wanted to know if there are built in utilities before I went to that extreme
07:52:57 <daey> why does this http://pastebin.com/qXNwqCTR not compile? im getting "Variable not in scope: (&) :: (String -> IO ()) -> String -> IO ()"
07:54:30 <jonored> infandum: I think you're interpreting the IO a wrong; IO a is "an action that produces an a" - until you actually do the action you don't have an a to look at.
07:54:49 <infandum> daey: Do you need parentheses? Like, writeFile (string & ioStuff)
07:54:59 <infandum> maybe not
07:55:11 <bjs> daey: did you mean $
07:55:18 <daey> omg T_T
07:55:31 <infandum> oh, I guess that makes more sense
07:55:33 <infandum> haha
07:55:55 <daey> thanks...
07:56:30 <infandum> jonored: Sure, but that action can be executed incrementally, right?
07:57:06 <infandum> jonored: The list is lazy, so I can do stuff like taking 10 BEFORE filtering
07:57:13 <infandum> although that does not inspect "a"
07:57:34 <infandum> Why can't I have that filter be lazy as well?
07:57:45 <chpatrick> hez
07:57:50 <chpatrick> *hey
07:58:09 <chpatrick> is it possible to "touchForeignPtr" a big structure containing foreign ptrs without actually touching each ptr individually?
07:58:19 <jonored> infandum: If you had some way of filtering the IO actions themselves, you could do that, but they aren't really inspectable, and certainly not for getting an a out.
07:58:25 <chpatrick> ie. I want to tell the GC that I want to keep this whole structure alive at that point
07:58:43 <chpatrick> maybe with touch#? 
07:59:02 <infandum> jonored: True, so is there no hope of filtering an infinite list of IO monads?
07:59:29 <jonored> infandum: What are you trying to accomplish?
07:59:34 <infandum> jonored: May main problem is that I need 10 of something, but a value can't be included in that 10.
07:59:51 <infandum> jonored: And those IOs are random values
08:00:03 <infandum> so I need 10 random values that aren't 1, for instance
08:00:17 <infandum> but range between 1 and 10
08:00:22 <infandum> or 0 and 10 or whatever
08:00:30 <infandum> I can't cut off the number as it could be within the range
08:00:34 <chpatrick> infandum: you want to filter the result of the IO action, not the IO actions themselves
08:00:43 <nshepperd_> infandum: Monad m => Int -> (a -> Bool) -> [m a] -> m [a]?
08:00:57 <chpatrick> :t take 10 . filter (/= 1) . randoms
08:01:00 <lambdabot> (Random a, RandomGen g, Num a, Eq a) => g -> [a]
08:01:09 <nshepperd_> infandum: I think there's a simple direct recursion solution for this
08:01:41 <chpatrick> :t (take 10 . filter (/= 1) . randoms) <$> getStdGen
08:01:43 <lambdabot> (Random a, Num a, Eq a) => IO [a]
08:01:50 <chpatrick> > (take 10 . filter (/= 1) . randoms) <$> getStdGen
08:01:54 <lambdabot>  <IO [Integer]>
08:01:58 <chpatrick> eh :P
08:02:17 <infandum> oh
08:02:19 <infandum> uh
08:02:21 <infandum> lemme see
08:02:29 <infandum> I'm using MWC but it's probably the same
08:03:46 <infandum> chpatrick yeah that worked
08:03:55 <infandum> now I need to figure out why that was fine
08:04:28 <fProgrammer> quick question: Is there any library which is equivalent to PGMPY(https://github.com/pgmpy/pgmpy) for probablistic graphical models in haskell?
08:05:00 <jonored> infandum: It's because it's operating on a single action that produces a lazy, infinite list of random numbers, rather than a lazy, infinite list of IO actions producing a single random number.
08:05:09 <infandum> I feel like I was doing something similar, but I was repeating the whole IO monad rather than fmapping in
08:05:37 <infandum> I guess I figured that the IO would have to be different for each number?
08:05:52 <chpatrick> infandum: what do you mean?
08:06:13 <chpatrick> the only IO is initializing the random number generator from the clock
08:06:16 <fProgrammer> anyone? 
08:06:59 <infandum> chpatrick: Yeah, so the IO monad is solely for getting a single seed, you don't need many of them right
08:07:13 <chpatrick> infandum: yes once you have the seed generating the next value is pure
08:07:17 <infandum> so each new number you extract doesn't need separate IOs
08:07:19 <infandum> I see
08:07:51 <infandum> Are they equivalent distributions? Is IO [a] the same as [IO a] because of the IO monad behavior?
08:07:55 <nshepperd_> takeFilterM _ _ [] = return []; takeFilterM 0 _ _ = return []; takeFilterM n p (m:ms) = do { x <- m; if p x then (x:) <$> takeFilterM (n-1) p ms else takeFilterM n p ms }
08:08:02 <infandum> or would the latter create different seeds?
08:08:16 * hackagebot threepenny-gui 0.7.0.0 - GUI framework that uses the web browser as a display.  https://hackage.haskell.org/package/threepenny-gui-0.7.0.0 (HeinrichApfelmus)
08:08:23 <nshepperd_> :t randoms
08:08:26 <lambdabot> (Random a, RandomGen g) => g -> [a]
08:08:47 <nshepperd_> randoms is special since actually creating the infinite list of random numbers is pure there
08:09:18 <infandum> nshepperd_: Unfortunately I needed a standard gaussian distribution so I was using MWC standard generators
08:09:25 <infandum> so I needed to use repeat
08:11:49 <infandum> WAIT! I'm not crazy!
08:12:12 <infandum> chpatrick: I just realized it did not work -- the numbers in the list are identical, so there IS a difference
08:12:17 <infandum> at least, when I use repeat
08:12:41 <chpatrick> infandum: randoms doesn't actually update the PRNG
08:12:50 <chpatrick> so every time you call it it'll start with the same seed
08:13:47 <infandum> chpatrick: Using (take 10 . filter (/= 1) . repeat) <$> standard r, where r is r <- createSystemRandom
08:13:57 <infandum> using the MWC library
08:14:11 <infandum> it repeats the same number, so each IO action must be separated I assume
08:14:22 <infandum> so IO [a] is not [IO a] in terms of the numbers
08:14:22 <chpatrick> infandum: well yeah, you only get one value out of it right?
08:14:27 <chpatrick> then you repeat that
08:14:43 <infandum> chpatrick: That's why I was confused why it "worked", when I didn't actually look that hard
08:14:45 <infandum> :(
08:14:56 <chpatrick> MWC isn't pure so generating every number is an IO action
08:15:00 <chpatrick> but System.Random is
08:15:32 <infandum> but System.Random doesn't have distributions other than uniform
08:15:38 <infandum> unfortunately
08:17:08 <infandum> With System.Random the analogy would be the same, but it would be randomRIO instead of getStdGen
08:17:29 <infandum> in which case I fall into the same issue, but let me check if there is an alternative which there probably is
08:18:00 <codedmart> Looking for some help on this error: https://gist.github.com/codedmart/160728d491a87228e863eb335c774427#file-error
08:19:24 <infandum> chpatrick: Ok yes, using getStdGen and randomRs does ALMOST do what I want, as the list contains different numbers, but the list is the same each time
08:19:39 <infandum> for obvious reasons
08:20:09 <infandum> aaaaand with newStdGen it's all good. Let me see if I can use the gaussian with htis
08:20:35 <chpatrick> infandum: if you want to use MWC the best thing to do is to write a recursive function
08:21:02 <nshepperd_> I prefer the random-fu library
08:21:22 <chpatrick> genN 0 = []; genN n = do x <- standard r; if x == 1 then genN n else do xs <- genN (n - 1); return (x : xs)
08:21:23 <infandum> chpatrick: Probably, I just didn't want to rewrite the whell
08:21:26 <infandum> wheel
08:21:29 <infandum> reinvent it
08:21:31 <infandum> whatever
08:21:53 <nshepperd_> but another option is to use untilJust from monad-loops
08:22:38 <nshepperd_> replicateM 10 (untilJust (\x -> if x > 1 then Just x else Nothing) (standard r))
08:22:51 <nshepperd_> will give you 10 numbers more than 1
08:23:33 <infandum> yeah
08:23:43 <infandum> so many solutions to choose from! :D
08:24:50 <nshepperd_> untilJust :: (a -> Maybe b) -> m a -> m b
08:25:05 <nshepperd_> retries the action until it returns something valid
08:27:38 <infandum> nshepperd_: Yeah, that's a good technique to use with MWC
08:28:11 <lgstate> is there anything in haskell land that matches the awesomeness of http://www.parens-of-the-dead.com/ ?
08:28:18 <lgstate> it makes all other dev environments look primitative
08:29:41 <infandum> lgstate: What part of it? It's emacs, and the author apparently did emacs rocks as well
08:29:59 <lgstate> the refactoring, auto adding dependencies, things reloading in the repl
08:30:02 <lgstate> it feels like the matrix
08:30:07 <lgstate> have you watched him code?
08:30:23 <infandum> I have not
08:30:50 <lgstate> it's amazing
08:30:54 <lgstate> I've never seen anyone code like that
08:30:58 <lgstate> and I have been coding for 15+ years
08:30:58 <systemfault> lgstate: These clojure tutorials look great
08:31:14 <lgstate> it's his emacs setup that I'm jealous of
08:32:32 <systemfault> lgstate: https://github.com/magnars/.emacs.d ?
08:33:14 <lgstate> systemfault: yeah, I know where it's located
08:33:21 <lgstate> but git cloning it doesn't magially give it to me for haskell
08:33:29 <lgstate> I want haskell -- but with all the cool emacs setup he has
08:34:19 <systemfault> He does Haskell?
08:34:50 <systemfault> Anyway, I mostly use vim but from I've recently read, "Intero" for emacs is great
08:34:50 <lgstate> lol no
08:35:04 <lgstate> He does emacs+clojure
08:35:04 <lgstate> I want a similarly awesome setup fo rhaskell.
08:35:32 <systemfault> lgstate: That thing: http://commercialhaskell.github.io/intero/
08:37:36 <infandum> lgstate: If he's zipping around the AST and you want that, check out haskell-structured-mode
08:37:50 <infandum> although it doesn't work in spacemacs, which makes me pretty jealous
08:38:00 <lgstate> so I think there's another of small things which I don't know how ot do in haskell
08:38:17 <nitrix> "Cannot instantiate unification variable `a0` with a type involving foralls: ...  GHC doesn't yet support impredicate polymorphism"
08:38:18 <nitrix> Aww :(
08:38:28 <lgstate> (1) dependency adding: C-c r a p // adds a dependency; it talks to cider (the repl), gets a list of all dependencies + their versions, and it narrrows it for you
08:38:32 <nitrix> I broke GHC :(
08:38:42 <lgstate> this is very convenient vs looking @ hackage, then updting stack.yaml, then restarting ghci
08:39:00 <nitrix> *impredicative
08:39:00 <lgstate> (2) adding :require/:use (equiv to haskell's use of import) -- again, it's not a big deal, but it's just very convenient
08:39:16 <lgstate> (3) refactoring: taking a block of code, and moving it to another *.clj file (vs another *.hs) file -- it's like 3 keystrokes
08:39:30 <lgstate> (4) use of yasnippet -- (maybe this is already possible in ahskell) for standard boiler plate
08:39:53 <lgstate> (5) code auto-reindents on every keystroke -- it just feels so activice/nice to get the feedback
08:40:01 <lgstate> again, all these are minor things, but put together, it's magical
08:40:07 <lgstate> and I'd say *fun*
08:46:59 <dememorized> s
08:47:05 <dememorized> :/
08:47:23 <dememorized> tmux is acting weird, sorry
08:51:04 <cocreature> nitrix: sadly breaking impredicativetypes is easy, one workaround is to throw a newtype on the type that has the forall in it
08:57:48 <MarLinn> lgstate: eclipse has most of these are things. I haven't tried the eclipsefp plugin in a long time, but maybe give it a look and see how what it has to offer?
08:58:11 <lgstate> eclipse + haskell ?
08:58:15 <lgstate> that sounds disguisting :-)
08:58:25 <cocreature> eclipse has these features for java, not for haskell :)
08:58:59 <lgstate> why would I want to code in java?
08:59:37 <MarLinn> lgstate: some of the things should be easy to port to Haskell, so it might be someone has done it already
09:01:27 <MarLinn> leksah also does a lot, including some things with imports and stuff I think... haven't looked at it too closely either
09:02:25 <lgstate> it's probably for the good of humanity such a tool deos not exist
09:02:36 <lgstate> if someone could write haskell with the clojure-tooling support, they'd be a step closer to creaitng skynet
09:02:46 <lgstate> it's only by spreading the power among different programming langauges that we maintain balance
09:07:13 <MarLinn> heh. I think a better tool for Haskell would be one that helps think about the underlying structures. Once you understand that X is a braided category of hilbert spaces and Y is an unfolding of a monoid through a profunctor all your code shrivels down to three library calls anyway
09:07:50 <ricky_clarkson> S, K and I?
09:08:17 * hackagebot dynamic-plot 0.1.4.0 - Interactive diagram windows  https://hackage.haskell.org/package/dynamic-plot-0.1.4.0 (leftaroundabout)
09:08:19 * hackagebot irc-core 2.2.0.0 - IRC core library for glirc  https://hackage.haskell.org/package/irc-core-2.2.0.0 (EricMertens)
09:08:22 * hackagebot wai-make-assets 0.2 - Compiling and serving assets  https://hackage.haskell.org/package/wai-make-assets-0.2 (SoenkeHahn)
09:08:30 <MarLinn> ricky_clarkson: Exactly! XDD
09:09:50 <mniip> MarLinn, yeah!
09:09:58 <mniip> or when you rewrite parsec in 2 lines of code
09:19:04 <clueless> can someone help me explain the logic of why this isn't functional? http://lpaste.net/195363
09:22:50 <glguy> clueless: First step will be to get rid of the tabs or fix your editor to set tabstops at 8
09:27:14 <lpaste> glguy annotated “No title” with “No title (annotation)” at http://lpaste.net/195363#a198649
09:28:25 <clueless> glguy: woooow, im new to lpaste too, did u just editted my code?
09:28:44 <glguy> clueless: You've got your function arrows backward, int isn't Int, you need Ord to compare elements, you need Show to show them, you can't add strings together with +, you can't return both a String and an Int from the same function, (You can raise an error or return an Either String Int), where clauses look like what I wrote, the expression to return goes before the where clause
09:29:21 <glguy> You don't indicate function application with ()s, so f(x) is just f x, 
09:29:45 <glguy> length(arr-1) tries to subtract 1 from arr, arr isn't an array, it's a list!
09:30:21 <glguy> Be warned that indexing a list with !! requires searching from the beginning of the list to find the requested element
09:31:49 <glguy> Oh, and to use div in an infix position you need to wrap it in backticks (`)
09:32:41 <clueless> glguy: thanks a lot, I see what you mean, do you mind explaining me a little bit more about the type declaration?
09:33:02 <clueless> glguy: whats a show type or how does that work?>  bin_search :: (Show a, Ord a) => [a] -> a -> Int
09:33:30 <glguy> (Show a, Ord a) => [a] -> a -> Int; Show a and Ord a are constraints, they indicate that this definition will only work for types 'a' that are both "showable" and "ordered"
09:33:48 <glguy> We need show for the error case and we get (<) from being ordered
09:34:22 <glguy> [a] -> ... ; function that takes a list of values each having type 'a'
09:34:36 <glguy> a -> .... function that takes a value of type 'a'
09:35:00 <glguy> and the final Int is because we return the index as the ultimate result which is the list index; Int being the type of machine integers
09:39:54 <joe9> http://dpaste.com/3GHAX9D is my haskell program. Line 84 to Line 91 processing is taking 13 seconds. The time is taken while writing to a file at line 105. I am measuring the time time taken from Line 84 to Line 90
09:40:05 <joe9> I suspect that it is haskell's lazy evaluation in play.
09:40:16 <joe9> How do I debug/figure out what is causing the slowdown?
09:41:45 <c_wraith> laziness shouldn't be a factor there. 
09:42:36 <c_wraith> everything seems to be an IO action that forces all of its arguments 
09:44:06 <cocreature> joe9: what’s the length of ps?
09:44:46 <joe9> cocreature: should be just 3
09:45:09 <c_wraith> joe9, I suspect the problem is that Show is generally slow.
09:45:22 <joe9> https://github.com/joe9/baby-steps-to-building-a-realtime-chart-with-haskell/blob/master/app/Step026OpenGL.hs is my main program. the line 77 is the call to drawPictures
09:45:44 <joe9> https://github.com/joe9/baby-steps-to-building-a-realtime-chart-with-haskell/blob/master/app/ChartOpenGL.hs is where the chart function is defined.
09:46:23 <joe9> https://github.com/joe9/baby-steps-to-building-a-realtime-chart-with-haskell/blob/master/app/PriceGraphOpenGL.hs is where I am defining pChart
09:46:25 <c_wraith> joe9, oh, I see. ps might not be evaluated 
09:47:20 <joe9> c_wraith: oh, ok. Let me check on google on how to strictify ps evaluation.
09:47:54 <joe9> c_wraith: cocreature: instead of assuming that the issue is lazy evaluation, is there a way to profile it to see where the issue is?
09:48:41 <nmdanny> question: if I have a certain datatype which I want to be ordered and compared equal by a certain field (e.g User by userName), do I need to manually implement both Eq and Ord or is just Ord enough?
09:50:16 <pavonia> nmdanny: You need Eq and Ord as the default instances are different from what you want
09:50:52 <nmdanny> also, if the following would hold, would that break the eq/ord laws(if there are such):  val1 /= val2 && val1 `compare` val2 == EQ ?
09:50:53 <c_wraith> joe9, wow, there's a huge chain of unevaluated data being passed in. and yeah, all that evaluation is being charged to the cost of writing the file. 
09:51:11 <cocreature> joe9: you could try runtime profiling. you probably need to put a cost center in manually
09:52:13 <cocreature> but c_wraith seems to have pinned down your problem
09:52:38 <cocreature> joe9: leaving that aside, that seems like a cool project. is it already in a state where you can play around with it?
09:53:26 <ejbs> So I made a datatype tree as such: data Tree a = Leaf a | Node (Tree a) (Tree a)  . Can I not have a list containing both (Leaf a) and (Node (Leaf a) (Leaf a))|
09:53:28 <ejbs> ?
09:53:52 <cocreature> ejbs: sure you can. that would have type [Tree a]
09:54:21 <ejbs> cocreature: Alright, great! Then it's just something wrong with my code :)
09:54:44 <joe9> cocreature, I rendered with OpenGL. but, the fact that it is taking 13 seconds to render 10,000 points makes it equal in performance to cairo or gloss.
09:54:46 <cocreature> ejbs: if you poste it on lpaste.net (or some other pastebin site) you can probably get someone here to take a look at it :)
09:54:54 <joe9> cocreature:  That is where I am stuck now.
09:55:14 <joe9> c_wraith: would you recommend !ps with the language pragma of BangPatterns?
09:55:29 <c_wraith> joe9, wouldn't help. 
09:55:32 <ejbs> cocreature: Thanks for the advice, I'll try that if I get stuck for much longer
09:55:36 <cocreature> joe9: btw it seems like you could use nanovg to simplify the drawing. it’s basically a thin layer over opengl to draw vector graphics with a pretty nice api
09:55:49 <c_wraith> joe9, that only force the top-level constructor. 
09:56:10 <c_wraith> joe9, and lists are recursive data structures 
09:57:19 <c_wraith> joe9, I'd change my data representation to unboxed vectors, and then make sure they were evaluated before returning them from wherever they are generated. 
09:59:22 <joe9> c_wraith: ok, Thanks. will check out nanovg. most opengl implementations use the OpenGL<3.3 implemantations. They do one vertex at a time. I am using OpenGL>=3.3 modern version to put all the data in the buffer once and render with a single draw call.
09:59:59 <joe9> c_wraith: will change the ps to an unboxed vector and try. What do you think of using deepSeq?
10:00:14 <c_wraith> joe9, managing these things is all about creating evaluation dependencies in your data. "I know that if this is evaluated, that also will be" 
10:00:20 <ejbs> I get an infinite type error and I can't see why: http://lpaste.net/5355025964080300032
10:00:41 <ReinH> deepSeq is almost never appropriate
10:00:49 <c_wraith> joe9, deepSeq is best used as a debugging tool to find out where your growth is coming from 
10:01:23 <ReinH> @google space invariants Haskell
10:01:25 <lambdabot> http://apfelmus.nfshost.com/blog/2013/08/21-space-invariants.html
10:01:25 <lambdabot> Title: apfelmus - Reasoning about space leaks with space invariants
10:01:31 <c_wraith> joe9, attempting to solve problems with it usually leads to horrible performance issues as your data gets traversed many times unnecessarily 
10:01:57 <cocreature> ejbs: please paste some self-contained example or at least also paste the error message so we can see which part it is pointing to
10:02:22 <cocreature> ejbs: also instead of making type annotations in comments, just make a proper type declaration. that’s what type systems are for :)
10:02:55 <ejbs> cocreature: As in, inline them with :: ?
10:03:09 <ReinH> You don't need to convince us what the types are, you need to convince the compiler.
10:03:39 <cocreature> ejbs: not sure what you mean by inline, just let minp :: (Tree a, …)\n  minp = minimize f
10:04:20 <cocreature> adding more type annotations also tends to give you better error messages in most cases
10:04:21 <ejbs> cocreature: That's what I'd call inline them :P ReinH: I guess, I do actually get a different error now
10:05:19 <ejbs> What the code essentially does is find the pair from f which minimizes some function, construct them into a tree and then remove those two and insert the tree into f'
10:05:21 <ReinH> Convincing us won't make your program compile, but if you convince the compiler I guarantee you will also convince us.
10:05:27 <ejbs> (or is supposed to do)
10:05:47 <ReinH> Also we still haven't seen an error message
10:05:49 <ejbs> ReinH: I know! Sometimes I just can't see why the types are wrong
10:06:02 <joe9> c_wraith:  Thanks.
10:06:04 <monochrom> No no, I think ejbs is now explaining the intention so we can suggest how to code it right.
10:06:33 <ejbs> Haha, nah, it just helps to know the context (I think)
10:08:19 * hackagebot partial-order 0.1.2 - Provides typeclass suitable for types admitting a partial order  https://hackage.haskell.org/package/partial-order-0.1.2 (mtesseract)
10:08:53 <monochrom> actually "minp :: (Tree a, Tree a, Double)" is unlikely to help. Recall that it is part of a local definition.
10:08:55 <ejbs> Seems like there are other parts of my program which doesn't really work nicely, I'll work it all thru first
10:09:25 <monochrom> But I need either self-contained code or the error message to proceed.
10:09:49 <ejbs> monochrom: Yeah, I'm gonna work it through myself for a while
10:20:32 <ejbs>  Could not deduce (a ~ a2) what does that mean?
10:20:34 <ejbs> Not equal?
10:20:38 <ejbs> (the tilde)
10:20:46 <mauke> ~ means equal
10:21:38 <ejbs> Well, I noticed there were a couple of bugs, I solved those and now it works!
10:22:31 <joe9> c_wraith: quick question. can I leave ps to be [Picture]. but, change the Picture to   Picture (VU.Vector (V2 Double)) -- x and y vertices? Would it provide the same benefit?
10:22:51 <joe9> It is just that Picture is my own data type and does not have an instance for Vector.Unboxed.
10:24:00 <c_wraith> joe9, yeah, that works. 
10:24:35 <joe9> c_wraith, ok. Thanks.
10:27:11 <tippenei1> trying to build a project with friday-juicypixels and ffmpeg as dependencies.. not going well
10:27:39 <joe9> c_wraith: Would an unboxed vector be faster than a Strict IntMap?
10:28:15 <c_wraith> if the keys are dense in the 0 to N range, sure. 
10:28:32 <joe9> c_wraith: http://dpaste.com/14NFXEX I have another data structure that is the source of this Unboxed vector that I have represented as an IntMap.
10:28:51 <joe9> c_wraith: not sure, if I should change that or just leave it as an IntMap.
10:29:08 <c_wraith> joe9, is it being used as a histogram of some sort? 
10:29:47 <lgstate> I need a language with the tooling of clojure and semantics of haskell
10:29:51 <nmdanny> is it possible for emacs intero to infer types in a file that has errors?
10:29:58 <joe9> c_wraith: The keys are dense (I understand "dense" to mean that there is an item for every index starting from 1, and the keys are in order)
10:30:01 <cocreature> lgstate: write one
10:30:15 <cocreature> nmdanny: afaik no
10:30:18 <monochrom> simply change clojure's semantics.
10:30:21 <joe9> c_wraith: I am using the IntMap to represent the chart data. (bar chart and area chart)
10:30:31 <c_wraith> joe9, in this case, that is what I meant. 
10:30:38 <lgstate> monochrom: enforing purity is nontrivial
10:30:45 <ReinH> lgstate: or write tooling for Haskell
10:30:59 <lgstate> ReinH, monochrom: you two are brilliant :-)
10:31:05 <lgstate> is there a third option?
10:31:10 <ReinH> Want something else instead.
10:31:16 <joe9> c_wraith: just to confirm, make all data structures (list and IntMap) to an unboxed vector, correct?
10:31:28 <monochrom> Yes. Take C and change both its semantics and its tooling.
10:31:32 <c_wraith> joe9, there are functions for creating unboxed vectors efficiently by folding over some sort of input stream. those would likely help. 
10:31:33 <remake> hello! can anybody explain this anomaly to me?
10:31:38 <remake> *Main> sqrt (fromIntegral (10^33)) 3.1622776601683792e16 *Main> sqrt (fromIntegral 10^33) 3.1622776601683796e16
10:32:29 <c_wraith> joe9, in all honesty, those aren't automatic suggestions for improving things. you really do need to understand the tradeoffs you're making. 
10:32:38 <ski> remake : `fromIntegral 10^33' means `(fromIntegral 10)^33', which apparently, due to floating-point, is slightly different from `fromIntegral (10^33)'
10:32:41 <monochrom> > 3.1622776601683792e16 * 3.1622776601683792e16
10:32:43 <lambdabot>  1.0e33
10:32:55 <monochrom> looks correct to me. what anomaly?
10:33:09 <c_wraith> joe9, but it sounds like these are cases where those structures fit. 
10:33:56 <remake> monochrom, ..2e16 vs ..6e16
10:34:00 <remake> hmm
10:34:01 <monochrom> Oh I see.
10:34:16 <monochrom> sqrt (fromIntegral 10       ^   33)
10:35:00 <ReinH> Floating point operations are not associative
10:35:44 <ski> (in this case, `fromIntegral' doesn't commute with `(*)', i believe is the issue)
10:35:49 <remake> root = sqrt . fromIntegral
10:35:53 <remake> root (10^33)
10:35:58 <remake> gives the same false result :/
10:36:06 <ricky_clarkson> > (16777216 :: Float) + 1 + 1
10:36:08 <lambdabot>  1.6777216e7
10:36:15 <ricky_clarkson> > (16777216 :: Float) + (1 + 1)
10:36:17 <lambdabot>  1.6777218e7
10:36:20 <prsteele> > (1e100 + (-1e100)) + 1 == 1e100 + ((-1e100) + 1)
10:36:22 <lambdabot>  False
10:36:24 <ski> `let root = sqrt . fromIntegral in root (10^33)' is the same as `sqrt (fromIntegral (10^33))', yes
10:36:27 <ReinH> Floating point operations are not associative.
10:36:46 <ReinH> That is the explanation of this anomoly.
10:37:15 * ski doesn't see which different associations would apply here
10:37:33 <monochrom> (^) is done by software: a lot of (*)'s. If you use (x :: Double)^y, every (*) step incurs a little loss.
10:38:16 <c_wraith> perhaps Float and Double shouldn't be in the Prelude. they should be in a module named Numeric.ISwearIUnderstandFloatingPointIsUtterlyBizarre
10:38:20 <monochrom> Whereas (x :: Integer)^y gives you an exact thing before converting to Double.
10:38:23 <ski> > sqrt (10 ** 33)
10:38:26 <lambdabot>  3.1622776601683792e16
10:39:05 <ski> > sqrt (10 ^ 33)
10:39:06 <prsteele> just make everyone use Rational unless they opt out?
10:39:07 <lambdabot>  3.1622776601683796e16
10:39:37 <ski> prsteele : what about the trigonometric operations then, e.g. ?
10:39:41 <monochrom> Rational is a great idea. Until you need trig. But then there is also Rational Trig.
10:39:46 <cocreature> ReinH: thanks for linking that blogpost on space invariants earlier, that’s a neat way to tackle this
10:40:00 <ReinH> cocreature: yw
10:40:27 <joe9> c_wraith: In my situation, for the data structure holding the data for the chart, I only append and traverse the whole list from the start to the end (while computing the scaled points to render to a chart). I do not care for indexed access being fast. But, I do care that append and traversal are fast.
10:40:27 <c_wraith> cocreature, I really liked that article. it was the first good explanation that explained it the same way I thought about it. 
10:40:32 <prsteele> ski: it is not without it's downsides, I agree
10:40:37 <ski> cocreature : link ?
10:40:37 <monochrom> As for square roots, the students of Pythagoras assert that square roots are rational, too. :)
10:40:50 <cocreature> ski: http://apfelmus.nfshost.com/blog/2013/08/21-space-invariants.html
10:40:53 <ski> danke
10:41:12 <knupfer> Are there anywhere operators for manipulation of Symbol?
10:41:27 <joe9> c_wraith, this is the same for the chart points that are to be rendered. It is just a list and I traverse from the start to the end. The only difference from the other is that there is no append to this data. I just recreate the list for each chart rendering.
10:41:28 <cocreature> c_wraith: yeah it’s really good, hadn’t seen it written down so concise anywhere before
10:41:41 <remake> well, thanks for the explanation
10:42:58 <joe9> for thhe chart points to be rendered, creation time and traversal time matter. whereas, for the chart data, append time and traversal time matter.
10:43:15 <c_wraith> joe9, ultimately, it's not completely clear to me what the best solution is. I just know that all of that data wasn't evaluated between reading it in and writing it, and it probably would have benefited from it. 
10:43:21 <tippenein> Any recommendations of how to create a lazy stream of frames from a .mov file?
10:43:24 <gummibears> hi room
10:43:41 <tippenein> ffmpeg-light and juicy pixels or other options?
10:43:43 <dazz> hiii
10:43:45 <c_wraith> joe9, though if you have a lot of appends going on, you might need to get tracker to make them efficient 
10:43:58 <c_wraith> .. trickier 
10:44:40 <tippenein> I'd like to take every nth frame from a video stream
10:44:52 <knupfer> Or, how could I extract the head from a type level String?
10:45:04 <gummibears> can someone please help me understand why this implementation is so much faster than vanilla product [1..100000]?
10:45:06 <gummibears> binomial :: Int -> Int -> Integer
10:45:06 <gummibears> binomial n k = binomialTab!!n!!k
10:45:06 <gummibears> Here is an infinite table of binomial coefficients:
10:45:06 <gummibears> binomialTab :: [[Integer]]
10:45:06 <gummibears> binomialTab = [[ binomialCalc n k | k <- [0..n]] | n <- [0..]]
10:45:11 <gummibears> Sorry
10:45:14 <remake> sqrt (fromIntegral (10^33)) - 31622776601683793
10:45:17 <remake> floor (sqrt (fromIntegral (10^33)))
10:45:31 <remake> first one gives 0.0, second one gives 31622776601683792
10:45:35 <gummibears> I mean ths one 
10:45:36 <gummibears> binaryProduct :: [Integer] -> Integer
10:45:36 <gummibears> binaryProduct [] = 1
10:45:37 <gummibears> binaryProduct [x] = x
10:45:37 <gummibears> binaryProduct xs = binaryProduct (bip' xs)
10:45:37 <gummibears>      where
10:45:38 <gummibears>          bip' (x1:x2:xs) = x1*x2 : bip' xs
10:45:40 <gummibears>          bip' xs = xs
10:46:14 <prsteele> gummibears: try to use lpaste for examples longer than a line or so
10:46:29 <gummibears> Ok I'm sorry. I'll do it next time
10:46:52 <prsteele> gummibears: it's fine, but it helps us answer your question too. Easier to copy+paste, and it doesn't go away as we keep talking
10:47:49 <lpaste> gummibears pasted “binaryProduct” at http://lpaste.net/198744
10:47:56 <schell> has anyone ever had trouble generating overloaded field accessors in lens using “makeFields” ?
10:48:20 <schell> i’m looking at my file’s TH splices and makeFields simply makes no fields!
10:48:31 <joe9> c_wraith: is there a haskell material, comparing the performance (create, insert, append,  traversal) of different structures?
10:48:45 <gummibears> This implementation is magnitudes faster than a normal list multiplication. but I don't understand why so
10:49:32 <c_wraith> joe9, there's really nothing haskell-specific about the topic. you just need to be aware of immutability 
10:49:34 <lpaste> schell pasted “makeFields weirdness” at http://lpaste.net/198746
10:50:24 <prsteele> gummibears: I think it is because it is making fewer thunks. you're pattern matching two at a time, so it is able to realize it should make fewer lazy multiplications
10:50:31 <glguy> schell: makeLenses and makeFields have different naming conventions, you don't typically use them together
10:50:33 <prsteele> gummibears: it speeds up again if you pattern match against a third
10:50:53 <gummibears> let me try
10:51:12 <schell> glguy: yes, that makes sense now that i look - and i don’t think i’m using the lenses actually, just the fields
10:51:20 <schell> but it does work with another data type
10:51:42 <glguy> schell: It can work, but you have to follow the naming convention or make a new naming convention
10:51:53 <c_wraith> gummibears, are you compiling with optimizations? the Prelude product function is pretty slow without them. 
10:52:18 <lpaste> schell annotated “makeFields weirdness” with “makeFields weirdness (annotation)” at http://lpaste.net/198746#a198749
10:52:28 <ReinH> gummibears: how many multiplication operations does it perform? How many does the equivalent product perform?
10:52:37 <gummibears> c_wraith: No optimizations. This is actually for a class assignment. AFAIK, the prof will just be compiling with ghc -Wall
10:53:03 <prsteele> gummibears: looking into using BangPatterns and adding a strictness annotation on your accumulator
10:54:11 <schell> glguy: it seems that even removing makeLenses ‘’UiState - fields are still not generated
10:54:23 <schell> what’s the naming convention that i’m missing?
10:54:26 <glguy> schell: Yeah, you'll still have to follow the naming convention
10:54:37 <glguy> It's written up in the docs
10:54:39 <ReinH> gummibears: And by comparison, how many extra (:) constructors are created?
10:54:41 <schell> glguy: maybe i’m glossing over it (http://hackage.haskell.org/package/lens-4.14/docs/Control-Lens-TH.html#v:makeFields)
10:54:43 <gummibears> ReinH: I tested it with product[1..100000]. Vanilla product takes 11.98 seconds while binaryProduct takes 0.12 seconds
10:54:52 <schell> ohhhhh
10:54:53 <schell> duh
10:55:02 <schell> glguy: sheesh, i see
10:55:06 <schell> thanks :)
10:55:16 <ReinH> gummibears: Ok, but that doesn't answer any of my questions.
10:55:35 <gummibears> prsteele: I haven't come as far in haskell yet. Just started a few days back. Haven't come across bangpatterns
10:55:54 <c_wraith> gummibears, now compare to foldl' (*) 1
10:56:06 <gummibears> foldl' is slower too
10:56:32 <nitrix> @let x = (2+2, 42) :: (Int, Int)
10:56:33 <lambdabot>  Defined.
10:56:36 <c_wraith> that's shocking, since it does so much less work. 
10:56:36 <nitrix> @sprint x
10:56:36 <lambdabot> Unknown command, try @list
10:56:48 <schell> glguy: thanks - i fixed my bug and got rid of a bunch of unused lenses (thumbsup)
10:57:02 <YellowOnion> Anyone know of a nice solution to modifying files painless in Haskell?
10:57:11 <gummibears> foldl' takes 1.79 secs on my machine for the same
10:57:17 <prsteele> gummibears: they let you write functions like `f !x !y = ...` where the !s mean to evaluate the term (to some extent). This means expressions like a * b * c * d get compressed down to a single value
10:57:19 <ReinH> c_wraith: binaryProduct is a tree fold
10:57:27 <nitrix> Would adding sprint to lambdabot be interesting to demonstrate lazy evaluation?
10:57:50 <ReinH> gummibears: if you try to answer my questions, you might learn something interesting about binaryProduct
10:57:59 <c_wraith> ReinH, that should only matter if subproducts get big enough for multiplication time to dominate 
10:58:09 <joe9> c_wraith: found this: http://blog.malde.org/posts/frequency-counting.html . seems to recommend judy, followed closely by mutable vector and unboxed vector.
10:58:23 <gummibears> ReinH: I'm sorry but I do not know the answers to them yet. Perhaps if you point me in some direction, i'd be grateful
10:58:39 <gummibears> I'm just a newb trying to learn
10:58:44 <ReinH> gummibears: You aren't supposed to know the answers, you're supposed to figure them out.
10:58:48 <c_wraith> joe9, that's ancient, then. judy is only good for very limited cases. 
10:58:51 <YellowOnion> For example, I have a small file that has a sha1 header, and a single value with it I want to modify
10:58:59 <ReinH> Trace out the evaluation of both for some small input, draw things if it helps.
10:59:07 <nmdanny> how can I do the following : (MonadExcept e a) => Either e a -> m a ? I tried using the ExceptT constructor but it doesn't work
10:59:24 <ReinH> c_wraith: well, do you have another explanation? :)
10:59:42 <ReinH> I'm interested to see the behavior of all three with optimizations turned on. I guess I can try that now.
11:00:24 <c_wraith> ReinH, actually, huge factorials might be big enough for multiplication time to dominate 
11:00:27 <gummibears> ReinH: I understand that binaryProduct is recursively creating smaller lists by multiplying the first two elements which becomes the head of the new list
11:00:46 <ReinH> c_wraith: I mean, 100000! is a big number.
11:01:37 <ReinH> you're trading a log factor increase in space and time to evaluate (:) constructors for a log factor fewer multiplications
11:01:59 <ReinH> At some point, this becomes worthwhile. I wouldn't be surprised if 100000! is past that point.
11:02:01 <ReinH> But we can benchmark.
11:02:38 <ReinH> Also note that this is Integer math
11:02:40 <ReinH> not Int math
11:02:49 <ReinH> so multiplication is already an order of magnitude(?) slower
11:03:06 <ejbs> I'd like to install this with Cabal: https://hackage.haskell.org/package/random-shuffle-0.0.4/docs/System-Random-Shuffle.html but how do I do that?
11:03:08 <prsteele> ReinH: alright I think I'm missing something. Where are you saving on the number of multiplications?
11:03:29 <ReinH> prsteele: This is a tree fold. At each step, you perform half the multiplications. You generate a binary evaluation tree.
11:03:48 <ReinH> the first step divides the list into half, the next step divides that list into half, and so on
11:04:19 <ReinH> you are recursing on a list half the size of the previous list each time
11:04:22 <prsteele> ReinH: yes, but there are still O(n) multiplications. The first pass alone does half of them
11:05:42 <joe9> c_wraith: I am doing a lot of double to float conversions. I read that haskell has a performance bottleneck with that and recommended the use of -sse2 flag. Is that still relevant?
11:06:02 <ReinH> prsteele: are you sure?
11:06:04 <joe9> or, should I just change to use float's all over? c_wraith
11:06:10 <prsteele> ReinH: if there are 2^n leaf nodes (input values) there are 2^n - 1 interior nodes (multiplication actions)
11:07:42 <prsteele> ReinH: in his code just calling `bip' xs` on the original list of xs does O(length xs) multiplications
11:07:47 <gummibears> ReinH: i apologize in advance if this is a dumb question, but why do you say that we are recursing on a list half the size of the previous list each time?
11:08:01 <gummibears> to me it seems that the newer list is only one element smaller
11:08:17 <ReinH> gummibears: what is the size of the result of bip' compared to its argument?
11:08:52 <gummibears> it's a list one element lesser so [3,4,5,6] gives back [12,5,6]
11:09:11 <prsteele> gummibears: write bip' out as a top level function and run it.
11:09:59 <ReinH> gummibears: is it?
11:10:14 <ReinH> You are justifying your conclusion by using it as your premise.
11:11:26 <ReinH> prsteele: the geometric series converges on 1, but we don't execute an infinite number of steps
11:12:19 <prsteele> ReinH: I'm not sure how that's relevant. You're claiming that his code does O(log (length xs)) multiplications? I claim running `bip' xs` on the original input list does O(length xs) multiplications alone
11:12:48 <ReinH> I retract that claim.
11:13:44 <prsteele> I think this function might be more numerically stable, since it will tend to multiply numbers of similar magnitude (should we be doing this in Double land, and assuming the original numbers are of similar magnitude)
11:16:15 <ReinH> It will do fewer multiplications, but that can't be the whole story.
11:16:40 <ReinH> Since it will do ~95% as many multiplications
11:17:28 <ReinH> prsteele: Maybe there are some details of Integer multiplication that combine with that property?
11:18:16 <prsteele> ReinH: ...that is possible. The total number of *s is constant, but more are done on smaller numbers and fewer on larger... I'd believe that
11:18:16 <ReinH> (the log factor claim came from thinking about tree folds in a data-parallel setting, which obv isn't the case here)
11:18:44 <prsteele> I still think the biggest issue is strictness, but that can be answered by benchmarking
11:18:46 <ReinH> prsteele: Well, the most multiplications are done on numbers smalle enough to fit into a singleton list
11:19:02 <ReinH> so they should almost as fast as Int multiplication
11:19:08 <prsteele> ReinH: I agree.
11:19:12 <ReinH> and the fewest are done on the largest numbers, which take the longest
11:19:16 <ReinH> That actually makes sense.
11:19:31 <ReinH> and only one is done on the largest numbers
11:20:19 <ReinH> prsteele: iirc tree folds can have some benefits for non-strict evaluation, but I haven't been able to source that claim
11:22:09 <gummibears> ReinH: You were right. Writing it as a top level function made me understand
11:22:54 <ReinH> (this is btw the nice way to write a tree fold that doesn't require partitioning)
11:23:56 <ashishnegi> i want a queue to share between multiple threads.. should i use (MVar Data.Sequence) between threads ?
11:24:12 <ReinH> ashishnegi: No, use a channel.
11:24:20 <ReinH> @google parallel and concurrent programming in Haskell
11:24:22 <lambdabot> http://chimera.labs.oreilly.com/books/1230000000929
11:24:22 <lambdabot> Title: Parallel and Concurrent Programming in Haskell
11:24:33 <bfrog> I've used erlang previously pretty extensively, and am thinking maybe haskell would be good for some upcoming projects, it seems like haskell by its nature would help create low numbers of program errors, even when compared to other functional (but dynamically typed) languages
11:24:33 <ReinH> ashishnegi: but more importantly, read this free book by the primary author of the Haskell RTS.
11:24:40 <ashishnegi> ReinH: i want to know when data has stopped coming on the queue..
11:24:47 <dmj`> ashishnegi: a channel is like a linked list of MVars, so it feels like a queue
11:25:02 <bfrog> is that the general consensus?
11:25:04 <prsteele> ashishnegi: you could always have a queue of Maybes, and send a Nothing to terminate
11:25:04 <ReinH> ashishnegi: Ok. How do you know that?
11:25:17 <dmj`> bfrog: yes
11:25:17 <ReinH> bfrog: we feel that way, certainly
11:25:18 <ashishnegi> ReinH: dmj` so, when queue is empty i know that.. this is for my own web-crawler
11:25:41 <ReinH> You can know that the queue is empty. You can't know that it will never be full again without extra work.
11:25:49 <ReinH> ashishnegi: but seriously, please read that book
11:26:09 <ReinH> e.g., without coordination between all the writers to that queue
11:26:25 <ReinH> which is now a hard distributed systems problem
11:27:05 <ashishnegi> ReinH: that is a good point, but writes are fast and reads are slow.. so queue emptiness should be ok for me
11:27:22 <dmj`> ashishnegi: you'd have to poll it to find if it's empty, if the queue is empty, the thread reading it will block until the queue receives a new write
11:27:41 <ReinH> bfrog: I think you will especially enjoy this lovely presentation about Haskell given at the Erlang User Conference https://www.youtube.com/watch?v=uR_VzYxvbxg
11:27:51 <ReinH> ashishnegi: good!
11:28:00 <prsteele> re: the talk, it's a nice watch
11:28:06 <ReinH> it's nice to not have to solve hard problems.
11:28:34 <ReinH> prsteele: SPJ is ridiculously charismatic for a stuffy British professor type. :)
11:29:01 <prsteele> ReinH: the talk was also very accessible. I knew *nothing* about Core before watching
11:29:05 <ashishnegi> ReinH: dmj` ok.. i would surely read book ( i have hard copy ) and watch video .. but what is the answer to my question  ? (MVar Sequence) is ok.. OR something else ?
11:29:26 <ReinH> ashishnegi: the video was for bfrog, but feel free to watch it as well
11:29:57 <ashishnegi> dmj`: also i can check with Sequence whether it is empty or not without doing blocking read..
11:30:00 <ReinH> ashishnegi: The answer is contained in the book, but you want to look for a *channel*, like Control.Concurrent.Chan or TChan (with STM)
11:30:03 <bfrog> ReinH: thanks, I'll take a look
11:30:19 <ReinH> A channel is the abstraction that wraps up something like an MVar Sequence into a usable queue.
11:30:37 <ashishnegi> ReinH: Chan is blocking on Read.. i think i need to go with TChan..
11:31:58 <leshow> any way to join a tuple into a list? splitAt returns a tuple
11:32:06 <leshow> and i'd like to return a list of lists instead
11:32:19 <ReinH> leshow: what is the type of the function you want?
11:32:29 <ReinH> @hackage split -- does something in here work?
11:32:29 <lambdabot> http://hackage.haskell.org/package/split -- does something in here work?
11:32:32 <dmj`> ashishnegi: check out TQueue
11:32:34 <leshow> > splitAt length 5 "abcdefghijk"
11:32:37 <lambdabot>  error:
11:32:37 <lambdabot>      • Couldn't match expected type ‘[Char] -> t’
11:32:37 <lambdabot>                    with actual type ‘([a0], [a0])’
11:32:40 <leshow> oops
11:32:45 <leshow> > splitAt 5 "abcdefghijk"
11:32:47 <lambdabot>  ("abcde","fghijk")
11:33:03 <ReinH> Oh right, TQueue. +1
11:33:09 <monochrom> You need to read responses instead of too busy asking.
11:33:55 <leshow> why does splitAt return a tuple
11:34:08 <leshow> i guess i probaly want chunksOf
11:34:12 <ashishnegi> dmj`: thanks that looks good.. i was just readying about 2-list amortized queue implementation :) thanks ReinH for video.. :)
11:42:09 <heebo> hi please forgive me for asking this question again
11:42:28 <heebo> can someone tell me why the following doesnt compile, http://lpaste.net/197413
11:42:57 <heebo> i should be able to compose fmap apply a function to a monad nested within a functor no?
11:43:00 <glguy> heebo: What reason did the compiler come up with?
11:43:03 <geekosaur> you should include the full error messages from the attempt, not everyone has the same ghc verson you do
11:43:45 <heebo> will get the error message
11:43:58 <glguy> heebo: fmap doesn't give you access to the Either e a of an EitherT e m a
11:44:17 <glguy> fmap :: (a -> b) -> Either e m a -> Either e m b
11:44:52 <heebo> please forgive me, its now compiling.... so sorry please ignore
11:46:19 <Henson> ReinH: I've been thinking about that large list folding problem.  You guys thought the binary folding method would result in fewer multiplications.  If you consider a list that goes up by tens, where determining the number of multiplications and additions to compute the product is easy, then the linear fold and binary fold are roughly evenly matched.  For an 8 element list the linear is slightly
11:46:47 <ReinH> Henson: It results in slightly fewer multiplications, but that is not the main source of the performance increase.
11:46:52 <ReinH> You should look at our later discussion.
11:46:56 <Henson> ReinH: more, but a 16 element list it's slightly less.  I'm doing it on paper so I haven't tried any large lists.
11:46:59 * Henson scrolls back
11:48:21 <ReinH> Henson: the number of operations for a tree of depth n is the result of the first n elements of the geometric series (1/2 + 1/4 + 1/8 + ..)
11:48:37 <joe9> c_wraith: as an fyi, https://github.com/joe9/judytest unboxed-vector is close to the fastest. confirming your suggestion: http://bpaste.net/show/ae756ab636eb
11:48:42 <ReinH> or rather, the ratio between the number of operations and the size of the list
11:48:53 <joe9> c_wraith: I should probably add Vector.storable there just for kicks.
11:49:00 <ReinH> so for 100000 elements, it's ~0.95, which is not very important
11:49:41 <c_wraith> joe9, the Storable variants are really only useful if you need to convert them to a foreign array 
11:49:53 <ReinH> Henson: the crucial factor (I believe) is that a large number of small numbers are multiplied with other small numbers (which is relatively fast), while a small number of large numbers are multiplied with other large numbers (which is relatively slow)
11:50:05 <c_wraith> joe9, so unless I'm doing FFI stuff, I mostly ignore them. 
11:50:29 <ReinH> e.g., the first level of the tree fits in an Int so the Integer multiplicaton is as efficient as possible
11:50:49 <ReinH> (the multiplicands and the result of multiplication fit in an Int, that is)
11:51:47 <Henson> ReinH: ok.  I was trying to address that fact by testing it out on paper, and for a list increasing by tens the number of multiplications and additions to write it out long-hand aren't much different.  But I suppose if you consider that smaller numbers can be done in hardware somehow faster than the larger ones, I'd buy that.  But it would still be nice to test it out to see if that's true,
11:52:06 <ReinH> Henson: an Integer is basically a list of Ints
11:52:07 <heebo> geekosaur: please see the following lpaste , i added the error 
11:52:09 <Henson> ReinH: because on paper (at least with my contrived example), it doesn't seem like it is.
11:52:13 <ReinH> If the list is tiny, the multiplication is much faster
11:52:16 <heebo> http://lpaste.net/198820
11:52:28 <ReinH> Small Integers multiply much faster than large Integers
11:52:28 <joe9> c_wraith:  I am rendering the points using OpenGL and the storable vector. so, just want to see how that works while I am doing this.
11:52:33 <heebo> glguy: ive added the error to http://lpaste.net/198820
11:52:42 <ReinH> and the tree fold arranges the operations so that small Integers are always multiplied with other small Integers
11:53:14 <ReinH> and the magnitudes increase as you get higher in the tree, where half as many operations are performed at each level
11:53:24 <ReinH> so as the operations get slower, you do fewer operations
11:53:35 <c_wraith> joe9, I think you might be letting yourself get pulled far afield. simplify what problems you are solving *first*
11:53:56 <Henson> ReinH: ok, intuitively it totally makes sense.  This will give me something to fiddle with to understand it better.
11:53:58 <ReinH> with a normal fold, the accumulator increases in magnitude at each step and gets slower and slower
11:54:09 <ReinH> (and multiplication gets slower and slower)
11:54:12 <c_wraith> joe9, your first task should be getting an appropriately flat heap profile. 
11:54:34 <c_wraith> joe9, once you have that, you can experiment with better FFI integration. 
11:54:43 <iomonad> Hey ! What is wrong with this function ? I have a problem with a & [a] \\ -> https://0x0.st/Sxn.hs
11:54:46 <ReinH> Henson: basically there are big (negative) jumps in performance as the Integer increases in size by MAXINT
11:54:59 <ReinH> because now an extra multiplication must be performed and possibly extra carries and etc
11:56:07 <aarvar> heebo: you want the outer fmap to be mapEitherT
11:56:23 <prsteele> ReinH: do you agree that if we are operating in floating point that the binary fold does the same number of multiplications as Prelude's `product`?
11:56:40 <Henson> ReinH: that all makes sense.
11:56:42 <joe9> c_wraith: ok, Thanks. Quick question, Is there a way to check out haskell heap profile while I am doing these changes?
11:56:47 <aarvar> mapEitherT f (EitherT x) = EitherT (f x)
11:56:53 <joe9> c_wraith: on a different note, nanovg seems  closed source?
11:57:46 <heebo> aarvar: Thank you!
11:57:58 <c_wraith> joe9, yeah, there is lots of information available on heap profiles in ghc. this is a topic I frequently revisit through real-world haskell chapter for. (it's available online for free) 
11:58:11 <c_wraith> joe9, I don't know anything about nanovg
11:58:35 <joe9> c_wraith: Thanks will check it out.
11:59:30 <Henson> prsteele: I think it should.  You are right about the number of elements in a binary tree.
11:59:59 <ReinH> prsteele: No I don't, but I believe that it approaches the same number of multiplications as the depth of the tree increases, and that for most cases the difference is negligible.
12:01:13 <prsteele> ReinH: so you think there should be gains for small trees? What about multiplying 8 numbers? What does the algorithm do in this case? (Or some other small n)
12:02:17 <ReinH> prsteele: A complete binary tree has 2^{k+1} - 1 nodes, where k is the depth of the tree
12:02:40 <aarvar> iomonad: for starters, [x:xs] matches a list containing a single list
12:04:11 <ReinH> (if k starts at 0)
12:06:00 <prsteele> ReinH: sure. And in our example the bottom layer (containing 2^k nodes) are the input numbers to the function.
12:06:00 <ReinH> > 2^3 - 1
12:06:00 <iomonad> aarvar :: I see thanks
12:06:00 <lambdabot>  7
12:06:00 <ReinH> 7 is not 8
12:06:16 <prsteele> ReinH: yes, but if we call binaryProduct [1..8] the depth of the tree will be k=3.
12:06:29 <ReinH> Yes, so that's 2 + 1 starting at k = 0
12:06:42 <ReinH> levels 0, 1, 2
12:06:55 <prsteele> ReinH: one more. 8, 4, 2, 1
12:06:55 <ReinH> I should have just said 2^k - 1 starting at k=1 and avoided this confusion
12:07:02 <ReinH> > 2^4 - 1
12:07:05 <lambdabot>  15
12:07:12 <ReinH> So you're suggesting that a binary tree has almost double the number of elements?
12:07:28 <prsteele> ReinH: Double, less one. Yes. 2^k leafs, and 2^k - 1 interior nodes.
12:08:25 <ReinH> Right, and the interior nodes are the multiplications
12:08:25 <ReinH> So we're back to 7
12:08:25 <ReinH> It can't possibly be 8
12:08:25 <prsteele> ReinH: In this example there are 8 leaves, and then there are layers of 4, 2, 1. so 7 interior nodes. I agree
12:08:25 <ReinH> There's no complete binary tree with 8 nodes
12:08:25 <prsteele> ReinH: 8 _leaf_ nodes
12:08:38 <prsteele> ReinH: each interior node represents one multiplication, so in this case there are 7 multiplications. The same as product
12:08:39 <aarvar> iomonad: are you trying to write the append function?
12:08:42 <ReinH> Either leaf nodes are numbers, in which case there are 8 but we don't count them
12:08:49 <ReinH> or leaf nodes are operations, in which case there are 4
12:08:50 <iomonad> aarvar :: yes
12:08:56 <aarvar> why do you need a Num constraint?
12:09:01 <Henson> if you draw it out on paper for some small trees, the linear fold and binary fold methods involve the same number of multiplications.
12:09:03 <ReinH> There is no complete binary tree with 8 nodes
12:09:16 <ReinH> The multiplications are the interior nodes (if the leaf nodes are the elements), not the leaf nodes themselves
12:09:25 <prsteele> ReinH: I never claimed there is. I said 'make the tree for an ipnut list of size 8', which will be a complete tree of 15 nodes.
12:09:36 <ReinH> And only 7 of those will be operations
12:09:41 <ReinH> because the leaf nodes are not operations
12:09:44 <iomonad> aarvar :: It can be avoid ?
12:09:54 <prsteele> ReinH: Yes. and `product [1..8]` involves 7 multiplications as well.
12:10:12 <ReinH> God damn it. *facepalm*
12:10:18 <aarvar> but your basic mistake is that [a] doesn't match any list a, it matches a list containing a single element a
12:10:23 <ReinH> Ok, moving swiftly on then
12:10:28 <aarvar> iomonad: sure, there's no need for a to be a Num
12:10:47 <aarvar> :t (++)
12:10:48 <lambdabot> [a] -> [a] -> [a]
12:11:00 <ReinH> prsteele: Let's ignore that and just focus on the thing I said that was correct
12:11:13 <iomonad> of course
12:11:22 <prsteele> ReinH: sure :) It was a good insight, I was stuck thinking in floating point land
12:11:49 <ReinH> So you do get a log factor fewer multiplications, but for an entirely different reason :D
12:12:04 <ReinH> And it only works with representations like Integer
12:12:35 <ReinH> where the time it takes to multiply two numbers increases with magnitude
12:12:48 <ReinH> (or the number of multiplications, if you look at the underlying impl)
12:13:28 <ReinH> Wait, how does the number of underlying multiplications increase? Exponentially, right?
12:13:50 <ReinH> (if both numbers have the same magnitude, i.e., length of underlying representation)
12:14:06 <dmwit> Depends a lot on the implementation.
12:14:35 <dmwit> Textbook multiplication costs mn multiplications (where the first integer is m digits long and the second is n digits long), but GMP uses a much fancier algorithm.
12:15:58 <dmwit> Karatsuba is a common alternate choice; it costs roughly m^0.75n^0.75 multiplications.
12:16:06 <iomonad> aarvar :: thanks a lot
12:16:13 <ReinH> dmwit: What does GHC use?
12:16:26 <ReinH> (Assuming default integer math)
12:16:45 <dmwit> GHC uses GMP. And GMP uses a very fancy algorithm that first looks at the sizes of the multiplicands, then chooses from a large portfolio of algorithms using that information.
12:16:47 <ReinH> It uses GMP right? It hasn't been replaced yet?
12:17:15 <geekosaur> yes
12:17:48 <ReinH> prsteele: I think I might use this as an interview question, once I fully understand it.
12:18:08 <ashishnegi> what should i use to do same side effecting thing n times ?.. i am using `forM_ (replicate n 0) $ \_ -> do ... ` 
12:18:21 <dmwit> :t replicateM_
12:18:23 <lambdabot> Applicative m => Int -> m a -> m ()
12:18:47 <ashishnegi> uh.. nice.. need to learn to search by types.
12:18:50 <dmwit> :t \n f -> forM_ (replicate n 0) $ \_ -> f
12:18:51 <lambdabot> Monad m => Int -> m b -> m ()
12:19:13 <dmwit> ?. hoogle ty \n f -> forM_ (replicate n 0) $ \_ -> f
12:19:14 <lambdabot> Plugin `compose' failed with: user error (Unknown command: "ty")
12:19:17 <dmwit> ?. hoogle type \n f -> forM_ (replicate n 0) $ \_ -> f
12:19:21 <lambdabot> Control.Concatenative apM_ :: Monad m => Int -> m a -> m ()
12:19:22 <lambdabot> Data.Vector indexM :: Monad m => Vector a -> Int -> m a
12:19:22 <lambdabot> Data.Vector unsafeIndexM :: Monad m => Vector a -> Int -> m a
12:19:28 <dmwit> ew
12:19:32 <dmwit> Those results are terrible.
12:21:53 <ashishnegi> lol
12:21:54 <joe9> I am working deep in a callback function. Instead of signalling the caller, is there a way to exit out of the program? I am testing a quick fix and I want the program to stop after executing a step? Should I use error for that?
12:23:41 <ReinH> Or, depending on context, you can throw some exception.
12:23:54 <ReinH> If it's just for debugging purposes, I don't see a problem with error.
12:24:11 <joe9> ReinH: Is exitSuccess an exception too?
12:24:18 <ReinH> (I suppose I should say some *other* exception)
12:24:30 <amalloy> :t exitSuccess
12:24:31 <lambdabot> error: Variable not in scope: exitSuccess
12:24:56 <ReinH> joe9: you tell me http://hackage.haskell.org/package/base-4.9.0.0/docs/src/System.Exit.html#exitSuccess
12:25:57 <joe9> ReinH: cool, Thanks. sorry for the bother.
12:26:03 <ReinH> np
12:26:10 <ReinH> Just teaching to fish and all
12:27:27 <joe9> ReinH: makes perfect sense. Thanks.
12:27:40 <joe9> http://bpaste.net/show/8382b0885dce c_wraith : this is my current profile without any changes.
12:42:29 <ydl> when i output core with ghc -c -O3 -ddump-simpl test.hs, with test.hs calling zipWith, zipWith doesn't get expanded to anything, i just see "zipWith @Type1 @Type2 @TypeOut function list1 list2". i would have expected some attempt at optimization (both are simple finite lists, should this not go down to a loop?), or at least a reference to zipWith. is it a ghc core primitive?
12:44:51 <ydl> sorry, i should also mention I expect some fusion, the call i am making is equivalent to "and $ zipWith (==) list1 list2". is there a way to make this fuse or do I need to rewrite it?
12:52:56 <ydl> to be precise i expect (/would like) the function "f :: [Int] -> [Int] -> Bool; f = (.) and . zipWith (==)" to compile to fuse to something like "f = go where go xs ys = case xs of [] -> True; (x:xs) -> (case ys of [] -> True; (y:ys) -> if x == y then go xs ys else False)" .
13:01:19 <ggole> ydl: iirc there is (was?) a restriction with inlining/specialising partially applied functions
13:01:26 <ggole> You might have some luck with eta-expansion.
13:04:00 <ydl> thanks, but just tried, eta expanding does nothing.
13:07:52 <codedmart> I seem to have used generics to much. How do I write a FromJSON instance for this -> https://gist.github.com/codedmart/fe4f6ce68843c6e7814136235b777a27
13:08:04 <codedmart> I was using Generic, but it was failing.
13:08:22 <mpickering> ydl: Can you paste your exact code?
13:08:23 * hackagebot cpuinfo 0.1.0.1 - Haskell Library for Checking CPU Information  https://hackage.haskell.org/package/cpuinfo-0.1.0.1 (TravisWhitaker)
13:08:25 <codedmart> DatabaseInformation is a record with a generic FromJSON.
13:08:28 <mpickering> and -O3 doesn't do anything more than -O2
13:08:57 <joe9> Is it ok to run every program with -xc?
13:09:01 <mpickering> try -O1000 if you want it to be really fast ;) 
13:09:34 <mpickering> "the call I am making to is equivalent to", what does that mean
13:10:08 <lyxia> codedmart: how was it failing
13:10:10 <mpickering> you can't talk about this without being specific as whether optimisations fire depends on precise criteria
13:10:17 <ydl> mpickering: exact test code: "module Test where f :: [Int] -> [Int] -> Int; f = (.) and . zipWith (==)"
13:10:45 <codedmart> lyxia: I was getting this error `Error in $: expected Object, encountered Array`
13:11:15 <mpickering> ok, functions are only inlined if they are fully applied
13:11:26 <mpickering> which is why zipWith is not inlined
13:11:54 <lyxia> codedmart: Ok, so you need to know how your JSON is structured.
13:11:55 <ydl> i tried "f xs ys = and $ zipWith (==) xs ys" with no difference in generated core
13:12:31 <mpickering> also the code doesn't type check for me
13:12:44 <mpickering> what is "and"?
13:12:54 <ydl> mpickering: sorry, i meant [Int] -> [Int] -> Bool
13:12:57 <ydl> in the type signature
13:13:21 <ydl> that typechecks. and :: [Bool] -> Bool.
13:13:45 <ydl> for what it's worth this is with ghc 7.10
13:13:53 <ydl> 7.10.3*
13:14:03 <codedmart> lyxia: I do, this is everything -> https://gist.github.com/codedmart/fe4f6ce68843c6e7814136235b777a27
13:14:14 <mpickering> err, I actually see ghc do a good job optimising this
13:14:23 <lyxia> codedmart: none of this tells me what the JSON is supposed to look like
13:14:34 <mpickering> which will be because "." gets inlined and thus the arguments get applied to zipWith
13:14:53 <mpickering> same with 7.10.3
13:14:56 <ydl> mpickering: could you say (1) your ghc version (2) your arguments to ghc?
13:14:57 <codedmart> lyxia: so result in DatabaseInfo can either be a Bool, [Text], or DatabaseInformation
13:15:13 <mpickering> ghc-7.10.3 -fforce-recomp -O2 test.hs -ddump-simpl -dsuppress-all
13:16:16 <mpickering> I have no idea what your code does but it does look like it optimises well
13:17:11 <ydl> mpickering: ah, it was because i omitted -dsuppress-all . what does that argument do?
13:17:37 <mpickering> just suppresses information about casts, strictness analysis etc
13:17:48 <mpickering> so you can see the terms more easily
13:18:06 <mpickering> oh you are right, I was looking at the wrong thing
13:18:21 <mpickering> sorry about that
13:18:31 <lyxia> codedmart: you mean, in JSON it's just one of a boolean, a list of strings, or some object?
13:18:42 <lyxia> codedmart: no tags?
13:18:47 <codedmart> lyxia: Correct
13:18:49 <mpickering> another useful flag is -ddump-inlinings btw
13:19:38 <ydl> mpickering: thanks! so what does -dsupress-all do? why does it trigger extra optimizations?
13:19:47 <mpickering> It doesn't
13:19:53 <mpickering> I was wrong, it just cleans up the output
13:19:54 <lyxia> codedmart: I guess you could the Generic mechanisms for this but you'll benefit more from trying to write the instance by hand
13:20:46 <ydl> mpickering: but try compiling without -dsuppress-all. it doesn't inline the zipWith. the core is significantly different
13:21:02 <mpickering> it doesn't with the flag either
13:21:04 <lyxia> codedmart: instance FromJSON DatabaseInfoResult where parseJSON ...  <- just pattern match on a value of type Value.
13:21:10 <mpickering> look at the f1 definition
13:23:48 <mpickering> if you define zipWith in the same module it does inline
13:24:42 <mpickering> you are expecting it to optimise to True right?
13:25:55 <mpickering> oh that's another lie, I defined zipWith wrong :)
13:27:15 <ydl> mpickering: ok, let's be precise. here is test.hs: http://lpaste.net/1136194389400354816 . here is core with "ghc -c -O2 -ddump-simpl -dsuppress-all test.hs": http://lpaste.net/8799559133830316032 . this is exactly what i expect it to optimize to. here is core with "ghc -c -O2 -ddump-simpl test.hs": http://lpaste.net/4387209458212667392 . note that the actual algorithm is different between the two.
13:27:16 <mpickering> oh right, it's obvious why it doesn't get inlined, it's a recursive function
13:27:30 <ydl> mpickering: no, it does. look at the examples i posted
13:27:30 <mpickering> sorry for all this confusion
13:27:59 <mpickering> the zipWith call is still there in the f1 definition
13:28:57 <ydl> mpickering: look at both cores. zipWith is not there in the first one.
13:29:10 <mpickering> It's on line 19?
13:29:24 <mpickering> the other one it's on line 33
13:29:59 <ydl> oh you're right, i'm sorry for the confusion from my end now :) i was seeing what i wanted to see
13:30:22 <mpickering> so was I, the correct answer is that zipWith is recursive so doesn't get inlined
13:33:15 <XRAA> is there a complete ocaml parser module?
13:33:19 <mpickering> have you read the secrets of the inliner paper?
13:33:27 <mpickering> it's quite instructive about how the inliner works
13:33:48 <ydl> hmm, but aren't map and foldr both recursive? maybe they don't get inlined but they still get fused. it's really about RULES instead of inlining in think.
13:34:08 <XRAA> i want to write a haskell library that can convert any ocaml module into a new one where all pure functions have become point-free
13:34:40 <mpickering> ydl: I think so yes
13:35:19 <mpickering> you can see which rules fire with -ddump-rules
13:36:24 <mpickering> there is a lot of confusion about the optimiser and the interaction of specialistion, inlining and rules
13:37:22 <ph88> hey guys, what does the compiler mean with f0 here ?  https://paste.fedoraproject.org/429130/14740581/
13:37:31 <joe9> I am trying to derive an Vector.Unboxed instance for  http://bpaste.net/show/a925d84babfa data type. This article mentions about not using GeneralizedNewTypeDeriving http://stackoverflow.com/questions/11399143/automatic-derivation-of-data-vector-unbox-with-associated-type-synonyms
13:38:01 <joe9> Is there a better material to explain how to derive an instance of a Vector.Unboxed?
13:39:22 <mpickering> does the answer not say to use the template haskell approach?
13:41:09 <mpickering> ph88: Seems clear to me, GHC is trying to infer the type of "underline_arb", but it isn't used anywhere so the type variable is ambiguous
13:41:19 <joe9> mpickering: yes, it does, but, I could not understand it. hence, wanted to check if there is other material.
13:41:24 <dmj`> joe9: you mean Unbox?
13:41:30 <mpickering> specifically the type of applicative you mean by "pure"
13:41:33 <joe9> dmj`: yes, please.
13:41:38 <mpickering> solution: add a type signature 
13:41:52 <joe9> dmj`: this one: http://hackage.haskell.org/package/vector-0.10.0.1/docs/Data-Vector-Unboxed.html
13:41:58 <ydl> ok, the problem is actually with "and", not with "zipWith" (!). proof: test.hs: http://lpaste.net/477308477337763840 . core: http://lpaste.net/4538440423550484480 . 
13:42:12 <dmj`> joe9: right, so check the instances
13:42:13 <ydl> this begs the question why is "and" not defined by "foldr (&&) True"?
13:42:14 <dmj`> https://hackage.haskell.org/package/vector-0.11.0.0/docs/Data-Vector-Unboxed.html#t:Unbox
13:44:04 <mpickering> ydl: Isn't the actual question why this causes it to optimise?
13:44:09 <dmj`> joe9: only primitive types are supported, types that have no level of indirection, aren't lifted.
13:44:36 <joe9> dmj`: there is an Unbox instance for Double and Int.
13:44:58 <joe9> dmj': so, I do not need an unbox instance for my data type, correct?
13:45:25 <ph88> ah yes that was it, thx mpickering 
13:46:43 <dmj`> joe9: what are you trying to do
13:47:01 <mpickering> ydl: You see here, https://hackage.haskell.org/package/base-4.9.0.0/docs/src/GHC.List.html#zipWith
13:47:13 <mpickering> there are two rules for zipWith which allow the optimisations to happen it seems
13:48:01 <codedmart> lyxia: Got it now thank!
13:48:25 <mpickering> other useful flags -ddump-simpl-stats -ddump-rule-firings
13:49:37 <joe9> dmj' I want to have an unboxed instance for this:  http://bpaste.net/show/7b5510661e93 , source code: http://dpaste.com/2QKQAHD . I want to change that from an IntMap to an unboxed vector.
13:50:06 <mpickering> "any" is defined in terms of foldable operations which probably doesn't specialise to something the rules deal with properly
13:50:19 <joe9> dmj' all code repo: https://github.com/joe9/baby-steps-to-building-a-realtime-chart-with-haskell/blob/master/app/MyData.hs
13:50:57 <ydl> mpickering: there are indeed two questions, but i can answer the first :) (1) why does "g" optimize? because "foldr (&&) True" uses the "build" machinery in a simple way, and the correct rules fire (as you suggested), whereas and is defined in a roundabout way using the All monoid. the remaining question is why and is implemented in such a seemingly silly way instead of just using "foldr (&&) True"
13:52:00 <joe9> dmj`: this example is implementing an unbox for maybe data type: https://github.com/liyang/vector-th-unbox/blob/master/tests/sanity.hs
13:52:39 <joe9> dmj`: from what I understand, he is converting the maybe value to (Bool,a)
13:52:57 <joe9> dmj`: correct? so, he can use the Bool value as the index of the vector?
13:54:31 <joe9> dmj`: I can understand the code in that file. but, I do not understand why Line 22 and Line 23 are needed
13:54:37 <joe9> dmj`: Does that make sense?
13:54:55 <ph88> i wrote i quickcheck function, the test fails, the output is huge .. how can i know what is the problem? Output: https://paste.fedoraproject.org/429133/47405926/
13:56:32 <joe9> c_wraith:  http://bpaste.net/show/fe551d91ea84 is the profiling output .  http://dpaste.com/3DCBNNY is the code of the file. I could not have imagined that minimum and maximum domain functions are the reason for the slowness.
13:58:31 <joe9> dmj`: are you still there?
14:00:03 <mpickering> ydl: It isn't clear that is the most performant way for all datatypes
14:00:24 <mpickering> foldMap is quite a bit more direct that foldr for a lot of other data types
14:00:51 <mpickering> I was actually studying exactly these methods today
14:00:54 <dmj`> joe9: yes
14:01:50 <geekosaur> ph88, it's showing the value that failed. I am working on prettyprinting the value but as I have no clue as to what is being tested, it won;t actually help much
14:02:04 <buttbutter> This is kind of a vague question, but given some m a, is it possible to do something like ((m a >>= f) >> = f) >> f etc until f fails? Is there a pre-existing function for this sort of thing? 
14:02:07 <geekosaur> and all QC can tell you is success/failure, it knows no more about the nature of the test than I do
14:02:11 <ph88> why in ghci do i have to import all the stuff that is already imported in my source file ? :/
14:02:13 <geekosaur> in fact it knows less
14:02:31 <buttbutter> Shit, I messed up soooo much syntax there.
14:02:43 <ph88> geekosaur, what do you mean you are working on prettyprinting the value ?
14:02:51 <mpickering> buttbutter: there is the monad-loops package
14:03:08 <geekosaur> I'm installing hindent to see if it can give me something more readable
14:03:11 <dmj`> joe9: why do you want your types to be instances of Unbox, why not just use boxed mutable vectors
14:03:12 <dmj`>  
14:03:16 <buttbutter> foo :: m a. f :: (a -> m a). I want to do ((foo >>= f) >>= f)... etc over and over. :)
14:03:20 <buttbutter> Until it fails!
14:04:38 <jonored> buttbutter: so... iterateM_ ?
14:04:40 <ph88> geekosaur, you doing all that for me ?
14:04:58 <geekosaur> I had intended to install it at some point anyway
14:05:10 <ydl> mpickering: so what did you find? do you have a reason to typically prefer foldMap over foldR?
14:05:25 <geekosaur> (in fact I thought I already had, and started the install when I found out I hadn't)
14:05:50 <joe9> dmj`: boxed mutable vectors need IO monad.
14:05:55 <buttbutter> jonored: I can't find iterateM_ on Hoogle :(
14:06:08 <joe9> dmj`: I have pure data and prefer to use pure functions.
14:06:11 <dmj`> joe9: they need PrimMonad m => m
14:06:23 <dmj`> joe9: unboxed mutables vectors require IO or ST as well
14:06:49 <dmj`> joe9: MVector is parameterized by a primitive state token, PrimState m
14:07:00 <joe9> I need just unboxed. not the unboxed mutable.
14:07:15 <jonored> buttbutter: it's in monad-loops ?
14:07:16 <buttbutter> jonored: Oh it's in the monad-loops.
14:07:31 <joe9> dmj`: just the Data.Vector.Unboxed version not the .Mutable one
14:07:42 <buttbutter> Okay. Cool. Thanks :D
14:07:46 <dmj`> joe9: ah, I see
14:07:52 <geekosaur> ...maybe that was a bad plan. how many dependencies does that thing have anyway?!
14:08:06 <Welkin> geekosaur: is it as much as yesod or hakyll?
14:08:21 <Welkin> or lens?
14:08:24 * hackagebot pg-store 0.1.0 - Simple storage interface to PostgreSQL  https://hackage.haskell.org/package/pg-store-0.1.0 (vapourismo)
14:08:25 <Welkin> those are my benchmarks
14:08:26 * hackagebot colour-space 0.1.0.0 - Instances of the manifold-classes for colour types  https://hackage.haskell.org/package/colour-space-0.1.0.0 (leftaroundabout)
14:08:34 <Welkin> it's a yesod-size dependency
14:09:01 <geekosaur> not quite, but seems excessive for such a tool
14:09:07 <ph88> geekosaur, how can i know what are dependencies ?
14:09:19 <geekosaur> @hackage hindent
14:09:19 <lambdabot> http://hackage.haskell.org/package/hindent
14:09:26 <mpickering> ydl: I was looking from the semantic angle but it would make sense that foldMap was more efficient as the instances are much more direct for data types which aren't lists
14:09:30 <geekosaur> which I should probably have loked at before pushing the button >.>
14:09:30 <ph88> geekosaur, maybe use stack ?
14:09:40 <mpickering> ydl: http://mpickering.github.io/posts/2016-09-16-motivating-foldable.html
14:09:43 <geekosaur> wouldn't help if I'd never installed the deps
14:09:59 <ph88> should i install hindent too ?
14:10:01 <mpickering> See the section "Foldable" for a little bit of discussion
14:10:06 <geekosaur> no
14:10:40 <mpickering> ph88: So you want to pretty print a show output? I find the pretty-show package helpful for that
14:10:55 <mpickering> there is a command line utility ppsh which you pipe in a derive show instance and it will try to pretty print it for you
14:10:59 * geekosaur was doing the prettyprinting just to get some idea of what was pasted
14:11:10 <dmj`> joe9: you could just represent your type with an Unboxed tuple
14:11:35 <lpaste> geekosaur pasted “that value, oy” at http://lpaste.net/3846186807486054400
14:11:51 <geekosaur> but I still have no idea what it is or means...
14:12:32 <geekosaur> ph88, anyway, as I said earlier, QC just shows the first failing value, it has no idea *what* it is testing or why the test function failed
14:13:08 <geekosaur> it can't "look inside" your test any more than any other Haskell code can
14:14:29 <ph88> mpickering, quickcheck found a example which my program does something wrong, i want to know what the error in my program is
14:14:57 <ph88> mpickering, i have my own pretty printer, which is also under test
14:15:00 <geekosaur> you will have to inspect the value and then work through the test yourself
14:15:23 <geekosaur> possibly you want to add debugging information to the function you are testing (see Debug.Trace)
14:15:41 <geekosaur> ...if inspection of the value that failed doesn't give you a clue
14:15:47 <mpickering> yes, I'm not sure what the question is, you just have to look at the failing output and look at your program
14:15:59 <geekosaur> (it doesn't tell me anything, but again I have no idea what you are testing)
14:16:59 <ph88> geekosaur, mpickering   the quickcheck test goes like this:   make syntax tree (this is what you see in the paste) -> pretty print -> source code -> parse -> syntax tree (should be the same one as input)
14:17:10 <ph88> how can i even know whether the error is in the print or parse section ?
14:17:35 <monochrom> Oh two moving parts? Then you will never know.
14:17:44 <joe9> dmj`: unboxed tuple of (Int,MyData)?
14:19:13 <joe9> dmj`: similar to this: (Unbox a, Unbox b) => Vector Vector (a, b)
14:19:34 <ph88> geekosaur, how can i format it like you did ? i changed something in my code now i have a new output
14:19:41 <geekosaur> you have to arrange to show the intermediates. again, QuickCheck has no idea what you have given it; it just throws values at it and reports the first value that fails
14:19:41 <geekosaur> where fails means the test returned False
14:19:41 <geekosaur> it has no clue what the test *is*
14:19:45 <XRAA> does anybody know a way to parse ocaml in haskell?
14:20:11 <joe9> dmj`: I would need an Unbox MyData instance, correct?
14:20:33 <ph88> sorry got disconnected, last thing i read was "<geekosaur> it has no clue what the test *is*"
14:21:28 <joe9> geekosaur:  can you please help? I want to create an unboxed vector instance for this data type:  http://bpaste.net/show/6c862bc56f17 and all the code snippets I can find are complicated.
14:21:47 <geekosaur> ph88, I slapped 'foo =' in front and ran hindent on it
14:21:58 <geekosaur> joe9, no, I cannot help
14:22:15 <ph88> geekosaur, i see    quickCheck :: Testable prop => prop -> IO ()     perhaps quickcheck has a function like    prop -> a   so i can then run a into another function ?
14:22:30 <geekosaur> ph88,m and you got the last thing I sent. you were expecting me to unveil the secret AI that does the hard part of debugging for you?
14:22:34 <ph88> so it's not IO immediately but i can put that syntax tree through the pretty printer so i can see the intermediate stage ..
14:22:54 <ph88> :/
14:23:10 <ph88> well i already spotted one mistake i made by your paste
14:23:14 <ydl> mpickering: in case you're interested, ghc 8.0.1 optimizes both functions to the same thing
14:23:51 <mpickering> why does it?
14:24:15 <geekosaur> sadly the right way to do this is painful. you collect test cases (AST, expected source) and test the functions in both directions. but this means having to pregenerate all the test cases, and hope you got full coverage.
14:24:45 <geekosaur> you can;t simultaneously generate the expected result *and* test that the generator is doing the right thing
14:25:28 <ph88> i'm not following .. not sure what you are talking about
14:25:47 <ph88> i just want that huge  data structure  to be available in ghci instead of printed to IO
14:25:52 <mpickering> ydl: The rules which get fired are slightly different between the two versions
14:25:52 <geekosaur> this is a good example of why debugging is harder than programming (cf. the Kernighan quote)
14:26:15 <geekosaur> ("Everyone knows that debugging is twice as hard as writing a program in the first place. So if you're as clever as you can be when you write it, how will you ever debug it?")
14:26:18 <ph88> as in    getFailTestCase = quickCheckRunWithoutOutputToIO myPropTestFunction
14:26:40 <EvanR> haha
14:27:07 <ph88> then if i want i can just do    show getFailTestCase ... but instead i will do   pretty getFailTestCase   which will attempt to pretty print it
14:27:31 <geekosaur> I don;t think QC is the tool for that
14:27:36 <ph88> ..
14:27:45 <geekosaur> there are other test frameworks that let you do more things
14:28:26 <geekosaur> tasty, hspec, etc. (better, they can incorporate QC tests when those are appropriate. for something like this, QC is a trifle underpowered)
14:29:02 <ph88> eeeeeeeeehhhhh
14:29:30 <glguy> You'll have to speak up for ph88
14:30:26 <ReinH> TASTY, HSPEC, ETC.
14:31:28 <ReinH> geekosaur: When a sentence begins, "Everyone knows", I automatically discount the rest ofi t.
14:31:42 <ReinH> Although I am willing to give Kernighan some benefit of the doubt
14:31:51 <glguy> Sure, you already knew it; no need to listen again
14:32:01 <ydl> mpickering: yep, that's right. are there techniques to be stable to what appear to be random rule firings? also is the reason additional rules fired changes to base in 8.0.1 or changes to the compiler?
14:32:03 <ReinH> glguy: touché
14:32:07 <geekosaur> yeh, the usual misquote comes out a bit differently
14:32:26 <mpickering> ydl: I think a change in the compiler as the rules look unchanged in the last decade
14:33:12 <mpickering> rules don't fire randomly. you can control whether the rule is consider to some extent using phases, see in the user guide
14:33:25 <geekosaur> but it's true at least to the extent that debugging is harder than writing it in the first place. it's arguably wrong in that the complexity of debugging goes up much faster than the complexity of programming >.>
14:34:25 <ydl> mpickering: but is it clear to you what in particular made the behavior change from 7.10.3 to 8.0.1? and what one would do to be stable to this?
14:34:49 <mpickering> I've got no idea what changed in that release window
14:34:59 <mpickering> what do you mean "to be stable to this"?
14:36:14 <ph88> geekosaur, what about MkResult ?
14:36:25 <EvanR> debugging and programming both have wildly different instantiations... some debugging is easy if you have the right tool/technique already set up
14:36:36 <ydl> mpickering: i mean i write code that optimizes properly in ghc 8.0.1 . can I be sure there will be no regressions in ghc 9.*?
14:36:54 <EvanR> and some ways of programming are inherently hard
14:37:12 <mpickering> no, the best way is to complain on the issue tracker with a concrete example of code which optimises differently
14:37:32 <EvanR> but that could be quote schema, one instance of that quote for each pair of real life programming and debugging conventions
14:37:33 <mpickering> people are sympathetic to performance regressions like this IF you provide code which shows the problem
14:38:40 <geekosaur> ph88, what about it?
14:39:06 <geekosaur> it still does not disassemble your test function and figure out exactly what failed
14:39:43 <geekosaur> it tracks whether it failed by returning False, by the optional predicate telling it not to test that input value, or by throwing an exception
14:40:01 <geekosaur> it still does not do the hard part of debugging for you
14:40:30 <ph88> it's not about debugging
14:40:40 <ph88> it's about getting the failtest case not to IO
14:40:47 <ph88> i found this   https://stackoverflow.com/questions/4772902/how-to-display-a-reason-of-a-failed-test-property-with-quickcheck
14:40:55 <ph88> this is what i want
14:41:11 <ph88> eh sorry wrong link, this one:  https://stackoverflow.com/questions/8191131/find-the-value-that-failed-for-quickcheck
14:42:07 <geekosaur> wow that's a bit of a hack
14:42:43 <mpickering> or the other way is to not rely on the optimiser at all but that makes things tricky
14:44:22 <ph88> geekosaur, about tasty and hspec .. can they do this with quickcheck ?
14:48:10 <steshaw> Is there a custom prelude that's designed to help with backward compatibility for GHC < 7.10. i.e. pre AMP
14:49:03 <Welkin> steshaw: why? just use 7.10.x or 8
14:49:07 <kamyar> hello all
14:49:32 <steshaw> Welkin: I'd like to but need to support GHC 7.6.x and 8.4.x
14:50:51 <kamyar> Please help me about haskell
14:52:05 <Gurkenglas> We would have looked at the crystal ball to find out the interface of 8.4.x but we need that interface to be determined by the Haskell committee rather than whatever entity resolves stable time loops
14:54:05 <kamyar> When should I use Chan and TChan exactly?
14:55:00 <steshaw> Ah, I was typing random version. The real ones are GHC == 7.6.3, GHC == 7.8.4, GHC == 7.10.3, GHC == 8.0.1
14:55:28 <steshaw> but really, Gurkenglas, there's no need to be like that
14:55:38 <oherrala> steshaw: check how some of the core libraries are handling backwards compability
14:56:20 <steshaw> oherrala: will do. I checked lens but it doesn't seem to use NoImplicitPrelude
14:56:23 <oherrala> steshaw: bytestring still builds with 7.0
15:01:40 <nitrix> @pl \f g x = (f x, g x)
15:01:40 <lambdabot> (line 1, column 8):
15:01:40 <lambdabot> unexpected "="
15:01:40 <lambdabot> expecting pattern or "->"
15:01:47 <nitrix> @pl \f g x -> (f x, g x)
15:01:47 <lambdabot> liftM2 (,)
15:03:12 <ph88> gotta go
15:03:17 <ph88> see you later
15:08:05 <joe9> From reading up about Unboxed vectors, I read that an unboxed vector cannot be created for records with different data types. This is the discussion here too http://stackoverflow.com/questions/22882228/how-to-store-a-haskell-data-type-in-an-unboxed-vector-in-continuous-memory
15:08:11 <joe9> Just want to check if this is correct.
15:08:26 * hackagebot pg-store 0.1.1 - Simple storage interface to PostgreSQL  https://hackage.haskell.org/package/pg-store-0.1.1 (vapourismo)
15:08:28 * hackagebot transient-universe 0.3.4 - Remote execution and map-reduce: distributed computing for Transient  https://hackage.haskell.org/package/transient-universe-0.3.4 (AlbertoCorona)
15:08:29 <joe9> There is a method proposed at the end to use Data.Serialize and a ForeignPtr
15:08:30 * hackagebot servant-ede 0.6 - Combinators for rendering EDE templates in servant web applications  https://hackage.haskell.org/package/servant-ede-0.6 (AlpMestanogullari)
15:08:39 <joe9> Not sure if that just makes it slower.
15:08:42 <ReinH> joe9: do you mean a polymorphic data type?
15:09:00 <ReinH> e.g., data Foo a = Foo a, rather than data Foo = Foo Int?
15:09:18 <joe9> ReinH: not like this: http://bpaste.net/show/594724b05c4c
15:09:20 <ReinH> Because that's not true: Foo can be unboxed if a can be unboxed.
15:09:33 <joe9> where the record has different data type'd fields
15:09:42 <ixor> any happstack user in here?
15:09:52 <ixor> is is beign redone with ghc8?
15:10:11 <joe9> ReinH, the comment by " nh2 Apr 15 '14 at 15:57" at the end of the STackoverflow page.
15:10:25 <ReinH> joe9: an unboxed vector can store, say, (Int, Int).
15:10:50 <joe9> ReinH: But, how about Int, Double, Double, Double?
15:10:58 <ReinH> And you can decide whether to use a vector of structs or struct of vectors approach
15:11:01 <ReinH> Why not?
15:11:34 <ReinH> There are already instances for (Unbox a, Unbox b) => Vector Vector (a, b), up to 6-tuples
15:11:44 <ReinH> there is no structural difference between them and records with the same number of fields
15:12:10 <joe9> ReinH: oh, ok. Let me try them. Thanks.
15:12:27 <ReinH> You can define your instance the same way as the instance for the tuple with the same size
15:12:35 <ReinH> and if for some reason it is larger, it should be obvious how to extend the example
15:15:04 <joe9> http://bpaste.net/show/586a21b00acc when I tried to follow this example used GeneralizedNewTypeDeriving as described here: http://stackoverflow.com/questions/10866676/how-do-i-write-a-data-vector-unboxed-instance-in-haskell
15:15:09 <joe9> ReinH ^^
15:15:26 <joe9> ReinH: That got me thinking that it might not be possible.
15:15:43 <ReinH> You can only use GeneralizedNewtypeDeriving on newtypes.
15:15:51 <ReinH> You can't derive Vector
15:15:51 <joe9> http://stackoverflow.com/questions/22882228/how-to-store-a-haskell-data-type-in-an-unboxed-vector-in-continuous-memory talks about creating an Unbox for 3 int's.
15:15:57 <ReinH> unless you are deriving it for a newtype
15:16:10 <ReinH> and your data type isn't and can't be a newtype
15:16:26 <ReinH> You have to write the instance.
15:16:45 <joe9> oh, ok. Can I follow the code snippet in this link? http://stackoverflow.com/questions/22882228/how-to-store-a-haskell-data-type-in-an-unboxed-vector-in-continuous-memory
15:16:52 <joe9> ReinH: define an instance of MVector
15:17:13 <joe9> ReinH: then an instance of Generic.Vector
15:17:51 <ReinH> You will probably be doing something similar to that, yes
15:18:05 <ReinH> Or similar to the instance implementations for tuples
15:18:08 <joe9> ReinH: ok, Thanks.
15:34:11 <joe9> ReinH: Thanks, I was able to use a triple without an issue.
15:45:37 <jellycode> Hello, i have an math problem i want to solve in a functional style.  curious how it could look in haskell.  
15:46:02 <jellycode> If anyone is willing to take a look, I have it here:   https://codeshare.io/Y3oLi
15:46:48 <jellycode> you can actually type in the codeshare window
15:47:40 <monochrom> This is creepy like Ouiji.
15:47:47 <jellycode> lol
15:47:50 <jellycode> i saw your cusor :)
15:48:15 <ReinH> 1400 people in one codeshare
15:48:18 <ReinH> let's see how it goes
15:48:27 <jellycode> haha
15:48:46 <amalloy> if it works for google, it should work for #haskell
15:49:01 <jellycode> if it were that easy to call 1400 haskell programmers to action, we could change the world
15:49:04 <monochrom> The treasure is on the Rhode Island, and your future spouse is Fiona.
15:49:56 <jellycode> initially, i was trying to do it imperatively
15:49:58 <ReinH> This is a job for folds!
15:50:05 <jellycode> thats what i was thinking
15:50:14 <jellycode> but my mind doesn't "think in folds" yet
15:50:31 <jellycode> also, another challenge is that the increments are essentially arbitrary
15:51:19 <amalloy> you'll have to fold over a tuple, won't you? or is this one of those cases where you can make the accumulator itself be a function, and fold up one of those?
15:54:16 <codedmart> Is it possible to use the `fieldLabelModifier` on just one field while still using generic with aeson?
15:54:46 <jellycode> i do believe there will need to be 2 accumulators
15:55:05 <jellycode> one to keep track of how much of the usage amount you've "decremented", the other to to keep track of the total
15:55:47 <jellycode> i guess i should put the correct answer at the bottom
16:06:55 <jellycode> One "hacky" workaround that i thought of is to do an intermediate step, of generating another map, basically what i have on lines 21, 23, and 25, and then folding those into the result.  i don't like this method. 
16:07:58 <jle`> jellycode: it sounds like something similar to calculating the decimal representation of a number
16:08:27 * hackagebot sets 0.0.5.1 - Ducktyped set interface for Haskell containers.  https://hackage.haskell.org/package/sets-0.0.5.1 (athanclark)
16:08:49 <jle`> jellycode: you can probably get some inspiration for that :)
16:09:01 <jle`> only instead of outputting a list of digits, you'd output a running sum of cash money
16:10:07 <jellycode> thanks, i'll look into that
16:10:56 <jle`> @let dig n = let (t,o) = n `divMod` 10 in o : if t == 0 then [] else dig t
16:10:57 <lambdabot>  Defined.
16:11:01 <jle`> > dig 38471
16:11:05 <lambdabot>  [1,7,4,8,3]
16:11:26 <jle`> backwards, but hopefully you get the picture :)
16:12:22 <jle`> the difference in your case is the "divMod" equivalent in your case will be different at every step
16:14:22 <jle`> maybe your function can be passed a list of different tiers/their amounts
16:14:45 <jle`> and pop off each item for the current tier, and pass of the rest of the list for the recursive call
16:16:43 <jellycode> Right jle, there's one extra variable thats really hurting my head
16:18:06 <ReinH> For one thing, you need to decide on what to do if you run out of rate tiers
16:18:17 <ReinH> one option is to have a final unlimited tier rate
16:18:23 <jellycode> Thats another factor
16:18:54 <jellycode> in my opinion, the way they've listed the rates makes it really hard to program against
16:19:45 <ReinH> One way you can do it is first prepare a list of amounts and the rate to apply to them
16:19:53 <jellycode> each tier could be read as, "starting from this key, and ending at the next tier IF it exists"
16:19:53 <ReinH> then folding over this list is easy to find the final total
16:20:09 <jellycode> thats kinda like i described before right??
16:20:19 <ReinH> that makes it a hylomorphism, which pleases me.
16:20:23 <jellycode> I think doing it in 2 steps will be much more readable
16:20:37 <jle`> oh yeah, i think it'd actually make a lot more sense as a fold than as what i suggested
16:20:39 <jellycode> even if it creates an intermediate map
16:21:00 <amalloy> jellycode: here's an interesting approach: https://gist.github.com/amalloy/ee99e4ea6279cca8e8a10431a517648d
16:21:02 <jellycode> it's less memory efficient that way, but in this case, there aren't likely to be more than a few rates
16:21:03 <ReinH> The intermediate list might not be created at all, depending on how optimization hits it
16:21:16 <amalloy> also, the arithmetic in your sample is parenthesized wrong or something
16:21:25 <jellycode> maybe i'm misunderstanding you ReinH
16:21:37 <ReinH> the intermediate list might fuse away
16:21:56 <jellycode> that part i get
16:22:05 <jellycode> but are you basically saying build a map that looks like lines 21, 23, and 25
16:23:38 <jellycode> i will study this for a bit amalloy
16:24:22 <amalloy> it has the same problem ReinH mentioned, in that it doesn't know what to do if you use more than the sum of all the tiers. as written it charges you 0 for overages, but you could change it to do something else
16:24:27 <jellycode> thank you for your help
16:24:36 <jellycode> ahh, right
16:24:42 <jellycode> ok, thanks
16:27:26 <ReinH> jellycode: exactly that, yes
16:28:54 <ReinH> build a list of (amount charged at that tier, tier rate)
16:29:03 <ReinH> then fold them together by multiplying and adding in the obvious way
16:29:36 <amalloy> ReinH: any feedback on my solution? it's a technique i am still somewhat inexperienced with
16:29:39 <jellycode> ok
16:29:51 <ReinH> amalloy: It's quite similar to what I was going to write.
16:30:07 <amalloy> folding over the tier lists to produce a function from Int -> Double
16:31:32 <ReinH> If I am reading it properly, if you have a default rate then you replace (const 0) with (* defaultRate)
16:31:49 <ReinH> possibly requiring fromIntegral, which I avoided by making everything a double. :D
16:31:52 <ReinH> hey, it works for JavaScript.
16:32:30 <ReinH> I mean, if we're using doubles for rates and multiplying moneys with them, we obviously don't care about accuracy.
16:32:36 <amalloy> yes, leaving everything doubles would have been simpler but grosser
16:33:14 <ReinH> We could leave everything as rationals and make it entirely non-gross.
16:33:15 <pyon> Would it be possible to make a Reflection Without Remorse version of Free Applicatives?
16:33:57 <mpickering> pyon: Why do you want this? 
16:34:07 <pyon> Just to know if it's possible.
16:34:33 <mpickering> First you have to define what you mean by "reflection without remorse version"
16:35:05 <ReinH> @google oleg "reflection without remorse"
16:35:08 <lambdabot> http://okmij.org/ftp/Haskell/zseq.pdf
16:35:08 <lambdabot> Title: Reflection without Remorse
16:35:10 <ReinH> presumably that
16:35:16 <pyon> Yep, that.
16:42:05 <pyon> mpickering: For instance, let's say I have “instance Functor Foo”, “test :: Foo a -> Bool” and “chain :: FreeA Foo Bar”. I want to check whether “chain” contains at least one action for which “test” returns True. With a naïvely implemented “FreeA”, this is an O(n) operation, because “chain” has to be traversed sequentially. With a RwR implementation, it could be parallelized.
16:43:03 <mpickering> I know about that paper but it doesn't explain what a "reflection without remorse version" is. It seems that you desire a certain property from an implementation
16:45:51 <pyon> Mmm, now that I think about it, a FreeA is itself just a type-aligned list containing a “Foo (y -> z)”, a “Foo (x -> y)”, a “Foo (w -> x)”, and so on, plus a “Foo a” at the end.
16:46:09 <pyon> a “FreeA Foo” *
16:49:52 <pyon> Errr, no. Plus a lone “a” at the end.
16:51:16 <ReinH> pyon: why do you need the optimization for monads?
16:51:26 <ReinH> that is, what's the property of bind that makes this optimization useful?
16:51:37 <ReinH>  Now, do applicatives have this property?
16:56:50 <ReinH> (Hint: what is "reflection" in the title, and why would one be remorseful about it?)
16:57:02 <pyon> ReinH: If I want to do “foo <*> bar”, and both “foo” and “bar” are long chains, I absolutely need the optimization.
16:57:13 <ReinH> You need *an* optimization.
16:57:16 <ReinH> CPS would do just fine.
16:57:36 <ReinH> There is a specific reason that CPS isn't fine *for monads*.
16:57:40 <ReinH> And it's explained in the paper.
16:57:50 <ReinH> Now, is there a way to use applicatives where CPS isn't enough?
16:58:12 <pyon> Maybe it's just me, but I find manually CPS'd code super unreadable. :-|
16:58:24 <ReinH> Ok, but that's not an answer to my question.
16:58:39 <c_wraith> Then use Codensity as an automatic monad CPS transform.  That's why it exists. :)
16:58:47 <ReinH> Can you, in point of fact, *reflect* on a free applicative's chain of aps?
16:58:59 <ReinH> Can you interleave building the chain with observing the chain?
16:59:14 <ReinH> Is there a reason you need efficient access to intermediate results?
16:59:51 <ReinH> If there's no reflection, there's nothing to be remorseful about.
17:00:51 <pyon> Ah, so it's just the fact that the right argument of >>= can produce a different kind of monadic action depending on the argument it's passed?
17:01:08 <pyon> Unlike <*>, where the kind of applicative action of the right argument is fixed beforehand.
17:02:24 <shachaf> Why is codensity called codensity, anyway?
17:02:30 <pyon> c_wraith: And my beef with Codensity is that it often produces a bigger monad than you need. Whatever happened to “make meaningless values to irrepresentable”.
17:02:38 <pyon> s/to/
17:02:40 <pyon> s/to//
17:03:54 <c_wraith> pyon: well, yes.  CPS transforms always introduce the ability to do dumb CPS tricks.
17:04:02 <ReinH> pyon: Yes, but you don't need codensity.
17:04:15 <ReinH> There's an optimization that is size-preserving
17:04:18 <shachaf> Looks like a bit of a stretch.
17:04:46 <shachaf> c_wraith: Is that true?
17:05:25 <shachaf> c_wraith: By analogy: You can turn [] to DList by using function :: [a] -> [a] instead of values :: [a], with the constraint that the functions are all of the form (xs++)
17:05:28 <c_wraith> Hmm.  I suppose it's not necessarily true if the functions are hidden behind non-exported constructors.
17:05:28 <shachaf> And the type is too big.
17:05:41 <shachaf> But you can also use functions :: forall r. (a -> r -> r) -> r -> r
17:05:52 <shachaf> And that type is the right size.
17:06:14 <glguy> You're supposed to go to a type that was too small before you go to the type that is just right
17:06:16 <shachaf> And up to constant factors has all the advantages of DList.
17:06:38 <shachaf> glguyilocks
17:08:28 * hackagebot configuration-tools 0.2.15 - Tools for specifying and parsing configurations  https://hackage.haskell.org/package/configuration-tools-0.2.15 (larsk)
17:08:31 * hackagebot sets 0.0.5.2 - Ducktyped set interface for Haskell containers.  https://hackage.haskell.org/package/sets-0.0.5.2 (athanclark)
17:08:33 * hackagebot type-spec 0.3.0.0 - Type Level Specification by Example  https://hackage.haskell.org/package/type-spec-0.3.0.0 (SvenHeyll)
17:17:58 <stm_> Quick question about do syntax (and maybe the ST monad). I've been trying to improve my understanding of both and attempted to rewrite an example without do syntax ... and apparently messed up somewhere. Can someone (pretty please) explain where?http://lpaste.net/199079
17:18:07 <stm_> I very much would appreciate the help.
17:19:02 <geekosaur> at a guess, your problem is neither
17:19:15 <geekosaur> your problem is that ($) is a very low precedence operator
17:19:51 <geekosaur> (newSTRef 0 >>= \count -> replicateM_ (10^6)) $ (modifySTRef' count (+1) >> readSTRef count) is not the same expression
17:19:53 <stm_> are you suggesting that parentheses would be the better option here?
17:20:06 <stm_> no?
17:20:13 <geekosaur> yes. I inserted the parentheses that ($) infers above
17:20:23 <geekosaur> if you look it over, you will note that they are in the wrong places
17:20:28 <buttbutter> I have a type that
17:20:34 <buttbutter> Shit, stupid keyboard. 
17:21:21 <stm_> I apologize if i'm misunderstanding. Is what you pasted here something that works?
17:21:42 <geekosaur> no, it is the ($) expression with the parentheses made explicit
17:21:58 <geekosaur> showing why it does the wrong thing
17:23:14 <geekosaur> the right thing would be: newSTRef 0 >>= \count -> (replicateM_ (10^6)) $ (modifySTRef' count (+1)) >> readSTRef count
17:24:26 <stm_> ah ok. I think I understand. This: runST $ newSTRef 0 >>= \count -> replicateM_ (10^6) ( modifySTRef' count (+1)) >> readSTRef count     
17:24:31 <geekosaur> the "do" version has newlines (expanded in the translation of "do" notation, but only after ($) is applied) in the right place to prevent ($) from "blowing up" to include too much.
17:25:14 <stm_> Are newlines significant in do notation?
17:25:17 <geekosaur> usually you get a type error when ($) grabs too much like that, but in this case it happens to change the meaning while still typechecking (with the wrong inferred type)
17:26:06 <geekosaur> they translate to (>>), unless they follow a ` binding <- ... ` in which case they translate to ` >>= \binding -> `
17:26:39 <geekosaur> but this happens *after* the ($) finds the limits of what precedence lets it use, so it in effect can't go past the newlines
17:26:59 <stm_> Thank you. Changing to parens fixed it. Does this mean it's perhaps better to use parentheses as a first option rather than the $ operator?
17:27:15 <geekosaur> (in reality it translates to semicolons which then translate later. the semicolons are what stop ($))
17:27:31 <geekosaur> often it's blindly recommended to only use parentheses
17:27:46 <geekosaur> if you;re unsure about how precedence works, that may be the safest option
17:27:48 <stm_> It seems as though the $ is more idiomatic when I look at other people's code buuuut after this I'm definitely leaning towards sticking to those
17:28:05 <ReinH> newlines that indicate a new layout section are significant
17:28:13 <ReinH> e.g.,
17:28:14 <ReinH> f
17:28:14 <ReinH>   a
17:28:15 <ReinH>   b
17:28:19 <ReinH> is not f >> a >> b
17:28:40 <geekosaur> right, but that's even more to have to figure out >.>
17:28:45 <ReinH> ok
17:29:50 <buttbutter> I have a type that is an instance of a Monad. It looks something like this: data mType a = mType a.  I have another type data Foo = Foo Int | ListFoo [Foo Int]. This might sound ridiculous, but all I want to do is define a function f :: mType Foo -> mType [Foo] that converts from a myType containg a ListFoo fs to a myType containg a list of the elements fs.. 
17:30:07 <buttbutter> How do I do it ;_;
17:30:43 <ReinH> I assume you mean data MType a = MType a
17:30:50 <buttbutter> Yes C:
17:31:04 <stm_> I'd been looking at this: http://dev.stephendiehl.com/hask/#do-notation . I tend to simply use parentheses as that is the more familiar syntax compared to the  operator. The layout thing I think I understand. The semantics of the do notation and the $ operator catch are a bit newer and catch me off guard and this happened to have both. I appreciate the help.
17:31:17 <ReinH> buttbutter: what does the fact that it is a monad have to do with the question?
17:31:25 <geekosaur> that it is a Monad is not relevant; Monad does not have knowledge of the internal structure of your type
17:32:07 <ReinH> Also what do you want to do in the event that your Foo is Foo 3, i.e., not a list?
17:32:07 <buttbutter> Uh. In reality, the Monad also stores a computational context that is required for evaluating each of the Foos in the list. 
17:32:10 <stm_> The monad part is not relevant. I didn't know what was happening (doubted it was the monad and assumed it was my do notation understanding but wanted to disclose noobish status)
17:32:20 <buttbutter> Oh, wait. Shit. Then that plan doesn't even work.
17:32:46 <buttbutter> I'm really confused :)
17:32:51 <buglebudabey> paging Cale
17:33:07 <geekosaur> stm_, that was aimed at buttbutter not at you
17:33:09 <buglebudabey> hmm doesn't seem to be on
17:33:14 <buglebudabey> pagin ReinH 
17:33:19 <ReinH> buglebudabey: hi
17:33:21 <buglebudabey> hai!
17:33:25 <buglebudabey> are you free?
17:33:30 <ReinH> for a bit, yeah
17:33:53 <stm_> oops sorry and thanks again
17:34:06 <buttbutter> Okay, let me ask something else first.
17:34:25 <buttbutter> I noticed that there isn't a function Monad m => (m a) -> a. Why is that?
17:34:55 <ReinH> buttbutter: Because it's not something you can do with all instances.
17:35:17 <buttbutter> Can you give me an example?
17:36:34 <geekosaur> any type for which the data constructors are not exported/visible
17:36:39 <glguy> buttbutter: You can't do it with [] (list), IO, Maybe, Proxy, Reader, etc
17:38:25 <geekosaur> a practical example is atomically :: STM a => IO a. the only way to ensure STM does what it is designed to do is to ensure that an STM transaction cannot be extracted as anything but an IO action --- and that only if the STM transaction does not `retry`.
17:39:09 <geekosaur> another example, albeit one with a hole in it, is you cannot "extract" an a from an IO a: there is no actual a to be had, only a "program" for the Haskell runtime that at some point will be executed to produce an a
17:39:43 <geekosaur> (the hole is that there is a hidden way to force execution --- but that comes with a price, and ignoring that price will lead to *very* strange behavior)
17:40:00 <ixor> what are best book online for learning haskell?
17:40:05 <amalloy> it seems easier to focus on something less complicated: you can't write a (total) function with type [a] -> a
17:40:31 <geekosaur> or for that matter Maybe a -> a
17:40:36 <geekosaur> (what happens if it's Nothing?)
17:41:01 <glguy> 17:36 glguy     : buttbutter: You can't do it with [] (list), IO, Maybe, Proxy, Reader, etc
17:41:30 <geekosaur> there are two functions to do that one; both provide ways to deal with Nothing. this cannot be done solely with Monad; it requires a function that knows that a Maybe a is either Nothing or Just a
17:41:39 <amalloy> glguy: yes, i'm aware. i was highlighting one in particular that's easy
17:43:50 <buttbutter> Okay. Now let me rephrase my question. Say I have a value v that has type MyMonad Foo. I know that I'm dealing with ListFoo value constructor and not the Foo constructor (data Foo = Foo Int | ListFoo [Foo]), I'm not sure if that's the proper way to say that but I hope you know what I mean. Let's say that I don't have the constructor available for MyMonad. How can I take the value v and go from MyMonad 
17:43:50 <geekosaur> (well, three; fromJust is not total and crashes on Nothing.)
17:43:56 <buttbutter> Foo to [MyMonad Foo] where the Foo in [MyMonad Foo] is the "Foo Int" value constructor and the Foo in MyMonad Foo is the "ListFoo [Foo]" one. I know what I said is probably retarded and is absolutely ridiculous terminology, but I don't know precisely how to talk about the types in that context.
17:45:01 <geekosaur> you cannot express "I know that I'm dealing with ListFoo value constructor and not the Foo constructor" in the type system, so you cannot solve that
17:45:24 <buttbutter> Well shit.
17:45:41 <buttbutter> Is that why I had so much trouble talking about that?
17:46:11 <geekosaur> there's some stuff you can do with generics but that just means everything carries around extra information so you can look up at runtime what constructor is used and make decisions based on it
17:46:57 <buttbutter> I see.
17:46:59 <ReinH> buttbutter: perhaps the most practical example is ((->) r), because the `a' that you get varies for each choice of 'r'.
17:47:14 <ReinH> There's no way to get an `a' out without providing an `r'.
17:47:24 <buglebudabey> would someone(s) like to critique my code? i'm looking for opportunities to use more advanced language features https://github.com/aneksteind/hext/blob/master/src/NLP/Hext/NaiveBayes.hs
17:47:25 <geekosaur> ("generics" here means either Data.Data or something like lens; when you derive lenses, you are invoking some compile time stuff that adds the extra runtime information)
17:47:29 <glguy> unsafePerformFunctionApplication?
17:47:30 <buttbutter> I see, kind of. 
17:47:39 <buttbutter> Guess that means I have to approach my problem differently. :(
17:47:52 <ReinH> buglebudabey: Can you write a function (r -> a) -> a?
17:48:15 <buglebudabey> ReinH i can but for what specifically are you meaning
17:48:23 <ReinH> Ok, write that function please.
17:48:30 <geekosaur> or if you define the lenses yourself, you're making the type information explicit in much the same way that the `maybe` or `fromMaybe` functions do
17:48:58 <ReinH> I'm asking for any defined function of type (r -> a) -> a, which means it must work for ALL r and a.
17:49:07 <ReinH> This is what you have asked for.
17:49:18 * glguy will take two
17:49:31 <ReinH> Because that is what a function of type Monad m => m a -> a is for the instance ((->) r)
17:49:41 <buglebudabey> extract r = (\x -> let a = r x in a) ReinH is that close?
17:49:50 <ReinH> buglebudabey: What is the type of extract?
17:50:03 <buglebudabey> extract :: r -> a -> a
17:50:18 <ReinH> :t let extract r = (\x -> let a = r x in a) in extract
17:50:19 <lambdabot> (t -> t1) -> t -> t1
17:50:27 <ReinH> That's the type of extract.
17:50:31 <ReinH> That is not what you asked for.
17:51:06 <ReinH> extract is just a weird way to write ($)
17:51:08 <geekosaur> note carefully what (r -> a) -> a means
17:51:21 <buglebudabey> it does?
17:51:37 <ReinH> You asked for Monad m => m a -> a, which for ((->) r) specializes to (r -> a) -> a
17:51:43 <ReinH> So that's the function you need to prove exists
17:52:00 <ReinH> You can prove that it exists by writing an implementation, and we will at least know that it exists for ((->) r).
17:52:09 <buglebudabey> i asked for that?
17:52:12 <ReinH> If you can't write it for ((->) r) then you can't write it for ALL monads, which is what you need to do
17:52:21 <ReinH> you asked why there isn't a function Monad m => m a -> a
17:52:26 <buglebudabey> mmm no i didn't
17:52:40 <ReinH> buttbutter> I noticed that there isn't a function Monad m => (m a) -> a. Why is that?
17:52:47 <ReinH> Yes you did.
17:52:57 <glguy> 17:34 buttbutter: I noticed that there isn't a function Monad m => (m a) -> a. Why is that? 
17:52:59 <buglebudabey> buglebudabey \= buttbutter
17:53:04 <ReinH> Wait
17:53:09 <ReinH> Oh.
17:53:10 <ReinH> Ok.
17:53:20 <ReinH> Sorry.
17:53:23 <buglebudabey> no worries
17:53:26 <buglebudabey> im interested now though
17:53:44 <geekosaur> ... actually buglebudabey I think you were the one who originally got confused
17:53:52 <geekosaur> [17 00:47] <buglebudabey> ReinH i can but for what specifically are you meaning
17:54:00 <geekosaur> think you responded to something intended for buttbutter 
17:54:01 <ReinH> geekosaur: no, I asked buglebudabey by accident
17:54:06 <geekosaur> oh
17:54:13 <buglebudabey> youre both right
17:54:22 <geekosaur> whoopss, yes, I see that now
17:54:26 <geekosaur> tab conplete fail?
17:54:28 <ReinH> buttbutter: Anyway, that's why you can't have a function Monad m => m a -> a in the Monad typeclass.
17:54:31 <ReinH> Yep.
17:54:32 <buglebudabey> yeah pretty much
17:54:36 <ReinH> Three letters is too many.
17:54:56 <ReinH> There are monads for which you can't write it, and ((->) r) is I think the most obvious example.
17:55:07 <ReinH> You'd have to produce an `r' out of thin air, and that's impossible.
17:55:19 <shachaf> I think Proxy is a pretty obvious example.
17:55:24 <buglebudabey> ok i get it now
17:55:28 <buglebudabey> with ($)
17:55:35 <ReinH> shachaf: Yeah, I suppose it is.
17:55:39 <buglebudabey> $ is :: f -> a -> a
17:55:44 <glguy> In glirc if you'd have tab completed "bu" at that point it would have gone with buttbutter due to the recency of chat
17:55:50 <ReinH> :t ($)
17:55:52 <lambdabot> (a -> b) -> a -> b
17:55:57 <buglebudabey> woops, nvm
17:55:59 <buglebudabey> fack
17:56:01 <ReinH> That's the type of extract as well
17:56:24 <buglebudabey> so buttbutter seems out of the pictures
17:56:26 <buglebudabey> picture*
17:56:34 <buglebudabey> substitute me for him
17:56:41 <buglebudabey> tag team
17:57:20 <buglebudabey> im not sure i can write (r -> a) -> a
17:57:37 <buglebudabey> what comes to mind is the (r -> a) monad but i get lost from there
17:57:53 <ReinH> You can't.
17:58:00 <ReinH> Because you don't know what the `r' is.
17:58:08 <ReinH> And the `a' value depends on the `r' value you choose.
17:58:19 <ReinH> So you can't write a Monad m => m a -> a for ((->) r)
17:58:20 <buglebudabey> ah i see
17:58:23 <buglebudabey> :t const
17:58:24 <lambdabot> a -> b -> a
17:58:38 <buglebudabey> thought of something similar to that
17:58:43 <ReinH> You also can't write one for Proxy, because there is no `a' anywhere at all, except in the type.
17:58:54 <buglebudabey> not familiar with Proxy myself
18:00:02 <ReinH> data Proxy t = Proxy
18:00:13 <ReinH> It has a trivial monad instance that throws any mention of t away.
18:00:33 <ReinH> e.g., the Functor instance is fmap _ Proxy = Proxy
18:00:46 <ertes> MarLinn: easy: it's an IRC client…  tab completion is so important that it's even specified in section 9.4 of the original RFC 1459 to be a mandatory client feature
18:00:50 <ReinH> or, rather, fmap _ _ = Proxy even
18:01:31 <ReinH> instance Monad Proxy where _ >>= _ = Proxy
18:03:14 <buglebudabey> nice, sounds pretty simple
18:03:28 <ReinH> about as simple as it gets, and obviously contains no `a's to retrieve.
18:06:57 <xuxu> i think i found a typo in the user guide
18:07:02 <xuxu> https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/ghci.html#using-do-notation-at-the-prompt
18:07:07 <xuxu> "You can also define functions at the prompt:"
18:07:11 <xuxu> Prelude> add a b = a + b
18:07:40 <glguy> xuxu: If that's not working for you, you're looking at the wrong version of the user guide for your local verison of GHC
18:07:41 <xuxu> that is going to fail, unless, is there some way to define functions without using the let syntax in ghci?
18:08:00 <xuxu> ghc 8.01
18:08:07 <glguy> OK, fire it up
18:08:19 <xuxu> aha ghc version 7.10.3 locally
18:08:30 * hackagebot type-spec 0.3.0.1 - Type Level Specification by Example  https://hackage.haskell.org/package/type-spec-0.3.0.1 (SvenHeyll)
18:08:32 * hackagebot pretty-types 0.2.3.1 - A small pretty printing DSL for complex types.  https://hackage.haskell.org/package/pretty-types-0.2.3.1 (SvenHeyll)
18:08:39 <geekosaur> xuxu that syntax works in 8.01
18:08:43 <geekosaur> only
18:09:06 <xuxu> is ghc 8.0.1 in beta?
18:09:15 <geekosaur> no, released several months ago
18:09:24 <xuxu> that's exciting
18:09:27 <geekosaur> they're prepping for 8.0.2 already
18:09:43 <geekosaur> (may be delayed to fix some new issues Sierra introduced)
18:11:05 * ertes is eagerly waiting for 8.0.2, because it finally fixes the deferred scope error non-sense
18:11:27 <xuxu> https://www.stackage.org/lts-7.0
18:11:27 <glguy> I'm hoping the fix for GHCi on linux gets in
18:12:22 <glguy> Oh, cool, a fix is merged https://ghc.haskell.org/trac/ghc/ticket/12433
18:12:24 <xuxu> stack install ghc?
18:13:20 <xuxu> https://docs.haskellstack.org/en/stable/install_and_upgrade i guess is what i need
18:48:04 <red13> I can't build hs-ffmpeg
18:49:09 <red13> or ffmpeg-light
18:50:20 <red13> The first fails on base (<4)
18:50:58 <red13> The second fails with cannot find pkg-config
18:52:23 <glguy> red13: then you'll either need to go back in time to 2009 when hs-ffmpeg worked or update it to work, or install pkg-config and the packages required by ffmpeg-light:    pkgconfig-depends:   libavutil, libavformat, libavcodec, libswscale, libavdevice
18:54:24 <red13> Thanks glguy
18:55:09 <glguy> I think that time-travel sounds the most exciting and that installing pkg-config and the libraries has the best likely outcome
18:55:10 <geekosaur> note that you need to install devel packages on those OSes that use them (fedora/centos, debian/ubuntu/mint)
18:55:14 <geekosaur> also pkg-config
18:55:24 <geekosaur> these are OS packages not haskell
18:57:12 <red13> It won't work on Windows?
18:57:46 <red13> What about cygwin or msys?
18:57:55 <koz_> red13: Those have their own package managers.
18:58:01 <koz_> (or at least, Cygwin does AFAIK)
18:58:12 <systemfault> I wonder if ghc works with ubuntu on windows
18:58:24 <systemfault> (The new bash on windows MS thing)
18:59:10 <red13> Without VirtualBox?
18:59:41 <red13> koz_ I'm not sure what that implies
19:00:45 <koz_> red13: Basically, there are Cygwin packages, which are basically system libraries that Cygwin provides. It has a package manager (i.e. they are centrally stored and managed).
19:00:52 <koz_> I think MSYS might do something similar, but I dunno MSYS.
19:01:19 <red13> How does that relate to pkg-config?
19:01:21 <systemfault> red13:re: http://www.howtogeek.com/249966/how-to-install-and-use-the-linux-bash-shell-on-windows-10/
19:01:35 <systemfault> I have no idea if ghc runs on it... but that's damn cool
19:01:36 <red13> Ok I'll try that
19:02:48 <glguy> Installing the Linux environment on Windows is going to be an adventure on its own
19:03:14 <zell> it says its not linux, just the gnu that normally runs on linux...
19:03:18 <systemfault> glguy: Even if it's a microsoft product?
19:03:35 <glguy> It's not being billed as a supported product last time I looked at it
19:04:53 <zell> wow password authenticated special development features for windows
19:05:21 <zell> yikes
19:05:31 <zell> (roots phone)
19:05:37 <geekosaur> pkg-config is going to be a pain for native windows, but I don't think ghc is quite a thing for that anyway; you want msys2
19:05:51 <geekosaur> which will have pkg-config, and pacman as its package manager
19:05:58 <zell> much better
19:07:04 <zell> i would rather avoid a proprietary solution, even if its just gnu on windows bash
19:08:18 <zell> why cant all that gnu stuff get built on a linux shell run as a haskell program?
19:08:50 <biglambda> plowboy
19:09:02 <zell> is that an app?
19:09:07 <biglambda> Uh
19:09:20 <biglambda> Wrong box :)
19:10:11 <zell> i would really like to be able to install linux on a vm such as virtualBox but which is written in haskell
19:12:11 <zell> i have no idea what would be required, but something like a device architecture emulator with various prescribed io functionality would do. is there a standard for this, a kind of standard virtual machine? is the jvm that?
19:13:03 <systemfault> Hmm, ghc doesn't seem to work on "ubuntu on windows" </sad>
19:13:23 <lingxiao> hey all
19:13:29 <geekosaur> 7.10 might. I'd be willing to be 8.0's new memory manager won't
19:13:30 <lingxiao> I have a parser that i want to express but im not sure how
19:13:32 <lingxiao> using attoparsec
19:13:52 <lingxiao> I want to parse this "(and) (even) foo"
19:13:59 <lingxiao> where (..) denotes optional ...
19:14:04 <zell> can i install linux on the jvm? is there a version of the jvm written in haskell?
19:14:06 <geekosaur> zell, jvm is a virtual cpu but services are only available via java entry points, not direct "hardware" access in a portable way
19:14:17 <lingxiao> so the express "(and) (even) foo" should be able to parse:
19:14:27 <lingxiao> {and even foo, and foo, even foo, foo"
19:14:30 <geekosaur> more specifically: no, and no
19:14:34 <lingxiao> {and even foo, and foo, even foo, foo}
19:15:07 <zell> what can i install linux on that i can write in haskell given superpowers?
19:15:40 <zell> is there a standard for device emulators?
19:16:07 <geekosaur> not really, no
19:16:34 <zell> i cant believe thats cutting edge, i believe one should be constructed
19:16:48 <zell> already*
19:17:13 <geekosaur> ob https://xkcd.com/927/
19:17:35 <geekosaur> everyone did, from virtualbox to vmware to bhyve; they're all different >.>
19:18:26 <biglambda> Is there a way to have a variable record update field
19:18:39 <geekosaur> nope
19:18:47 <zell> ah, so its just my use of "standard", well thats ok, a minimal clone of the minimal standard seems desired
19:18:50 <geekosaur> that is, in fact, why lens exists
19:18:56 <biglambda> I see
19:19:28 <geekosaur> (well, lens as the final evolution of the lens concept, starting with the first version of fclabels)
19:22:43 <zell> that package seems to suggest jan should move to amsterdam for retirement
19:23:00 <zell> https://hackage.haskell.org/package/fclabels-2.0.3/docs/Data-Label.html
19:23:33 <geekosaur> the last version of fclabels is much more like ekmett-lens than the first version was
19:23:59 <zell> to the time machine!
19:23:59 <zzing> There is a programming language called tiny that is designed for students to implement, but it is an imperative language. Are there any simple functional languages that are meant for the same? (to get an idea of tiny: https://en.wikipedia.org/wiki/Tiny_programming_language )
19:24:13 <geekosaur> hackage has links to older versions :)
19:24:19 <Welkin> zzing: scheme
19:25:11 <zzing> Welkin, I should add - something that has some syntax to it (the idea is for parsing it first and foremost)  Scheme doesn't have much to speak of there.
19:25:15 <zell> yes this looks more clear https://hackage.haskell.org/package/fclabels-0.4.0/docs/Data-Record-Label.html
19:25:25 <zell> template haskell, what is that?
19:25:27 <dazz> new bash in win 10 is awesome thing 
19:25:46 <zell> dazz, did you get ghc running?
19:25:50 <geekosaur> zell, evaluates haskell code at compile time to generate ghc ASTs
19:26:20 <grantwu> Is there a way to go from a number to a ByteString without roundtripping through String or Bytestring.Builder
19:26:22 <geekosaur> lens uses it to inspect user defined records and create its own accessors, for example
19:26:22 <zell> oh no, not abstract syntax
19:26:55 <zell> wait, so it writes new haskell code on the fly?
19:27:00 <dazz> hvnt tried yet .. . I'll check today
19:27:11 <geekosaur> yes
19:27:18 <zell> no wonder it breaks my mind
19:27:19 <geekosaur> but it has to write it as an AST, not source code
19:27:25 <zell> seems fair
19:27:25 <grantwu> wait, not through ByteString.Builder, I don't want the binary encoding, but still
19:28:04 <zell> is it a theorem that lens can be implemented without using AST's?
19:28:24 <geekosaur> lens itself does not need TH
19:28:28 <zell> whew
19:28:35 <geekosaur> it's only used to auto-generate record accessors
19:28:42 <geekosaur> you can generate them manually, it's just annoying
19:28:46 <zell> ah
19:28:54 <zell> ok that sounds less offensive
19:28:59 <shachaf> i,i or you can use upon
19:29:06 <glguy> nope nope nope
19:29:06 <shachaf> You shouldn't use upon, but upon is great.
19:29:44 <shachaf> > set (upon (!!3)) 'q' "abcdef"
19:29:45 <geekosaur> basically it's a lot of boilerplate code. most people prefer to just drop a makeLenses splice in it
19:29:47 <lambdabot>  "abcqef"
19:29:51 <zell> seems seriously mysterious
19:29:57 <glguy> Some things in lens exist because they are useful, some things exist because they "should exist", and then there's upon
19:30:17 <shachaf> > set (upon (drop 2)) " hi" "abcdef"
19:30:20 <lambdabot>  "ab hi"
19:30:31 <zell> omg
19:30:41 <geekosaur> accursesUnutterable...
19:30:44 <geekosaur> *accursed
19:30:53 <zell> is that a language extension?
19:31:08 <geekosaur> upon? that's lens being used for evil
19:31:27 <shachaf> Strictly speaking it's pretty independent of lens.
19:31:40 <zell> why not just use splitAt or implement a data structure to enable modification, such as a zipper
19:31:41 <shachaf> lens assimilated the evil
19:31:51 <shachaf> upon works on any type, not just lists.
19:32:17 <zell> what is the constraint to the type that upon can work on?
19:32:24 <zell> oh no dont tell me
19:32:24 <geekosaur> :t upon
19:32:26 <lambdabot> (Indexable [Int] p, Applicative f, Data a, Data s) => (s -> a) -> p a (f a) -> s -> f s
19:32:52 <zell> whats Indexable and Data?
19:33:17 <glguy> Indexable isn't really related, Data is from "Scrap your boilerplate" generics
19:33:17 <shachaf> Data is the important one.
19:33:20 <zell> https://hackage.haskell.org/package/fields-0.1.0/docs/Data-Record-Field-Indexable.html
19:33:20 <geekosaur> Data is the generics machinery
19:33:32 <zell> whats generics?
19:34:00 <geekosaur> keeps the constructor details and other type information around for runtime introspection and modification
19:35:19 <zell> http://lpaste.net/194272
19:36:00 <zell> i use; type Constructor f b a = (a,b,f a)
19:36:08 <zell> where b represents the "index"
19:38:17 <zell> so i guess the question is, am i implementing generics?
19:39:29 <zell> or is carrying a a single type parameter not sufficient to provide information around for runtime introspection and modification?
19:42:02 <zell> the example uses list which does not require type information for navigation, but the framework allows that a value of arbritrary type be required by construction
19:42:48 <zell> did i lose you?
19:44:43 <zell> (really this is an implementation for zebedde, which is a kind of directed unfold)
19:45:40 <reddy_>  hi
19:45:57 <zell> whats up reddy_?
19:46:01 <lambdabot> Hello.
19:46:25 <Welkin> lambdabot: what?
19:47:07 <reddy__> hi
19:47:12 <lambdabot> Hello.
19:47:18 <Welkin> hi
19:47:23 <Welkin> wtf
19:47:24 <reddy__> i am new here
19:47:30 * Welkin slaps lambdabot 
19:47:33 <zell> hi
19:47:38 <reddy__> what is the haskell about ?
19:47:46 <Welkin> reddy__: your mother
19:47:57 <zell> !?
19:48:18 <zell> has someone hacked #haskell/
19:48:19 <shachaf> @faq has the answers you want
19:48:19 <lambdabot> https://wiki.haskell.org/FAQ
19:48:29 <reddy__> ok tnx
19:48:30 <glguy> Welkin: No, that's not a good answer
19:48:49 <zell> lol
19:49:25 <zell> did anyone get what i was on about?
19:49:32 <reddy__> ghc can ru on ubuntu ?
19:49:43 <reddy__> *run
19:49:45 <Welkin> zell: something about swiss cheese
19:50:02 <zell> ...
19:50:11 <Welkin> reddy__: it runs on everything
19:50:27 <zell> is #haskell drunk because its friday night/
19:50:35 <Clint> drunk on swiss cheese
19:50:50 <zell> give it half an hour
19:50:53 --- mode: ChanServ set +q *!*@107-147-70-64.res.bhn.net
19:51:36 <reddy__> just asking .. beacuse i am now on win bash
19:51:53 <zell> why do browsers use javascript and not haskell?
19:52:03 * Clint sighs.
19:52:04 <zell> is ghci just a haskell browser?
19:52:34 <zell> can i get ghci to run in the browser?
19:52:40 <zell> do i have to compile to js?
19:52:53 <zell> (compile ghc to js)
19:52:56 <geekosaur> you'd have to use ghcjs, yes. 
19:54:07 <zell> if ghcjs exists can i download ghc compiled to js?
19:54:18 <zell> or can i compile ghc to js myself?
19:54:41 <geekosaur> ghcjs is still a bit scary to install >.>
19:55:00 <zell> can i install it in the browser?
19:55:02 <geekosaur> I suspect you could use it to compile itself to js, but I wouldn't want to wait around for the result to run
19:56:05 <geekosaur> some things optimize fairly well. others... not so much. and ghcjs goes for exact behavior instead of faster but less exact JS equivalents
19:56:22 <zell> i was using gpu accelerated web graphics that could combine server and client processing power to deliver realtime fluid simulation, but your saying its not fast enough to compile?
19:56:56 <geekosaur> I understood "compile ghc to js" as compiling the ghc source code itself
19:57:03 <systemfault> Same
19:57:23 <geekosaur> which may not have been what you meant, but then I'm a bit confused by your direction anyway
19:57:33 <zell> oh, your saying i couldnt use my "on browser ghc" copiled to js to compile anything usefull..
19:57:46 <zell> compiled*
19:58:22 <geekosaur> ghc's bytrecode backend is not really intended for anything but debugging and would need a fair amount of work and maturation to be suitable as the basis for a browser the way javascript is used
19:58:41 <geekosaur> I;m saying compiling ghc to js and using the resulting ghc-in-javascript would be slow
19:58:55 <zell> (this all stems from requiring a vm to use haskell on android, the browser seems a reasonably portable solution)
19:59:18 <zell> i dont mid if the compilation is slow
19:59:26 <zell> so long as the compiled programs are fast
19:59:47 <zell> i guess they wouldnt be if they were compiling to js for the reason you described
19:59:53 <geekosaur> since ghc is self-bootstrapping, therefore is written in its own dialect of haskell and needs to be built with a current or previous ghc version (or if you go back to version 1.x, lazy ml --- but then you have to find that, good luck)
20:01:01 <geekosaur> I am not talking about using ghcjs to compile arbitrary haskell, although that will also result in somewhat slow code because it wants to be exact. (but it will exactly do almost everything that ghc to native will do. including concurrency, STM, etc.)
20:02:00 <zell> would it be worth doing this before continuing the discussion or can it be seen to be a good thing to do at this point?
20:02:47 * geekosaur thinks one of us is just confused at this point
20:02:57 <zell> as i understand it, a browser is a javascript enabled html compiler, is that correct? do we have such a thing written in haskell?
20:03:35 <geekosaur> that's a ... rather massive oversimplification
20:04:02 <zell> forgive me
20:04:17 <geekosaur> it's theoretically correct, but the gap between theory and practice includes a web filled with malformed and incorrect html with random extensions and plugins
20:04:27 <zell> i just need to make sure im not completely understanding it wrong
20:05:06 <zell> i guess i only need a browser capable of running ghc-in-javascript
20:05:15 <zell> so not so many extensions needed
20:06:07 <geekosaur> you'll have trouble finding that too, unless you mean something along the lines of tryhaskell.org
20:06:08 <pavonia> Why do you need such a thing? What are you trying to do?
20:06:09 <zell> seems like its fixed by the type of js that can be output from ghc2js
20:07:47 <zell> pavonia, once i can compile and run a browser written in haskell on a browser written in haskell i will be satisfied
20:08:10 <zell> then i can run haskell on android in such a browser
20:08:28 <zell> with some care around the fixed point
20:09:24 <zell> eg, using a third party browser just one time, to compile the haskell browser that compiles and runs in itself
20:09:54 <zell> if thats not confused
20:10:16 <zell> i could do with some help ironing out the details
20:10:27 <zell> but it seems all the components exist
20:11:04 <hoppfull> Hey. I'm following a "tutorial" which has some code in it and I am trying to figure out how it works. However in the code the import statements are omitted so I don't know which modules I need. I've slooooowly managed to figure out all of them and now there is just one left and I'm thinking there's got to be some simple way of doing this.
20:11:14 <hoppfull> How can I find which module a function belongs to?
20:11:44 <geekosaur> @where hayoo
20:11:45 <lambdabot> http://hayoo.fh-wedel.de/ -- See also Hoogle: http://haskell.org/hoogle http://fpcomplete.com/hoogle
20:11:46 <pavonia> There's hayoo
20:12:32 <zell> i guess the point is that ghc compiles itself, so i need to run ghc inorder to obtain it, so i cant get one for android, if there was a browser version i could run that on android
20:12:34 <systemfault> The names are lame :(
20:12:53 <zell> seems like i need a kind of emulator running an android and the browser is that
20:12:56 <systemfault> What about Hing? And Huck Huck Go?
20:13:16 <systemfault> Ask Heeves?
20:16:28 <hoppfull> Thanks guys
20:18:12 <zell> does that make sense pavonia?
20:18:34 <pavonia> Not to me
20:19:39 <zell> haskell does not run on android but browsers do, so if i could compile in browser i could compile on android
20:20:37 <pavonia> You already can compile Haskell program to JS with ghcjs
20:21:07 <zell> not on android
20:22:38 <pavonia> Okay, but the programs run on androd too?
20:22:53 <zell> they should be compiled to js
20:23:19 <zell> so that they too can run in the browser
20:23:43 <zell> compiling programs written in haskell on the browser to run on the browser
20:24:03 <zell> motivating compiling ghc to js using ghcjs so that ghcjs can be used on ghc-in-browser
20:24:53 <zell> i guess then they will always basically be running in a third party browser
20:27:19 <zell> but conceivably, a browser written in haskell could be included in a library in a java program compiled to apk, which is the thing which the browser solution avoids having to do each time
20:27:33 <grantwu> As I understand it, quickcheck randomly generates test cases that are values of the type you specify, and then checks that the result of applying a function to the test cases satisfies a supplied property
20:28:12 <grantwu> Is there a way to just get the random generated test case thing?
20:28:48 <shachaf> Sure.
20:28:49 <zell> supply a function which emits the data
20:28:58 <zell> ?
20:29:36 <grantwu> er... I make the function under test something that prints the data?
20:29:39 <grantwu> that sounds like a hack
20:30:03 <zell> you could load it into an TVar
20:30:14 <zell> ont less hacky though
20:30:17 <zell> not*
20:30:19 <zell> probably there s something built in, im not familiar with it
20:30:24 <Gurkenglas> After inlining, all cases of a case statement might begin with the same constructor. How much sense would it make to pull it out of the case statement in that case, making the whole thing more lazy?
20:30:32 <shachaf> No, you use https://hackage.haskell.org/package/QuickCheck/docs/Test-QuickCheck-Arbitrary.html
20:33:23 <Gurkenglas> (That is, making the compiler pull out that constructor)
20:33:43 <geekosaur> Gurkenglas, I think that's only safe (ironically) when the expression is strict? otherwise you may be moving a bottom in a way that makes an expression not as defined as it would have been?
20:34:27 <Gurkenglas> Couldn't that only possibly make expressions more defined than they were?
20:35:38 <geekosaur> no because you may be lifting a bottom out to before case discrimination forces it instead of after, so more possible paths can be affected by a bottom that got propagated instead of forced by the case discriminator. I think
20:35:49 <zell> the arbitrary class seems pretty high level, why is it bundled up in quickcheck?
20:36:05 <lpaste> xuxu pasted “lambdabot installation failure” at http://lpaste.net/199154
20:36:07 <geekosaur> from what I've seen, this stuff gets tricky and there've been numerous ghc and library bugs from getting it wrong
20:36:26 <xuxu> upgrading to 8.0.1 broke my lambdabot integration with ghci
20:36:49 <xuxu> and now cabal install lambdabot is failing. how might i resolve this?
20:37:06 <glguy> xuxu: Read the first 2 lines of the error message to start
20:37:07 <geekosaur> xuxu, I know the lambdabot in here was updated but I suspect a new release was not made; you probably need to install it from git
20:37:19 <geekosaur> and yes, that message could matter as well
20:38:05 <grantwu> How do I randomly choose a data constructor for an ADT?
20:38:12 <grantwu> (with QuickCheck.Arbitrary)
20:38:39 <geekosaur> zell, it's kinda specific to QuickCheck insofar as it has QuickCheck Property-s interwoven with it (see the (==>) operator, for one)
20:38:50 <shachaf> I don't think Arbitrary will just choose a constructor. It'll choose the whole thing. Won't it?
20:39:07 <grantwu> Er, what I meant is that
20:39:09 <xuxu> cabal update did it
20:39:22 <shachaf> If you say what you're trying to accomplish people might be able to help you.
20:39:33 <grantwu> I want to generate some random values of a type, so I need to make the type an instance of Arbitrary
20:39:36 <shachaf> Even people who know Haskell but don't know the details of QuickCheck off-hand, which is most of this channel.
20:40:45 <zell> you could map over a list of results from System.Random?
20:40:57 <shachaf> First you wanted to get random values of a type using QuickCheck. But now the type doesn't have an Arbitrary instance, so you need to generate random values yourself anyway?
20:41:32 <grantwu> Well, doesn't QuickCheck supply helper functions for generating random values yourself?
20:41:58 <Clint> yes
20:42:11 <grantwu> I think oneof is what I wanted
20:42:19 <Clint> elements is useful too
20:43:02 <hoppfull> Do I even need FRP? It seems too complicated to implement and the modules available seem poorly documented.
20:44:37 <zell> elements uses choose
20:44:38 <zell> choose rng = MkGen (\r _ -> let (x,_) = randomR rng r in x)
20:45:20 <Clint> choose and frequency are also useful
20:46:01 <zell> frequency -- | Chooses one of the given generators, with a weighted random distribution.
20:46:05 <zell> nice
20:46:47 <zell> check out; https://hackage.haskell.org/package/hs-carbon-0.1.1.0/candidate/docs/Control-Monad-MonteCarlo.html
20:49:51 <zell> the class Result looks like a Monoid with a constructor, which is a nice thing
20:52:28 <zell> it uses an associated type as opposed to a type parameter as in Alt
20:53:43 <kurt11> Can anyone write a function to calculate `[0, x^1, x^2, ..., x^n]` reasonably efficiently in clean, reasonably idiomatic Haskell?
20:54:30 <glguy> If you meant: [x^0, x^1, x^2... there is  iterate (*x) 1
20:54:41 <kurt11> ... given `x` and `n` as integral function parameters
20:55:12 <shachaf> conal was talking about parallelizing that
20:55:29 <zell> there is no faster way than iterate (*x) ?
20:55:31 <shachaf> He suggested his general parallel-scan code, but I bet you could do something more efficient for this case using repeated squaring.
20:55:31 <kurt11> @glguy: wow! thanks!
20:55:32 <lambdabot> Unknown command, try @list
20:55:43 <shachaf> But anyway that's not relevant here.
20:56:03 <grantwu> that doesn't use repeated squaring
20:56:08 <kurt11> `take n $ iterate (*x) 1` works perfectly!
20:56:15 <grantwu> But that depends on your definition of reasonably efficient
20:56:35 <shachaf> grantwu: Yes, but you don't need it here because you're producing every power anyway.
20:56:49 <shachaf> But if you were doing it in parallel maybe you could use it.
20:56:50 <grantwu> Oh right.  Silly me.
20:57:56 <zell> baically its (sum  . (repeat (n^m)))
20:58:14 <shachaf> Actually computing it in parallel is an interesting question.
20:58:19 <zell> replicate not repeat sorry
20:58:47 <zell> you couldnt parellelise that its completely linear
20:59:16 <zell> right?
20:59:17 <grantwu> yes, you can, with repeated squaring
20:59:26 <zell> which is...
20:59:41 <grantwu> you can compute x^n as x^(n/2)^2
20:59:47 <grantwu> (obviously you have to deal with odd powers)
21:00:05 <shachaf> But I guess even without that you can do it with linear work and logarithmic depth.
21:00:07 <grantwu> this ends up taking log_2 n squares, I think
21:00:22 <shachaf> So this can give you better than logarithmic depth maybe?
21:00:41 <shachaf> O(log log n)?
21:00:42 <zell> but it still just generates a big list of numbers interspersed with *
21:01:11 <zell> oh, no i get it
21:01:14 <zell> cool
21:01:15 <hoppfull> what about [x, x, x ... x] * [1, x, x ... x] * [1, 1, x ... x] * [1, 1, 1 ... x] * ... * [1, 1, 1 ... x]?
21:02:29 <zell> why are you inserting infinitely many identity operations?
21:03:46 <hoppfull> That isn't haskell. I'm sure you can omit the identity operations somehow.
21:03:54 <zell> ((x*x)*(x*x))*((x*x)*(x*x)) etc
21:05:34 <zell> how do you call that, its like it has reduced representation using fewer symbol renamings
21:06:44 <zell> let (x2,x4,x8,x16 ..) = (x*x,x2*x2,x4*x4 ..)
21:08:34 * hackagebot indexed 0.1.1 - Haskell98 indexed functors, monads, comonads  https://hackage.haskell.org/package/indexed-0.1.1 (ReinerPope)
21:11:16 <zell> you can do that with functions too i guess, its related to krohn rhods theorom of prime decomposition of finite automate, id love some expansion on that
21:11:23 <shachaf> Maybe it's not log log n
21:12:07 <zell> i guess lambda calculus arose around the time of the study of the factorisation of programs
21:12:45 <shachaf> Do you really not get anything asymptotically for computing scanl of replicate, as opposed to scanl of an arbitrary list?
21:12:55 <zell> https://en.wikipedia.org/wiki/Krohn%E2%80%93Rhodes_theory
21:13:31 <zell> very well phrased shachaf
21:14:38 <zell> the operator must be associative of course
21:15:57 <zell> i guess you would have to inform the compiler to check against all decompositions to see if it had already computed one
21:16:26 <zell> i wouldnt expect it to do so automatically as that seems costly
21:17:33 <zell> maybe this direction could be supplied using a better unfold than replicate
21:18:56 <zell> replicate' 1 = [[1],[[1],[1]],[[[1],[1]],[[1],[1]]],[[[[1],[1]],[[1],[1]]],[[[1],[1]],[[1],[1]]]]...
21:19:11 <zell> please dont strain your eyes
21:23:06 <zell> i suppose even you could make a datatype which represents infinite repeated elements specifically to be folded using this square decomposition thing.
21:23:21 <zell> i bet that exists already
21:24:10 <zell> anyone?
21:42:25 <nshepperd_> if you do exponentiation by squaring, then 'drop n . scanl f z . repeat x' does get faster I guess
21:43:09 <nshepperd_> apart from the fact that you still have to traverse the front of the list's spine
21:47:55 <nshepperd_> you can produce a lazy patricia tree for all indexes
21:53:58 <zell> does it?
21:54:44 <zell> i find https://hackage.haskell.org/package/containers-0.5.8.1/docs/Data-IntMap-Lazy.html
21:55:45 <c_wraith> zell: https://hackage.haskell.org/package/data-inttrie-0.1.2/docs/Data-IntTrie.html
21:59:07 <zell> a lazy int labeled search tree?
21:59:26 <c_wraith> yep
21:59:36 <c_wraith> very handy for memoization problems
21:59:57 <zell> is it a radix trie?
22:00:11 <nshepperd_> it is a radix tree, yeah
22:00:28 <nshepperd_> trie. i forget what the difference is there
22:00:56 <zell> compact prefix tree (as they can be searched by prefixes)
22:01:10 <c_wraith> Data.IntTrie can't be compact - it has elements at every node.
22:01:15 <zell> i have never come accross them im just finding this on wiki
22:01:25 <c_wraith> or at least, not compact in the way patricia tries are
22:01:50 <nshepperd_> I was thinking you could use the tree to cache intermediate squares, but I'm now not sure that's actually useful
22:02:12 <zell> wiki says "a radix tree (also radix trie or compact prefix tree)" which seems wring given what you say
22:02:53 <zell> https://en.wikipedia.org/wiki/Radix_tree
22:03:00 <zell> i couldnt find what was a particia tree
22:03:29 <zell> it says a radix tree is a space-optimized trie in which each node that is the only child is merged with its parent.
22:03:53 --- mode: ChanServ set +o geekosaur
22:03:58 --- mode: geekosaur set -q *!*@107-147-70-64.res.bhn.net
22:04:06 --- mode: geekosaur set -o geekosaur
22:07:19 <zell> apparently Data.Map uses "Binary search trees of bounded balance"
22:08:13 <c_wraith> ah.  Radix trie is the more modern name for Patricia trie.  The original paper describing the Patricia Trie named it after the author's dauther.
22:08:36 * hackagebot indexed 0.1.2 - Haskell98 indexed functors, monads, comonads  https://hackage.haskell.org/package/indexed-0.1.2 (ReinerPope)
22:11:01 <zell> if every element is accessed i cant see how it could be faster than lists and let
22:12:28 <c_wraith> It gives faster random access
22:12:45 <zell> good for monte carlo
22:12:56 <zell> nice
22:13:09 <geekosaur> it's not going to be faster if all you do is in-order access of all elements, no. but for pretty much anything else it'll be faster
22:13:09 <c_wraith> It also doesn't allocate for paths that are never visited
22:14:14 <zell> its a shame i cant use my zipper though, that would be hard to unroll to a random position quickly
22:15:22 <zell> its just Free (\a->([a],[a]))
22:16:02 <zell> navigated eg ((fmap.fmap.fmap) forwards)
22:16:41 <zell> which gives speed increase on locally granular problems
22:16:58 <zell> such as eg paintbucket tool
22:17:10 <zell> of ms paint
22:19:17 <zell> the zipper must be refreshed after traversal, i dont know if this allocates for unviseted paths, does it?
22:19:34 <zell> i think thats a pretty big question for me
22:19:46 <zell> i think its lazy but i cant really show it
22:22:36 <ashishnegi> hi.. i am printing logs from two threads on stdout.. should not logs get mangled ? i find that each is printing on new line cleanly.. everytime i run. i am using `purStrLn` and `async` in code.
22:28:13 <zell> The main additional functionality provided by async is the ability to wait for the return value of a thread
22:28:40 <zell> https://hackage.haskell.org/package/async-2.1.0/docs/Control-Concurrent-Async.html
22:29:40 <geekosaur> ashishnegi, if you are always using putStrLn and stdout is always a terminal, line buffering is probably making it work "accidentally"
22:29:54 <geekosaur> (get enough concurrent writers and it will break down)
22:30:15 <geekosaur> if you redirect output to a file, you'll likely find interleaving happening
22:31:39 <koz_> I'm looking for a data structure X which stores unique strings, and can quickly determine if a given string s is a substring of any string stored in X. Can someone suggest something?
22:32:17 <ashishnegi> geekosaur:  uh.. ok.. i was afraid thinking that one thread runs at a time.. also had a doubt after i read "The downside of having lightweight threads is that only one can run at a time," at http://hackage.haskell.org/package/base-4.9.0.0/docs/Control-Concurrent.html#v:mkWeakThreadId.. ofcourse it was ok after i read full text. :P thanks
22:32:48 <ashishnegi> koz_: tries.. right ?
22:33:33 <pavonia> Aren't tries only for prefixes?
22:33:59 <koz_> ashishnegi: pavonia has it right - tries are for prefixes, but I need general substring checks.
22:34:13 <nshepperd_> koz_: suffix tree, I think
22:34:31 <ashishnegi> pavonia: you can store all substrings on a string.. and then there are efficient way to do that too.
22:34:31 <koz_> nshepperd_: Suffix trees are to store *one* string; I need a collection of unique strings.
22:34:44 <koz_> Is there some kind of suffix trie?
22:34:51 <koz_> (like, I really need both of those at the same time)
22:35:23 <zell> (\a-> zipn a (tail a)..)
22:35:40 <ashishnegi> koz_: one way is "ternary search tree" https://en.wikipedia.org/wiki/Ternary_search_tree
22:36:08 <nshepperd_> koz_: you can fake it by concatenating all your string with a new symbol in between
22:36:26 <koz_> nshepperd_: That's ... brilliant, thank you!
22:36:38 <koz_> I'll check out both your solution and ashishnegi's. Thanks!
22:37:28 <ashishnegi> koz_: nshepperd_ i also like your method :) :+1:
22:37:30 <zell> nshepperd_ like sequence?
22:37:37 <nshepperd_> there's probably some more principled way to union suffix trees or something but that would a bit of a project to figure out I think :)
22:38:36 <ashishnegi> nshepperd_: second thought, but searching on concatenation would be inefficient.. no ?? 
22:39:23 <zell> how else are you traversing your datastructure?
22:40:38 <nshepperd_> ashishnegi: I don't think so. searching for a string of length m is O(m) regardless of tree size, apparently
22:42:00 <koz_> nshepperd_: This also seems relevant: https://en.wikipedia.org/wiki/Generalized_suffix_tree
22:42:17 <zell> what are you referring to nshepperd_?
22:42:43 <ashishnegi> nshepperd_: are you talking about Knuth-Morris-Prath or similar algos.. if koz_ you have fixed known before hand strings.. then it is good.. otherwise i do not know of incrementally building as new strings come in..
22:43:16 <koz_> ashishnegi: Not exactly. I'm fishing for ideas at the moment - I'll see how these suggestions fit with what I'm doing and where they lead me.
22:43:24 <koz_> To use detective-speak, I'm hunting for leads.
22:44:11 <zell> i find http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.130.8022&rep=rep1&type=pdf
22:46:07 <ashishnegi> koz_: i am re reading Ternary serach tree and they are so simple to code and so efficient.. feel free to ping me if they interest you. they support search for "up" if you gave it "cup".. your case.
22:46:54 <zell> you should inspire us with your current musings
22:49:01 <koz_> zell: Yeah, I was *just* about to search for that! ashishnegi: Do you have a good reference describing ternary search trees? The Wikipedia article is a bit threadbare.
22:49:39 <zell> from what i can gather its a case of transforming trees tuned to various orderings, (conformations)
22:51:16 <zell> i guess you should not hold them all
22:51:53 <zell> is that about right?
22:52:41 <ashishnegi> koz_: they are just like binary tree; but apart from left and right.. there is middle pointer as well. middle is used for going to next-char if you matched.. also each node stores a bool.. where it is a end of some word or not
22:52:56 <ashishnegi> try http://www.geeksforgeeks.org/ternary-search-tree/
22:54:36 <zell> a :: (a,a,a,Bool)?
22:55:00 <nshepperd_> sounds like it's roughly isomorphic to a trie where you replace the array with a binary search tree
22:56:03 <zell> data T a :: T (T a) (T a) (T a) Bool
22:57:13 <ashishnegi> koz_: but it looks like you would have to insert each substring for each word.. not sure about that.. please have a look.
22:57:37 <nshepperd_> data TernaryLike c = T Bool (Map c (TernaryLike c))
22:57:43 <nshepperd_> assuming the alphabet is c
22:59:43 <nshepperd_> except the Map is sorta inlined
23:04:15 <zell> you mean the ternary search tree for eg cats,bug,cats,up,ug
23:05:01 <zell> ?
23:07:21 <nshepperd_> the example on that page is a search tree for {cats, bug, up}
23:08:37 * hackagebot spy 0.14 - A compact file system watcher for Mac OS X, Linux and Windows  https://hackage.haskell.org/package/spy-0.14 (StefanSaasen)
23:08:39 * hackagebot indexed 0.1.3 - Haskell98 indexed functors, monads, comonads  https://hackage.haskell.org/package/indexed-0.1.3 (ReinerPope)
23:08:51 <zell> i thought the problem was extending to match on suffix
23:09:41 <zell> and the question was basically do you have to just fill it up with tails
23:09:54 <zell> but i ould be way off
23:09:58 <zell> could
23:10:36 <luffy`> i'm new to haskell
23:10:46 <luffy`> can you recommend some books?
23:10:52 <luffy`> for me to read and practice?
23:11:01 <zell> rubber fruit guide to functional programming
23:12:07 <l3vel> is there a big difference between Yesod and Snap? I'm looking to use one for a small project.
23:15:28 <zell> i never got either to work
23:15:47 <zell> had to use port forwarding with nginx
23:16:06 <glguy> I prefer Snap for my own projects
23:16:49 <l3vel> Thanks
23:21:47 <joe9> c_wraith: just wanted to let you know that the performance went from 13 seconds to 500 ms after switching to unboxed vectors.
23:22:39 <zell> whats that?
23:23:46 <zell> i find; unboxed vectors of pairs are represented as pairs of unboxed vectors.
23:25:44 <zell> cant make sense of the source or documentation
23:26:37 <joe9> zell: What do you want to know? about unboxed vectors?
23:27:02 <zell> how they work and why thats fast
23:27:52 <fengshaun> rubber fruit guide? wut?
23:27:58 <ashishnegi> koz_: it looks like ternary-trees are not for "substring" finding.. it would be difficult to get back word(s) from a substring.. variant of them are also not working. apologizes if this wasted your time.
23:29:39 <zell> fengshaun, luffi ate a evil fruit which turned him into a rubber man
23:29:48 <zell> devil fruit*
23:29:49 <joe9> zell: https://github.com/joe9/judytest
23:30:31 <luffy`> zell: i tried to search that and didn't find anything
23:30:58 <zell> lol
23:31:08 <zell> craft of functional programming
23:31:24 <zell> but you probably want to use web resources
23:32:06 <joe9> zell: check out https://www.schoolofhaskell.com/user/commercial/content/vector the efficiency section
23:32:08 <zell> or just try to write simple programs
23:32:39 <luffy`> zell: thanks. i'll look on to that
23:33:18 <joe9> zell: another good article: https://wiki.haskell.org/Numeric_Haskell:_A_Vector_Tutorial
23:33:35 <zell> so the upshot is if i use vector i get fast mutability?
23:33:39 <ggole_> Switching arrays of records to records of arrays is a well known trick
23:34:17 <zell> that makes sense ggole_ thanks
23:34:37 <zell> it puts this direction into perspective well
23:35:00 <ggole_> The advantage is that elements of the same type are packed closely in memory, which is both friendly to caches and SIMD instructions
23:35:16 <zell> omg
23:36:19 <ggole_> And it avoids polluting the cache with elements of the record that are not being used in the current traversal, too.
23:36:33 <koz_> ashishnegi: Nothing to apologize for - it helps me to know this stuff anyway! I enjoy data structures.
23:36:53 <zell> my basic approach was to use a tree zipper
23:37:16 <zell> which took advantage of granular subtraversals in my algorithm
23:37:44 <zell> the Map approach does not as far as i can tell
23:38:39 <zell> https://hackage.haskell.org/package/repa-3.4.1.1/docs/Data-Array-Repa-Stencil.html
23:38:48 <joe9> c_wraith: now, it is hovering around 60 milliseconds per drawing. Thanks a lot again.
23:43:16 <ashishnegi> koz_: do you need all words that match a substring ?
23:49:35 <zell> my problem arises when the words are not strings but nd lists. the problem is that the corners overlap a lot for large stencils and even worse for high dimensions
23:49:39 <ashishnegi> koz_: "suffix-array" https://en.wikipedia.org/wiki/Suffix_array is another DS that you can use .. concatenate all strings with special character and end with another special character, build "suffix-array" and then all occurances of a substring can give you all words having that substring in O(log n) where n is length of concatenated string.
23:54:33 <zell> the speed from the ternary search tree seems to come from the small alphabet. if the text is smaller than the alphabet it may be unlikely that many of the words have the same prefix, in which case its better to simply traverse the text and match locally
23:55:28 <zell> in numerical simulations, this would be the same as replacing Char for Double and expecting that it is very unlikely that any two values are the same
23:56:11 <zell> so that operations over local index in a subtraversal become a priority
23:57:45 <zell> drawing contours on an ordinance survey map by traversing locally path of constant gradient
