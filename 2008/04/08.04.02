00:12:27 <cjs> One thing I miss about ruby was adding methods to Integer to make it "read" like units, e.g., "7.seconds; 300.milliseconds". Is there any way to do that in Haskell?
00:12:51 <dmwit> Not really.  There are some type-system based units packages, though.
00:13:28 <cjs> Hm. We need functions where you put the arguments on the left.
00:14:17 <Cale> cjs: Of course, you can write functions to do that, or write your own type which keeps track of units.
00:14:57 <cjs> Well, e.g., now I do "threadDelay $ fromSeconds 3.5", but it's not quite as clean.
00:20:37 <cjs> In FP I seem to struggle a bit more to keep argument lists from getting out of hand, as compared to OO.
00:22:10 <Cale> cjs: Well, you're expressing the dependence of the result on the inputs. In OO, there tends to be a lot of implicit inputs which arrive from side effects.
00:22:11 <dmwit> Time for a new data type, maybe?
00:22:27 <Cale> You can construct datatypes to bundle things together, yeah.
00:23:17 <cjs> Hm. When I think about it that way, I tend to think I wasn't using all that many side effects in OO, because FP doesn't feel all that much worse.
00:24:05 <Cale> Well, there are output effects too :)
00:24:59 <Cale> But yeah, the way that OO emphasises structuring data, you can copy that. So define types which have a bunch of fields that bundle together related data, so you don't end up passing that as separate parameters, for instance.
00:26:10 <cjs> Yeah, I've been working on that a bit. And the named fields very much parallel the way I deal with value (immutable) objects in OO.
00:26:58 <Cale> You can even copy the polymorphism in OO languages a bit with existential types. (though not really the subtyping so much)
00:27:13 <Cale> Or you can just use typeclasses of course.
00:27:40 <cjs> Or just pass around different functions that take the same arguments.
00:27:49 <Cale> yeah
00:28:40 <Cale> (which is basically how existential types work behind the scenes anyway)
00:32:22 <quicksilver> many people leap straight to typeclasses to solve a problem when higher order functions would be clearer and more flexible.
00:32:45 <quicksilver> (because, possibly?, the typeclasses remind them of the way they would solve it in OO)
00:34:02 <Cale> Well, typeclasses are actually a pretty nice way to structure ideas as well.
00:34:16 <Twey> quicksilver: Example?
00:34:41 <Cale> But yeah, you have to weigh that against the cost in flexibility (since you can't define more than one instance per type)
00:34:50 <quicksilver> Data.Binary
00:35:01 <quicksilver> (not that I'm really criticising the decisions the authors took there)
00:35:06 <quicksilver> but it is an instructive example.
00:35:16 <quicksilver> it might be much nicer just to have pairs of (encode,decode) functions
00:35:20 <quicksilver> rather than a typeclass
00:35:38 <Twey> Hmm
00:35:40 <quicksilver> since a typeclass restricts you to one encod/decod'ing per type.
00:36:09 <quicksilver> if module A wants a special way to encode [Int], and module B wants another, they can't coexist in the same program
00:36:13 <quicksilver> well, they have to use newtypes.
00:36:17 <quicksilver> which is fine, I guess.
00:36:30 <quicksilver> but newtypes is a solution to a problem which needn't have been there in the first place :)
00:36:53 <quicksilver> the point about typeclasses is they *are* just a bundle of higher order functions.
00:36:54 <cjs> Given a list of String -> IO () functions, how do I sequence in that order the applications of them to string "s"?
00:36:57 <solrize_> is there a speed hit from using a typeclass?
00:37:21 <quicksilver> but you allow the compiler to choose which bundle, based only on the type
00:37:28 <quicksilver> which is conveninent, when that's what you want
00:37:31 <quicksilver> but restrictive, when it isn't.
00:37:49 <quicksilver> and IMO people use it when it isn't what they really want, simply because they are unfamiliar with the 'simpler' alternative.
00:37:55 <quicksilver> solrize_: no, not in this context.
00:38:26 <quicksilver> (there is an overhead to using a typeclass over a specialised function. There isn't an overhead to using a typeclass compared to bundleing the higher-order-functions by hand)
00:39:35 <quicksilver> cjs: mapM ($s) functions
00:39:41 <quicksilver> or mapM_, even.
00:40:08 <quicksilver> Cale would do it using sequence in the (->) monad followed by sequence_ in the IO monad though.
00:40:15 <Cale> heh
00:41:01 <quicksilver> sequnece_ (sequence functions s)
00:41:04 <quicksilver> (is that right?)
00:41:09 <Cale> :t sequence (sequence [putStrLn] "hello")
00:41:10 <lambdabot> IO [()]
00:41:14 <Cale> :t sequence_ (sequence [putStrLn] "hello")
00:41:14 <cjs> Oh, right, *that* use of $.
00:41:15 <lambdabot> IO ()
00:41:16 <quicksilver> modulo spelling.
00:41:53 <quicksilver> the mapM_ way is shorter and clearer, but it's nice to be aware of sequence in the (->) monad.
00:42:06 <quicksilver> for slightly different versions of the question it's exactly what you want.
00:43:52 <cjs> Actually, what I was really looking for was the basic idiom "map ($ 5) [(*2), (*3), (*4)]"
00:44:05 * quicksilver nods
00:44:29 <quicksilver> a.k.a. sequence [(*2),(*3),(*4)] 5
00:44:44 <dmwit> sequence (map (*) [2..4]) 5
00:45:05 <dmwit> ;-)
00:45:33 <dmwit> But yeah, that doesn't really generalize much.
00:48:49 <cjs> So "sequence_ (map ($ response) handlers)" is "mapM ($ response) handlers"?
00:49:12 <quicksilver> mapm_
00:49:25 <quicksilver> sequence_ (map  is mapM_
00:49:32 <quicksilver> and ditto without the underscore
00:49:35 <cjs> Ah.
00:49:46 <cjs> Hm. mapM worked for me. Of course, I have only one handler....
00:49:47 <quicksilver> just indicates whether you collect results or discard
00:50:43 <Baughn> > sequence [(*2),(*3),(*4)] 5
00:50:44 <lambdabot>  [10,15,20]
00:51:14 <haraldk> :t sequence
00:51:15 <lambdabot> forall (m :: * -> *) a. (Monad m) => [m a] -> m [a]
00:52:04 <cjs> Actually, "sequence [(*2),(*3),(*4)] 5" gives me an error in my ghci.
00:52:16 <cjs> "No instance for (Monad ((->) a))"
00:52:22 <Cale> cjs: Yeah, the instance isn't in the Prelude, sadly
00:52:33 <Cale> cjs: You have to import Control.Monad.Instances
00:52:38 <Cale> or Control.Monad.Reader
00:52:51 <haraldk> mhh
00:55:45 <cjs> I'm kinda feeling like I'm getting the hang of the "create actions and at some point put them into a sequence" style of I/O.
00:56:55 <opqdonut> :9
00:59:11 <haraldk> :t (->)
00:59:12 <lambdabot> parse error on input `->'
00:59:20 <Cale> :k (->)
00:59:21 <lambdabot> ?? -> ? -> *
00:59:30 <haraldk> of course :-)
01:00:07 <Cale> You can largely think of it as having kind  * -> * -> *  though
01:00:26 <Cale> The ?? and ? kinds have to do with unboxed types
01:00:49 <Cale> So it takes two type parameters and produces a type
01:01:02 <Cale> Or one type parameter and produces a type constructor of kind * -> *
01:01:27 <haraldk> :k (int ->)
01:01:27 <lambdabot> parse error on input `)'
01:01:40 <Cale> :k (->) Integer
01:01:41 <lambdabot> ? -> *
01:01:57 <Cale> Sadly, types don't support section syntax
01:02:05 <Cale> :k (Integer ->)
01:02:06 <lambdabot> parse error on input `)'
01:02:28 <Cale> :k (->) Integer String
01:02:29 <lambdabot> *
01:02:36 <Cale> :k Either
01:02:37 <lambdabot> * -> * -> *
01:02:46 <Cale> :k Either Integer
01:02:47 <lambdabot> * -> *
01:02:51 <Cale> :k Either Integer String
01:02:52 <lambdabot> *
01:03:03 <Cale> :t Control.Monad.RWS.RWST
01:03:04 <lambdabot> forall r s (m :: * -> *) a w. (r -> s -> m (a, s, w)) -> RWST r w s m a
01:03:06 <Cale> :k Control.Monad.RWS.RWST
01:03:07 <lambdabot> * -> * -> * -> (* -> *) -> * -> *
01:03:14 <haraldk> :k (->) ((->) Integer)
01:03:15 <lambdabot>     `(->) Integer' is not applied to enough type arguments
01:03:15 <lambdabot>     Expected kind `??', but `(->) Integer' has kind `? -> *'
01:05:36 <haraldk> :k (->) Integer [String]
01:05:37 <lambdabot> *
01:05:46 <Cale> :k []
01:05:47 <lambdabot> * -> *
01:06:12 <Cale> :t StateT
01:06:13 <lambdabot> forall s (m :: * -> *) a. (s -> m (a, s)) -> StateT s m a
01:06:14 <Cale> :k StateT
01:06:15 <lambdabot> * -> (* -> *) -> * -> *
01:06:20 <Cale> :k StateT Integer
01:06:21 <lambdabot> (* -> *) -> * -> *
01:06:29 <Cale> :k StateT Integer ((->) Integer)
01:06:30 <lambdabot> * -> *
01:06:33 <Cale> :k StateT Integer ((->) Integer) Integer
01:06:34 <lambdabot> *
01:07:21 <solrize_> :k Int
01:07:22 <lambdabot> *
01:07:53 <solrize_> :k (forall a). a -> b
01:07:54 <lambdabot> parse error on input `)'
01:08:05 <solrize_> :k forall a. a -> b
01:08:06 <lambdabot> Not in scope: type variable `b'
01:08:10 <solrize_> :k forall a. a -> a
01:08:11 <lambdabot> *
01:08:20 <solrize_> :k exists a. a -> a
01:08:21 <lambdabot> parse error on input `.'
01:12:07 <solrize_> hey cale is it a wart that unsafecoerce can exist at all?  like what if the haskell implementation starts using more tagged words so you can't always convert one type into another?
01:13:05 <Cale> Yes, it is indeed a serious wart that unsafeCoerce exists, and you should pretend that it doesn't and take steps to avoid the possibility that it could be written.
01:13:22 <dmwit> Well, it's got that "unsafe" bit on the beginning.
01:13:24 <Cale> (For instance, if you write certain bad instances of Typeable)
01:13:26 <dmwit> That should be a hint. ;-)
01:13:52 <Cale> It should be treated with the same gravity as modifying the compiler. You're essentially telling the type checker "trust me"
01:14:27 <Cale> "these two things really are represented the same way in the implementation"
01:14:50 <Cale> Of course, you can't know that unless you know how the implementation represents things.
01:15:40 <quicksilver> solrize_: unsafeCoerce is not haskell.
01:15:44 <quicksilver> it's a GHC primitive
01:15:53 <quicksilver> it is part of a lower-level language which GHC uses to implement haskell.
01:16:04 <quicksilver> In my opinion, that is the correct way to view it.
01:16:04 <Cale> Or unless you've somehow sneakily arranged to have the information that two types are identical by some other means (like runtime type representations)
01:16:23 <Baughn> Like Typable? ;)
01:16:29 <quicksilver> it is good that GHC provides the unsafeCoerce primitive as part of its low-level (impure) language.
01:16:48 <quicksilver> it enabled JaffaCake to write Data.Dynamic in this low-level language
01:16:51 <quicksilver> without having to resort to C.
01:17:05 <quicksilver> and then Data.Dynamic itself can be safely used from haskell.
01:17:11 <Cale> You can actually write unsafeCoerce in terms of Dynamic and/or Typeable, very unfortunately :)
01:17:25 <quicksilver> only with broken Typeable instances.
01:17:29 <Cale> right.
01:17:29 <quicksilver> which is "cheating".
01:18:02 <solrize_> well unsafePerformIO is sort of necessary
01:18:13 <solrize_> and unsafeCoerce can be written with it
01:18:15 <Cale> How so?
01:18:26 <quicksilver> unsafePerformIO is necessary as part of a low-level language for writing C bindings.
01:18:32 <quicksilver> it's not necessary as part of haskell.
01:18:50 <quicksilver> it's convenient to be able to use the syntax + conveniences of haskell, as part of this low-level impure FFI language.
01:20:17 <Cale> unsafePerformIO again should be treated with the same level of seriousness as modifying the compiler, and to know what it will really do, you actually have to know how the compiler will treat the code.
01:20:49 <solrize_> i thought that unsafePerformIO was written with the FFI rather than the other way around
01:20:53 <Cale> No.
01:21:05 <solrize_> hmm
01:21:08 <Cale> Well, it's part of the FFI.
01:21:40 <Cale> But it can't really be written in terms of anything else the FFI gives you, I'm fairly sure.
01:22:10 <solrize_> how does the runtime call "main"?
01:22:16 <quicksilver> Cale: the FFI spec gives a weak, but compiler-independent, spec for unsafePerformIO
01:22:29 <quicksilver> (IIRC)
01:22:46 <Cale> quicksilver: It would have to be pretty weak :)
01:22:49 <quicksilver> solrize_: how does the C runtime cal "main" ?
01:23:10 <Cale> I doubt it could even make any guarantees about how many times the IO action was run :)
01:23:11 <quicksilver> Cale: well basically it tells you that you can use it to convert an IO action into a pure function if you're confident it is in effect pure.
01:23:32 <solrize_> crt0.s calls it
01:23:34 <Cale> ah, I suppose you can say that, yeah
01:23:42 <quicksilver> solrize_: (I'm not sure on what level of abstraction you want an answer)
01:23:50 <Cale> unsafePerformIO (return x) == x   is a pretty safe law.
01:23:52 <quicksilver> solrize_: the GHC version of crt0.s calls it, then.
01:23:57 <quicksilver> solrize_: is that the answer you want?
01:24:29 <solrize_> um
01:24:41 <quicksilver> the pictures are quite similar to any other compiled language.
01:24:57 <quicksilver> the compiler supplies some kind of runtime shell which does some kind of OS initialisation and then 'jumps' to main.
01:25:10 <solrize_> IO is something like State Realworld
01:25:14 <solrize_> or something like that
01:25:24 <solrize_> and it calls unsafeInterleaveIO somehow
01:25:32 <Cale> hm?
01:25:34 <quicksilver> I think you're muddling your levels of abstraction.
01:25:59 <quicksilver> IO isn't really State Realworld, although the GHC representation for it *looks* like that.
01:26:06 <quicksilver> it's not, though, in quite a deep way.
01:26:13 <quicksilver> State RealWorld could duplicate the world.
01:26:18 <solrize_> anyways yeah i didn't want to delve into that, i mean unsafePerformIO is used in real programs if you want to do something conceptually pure but that's doing io under the covers
01:26:39 <quicksilver> GHC's rep of IO as State RealWorld# is another one of these "not haskell things"
01:26:49 <Baughn> It's cheating and using side-effecting functions while the monad sequencing forces the compiler to have them run in the right order (right?).
01:26:49 <quicksilver> ..written in an impure language which looks like haskell but isn't haskell...
01:27:32 <opqdonut> yeah, the ghc IO implementation is really a hack on the evaluation order :)
01:27:37 <Cale> Baughn: yeah. The trouble with it is that the inliner, say, can come along and inline something which has an unsafePerformIO in it.
01:27:48 <Cale> Baughn: Now your IO happens multiple times :)
01:28:05 <Baughn> Cale: I try to keep any uses of unsafePerformIO idempotent. ^^;
01:28:05 <Cale> Baughn: and of course, it will happen randomly out of sequence with everything
01:28:46 <quicksilver> Baughn: idempotent is one of the requirement.
01:28:53 <Cale> Basically, if you put an action inside an unsafePerformIO, you should be sure that you don't give a damn how many times that action happens, or when, or if it never happens at all.
01:28:57 <quicksilver> Baughn: don't-care-if-it-isn't-even-run-once is another
01:29:05 <quicksilver> Baughn: and don't-care-when-it-runs is the third
01:29:08 <quicksilver> (and I think that's all)
01:29:25 <opqdonut> idempotent in what sense?
01:29:31 <solrize_> so if i have a function "factorize n" which figures out the prime factors of a number
01:29:36 <Baughn> quicksilver: So.. http get?
01:29:55 <solrize_> then abstraction and modularity says i should be able to use that inside a pure function
01:30:04 <solrize_> since it's a unique function on integers
01:30:24 <solrize_> even though it might be spawning a distributed number field sieve on a 1000 node server cluster
01:30:29 <Cale> opqdonut: Running it twice is the same as running it once.
01:30:38 <solrize_> thus, unsafePerformIO
01:30:40 <Baughn> solrize_: Sure. So long as it can't /fail/ or anything..
01:30:44 <solrize_> right :)
01:30:51 <solrize_> that was sort of the next question
01:31:07 <opqdonut> Cale: ah okay
01:31:14 <Baughn> solrize_: You'd have to make any errors fatal. That may or may not be okay with you - the code /inside/ unsafePerformIO can still handle errors, so long as they stay inside.
01:31:26 <solrize_> hmm ok
01:31:32 <solrize_> yeah not much other choice
01:32:08 <quicksilver> it's quite confusing that parts of the GHC runtime are written in a language which looks like haskell but isn't.
01:32:12 <solrize_> of course could they throw out to a surrounding IO function?
01:32:20 <quicksilver> but, the needed an impure language somewhere.
01:32:33 <Baughn> solrize_: No. If you do that, you risk corrupting the rts.
01:32:37 <quicksilver> and it's ncie for them to have an impure language which has many of the useful properties of haskell.
01:32:44 <solrize_> ugh
01:32:44 <Baughn> solrize_: You could get /all/ sorts of odd results..
01:32:48 <quicksilver> Baughn: eh? I don't think you risk corrupting anything.
01:33:02 <quicksilver> generally speaking catching errors raised in pure code is considered a bit of a fugly hack.
01:33:08 <quicksilver> it breaks referential transparency
01:33:14 <quicksilver> and is probalby unwise
01:33:18 <quicksilver> but I don't think it's actually dangerous
01:33:36 <mofmog> great, hscurses has barely any documentation
01:33:50 <Cale> As if "It breaks referential transparency" isn't dangerous enough ;)
01:33:50 <Baughn> let a = unsafePerformIO foo, b = same -- if foo throws an error on /one/ of the evaluations, and you catch it, a and b will be different
01:34:12 <Baughn> But the compiler could still statically optimize "a==b" to True and such horrors
01:34:45 <Cale> Actually, that particular one will never happen.
01:34:59 <Cale> > (0/0) == (0/0)
01:35:00 <lambdabot>  False
01:35:12 <Baughn> I'm sure if I look long enough I will find something. It just doesn't feel safe.
01:35:26 <opqdonut> yeah == needs to be as strict as it's definition in the instance
01:35:27 <Cale> But yeah, it's not really safe.
01:36:11 <Baughn> That's what I mean by corruption. It may not be the heap-corruption-causing-errors-ten-minutes-later corruption of C, but it still breaks your expectations
01:36:17 <Cale> I'm not sure you can get memory corruption, but you can certainly get ridiculous behaviour. Anything which breaks referential transparency will tend to have bizarre consequences under optimisation.
01:37:10 <solrize_> well, it's IO throwing to IO, so it's already expected to not be transparent if the inner action is being run
01:37:21 <cjs> You know, sometimes using >>=, even within do notation, is more comfortable than <-.
01:37:33 <solrize_> so as long as i don't call the function a second time it's ok?
01:37:45 <Cale> cjs: =<< is even more comfortable inside do-notation
01:37:45 <Baughn> Actually, that's a good question. There's a lower layer of code stopping memory corruption (by not providing corrupting operations); it is dependent on haskell semantics being correct at all?
01:37:48 <solrize_> cjs yeah that was an epiphany for me too :)
01:38:05 <Cale> cjs: Then the arrows line up nicely :)
01:38:08 <Baughn> solrize_: After using unsafePerformIO, the compiler no longer /knows/ that
01:38:20 <solrize_> hmm
01:38:25 <Baughn> solrize_: And you can't prevent it from running the code twice. It needs to be idempotent.
01:38:53 <Baughn> solrize_: Oh, and functional. Which anything randomly throwing errors isn't.
01:39:04 <solrize_> well, the stuff inside the unsafePerformIO can remember that it was called before, since it's impure...
01:39:05 <Cale> Basically, unless your first name is Simon, don't use unsafePerformIO ;)
01:39:09 <solrize_> hehe
01:39:15 <opqdonut> :D
01:39:32 <Cale> It should really be called  unsafeSimonIO
01:39:41 <mofmog> hmm why doesnt ghc come with curses
01:39:45 <solrize_> simonize
01:39:49 <Baughn> Come, now. It isn't /that/ hard to use
01:39:51 <mofmog> or more importantly why does no one use curses with haskell
01:40:07 <vegai> is it hard to use correctly?
01:40:08 <solrize_> haskell should come with tkinter, then it can beat python ;)
01:40:11 <dmwit> I think hmp3 uses hscurses.
01:40:16 <Baughn> mofmog: Because "cabal install hscurses" is so very easy to write
01:40:28 <Cale> Baughn: well, sure. You *can* use it, but it's usually ill-advised to do so.
01:40:46 <mofmog> there simply isn't enough documentation on hscurses
01:40:51 <mofmog> i've been stumbling around for days
01:40:51 <Baughn> Cale: I am /not/ going to put decodeImage :: ByteString -> Image in IO.
01:40:54 <dmwit> So use something else?
01:41:07 <mofmog> what else is there 0_0
01:41:11 <dmwit> I don't know what, though, if it really has to be console-based.
01:41:19 <dolio> simonSaysPerformIO
01:41:19 <solrize_> hsgtk?
01:41:19 <Cale> Baughn: are you importing it from a C library?
01:41:26 <Baughn> Cale: Yes, imagemagick
01:41:37 <dmwit> solrize_: gtk2hs
01:41:48 <solrize_> dmwit, yeah, that :)
01:41:54 <Baughn> Cale: (And I've achieved a 2000x speedup over the last three days. Which would be nice if it wasn't 3000x slower than the C version to begin with.)
01:42:09 <Spockz> Who has experience with wxcore?
01:42:09 <opqdonut> :D
01:42:14 <Cale> Baughn: well, yeah, FFI bindings are an appropriate use of course :)
01:42:15 <dmwit> Oh, hey, within 1.5x of C is pretty good...
01:42:37 <Cale> (especially to pure functions like that)
01:42:43 <quicksilver> dmwit: I think it doesn't.
01:42:44 <Cale> It's when people start to use it for other things that it becomes questionable :)
01:42:44 <Baughn> dmwit: The actual program I'm using it for is now 5x faster than the C program. :D
01:42:52 <Baughn> dmwit: (Over eight cores, but still)
01:42:55 <quicksilver> dmwit: I think dons hacked up his own curses binding because he was unhappy with the existing ones.
01:42:59 <Cale> I don't even agree with using it to create top-level IORefs.
01:43:07 <solrize_> is there a way to read bytestrings on the foreign side of a "safe" ffi call?
01:43:13 <dmwit> quicksilver: ...oh =/
01:43:24 <quicksilver> solrize_: sure, by poking in the internals
01:43:32 <quicksilver> solrize_: ByteStrings are a ForeignPtr internally
01:43:42 <Baughn> solrize_: http://www.haskell.org/ghc/docs/latest/html/libraries/bytestring/Data-ByteString-Unsafe.html <-- This stuff should be about good enough
01:43:42 <quicksilver> solrize_: you can pull that out and pass it to the foreign side
01:43:43 <lambdabot> http://tinyurl.com/2mkv5f
01:43:47 <quicksilver> (where it is just a Ptr() )
01:43:53 <solrize_> quicksilver, hmm, that actually doesn't sound too bad
01:43:54 <dmwit> mofmog: The binding looks like it might be a really close translation of curses, have you tried the curses man pages for documentation?
01:44:02 <quicksilver> (I mean, where it is a void* )
01:44:06 <solrize_> cool
01:44:21 <quicksilver> you can also use withArray and unPack
01:44:24 <mofmog> i'm trying to figure out these widgets
01:44:25 <quicksilver> if you want to be "safe"
01:44:28 <quicksilver> btu slower.
01:44:36 <solrize_> do you happen to know where the default chunksize is defined?  i looked for it with no luck
01:44:48 <quicksilver> in the code for lazy bytestrings
01:44:53 <quicksilver> it's not configurable
01:45:02 <solrize_> hmm, the docs say it's ok to change it
01:45:02 <quicksilver> on the other hand, code which cares what the chunk size is is broken :)
01:45:12 <quicksilver> oh, maybe it is configurable?
01:45:18 * Baughn wonders if there is a safe array library somewhere
01:45:32 <solrize_> i have the impression it's a compile time constant more or less.  but i couldn't find it even that way.
01:45:44 <dmwit> Baughn: Data.Array?
01:45:49 <solrize_> i want to crank it up a lot higher so i can read lots of files at once without a bunch of seek latency
01:45:56 <Baughn> dmwit: No.. um, let me write an example
01:46:01 <cjs> =<<? Hmm!
01:46:31 <solrize_> actually what i want is to read ahead a bunch of chunks, using AIO, so if i get the early chunks first i can start computing with them immediately
01:46:51 <cjs> Console based? What's that one that yi uses?
01:47:33 <dmwit> I think it's just straight VTY.
01:47:41 <cjs> Right. That's what I was thinking of.
01:48:45 <cjs> solrize_: what kind of application is this?
01:48:48 <quicksilver> solrize_: you know best what you're doing, but you know the OS will kind-of do that for you anyway?
01:49:09 <solrize_> cjs, the merge phase of an external sort
01:49:09 <Baughn> > array ((0,0),(2,2)) [((2,2),42)] ! (0,8) -- This is what I mean by unsafe
01:49:10 <lambdabot>  42
01:49:23 <solrize_> quicksilver i didn't realize the OS would do that
01:49:33 <dmwit> Baughn: eep!
01:49:36 <quicksilver> using Data.Binary and lazy bytestring at its default settings, dons has demonstrated hundreds of MB/sec
01:49:45 <cjs> Yes, many OSes will detect long sequential reads and do a bit of read-ahead.
01:49:46 <quicksilver> so I doubt tuning the settings is really all that important
01:50:00 <Baughn> dmwit: It doesn't let you actually access out of bounds, but the index-generator is broken
01:50:10 <dmwit> > array ((0, 0),(2, 2)) [((2, 2), 42)] ! (0, 70)
01:50:10 <lambdabot>  Exception: Error in array index
01:50:32 <dmwit> Baughn: That sounds like a problem with the Ix instance, mostly.  You could just newtype a better one.
01:50:34 <solrize_> cjs, the trouble is that the reads are probably mostly sequential but maybe not perfectly sequential, if there is any fragmentation on the disk from files being deleted or whatever
01:50:54 <solrize_> i'd have to make measurements but am trying to think ahead of what i'd run into
01:51:06 <dmwit> Baughn: Do a check in indexOf or whatever.
01:51:06 <solrize_> the issue is that i want to read 100's of sequential files simultaneously
01:51:15 <cjs> That's not generally worth worrying about, at least with filesystems such as FFS, which cluster things fairly well if there's space to do so.
01:51:19 <quicksilver> solrize_: point is, you are prematurely optimising, I think.
01:51:20 <Baughn> dmwit: I could do that, I guess
01:51:30 <quicksilver> solrize_: the OS already tries quite hard to do this right.
01:51:43 <quicksilver> and haskell already has simplistic but effective asynch IO built in.
01:51:48 <Baughn> solrize_: Or you could use xfs, which comes with an online defragmentor
01:51:54 <quicksilver> (as does the OS itself)
01:52:08 <dmwit> I guess you can make disk requests *much* faster than you can get the responses.
01:52:12 <solrize_> hmm i will run some tests but it seems to me that in a typical file-based app i never get anything like the real speed of the disk
01:52:12 <cjs> Mmm, if it's 100s, the OS is moderately likely to miss that.
01:52:25 <solrize_> if i really go crazy i may try to run in a raw disk partition instead of a file system
01:52:27 <dmwit> So the OS and the disk hardware can both have quite a bit of data about what you want next.
01:52:50 <cjs> solrize_: you definitely don't want to do that in a modern OS. It's really not worth the effort.
01:52:59 <solrize_> cjs probably true :)
01:53:04 <quicksilver> solrize_: I doubt it's for the reason you think it is :)
01:53:12 <quicksilver> I doubt it's something you can solve with IO scheduling.
01:53:23 <Baughn> solrize_: It's an old, old problem, and it's about to go away anyway - flash disks don't have the problem.
01:53:32 <Baughn> solrize_: So there's no point in fixing it /now/
01:53:33 <cjs> If you're writing the files in such small chunks that they fragment badly, you could try writing out a bunch of large empty files and then rewinding and rewriting them.
01:54:27 <Spockz> http://bin.mrbondt.nl/?show=650
01:54:28 <lambdabot> Title: MrBondt.nl: Pastebin
01:54:30 <cjs> But even with small writes, I'd be surprised if they fragment all that much. Assuming FFS or similar, of course. I can't recall if ext3fs uses cylinder groups and attempts to cluster data, but I thought it did.
01:54:44 <Spockz> that's what I get if I want to configure wxcore
01:55:17 <solrize_> i measured seek time of a CF card on a PATA adapter at around 0.7 msec, transfer rate around 20 mb/sec
01:55:47 <dmwit> Spockz: O_o
01:55:57 <dmwit> Spockz: ...do you have x permission on .? =P
01:56:06 <Baughn> solrize_: Apples and oranges. Most of that is in the /flash interface/, not the technology itself
01:56:11 <Spockz> dmwit: it's windows :+
01:56:19 <dmwit> Spockz: Windows... ah.
01:56:41 <Baughn> solrize_: Certainly you can parallelize to your heart's desire and get arbitrarily high transfer rates; there will still be seek time, of course, btu even 0.7msec is pretty low
01:56:48 <dmwit> Spockz: Can you run the configure step yourself?
01:56:55 <solrize_> baughn, yeah, i haven't tried any high end ssd's, maybe the seek time can be faster
01:57:09 <dmwit> Spockz: Also, you may want to report this as a Cabal bug.
01:57:11 <Spockz> dmwit: How? This is the first time I use cabal
01:57:15 <dmwit> (Not sure about that.)
01:57:33 <dmwit> Spockz: "configure --with-hc=ghc"
01:57:48 <dmwit> Spockz: (Just copying the command it tried to do, but without the "./" at the beginning.)
01:57:48 <Spockz> nop
01:57:54 <Spockz> not found
01:58:07 <dmwit> Oh, is there a file called "configure" in that directory?
01:58:09 <Spockz> let's see where that file is
01:58:13 <solrize_> baughn what i'm concerned about is using bytestring.lazy on 100 files at once, and having it read 64k from one file, seek to the next file, read 64k, etc
01:58:23 <solrize_> this would be on a hard disk, so around 8 msec/seek
01:58:26 <dmwit> Does autoconf even work at all on Windows?
01:58:43 <solrize_> i'd want to do at least 50 msec of transfer per seek, i.e. read chunks of 1 mb or so, maybe more
01:58:52 <Spockz> dmwit: I hope so
01:59:04 <Spockz> in what folder would that configure be?
01:59:05 <Baughn> solrize_: That could happen. If you want larger chunks you can force parts of the bytestring, but in general.. it's down to the OS's lack of async transfer mechanisms, not haskell.
01:59:09 <quicksilver> I think autoconf only works with a unixy setup
01:59:10 <johnnowak> autoconf doesn't really "work" anywhere
01:59:20 <quicksilver> mingw or whatever it's called
01:59:22 <dmwit> Spockz: You may want to get a cygwin install going.
01:59:27 <quicksilver> cygwin, yeah.
01:59:37 <solrize_> baughn is AIO not enough for async transfer?
01:59:48 <Baughn> solrize_: Oh, AIO would be fine, if it /worked/
01:59:58 <solrize_> ahh
02:00:01 <Baughn> solrize_: Linux' version right now is just a wrapper for read
02:00:03 <solrize_> the plot thickens
02:00:05 <solrize_> hmm
02:00:12 <Baughn> http://lse.sourceforge.net/io/aio.html <-- Details
02:00:13 <lambdabot> Title: Kernel Asynchronous I/O (AIO) Support for Linux
02:01:22 <solrize_> what's o_direct?
02:01:34 <Baughn> solrize_: "Don't use the disk buffer". Don't use it.
02:01:59 <solrize_> that's what o_direct is?  er, ok :)
02:02:12 <solrize_> is bsd aio any better?
02:02:19 * Baughn has no idea
02:02:48 <quicksilver> AIO is a slightly sore subject
02:02:53 <quicksilver> but it shouldn't be exagerrated
02:03:06 <quicksilver> the currently setup does actually work, and you often get very good read speeds out of it.
02:03:16 <cjs> @hoogle connectTo
02:03:17 <lambdabot> No matches found
02:03:20 <cjs> ???
02:03:28 <quicksilver> solrize_: are your files large or small?
02:03:36 <solrize_> quicksilver, large
02:03:39 <solrize_> GB's
02:03:43 * quicksilver nods
02:03:56 <dmwit> cjs: It's probably in an external library or package.
02:04:06 <quicksilver> I would strongly suggest you try it (with the default lazy bytestring) and then see how it performs in practice.
02:04:08 <cjs> solrize_: BTW, did you read that paper on Fast IO in Haskell?
02:04:10 <dmwit> cjs: Have a look at what modules are imported, and start googling for docs?
02:04:20 <solrize_> cjs, no, what paper?
02:04:21 <quicksilver> do you have reason to believe that your program is likely to read 64k in one thread then 64k in another?
02:04:28 <quicksilver> (rather than reading more than 64k in the first thread)
02:04:29 <cjs> It's in Network. I would think that Hoogle would find it.
02:04:56 <solrize_> quicksilver i hadn't figured on using multiple threads
02:05:10 <solrize_> basically i have N sorted files and want to merge them
02:05:21 <cjs> http://cryp.to/blockio/fast-io.html
02:06:33 <solrize_> oh yes i've seen that paper, it's good
02:06:55 <quicksilver> solrize_: ah, right.
02:06:56 <solrize_> but doesn't use bytestring
02:07:10 <quicksilver> solrize_: you did say that, but I forgot.
02:07:14 <solrize_> np
02:07:39 <solrize_> so the idea is the read pointers would be moving thru all the files at approx the same speed
02:07:54 <quicksilver> well it's easy enough to write. I say you write it, and tell us :)
02:07:59 <quicksilver> easier than all this speculation.
02:08:09 <solrize_> which?
02:08:09 <quicksilver> Find out how far short of the ideal IO speed you are.
02:08:12 <quicksilver> and if you're CPU bound.
02:08:16 <quicksilver> just the naive lazy bytestring impl.
02:08:32 <solrize_> ok
02:08:35 <quicksilver> then try changing the chunk size to, e.g., 512k, and see if that's any better :)
02:08:36 <solrize_> yeah i'll try that
02:11:21 <solrize_> i wrote something in python using mmap on files of just a few gb and found i took a lot of delays from i/o waits
02:11:44 <solrize_> that are completely uncontrollable using mmap, they're basically page faults
02:11:51 <solrize_> so that's why i wanted to use async io if possible
02:12:36 <solrize_> and aio sounded really cool when reading about it but i didn't realize it wasn't really implemented
02:12:43 <solrize_> i guess it is overhyped
02:13:20 <Baughn> It isn't that, it's just..
02:13:28 <Baughn> Linux isn't a very good unix implementation. ^^;
02:13:43 <solrize_> hmm is solaris better?
02:13:56 <quicksilver> yes, it's better at this particular point, I think.
02:14:10 <quicksilver> to be fair to linux, async disk IO turns out not to be very important to many people.
02:14:55 <yuriyp> cjs: which paper are you reffering to?
02:15:10 <yuriyp> cjs: already see the answer
02:15:27 * Baughn notes that it'a /very hard/ to design a denormalized, overlapping, awful database when you're /told/ to do so
02:16:37 <solrize_> are there any good books or articles about sorting on modern hardware?  i.e. knuth vol 3 for the 21st century :)
02:16:51 <quicksilver> I don't think so, no.
02:17:10 <solrize_> hmm
02:17:32 <cjs> Look at DB stuff for modern hardware, perhaps particularly column-oriented DBMSes.
02:17:32 <solrize_> thanks
02:18:11 <quicksilver> there will certainly be papers on it, yes :)
02:18:15 <solrize_> cjs thanks, good idea, we're using postgresql for some stuff now so i might look at how it sorts
02:18:23 <quicksilver> it uses the knuth sort
02:18:31 <solrize_> what's the knuth sort?
02:19:27 <Botje> you first take a 7 year break to write a programming language specially designed for sorting, then invoke the knuth() primitive
02:19:39 <solrize_> botje :)
02:19:40 <solrize_> hehe
02:19:42 <solrize_> metasort
02:19:44 <quicksilver> solrize_: I think some people call it the "Replacement selection" sort?
02:19:45 <quicksilver> or something.
02:19:56 <solrize_> hmm
02:20:06 <solrize_> ok that sounds like an in-core algorithm
02:20:12 <cjs> The column-oriented stuff you might find particularly interesting; in many common types of "data-warhousing" systems you can speed things enormously by changing your data formats.
02:20:17 <quicksilver> basically, pg uses a variant of what knuth recommends on his chapter on external sorting
02:20:25 <quicksilver> replacement selection plus merge, perhaps?
02:20:28 <cjs> In fact, one of my ambitions is to write a DBMS in Haskell.
02:20:39 <solrize_> cjs, happs state :)
02:21:15 <cjs> ?
02:21:22 <solrize_> knuth had 100's of pages on how to merge on N tape drives that could read and write backwards and forwards etc.
02:21:32 <solrize_> cjs, joking.  happs state = MACID database
02:22:16 <solrize_> cjs that sounds cool, a db in haskell
02:22:32 <solrize_> i just get the impression that SQL has gone out of style
02:22:37 <paolino> @pl \x y -> x >>= \x -> fmap (x:) y
02:22:37 <lambdabot> liftM2 (:)
02:23:13 <cjs> It was never in style.
02:23:51 <jeffz> it's always in style with "business programmers"
02:24:17 <dmwit> What is better than SQL for database control?
02:24:33 <solrize_> i've been wanting to look at couchdb
02:24:47 <dmwit> Good-quality DSL's take longer to go "out of style" than I think you are giving them credit for.
02:24:48 <dolio> Relational algebra?
02:25:17 <dmwit> dolio: Name one (high-performance, for bonus points) database that implements a relational algebra.
02:25:22 <cjs> There's nothing commonly used out there that's not even worse.
02:25:53 <solrize_> i've had the idea of writing a db using serialized data.maps or something like that
02:26:03 <solrize_> it would be low performance but hopefully very simple
02:26:14 <cjs> Mine would use a language much more closely based on the relational algebra, and with a lot more expressive power than SQL.
02:26:17 <solrize_> and handle concurrency and rollback etc trivially
02:26:40 <cjs> The other thing I wanted to do was separately define the logical and physical layouts.
02:27:02 <dmwit> I think Datalog would be nice, if only it could be fast.
02:27:03 <dolio> dmwit: I'm not aware of any databases off hand that don't use SQL. That doesn't stop people from saying it sucks and something based more closely on the relational algebra/calculus would be better, though, it seems.
02:27:17 <dmwit> yeah
02:27:19 <solrize_> dolio couchdb doesn't use sql.  it's, like, forward into the psat
02:27:19 <solrize_> past
02:27:23 <osfameron> BDB?
02:27:41 <dolio> I'm not much of a database afficionado, really.
02:27:43 <dibblego> I use databases without SQL; far better than relational gunk
02:27:46 <solrize_> it's a denormalized triple store or something
02:28:01 <solrize_> couchdb.org
02:28:03 <solrize_> it's written in erlang
02:28:35 <solrize_> http://incubator.apache.org/couchdb/docs/faq.html
02:28:36 <lambdabot> Title: Apache CouchDB: Frequently Asked Questions
02:29:06 <quicksilver> I could hardly disagree more dibblego
02:29:19 <quicksilver> the point of using a relational database is convenient declarative expression of complex queries.
02:29:31 <dibblego> and it fails very miserably at that
02:29:36 <solrize_> hmm haskell sounds better than sql for that :)
02:30:00 <quicksilver> dibblego: it's rather poor compared to what it could be.
02:30:08 <quicksilver> and distinctly awkward syntactically
02:30:24 <osfameron> powerful nonetheless
02:30:24 <dibblego> I'd much prefer to call filter than write a stupid SQL query with WHERE
02:30:25 <quicksilver> but still orders of magnitude better than morasses of client code written in C, C# or Java
02:30:31 <solrize_> aha my 1.5 hour python script finished, i can start the next thing and go to bed :).  i can't wait to rewrite this stuff with ghc and have it run in 5 minutes :)
02:30:43 <dmwit> Also: people need to realize that SQL is completely case-insensitive and that lower-case is about 300,000x more readable than upper-case.
02:30:50 <quicksilver> as for whether *haskell* is a better language, that is certainly a much more interesting discussion.
02:30:55 <Spockz> dibblego: but, that would produce a big set at first
02:30:59 <Spockz> and then reduce it
02:31:08 <dibblego> Spockz, no it wouldn't
02:31:30 <osfameron> and SQL has the advantage that many clients speak it.  Which is a potential advantage over an in-house query/store in *your-favourite-language*
02:31:32 <dmwit> laziness++
02:31:35 <quicksilver> but it's certainly true that good SQL databases optimise queries in ways that naive haskell expressions don't.
02:31:56 <quicksilver> and the model of a large powerful DB server doing queries server-side also has its merits.
02:31:57 <dmwit> right
02:32:00 <solrize_> you'd have some combinators and a query optimizer that ran them a bunch of different ways
02:32:06 <quicksilver> solrize_: yes, you can do it.
02:32:08 <quicksilver> I don't deny it.
02:32:16 <dibblego> I call a lazy filter function from Scala when I use db4o; I'll let others write SQL if they feel punishing themselves
02:32:20 <Spockz> dibblego: sure, if you first execute a sql query with no where. You get the whole table (suppose) and then afterwards you filter out the things you don't want
02:32:23 <osfameron> quicksilver: you could have a haskell-db server being passed haskell queries and performing them server side...
02:32:35 <dibblego> Spockz, no, you're wrong; are you aware that Haskell is a lazy language?
02:32:37 <Spockz> Only in between, that whole set of data has to be passed through the channel
02:32:38 <quicksilver> there are a whole host of advantages of using an RDBMS
02:32:57 <dibblego> Spockz, no it doesn't; you can produce indexes and you can abstract over the container type to help
02:33:00 <quicksilver> each one separately ccan be acheived in a different way.
02:33:03 <Spockz> dibblego: I don't know how you interface with a db
02:33:15 <quicksilver> achieving them all is a lot of work.
02:33:21 <quicksilver> (not impossible : just a lot of work)
02:33:32 <dibblego> Spockz, assuming that filter requires the entire list is wrong
02:33:47 * dibblego must go
02:33:51 <Spockz> ok
02:33:53 <Spockz> :w
02:33:55 <Spockz> hf
02:33:58 <solrize_> nite dibblego
02:34:12 <quicksilver> I don't really know what dibblego is talking about.
02:34:21 <quicksilver> filter obviously requires at least part of the whole list
02:34:24 <quicksilver> otherwise it can't do its job
02:34:46 <quicksilver> [ n | n <- names, "James" `isInfixOf` n]
02:35:03 <quicksilver> requires at least "enough" of every item to check if James is part of it.
02:35:25 <solrize_> well sometimes you might only want the first...
02:36:53 <quicksilver> true.
02:37:12 <quicksilver> in my experience that limited kind of laziness is handy but not game-breaking.
02:37:40 <quicksilver> it's cool to be able to do that, but more often you don't have a LIMIT 1 or similar.
02:38:02 <solrize_> yeah
02:38:06 <sclv> indexes ftw. i don't get why people keep reinventing the database wheel.
02:38:26 <solrize_> cause the existing stuff all sucks
02:38:34 <osfameron> unless you're doing web navigation of large datasets, where LIMIT x OFFSET y is crucial
02:38:45 <osfameron> or if not crucial, at least the most obviously effective approach
02:38:59 <sclv> on the other hand, a linq-like syntax that compiled into real db stuff would be very nice indeed.
02:39:11 <solrize_> how does limit x offset y work underneath?  cursors?
02:39:20 <vegai> erlang's mnesia has something like that
02:39:32 <Spockz> quicksilver: good that you said that, I was a bit puzzled :p
02:39:34 <osfameron> I think mysql hacks it in specially
02:39:41 <sclv> functional doesn't give you the object/relation mismatch -- just type marshaling issues that should be very solvable, but not without compiler support.
02:39:46 <solrize_> mnesia seems really nice, might be good for what i'm doing now, which is currently using a triple store on top of psql
02:39:51 <osfameron> in postgres/oracle it doesn't exist, so you have to do a subquery and throw away the OFFSET part iirc
02:39:55 <quicksilver> sclv: because it is a particularly interesting wheel
02:40:12 <Spockz> it's round
02:40:33 <quicksilver> sclv: and there are so many shiny alloys you can imagine to use for it.
02:40:54 <sclv> the stonebreaker stuff -- both columnar and h-base seems the most interesting, but it comes out of ppl that really know the issues and the space.
02:41:08 <quicksilver> osfameron: most DBs don't optimise LIMIT x OFFEST y in an interesting way for complex queries.
02:41:24 <quicksilver> osfameron: they do, essentially, calculate the entire query and just only return some rows to the client.
02:41:27 <solrize_> i hang out on various web bbs systems that are LAMP apps and they bog down horribly under load, even on beefy hardware, and something just seems wrong
02:41:42 <sclv> that's a coding issue.
02:41:53 <quicksilver> osfameron: it's very hard to optimise that for complex queries. Obviously it's trivial for certain simple ones.
02:42:01 <osfameron> yeah indeed
02:42:06 <sclv> its like complaining that naive haskell programs run into space leaks.
02:42:09 <solrize_> well it's the amoutn of db traffic and the mismatch between the relational model and what the app really wants
02:42:16 <osfameron> I think navigating to arbitrary pages of data is the wrong thing to do anyway
02:42:27 <sclv> its a bad relational model.
02:42:32 <osfameron> you probably either want "First, Prev, Next", or to sort/search by another criterion
02:42:33 <solrize_> i mean these programs weren't written by total fools afaik (but then what do i know)
02:42:55 <osfameron> but people seem to like those page lists in web forms
02:42:57 <cjs> If they use MySQL, they're probably just treating the DBMS as a dumb data store, anyway.
02:43:01 <quicksilver> solrize_: cursors are a disaster.
02:43:06 <quicksilver> solrize_: (long lived cursors)
02:43:09 <sclv> and then its also hitting the database all the time for things that should be cached.
02:43:11 <solrize_> yeah
02:43:21 <sclv> enumerators ftw.
02:43:21 <solrize_> so i'm wondering how they handle those offsets
02:43:23 <quicksilver> long lived cursors consume massive DB-side resources.
02:43:31 <solrize_> aside from computng the big result set each time
02:43:34 <quicksilver> solrize_: they just recalculate the query every time.
02:43:35 <quicksilver> much easier.
02:43:50 <solrize_> well that's why they suck so bad!  they need functional data structures
02:43:59 <quicksilver> the problem is fundamentally hard.
02:44:01 <osfameron> zipperdb!
02:44:07 <quicksilver> I'm not saying there aren't interesting solutions.
02:44:09 <quicksilver> but it's HARD.
02:44:12 <quicksilver> for a general query.
02:44:20 <solrize_> yeah
02:44:25 <quicksilver> for SELECT * FROM foo ORDER BY x, it's trivial.
02:44:28 <quicksilver> if there is an index on x.
02:44:31 <sclv> right: you need to specialize, and have some sense of what's goin on.
02:44:35 <quicksilver> and most DBS manages to get that simple case :)
02:44:58 <solrize_> well the usual selection joins two tables on one field
02:44:58 <quicksilver> anythign involving a complex subquery or something, and it's no longer so simple :)
02:45:06 <solrize_> that shouldn't be too hard either with data.map
02:45:14 * solrize_ sleepy
02:45:19 <paolino> >  foldr (liftM2 (:)) (Just []) [Just 1,Just 2,Nothing,Just 3]
02:45:20 <lambdabot>  Nothing
02:45:26 <quicksilver> I am one of the many people here who has written some bits of a relational DB.
02:45:31 <quicksilver> more than once, actually.
02:45:44 <quicksilver> at least once in C++ once in python and once in haskell.
02:45:53 <quicksilver> I might actually finish something one day. But probably not.
02:45:55 <solrize_> i did one in c but it wasn't a serious one of any scale
02:46:00 <quicksilver> starting stuff is much more fun than finishing stuff.
02:46:06 <solrize_> yeah :)
02:46:13 <osfameron> word
02:46:27 <cjs> It would be interesting to use persistant, lazy, memoized queries.
02:46:58 <paolino> > foldr (liftM2 (:)) (Just []) [Just 1,Just 2,Just 3]
02:46:59 <lambdabot>  Just [1,2,3]
02:47:03 <sclv> some dbs sort of do, I think?
02:47:15 <cjs> None that I'm aware of.
02:47:57 <dmwit> > sequence [Just 1, Just 2, Just 3] -- paolino
02:47:57 <lambdabot>  Just [1,2,3]
02:48:02 <sclv> views are sort of that...
02:48:10 <sclv> semantically they're non-strict at least.
02:48:21 <paolino> ehm ,hotwater reinventing, thanks dmwit
02:48:50 <quicksilver> they're not memoized
02:48:55 <quicksilver> unless they are materialised
02:48:59 <quicksilver> in which case they are not lazy :)
02:49:11 <quicksilver> I'm not aware of a DB which offers "lazy materialisation"
02:49:18 <quicksilver> although I've often thought it would be fun to have.
02:49:25 <sclv> ah, right. i see what you mean.
02:49:27 <quicksilver> You could write it in Pg RULES and similar systems.
02:49:32 <dmwit> :t foldr (liftM2 (:)) (return [])
02:49:33 <lambdabot> forall a1 (m :: * -> *). (Monad m) => [m a1] -> m [a1]
02:49:41 <dmwit> ?src sequence
02:49:41 <lambdabot> sequence []     = return []
02:49:41 <lambdabot> sequence (x:xs) = do v <- x; vs <- sequence xs; return (v:vs)
02:49:41 <lambdabot> --OR
02:49:41 <lambdabot> sequence xs = foldr (liftM2 (:)) (return []) xs
02:49:50 <sclv> concurrency issues would sort of mess with it though, no?
02:49:50 <dmwit> heh
02:50:13 <quicksilver> sclv: I think pg rules give you enough power to make it safe, but I don't remember.
02:50:22 <quicksilver> yeah, they do.
02:50:26 <quicksilver> if you're prepared to work on it.
02:50:37 <quicksilver> you need a trigger on every referenced table which explicitly invalidates the cache
02:50:40 <quicksilver> on insert/update
02:50:51 <quicksilver> so a bunch of rules and triggers and you can pull it off.
02:51:11 <quicksilver> I imagine there are some potted recipes out there.
02:51:28 <quicksilver> there is some pretty heavy pg/plsql hackery available on the intertubes.
02:51:29 <cjs> quicksilver: actually, you could get more clever, and for each new insert, decide which cached queries it will, won't or might match.
02:51:32 <sclv> right but if your cache goes invalid on every insert, then the persistence isn't v. useful... depending.
02:51:50 <quicksilver> sclv: well it's useful assuming invalidations are "relatively" rare.
02:51:59 <quicksilver> sclv: or, if there is a way to "incrementally" update the cache
02:52:03 <quicksilver> rather than invalidating it.
02:52:08 <cjs> (I'm not suggesting you'd want to do this in something like pl/pgsql, of course. That would be insanity, unless you wrote a program to write the code for you.)
02:52:13 <quicksilver> eitehr of those assumptions may be correct in certain cases.
02:52:17 <sclv> plus it might invalidate the earlier part of the query too, conceptually, at which point, you need to signal that too...
02:52:37 <quicksilver> I'm concerned about making concurrency safe
02:52:45 <quicksilver> not shooting yourself in the foot in a single transaction
02:52:51 <quicksilver> "earlier" queries are your problem.
02:52:59 <quicksilver> it's queries in other threads you want to be safe from.
02:53:15 <quicksilver> (to the extent any DBMS actually guarantees ACID)
02:53:20 <quicksilver> (hint : none of them do)
02:53:24 <sclv> without genuine stm-style world splitting you're not getting what i want, i think, and with it then it seems there's a whole new ballgame.
02:53:38 <quicksilver> sclv: Pg and oracle both implement something like stm-style world splitting, though :)
02:53:41 <quicksilver> sclv: so that's OK then.
02:54:02 <quicksilver> it's not perfect, which is why neither of those is a true ACID database.
02:54:11 <quicksilver> but it's good enough for quite a few natural behaviours.
03:04:08 <solrize_> pg and sql use traditional table representations and then if you have concurrent clients they figure out the old value from the rollback log (oracle) or use writeahead (pg), i think
03:04:21 <solrize_> i like the idea of using avl trees instead of tables
03:04:43 <solrize_> or trying to understand what zipperdb was supposed to be
03:04:46 <cjs> pg stores a timestamp with every row, and returns the row with the timestamp that says it was current at the time you started your query.
03:04:52 <solrize_> hmm
03:05:03 <cjs> So there may be three or four different versions of a single row.
03:05:08 <solrize_> ic
03:05:33 <quicksilver> not in the tables, though.
03:05:38 <cjs> That's where the MV comes in MVCC (multi-version concurrency control).
03:05:39 <quicksilver> or is it?
03:05:51 <quicksilver> I thought the 'other versions' of the rows were off-table.
03:06:06 <cjs> No, all rows are in the table, in PG. Oracle is different.
03:06:10 <quicksilver> ah, right.
03:06:12 * quicksilver nods
03:06:13 <cjs> But Oracle doesn't use MVCC, as far as I can tell.
03:06:25 <quicksilver> oracle's solution is often described as being MVCC
03:06:27 <cjs> BTW, this lets you do time travel; you can ask PG to do a query as of, say, an hour ago.
03:06:34 <solrize_> oh cool
03:06:39 <quicksilver> it may be that the people who describe it that way are lying.
03:06:40 <cjs> Oracle won't let you do that.
03:06:41 <solrize_> thats what i wanted the functional map for
03:06:46 <quicksilver> cjs: that is not true.
03:06:50 <quicksilver> oracle does support time travel.
03:06:56 <cjs> Really? Ok. I lied. :-)
03:07:13 <quicksilver> pg's time travel is not, as far as I know, based on the MVCC
03:07:14 * SamB notes that gmail apparantly supports time travel too now
03:07:17 <solrize_> but that means, ugh, every pg query has to postselect on the highest timestamp before the transaction?
03:07:18 <quicksilver> I think it's based on the log.
03:07:30 <quicksilver> I think obsolete rows get removed from the table files.
03:07:38 <quicksilver> but the time travel (PITR, they call it) can find it in the log.
03:07:52 <SamB> ... except that gmail supports time travel in the other direction
03:07:53 <cjs> pg's time travel uses mvcc. The "obsolete" rows get removed only when you VACUUM, and I believe you can specify limits.
03:08:04 <dmwit> SamB: Don't worry each person gets only 10 temporal incursions.
03:08:25 <SamB> I know
03:08:34 <quicksilver> cjs: can you source that?
03:08:42 <quicksilver> cjs: I think you might be wrong or out of date.
03:08:59 <quicksilver> "Long ago, PostgreSQL had a built-in time travel feature that kept the insert and delete times for each tuple."
03:09:04 <cjs> The big disadvantage of PG's system, of course, is that the indices don't have timestamps, so even if you have the data you need from an index, you still have to go to the row to check the timestamp.
03:09:18 <cjs> quicksilver: where is that from?
03:09:26 <quicksilver> "PostgreSQL now uses a WAL recovery scheme, and ditched time travel."
03:09:33 <quicksilver> http://www.postgresql.org/docs/current/static/contrib-spi.html
03:09:34 <lambdabot> Title: PostgreSQL: Documentation: Manuals: PostgreSQL 8.3: spi
03:09:39 <quicksilver> (although that's not the only place I see that)
03:09:49 <SamB> hmm, seems like a fairly reliable source!
03:09:59 <solrize_> wal = writeahead L.*=?
03:10:02 <SamB> for pg info, at least
03:10:12 <quicksilver> AIUI, Pg still supports "something like time travel" but it's now called PITR, and it's via the WAL log.
03:10:17 <solrize_> log
03:10:19 <quicksilver> not via mvcc.
03:10:24 <quicksilver> solrize_: yes.
03:11:18 <quicksilver> Oracle supports native time travel at the prompt. As in "SELECT foo FROM bar 3 days ago"
03:11:38 <solrize_> zomg it's late, i gotta get some sleep, morning phone call, this script will run for 2 hours or so and then i may wake up to check on it
03:11:40 <solrize_> nite
03:11:42 <quicksilver> (syntax invented, I don't actually have an enterprise copy of oracle nor the patience to browse menus)
03:11:45 <quicksilver> manuals
03:12:02 <cjs> Oh, huh! They didn't bring back time travel!
03:12:24 <SamB> not even ... chinese food menus?
03:12:50 <cjs> I thought that they had. Anyway, the stuff you need is there, although I just realised that time in PG is in units of transaction IDs, so you'd also need something to timestamp those in order to be practical.
03:13:09 <cjs> But definitely the on-disk tuple format includes the transaction IDs.
03:14:04 <quicksilver> of course my unwritten RDBMS will have time travel.
03:14:08 <quicksilver> and true ACID.
03:14:11 <osfameron> summon a distributed database using git!
03:14:17 <quicksilver> take that, so-called enterprise DBs.
03:14:26 <SamB> quicksilver: will it let you write stuff in the past?
03:14:28 <quicksilver> oh, and transparent replication, thanks for remninding me.
03:14:35 <cjs> PITR is not for queries; it allows you to restore a database to a given point in time.
03:14:42 <quicksilver> SamB: it will let you buy shares in apple the day before the iPod was released.
03:14:51 <SamB> whoa
03:14:52 <quicksilver> cjs: yeah. I think that's the closest they have now.
03:15:08 <SamB> most time machines don't let you travel back to before the time they were made, do they?
03:15:09 <quicksilver> cjs: (it does give you time travel modulo considerable inconvenience and the need for lots of spare disk space :)
03:15:20 <quicksilver> SamB: the one in terminator did.
03:15:34 <cjs> Perhaps I'm just remembering some plans for re-introducing TT that I saw on the mailing list a few years ago.
03:15:38 <osfameron> ditto the tardis
03:15:39 <SamB> I still have to see that...
03:15:45 <cjs> The problem with PITR is, the time travel takes too long! :-)
03:17:39 <cjs> So, "modifyMVar counter (\i -> return (i+1, ()))" does not seem like the proper idiom for using modifyMVar...
03:17:53 <quicksilver> cjs: that's one of my pet hates
03:18:10 <quicksilver> cjs: I define modifyMVarPure for that reason.
03:18:23 <cjs> Really? That's actually the way it's used!?
03:18:25 <quicksilver> cjs: I made my case to the MVar czar but he didn't agree with me.
03:18:34 <quicksilver> too late to change the names now.
03:18:40 <cjs> I thought I was just missing something here....
03:18:42 <quicksilver> IMO modifyMVar should have been called modifyMVarM
03:18:44 <quicksilver> or, better
03:18:45 <quicksilver> withMVar
03:18:56 <quicksilver> and modifyMVar should be the pure version.
03:19:20 <quicksilver> modifyMVarPure m f = modifyMVar_ m (return . f)
03:19:31 <quicksilver> cjs: that is in the source of every program in which I use MVars :)
03:21:04 <cjs> That is nicer.
03:21:53 <cjs> Ah, but I missed modifyMVar_, which is a little better, anyway.
03:22:01 <cjs> But I think I'll use yours. I like that.
03:22:40 <quicksilver> cjs: http://www.haskell.org/pipermail/libraries/2007-September/008237.html
03:22:42 <lambdabot> Title: MVar API - modifyMVar, http://tinyurl.com/2swwlo
03:22:48 <quicksilver> cjs: nobody agreed (or even responded)
03:23:07 <quicksilver> cjs: I took it up with jaffacake personally but he didn't seem to think it was very important :)
03:26:17 <cjs> Hm. I guess it would be painful to change now. Except for me! :-)
03:27:50 <quicksilver> cjs: yeah, I think that's the issue
03:28:09 <quicksilver> cjs: it's not really important enough to do a change to a widely used function.
03:28:19 <quicksilver> cjs: even if they do acknowledge my point :)
03:42:19 * SamB isn't sure it would be intuitive to have withMVar using the return value of the computation...
03:47:02 <cjs> Time for another sanity check. In my main thread, I send a login request followed by some other stuff. I've got a response processing thread that processes responses to requests. Mostly the stuff is quite async, but after sending a login request I need information from the response to that before I can send further requests (that being the "source ID" I've been assigned).
03:48:01 <cjs> Does it make sense to have an MVar holding this ID that starts out empty, have my sending thread send the login request, and then wait on that MVar for it to be filled by the response processing thread before continuing on to send further requests (which require that ID)?
03:48:44 <Baughn> Makes sense to me
03:49:16 <quicksilver> SamB: the other 'with' values return the results of the computation. Or isn't that what you meant?
03:49:42 <quicksilver> cjs: Yes. That sounds like a typical approach.
03:50:06 <quicksilver> cjs: another approach is for the main thread to register a 'callback' which will be called when the login response is returned.
03:50:19 <quicksilver> cjs: but the approach you suggest is probably more straightforward looking.
03:50:29 <cjs> Hm, so I could make a general case of this kind of thing; when I need to wait for a result (which, due to the async nature of the protocol could be well after I send the request, and I may need to send other requests in the meantime), just fork off a thread that waits for whatever it needs to be filled in by the response processor.
03:50:41 <cjs> Hmm, I can see occasions where a callback might be useful, too.
03:51:03 <cjs> Hm. I can see a structure for this app starting to come together.
03:51:09 <SamB> quicksilver: yes, but I don't think many of them do anything ELSE with the value...
03:51:40 <SamB> I would kind of expect withMVar to just grab the MVar, hold onto it during the computation, and put the original value back ;-)
03:53:44 <cjs> Interesting! When I initialize a data with named fields, I get a warning, not an error, when I leave out a field.
03:53:53 <quicksilver> cjs: yes. I think that's odd, too.
03:54:08 <cjs> So what's in those fields, then?
03:54:12 <quicksilver> the unset ones are set to 'error "You forgot this field"'
03:54:15 <quicksilver> basically
03:54:21 <cjs> Oh, so it will blow up.
03:54:50 <cjs> Kinda seems to me as if that should be a type error.
03:57:23 <MarcWeber> Someone using shim and vim? I've added some new features (GrepScope will list all functions whose name or type contains a regex), and FindModulesExportin <id> as well as camelCaseCompletion (thus sH finds simpleHTTP which seems to work pretty well)
03:58:19 <MarcWeber> Anyone knows wether the author Benedikt Schmidt is online here ? Is his nick beschmi ?
03:59:33 <cjs> Huh! Didn't know about shim. Gotta check that out one day.
03:59:40 <MarcWeber> The next thing I'd like to do is adding support for "autmoatically add import statement for identifier X"
04:03:02 <cjs> testconnect: /u/cjs/co/client/tsuru/trader/src/trader/src/TestConnect.hs:23:20-46: Missing field in record construction TestConnect.responseCount
04:03:21 <cjs> Why is the missing field initialization a warning rather than a compile error?
04:04:10 <quicksilver> cjs: like I said, I think it's a surprising choice.
04:05:58 <quicksilver> "Fields not mentioned are initialized to _|_."
04:06:01 <quicksilver> from the haskell report
04:06:06 <quicksilver> so it's mandated by the h98 standard.
04:06:11 <quicksilver> (I disagree with it, though)
04:06:47 <cjs> Right, I just found that. I think I disagree, too. (Not that I ought to have an opinion. :-)
04:07:17 <quicksilver> would be nice if GHC had a "treat this particular warning as an error" config
04:08:57 <cjs> Does it not?
04:09:28 <quicksilver> maybe it does. I don't know :)
04:09:43 <cjs> -Werror
04:10:31 <quicksilver> cjs: I meant one particluar warninf.
04:10:36 <quicksilver> not the whole lot
04:11:09 <quicksilver> -fwarn-missing-fields-as-error
04:11:13 <quicksilver> -ferror-missing-fields
04:11:15 <quicksilver> something liek that.
04:27:45 <hpaste>  forkiliens pasted "runInteractiveCommand" at http://hpaste.org/6783
04:28:28 <forkiliens> hi everyone.
04:28:47 <byorgey> hi forkiliens
04:28:53 <forkiliens> does somebody see why the error occurs?
04:29:09 <forkiliens> i put it on hpaste
04:29:48 <forkiliens> hi byorgey
04:30:06 <opqdonut> forkiliens: you can't escape the IO monad
04:30:32 <opqdonut> the result type of do notation needs to be "m something" for some monad m
04:30:48 <opqdonut> try making the last line "return s" and the type "... -> IO String"
04:30:48 <forkiliens> ah,
04:30:58 <forkiliens> ok,
04:31:15 <byorgey> in fact, you don't need 'return s', just make the last line 'hGetContents o'
04:31:30 <forkiliens> done
04:31:37 <opqdonut> yeah, that too
04:31:55 <forkiliens> thanks byorgey
04:32:01 <forkiliens> and opqdonut
04:32:04 <EvilTerran> and then you'll need to run renderArithm2 as an IO action instead of just getting a value back - so, where you're using it, you'll need to make that "s <- renderArithm2 a" in a do block (or something)
04:32:06 <forkiliens> :)
04:32:34 <EvilTerran> ... now the IO seeps through your program, getting into everything ;)
04:32:42 <forkiliens> i now
04:33:02 <forkiliens> maybe i should do as opqdonut said
04:33:15 <forkiliens> and then bind the result to a string
04:33:50 <cjs> Moving it from the tidy world of purely functional programming into the gory mess of I/O-intensive programs.
04:34:15 <forkiliens> EvilTerran: my result type is now IO String. do you think this is still a problem?
04:34:58 <EvilTerran> that's exactly what it should be
04:35:06 <forkiliens> ok, thanks.
04:35:17 <quicksilver> if IO seeps through your program into everything you're doing it wrong.
04:35:58 <forkiliens> i guess that's the price to pay for using haskell  :)
04:36:01 <quicksilver> but renderArithmetics certainly deserves an IO type, cos it does do some real IO, it forks a program.
04:36:11 <cjs> No, it's just that Haskell makes it too hard to do things that way. We should use a superior language, such as Java.
04:36:40 <EvilTerran> lart cjs :: IO Pain
04:36:50 <forkiliens> exactly
04:37:25 <forkiliens> haskell is far easier to read and write than any other language i have met.
04:38:11 <forkiliens> i had to decide for myself one month ago, which "language" to learn: php, lisp, perl, ruby on rails or haskell
04:38:22 <forkiliens> i think that i made the right decision...
04:38:29 <EvilTerran> but of course
04:38:34 <Maddas> You're in #haskell, of course you'll hear what you want to hear :-)
04:38:43 <forkiliens> :)
04:38:45 <cjs> RoR is not even a language!
04:39:19 <cjs> Ruby is nice, if rather flawed in many ways; RoR is the bastard spawn of a PHP programmer and a web "designer".
04:39:27 <forkiliens> but really: i study mathematics, so haskell is really designed to solve problems as i'm used to do it.
04:39:27 * Maddas recommends learning more languages as you get the chance to. Not that all of them are worth learning, but there are many that are.
04:39:53 <cjs> forkiliens: Yes, you'll find Haskell quite comfortable, then.
04:40:21 <forkiliens> so again: thanks for the help: see you later.
04:40:27 * EvilTerran plans to learn python and agda, and maybe c# and j when he can be bothered
04:40:40 <cjs> Actually, different is more important than more. I think that there's no point in learning Python if you know Ruby, or vice versa, at least when it comes to doing it for educational purposes.
04:40:59 <vegai> python is not so much like ruby
04:41:48 <cjs> They seem much of a muchness, to me, though I must admit I know very little Python.
04:42:09 <byorgey> EvilTerran: you will pick up python in approximately 1.5 days.
04:42:10 * Maddas didn't find learning Python particularly educational in the way of learning new concepts/ways of looking at things.
04:42:28 <byorgey> EvilTerran: agda and J might take a bit longer but are more worth it =)
04:42:30 <Maddas> But I didn't delve very deep, and I was exposed to other 'dynamic' languages before, so YMMV.
04:42:33 <EvilTerran> byorgey, huh ... i might do that now then :D
04:42:43 <EvilTerran> (python)
04:42:48 * Maddas still intends to learn J but has yet to find a sufficiently motivating thing to do with it :-(
04:43:24 <byorgey> EvilTerran: beware, the dynamic typing will drive you nuts, but have fun =)
04:44:19 <byorgey> Maddas: how about Project Euler?  have you already done a bunch of those?
04:44:25 <byorgey> that's how I learned J =)
04:44:25 <quicksilver> there are some axes along which python and ruby look the same
04:44:39 <quicksilver> however the 'everything-is-an-object' part of ruby gives it a differetn character
04:44:42 <quicksilver> IMO
04:44:53 <quicksilver> (reminding one of smalltalk, modulo the syntax)
04:45:06 <cjs> Very smalltalkish.
04:45:14 <EvilTerran> except the object-ness of numbers is broken in a few ways :P
04:45:30 <quicksilver> EvilTerran: I am referring more to the idea than the implementation :)
04:45:41 <quicksilver> I don't have enough practical ruby experience to have an opinion on the implementation.
04:45:42 <vegai> EvilTerran: in ruby? How?
04:46:02 <vegai> oh, you meant generally?
04:46:02 <EvilTerran> vegai, numbers are assigned by value, everything else by reference, for starters
04:46:14 <vegai> ah
04:46:16 <cjs> The implementation is full of flaws. That said, it sucks less than a lot of other languages.
04:46:37 * dolio never could get into python.
04:46:47 <dolio> I blame the book I used, though.
04:46:59 <EvilTerran> also, they have no dup method, so you can't write a function that blindly  copies its parameter before mucking about with it
04:47:40 * EvilTerran would solve the first problem, at least, by splitting assignment up into "reference assignment" and "copy" - the former being what = usually means in ruby, and the latter being class-dependent
04:51:02 <quicksilver> dolio: I view python as 'perl tidied up'
04:51:13 <quicksilver> dolio: only it lacks lots of the advantages of perl so that's not very compelling.
04:51:26 <dolio> Heh.
04:51:52 <opqdonut> i view php as "perl tidied up - gone bad"
04:51:54 <dolio> Well, I think at the time I was coming out of Java, so the 'self' parameter everywhere annoyed me.
04:52:08 <opqdonut> python is really it's own genre
04:52:46 <dolio> I remember the book explaining 'lambda' but not explaining why you'd ever want them.
04:53:09 <Maddas> byorgey: Ah, no, I didn't think of using it for that. Did it work well for you? Did you find J's strengths could be use effectively for that kind of problem? :)
04:53:20 <byorgey> Maddas: for many of them, yes
04:53:21 <opqdonut> well python lambdas are crippled anyways
04:53:25 <dolio> And I quit reading when the book went through a page or two description on how to determined whether hash1 < hash2.
04:53:26 <opqdonut> they only allow expressions
04:53:36 <opqdonut> and almost nothing is an expression in python
04:53:49 <Maddas> dolio: Lambdas ar every restricted :-(
04:54:00 <oklopol> i once wrote a brainfuck interp in a python lambda
04:54:03 <opqdonut> yeah, practically useles
04:54:23 <oklopol> (let's not mention it took ages)
04:54:47 <Maddas> I didn't see it as Perl cleaned up, I found it too dirty in its own ways, unfortunately. It looked very tidy, but I found its design dirty in that I couldn't understand many restrictions or why they didn't do other things 'properly'. But I may have been too harsh on it; knowing Haskell doesn't help with that :-)
04:55:00 <quicksilver> one of the things which haskell gets outstanding right is that expressions and statements *are* different.
04:55:03 <opqdonut> the python object system is just ugly
04:55:04 <quicksilver> but they are both expressions.
04:55:08 <quicksilver> now that is clever.
04:55:38 <Twey> opqdonut: They're meant to be crippled
04:55:46 <opqdonut> of course
04:55:52 <Twey> The idea is, if it doesn't fit within a lambda, make it a named function
04:55:52 <dolio> I can't recall if I'd seen ruby at the time, either. If I had, it'd odd that I didn't realize what lambda expressions were for, since ruby effectively uses them all over the place.
04:55:52 <opqdonut> but i don't agree with the reasons
04:56:07 <opqdonut> Twey: simple callbacks are hell in python
04:56:08 <Twey> There were great debates over whether there should be lambdas at all
04:56:20 <opqdonut> i prefer lua-style functions
04:56:21 <Twey> opqdonut: Not really
04:56:25 <opqdonut> which are just values that get assigned
04:56:34 <Twey> Python functions are too
04:56:46 <opqdonut> so you can say f(function (a) print(a) end)
04:57:00 <opqdonut> Twey: yeah but it's not really practical
04:57:06 <dmwit> Hey, I know a language where you can do that!
04:57:10 <Twey> Heh, dmwit
04:57:11 <dmwit> f = \a -> print a
04:57:16 <Twey> f print
04:57:16 <opqdonut> dmwit: exactly :)
04:57:26 <Twey> As of Python 3: f(print)
04:57:29 <opqdonut> Twey: yeah i could've said f(print) too
04:57:32 <opqdonut> bad example
04:57:50 <opqdonut> perl6 >>> python3
04:58:13 <Twey> Highly unlikely
04:58:50 <quicksilver> that's a vacuous truism
04:58:58 <quicksilver> perl6 is better than everything, because it doesn't exist :)
04:59:02 <opqdonut> :)
04:59:05 <Twey> Heh
04:59:19 <Twey> Perl is highly practical, but that practicality comes of being a load of things just nailed together, with features tacked on later where appropriate
04:59:20 <opqdonut> (well it exists as much as python3 i'd say)
04:59:35 <opqdonut> no, stuff doesn't get "tacked on" to perl
04:59:38 <opqdonut> perl evolves
04:59:50 <opqdonut> it might look the same but there really is a great amount of cohesion
04:59:54 <Twey> That's a nicer way of putting it, yes :-)
05:00:11 <opqdonut> i really like perl
05:00:18 <quicksilver> http://lolgeeks.com/wp-content/uploads/2007/05/lolgeeks016.jpg
05:00:18 <Twey> I noticed :-P
05:00:41 <opqdonut> some perl code (esp. old cgi stuff) is horrible
05:00:45 <Twey> Anything that can seriously use $_ or $@ as an identifier gets marked down several places in my book :-P
05:00:51 <opqdonut> but the language is beautiful
05:01:15 <Twey> But there are things like chomp
05:01:21 <opqdonut> it really does have the power of natural languages like wall intended
05:01:22 <quicksilver> anything that can seriously use \ and -> as syntactic constructs whilsts <-> is an identifier gets marked down in my book.
05:01:25 <Twey> Which are so imperative they make my eyes hurt
05:01:31 <opqdonut> true
05:01:33 <quicksilver> </irony>
05:01:49 <Twey> quicksilver: But they're *syntax*
05:01:52 <quicksilver> (the point is that concrete syntax at the level of individual characters really isn't important)
05:01:55 <dolio> > let f $_ x = f (f x) in succ $_ 1
05:01:55 <lambdabot>  Parse error in pattern at "in" (column 22)
05:02:04 <dolio> > let f $@ x = f (f x) in succ $@ 1
05:02:06 <lambdabot>  3
05:02:07 <Twey> $_ and $@ are actual built-in identifiers
05:02:25 <quicksilver> $ is an actual built-in identifier to haskell
05:02:33 <quicksilver> as far as anything is built-in to haskell, which is not very far.
05:03:11 <Maddas> Arguing about what is 'ugly' without actually looking at the language as a whole is meaningless and only leads to petty arguments.
05:03:25 <doserj> it is not as built-in as []
05:03:31 <cjs> Perl totally nailed me the other day by being extremely impure and modifying a parameter that I wanted only to read. Took me half an hour to debug before I realized what it was doing.
05:03:33 <quicksilver> it's not meaningless
05:03:40 <quicksilver> but you have to be careful
05:03:43 <dmwit> It is possible to write ugly code in any sufficiently powerful language.
05:03:45 <quicksilver> ugly/beautiful does matter.
05:03:55 <dmwit> It is even possible to write ugly code in many under-powered languages!
05:04:08 <quicksilver> but you have to try to distinguish between superficial syntax (which is subjective, and you become accustomed to)
05:04:09 <Twey> dmwit: Indeed
05:04:10 <Maddas> quicksilver: It is meaningless if you are just criticizing from the stand-point of some other language.
05:04:25 <quicksilver> and fundamental syntactic issues (which affect your programs no matter how objective you are)
05:04:39 <Twey> dmwit: What one has to worry about is when the core language itself dictates functions/features that are that ugly
05:04:41 <quicksilver> $@ is superficial.
05:04:55 <quicksilver> the syntax for defining a lambda in Java is fundamental.
05:05:06 <Twey> Er, non-existant
05:05:09 <Twey> Last I looked
05:05:20 <dolio> You can do it, sort of.
05:05:38 <Maddas> quicksilver: If you pull it out of context, it is meaningless, as you're not looking at how it is used and what it is used for.
05:05:46 <dmwit> new Runnable() { public void run() { ... } } // longish
05:05:48 <Twey> Anonymous classes were the closest it got last time I used Java, which was admittedly a while ago.
05:05:54 <cjs> Anonymous class instantiation against an interface. As with anything in Java, it takes a lot of lines to do it.
05:05:55 <quicksilver> yes, what dmwit said.
05:05:59 <Twey> Yeah.
05:06:05 <Maddas> It is very easy to make fun of Haskell because it looks different from other languages, for example.
05:06:19 <Maddas> (And works differently :-))
05:06:29 <quicksilver> you can construct pretty complex lambda expressions in java
05:06:34 <quicksilver> including partial applications and stuff
05:06:40 <dolio> Of course, that only works for void functions taking no arguments.
05:06:40 <quicksilver> there are libraries for it
05:06:49 <quicksilver> but the syntax remains horrid horrid horrid.
05:06:54 <quicksilver> no matter how many clever tricks you pull off.
05:06:59 <dolio> So you need a different interface for each type of lambda expression. :)
05:07:00 <quicksilver> just like C++ and boost.
05:07:07 <quicksilver> but C++ has more tricks up its sleeve.
05:07:10 <Twey> Mmm, it's not *horrid*
05:07:13 <Twey> It's just clunky
05:07:30 <Twey> It's kind of regular, in its own way (moreso than C++', at least)
05:07:33 <dmwit> Boost is actually pretty clever.
05:07:35 <Maddas> quicksilver: I maintain that all of this is still meaningless if you don't take into consideration how people usually use the languages.
05:07:36 <dolio> Or, maybe with generics, one for each arity.
05:07:47 <quicksilver> dmwit: boost is extraordinarily clever.
05:07:48 <dmwit> Though I challenge anyone to have the patience to understand one of its errors.
05:07:55 <Twey> Haha
05:08:03 * Maddas prefers Oleg's C++ lambda to Boost's without fully understanding Boost :-)
05:08:03 <quicksilver> dmwit: I would go as far as to say it is happs-level-clever :)
05:08:13 <quicksilver> possibly even oleg-level-clever.
05:08:31 <quicksilver> but its mere existence is a testament to the flaws in C++, in a way.
05:08:37 <quicksilver> Maddas: I agree with what I think you mean.
05:08:44 <quicksilver> Maddas: I don't agree with what you said.
05:09:00 <quicksilver> I couldn't care less how people usually use a language. I'm not people and I'm not usual.
05:09:14 <quicksilver> I care about the best way to solve problems in a language.
05:09:57 <Maddas> Yes, and that's usually not the same across languages.
05:10:14 <Maddas> I think I do agree with what you mean :-)
05:10:17 <quicksilver> of course.
05:10:29 <quicksilver> but, e.g., "people" "usually" use Java extremely poorly.
05:10:34 <opqdonut> mhmm
05:10:34 <Maddas> Oh, of course.
05:10:38 <quicksilver> however, I don't base my dislike of Java on that.
05:10:43 * Twey nods.
05:10:49 <quicksilver> I base my dislike of Java on my belief that even the 'best way available'
05:10:54 <quicksilver> is pretty unsatisfactor.
05:11:00 <Twey> y.
05:11:04 <quicksilver> Although I will definitely say that each version gets better.
05:11:14 <quicksilver> and they have addressed a lot of the problems in the recent ones.
05:11:15 <Twey> 1.5 was a definite improvement.
05:11:22 <quicksilver> (I don't really follow it closely any more, though)
05:11:25 <Maddas> It's just that many of these discussions strike me as silly as someone new to Haskell complaining about pointer arithmetic not being as easy as in C, or strings not being arrays by default, or something like that.
05:11:33 * quicksilver nods
05:11:35 <Maddas> It's easy to complain, that doesn't make the complaint valid :-)
05:11:45 <quicksilver> it sometimes makes the complaint fun.
05:11:57 <opqdonut> Twey: well they really messed up generics
05:12:16 <Twey> opqdonut: But at least they had them.
05:12:33 <opqdonut> for instance, for a map there is "void put(K key, V value)" but "Object get(K key)" wtf?
05:12:38 <opqdonut> no, sorry
05:12:40 <quicksilver> they really messed up threads, too.
05:12:47 <opqdonut> "V get(Object o)" it was
05:12:49 <Maddas> quicksilver: But I think the damage it does to any language to see such bad comparisons all over the internet isn't worth it. Many languages are eagerly dismissed based on opinions of people who never really learned them.
05:12:54 <quicksilver> at least they have the good grace to admit their mistakes in public :)
05:13:04 <dmwit> Yeah, Generics weren't done correctly, and primitives need to die.
05:13:28 <dmwit> I've got other complaints, too, but they just don't seem appropriate to bring up...
05:13:35 <quicksilver> didn't they mess up array subtyping, too?
05:13:37 <quicksilver> ;)
05:13:47 <opqdonut> oh they did?
05:13:59 <quicksilver> yes, they did.
05:14:11 <quicksilver> They didn't spot the covariance problem.
05:14:27 <Twey> Maddas: People who eagerly dismiss a language based on the dubious complaints of someone in an IRC channel don't deserve to find their new favourite language :-)
05:15:22 <ddarius> How do you fail to spot that in the mid 90's?
05:16:08 <dolio> Mid 90s?
05:16:17 <dolio> I thought they just got generics recently.
05:16:30 <quicksilver> array subtyping was a 'bug' in the in first java spec
05:16:37 <quicksilver> they 'fixed' it with runtime type checks.
05:16:44 <quicksilver> (in the implementation, not the spec)
05:16:46 <dolio> Oh, okay.
05:16:51 <opqdonut> ah array downcasting?
05:16:55 <quicksilver> I don't know if a later version of the spec mentioned it explicitly.
05:17:04 <quicksilver> the original version didn't mention the problem at all
05:17:07 <quicksilver> so it really was a 'bug'
05:17:40 <quicksilver> a :: Circle[]; b :: Shape[]; b = a; b[0] = new Square()
05:17:50 <quicksilver> to mix haskell and java notation in a weird way :)
05:18:11 <opqdonut> oh, okay
05:18:15 <quicksilver> the fact that with reads it's safe to upcast but with writes it's safe to downcast.
05:18:22 <quicksilver> or is it the other way around? :)
05:19:40 <dmwit> I left-cast on inscriptions.
05:25:42 <dolio> Also, how'd they get generics wrong? Wasn't Wadler working on them?
05:25:57 <opqdonut> well not wrong per se
05:26:11 <opqdonut> they're not just utilized properly in the standard library and so forth
05:26:56 <ddarius> opqdonut: There are some issues related to backwards compatibility.
05:27:08 <opqdonut> that's what they tell me
05:27:28 <dolio> I suppose that does seem to be the source of their problems.
05:27:43 <opqdonut> luckily scala is quite nice
05:27:46 <opqdonut> tho impure
05:30:14 <quicksilver> generics with type-erasure to Object is actually quite comparable to the way GHC implements true polymorphism
05:33:15 <dolio> Yeah, it's odd that one doesn't run into problems with that like people seem to complain about with Java generics.
05:33:30 <dolio> Then again, I don't know how prevalent the issues people bring up actually are in practice.
05:33:40 <opqdonut> it's because the erasure in java is done too early
05:34:01 <dolio> Like implementing 'interface I<T> {...}' at two different types.
05:34:16 <dmwit> I've run up against erasure problems, and I only used Java for an introductory CS class.
05:34:17 <opqdonut> for example with the "V get(Object o)" example i cited, giving an argument of the "wrong" type didn't generate an error
05:35:27 <qwr> what kind of erasure problems?
05:35:49 <dolio> I'm too tired to think what the haskell analogue to that would be.
05:36:15 <opqdonut> well just describe the java problem
05:36:59 * qwr thinks there are basically 3 of them - java array's have they're own non-erasured typing, boxing overhead (although jvm could optimise that?) and using reflection
05:37:01 <dolio> Try to implement 'interface I<T> { void foo(T); }' at two different concrete types.
05:37:07 <dmwit> I'm not sure I can remember for sure, but I think there's was some issue where there was no type-safe way to create an array of objects.
05:37:12 <qwr> in haskell only the boxing is relevant afaik
05:37:24 <dolio> It won't work, because they get erased to 'void foo(Object)'.
05:37:36 <quicksilver> dolio: some of the problems are specific to mutation
05:38:19 <dmwit> Like "<T> class Foo { T[] bar() { return new T[5]; } } // not valid".
05:38:31 <dmwit> (modulo syntax errors)
05:38:53 <opqdonut> ah
05:39:33 <dolio> I was talking with someone in #haskell-blah about this a while back, and apparently C# generics let you implement such parameterized interfaces twice.
05:40:03 <dolio> Although, not at arbitrary parameters, of course, since you could get into trouble if T1 = T2.
05:40:18 <dolio> But with concrete types that shouldn't be a problem.
05:42:22 * qwr thinks, that java could actually support this with type erasure too. it would have to join the methods and do runtime argument typechecking though
05:42:58 <dmwit> Yeah, there's a bug open to do some runtime type-checking.
05:44:20 <dolio> Like I said, though, I don't know how often that comes up in practice.
05:46:11 <vegai> Cale: Is imports.h supposed to be somewhere in lambdabot? I darcs pulled just now and get State/L.hs: error: imports.h: No such file or directory
05:52:38 <blahblah> hello is there anyone who would help me out with some questions i need to ask in haskell
05:52:47 <EvilTerran> ?quote meta-ask
05:52:47 <lambdabot> No quotes match.
05:52:52 <EvilTerran> gr
05:52:59 <EvilTerran> ?remember meta-ask don't ask to ask, just ask!
05:52:59 <lambdabot> Nice!
05:54:32 <blahblah> does anyone know how to write pattern matching code?
06:00:26 <opqdonut> http://www.lisperati.com/landoflisp/ <- this has probably been here already?
06:02:32 <shepheb> opqdonut: yeah, that was discussed yesterday
06:02:51 <shepheb> there's a comment from the author out there somewhere, too
06:03:15 <opqdonut> ok :)
06:04:24 <quicksilver> blahblah: yes, I'm sure everyone here does.
06:04:30 <quicksilver> blahblah: you should ask a specific question.
06:11:11 <blahblah> normalise [VarPair "x" 0,VarPair "y" 2,VarPair "z" 1,VarPair "y" 4] and the output should be [VarPair "y" 6,VarPair "z" 1] any hints/ideas? and normalise is the function i havent actually implemented
06:11:38 <quicksilver> this looks a little like a homework exercise, perhaps?
06:11:44 <quicksilver> how far have you got so far?
06:11:48 <quicksilver> what did you try which didn't work?
06:13:21 <opqdonut> :t groupBy -- a hint
06:13:23 <lambdabot> forall a. (a -> a -> Bool) -> [a] -> [[a]]
06:15:11 <quicksilver> addTogether (VarPair n a) (VarPair m b) = VarPair n (a+b)
06:15:18 <quicksilver> blahblah: this is a clue on how to add together two
06:15:24 <quicksilver> which shows how pattern matching works in general.
06:15:27 <quicksilver> does that help?
06:17:27 <blahblah> normalise :: [VarPair] -> LinearSum this is my method stub is this correct?
06:19:09 <shepheb> @seen byorgey
06:19:09 <lambdabot> byorgey is in #xmonad, #haskell-blah and #haskell. I last heard byorgey speak 1h 25m 49s ago.
06:21:15 <quicksilver> blahblah: doesn't look like it to me, no
06:21:28 <quicksilver> blahblah: the example you gave takes a list of varpairs and returns a list of varpairs
06:21:36 <quicksilver> I would expect type [VarPair] -> [VarPair]
06:22:13 <blahblah> yea but LinearSum is the same as list of VarPairs
06:22:22 <quicksilver> then, yes.
06:22:23 <quicksilver> :)
06:24:04 <yitz_> ha! landoflisp is very funny.
06:26:30 <blahblah> 1 sec guys going 2 use my laptop.
06:27:35 <blahblah> the main problem i am having is that i don't no what functions to use, to extract the data i want.
06:29:03 <quicksilver> blahblah: did you see my example?
06:29:10 <quicksilver> blahblah: that shows how to extract information
06:29:26 <blahblah> 1 sec
06:29:54 <blahblah> oh yeah thanks ill give it a try :>
06:29:58 <blahblah> :)
06:30:43 <quicksilver> you can also write little "accessor" functions if that suits you
06:30:54 <quicksilver>  getName (VarPair n a) = n
06:30:59 <quicksilver> getValue (VarPair n a) = a
06:31:00 <quicksilver> for example
06:31:05 <quicksilver> more than one way to do it ;)
06:31:08 <yitz_> blahbla: also look at opqdonut's hint above
06:31:21 <quicksilver> as an anonymous function you can also write (\(VarPair n a) -> n)
06:46:21 <cjs> Time to head home. Good day. Thanks, guys.
06:46:42 <blahblah> is this a good idea, i am going to make 4 more methods 1 to sort the [], 2 make a sub list ie group like terms, 3 add the [] together and 4 put the [] together? i will call these methods from my main method normalise
06:48:11 <quicksilver> that sounds good.
06:48:23 <quicksilver> number (2) is already in the standard library
06:48:24 <quicksilver> groupBy
06:48:50 <quicksilver> indeed, (1) you can use sortBy
06:50:09 <blahblah> oh thanks that makes things a hole lot simpler. if i had a [1,2,3] and i wanted to add all elms to get [6]. is there a function that does this?
06:50:21 <blahblah> whole*
06:50:33 <tromp> > sum [1,2,3]
06:50:35 <lambdabot>  6
06:50:46 <shepheb> :t sumBy
06:50:47 <lambdabot> Not in scope: `sumBy'
06:51:35 <quicksilver> shepheb: it's called "foldMap" and it's in Data.Foldable.
06:51:47 <quicksilver> (closest generalisation of 'sumBy')
06:51:56 <quicksilver> :t F.foldMap
06:51:57 <lambdabot> Couldn't find qualified module.
06:52:02 <shepheb> or "flip foldl' 0"
06:52:04 <quicksilver> :t Data.Foldable.foldMap
06:52:05 <lambdabot> forall a m (t :: * -> *). (Monoid m, Data.Foldable.Foldable t) => (a -> m) -> t a -> m
06:52:14 <quicksilver> shepheb: that's just sum. not sumBy ;)
06:52:56 <shepheb> > (flip foldl' 0) (flip subtract) [1,2,3]
06:52:57 <lambdabot>  -6
06:54:13 <blahblah> yeah sum works fine
06:59:27 <yitz_> sumBy f = sum . map f
06:59:43 <blahblah> I did this   sort ["x","y","z","z","w"] and got  ["w","x","y","z","z"]. but i want to no how to group elms together in separate [] ie ["w"],["x"], ["y"], ["2Z"]
07:00:03 <yitz_> group
07:00:19 <scook0> > group "mississippi"
07:00:20 <lambdabot>  ["m","i","ss","i","ss","i","pp","i"]
07:00:32 <yitz_> > group $ sort ["x","y","z","z","w"]
07:00:33 <lambdabot>  [["w"],["x"],["y"],["z","z"]]
07:00:46 <scook0> > map (head &&& length) . group . sort $ "mississippi"
07:00:46 <lambdabot>  [('i',4),('m',1),('p',2),('s',4)]
07:01:58 * shepheb ponders the terrifying amount of C code that line would take
07:02:35 * yitz_ ponders the potential buffer overflows in the C code
07:02:41 <Deewiant> can anybody implement (group . sort) in a one-liner without requiring Ord (i.e. without sort)? :-)
07:02:53 <yuriyp> :t ($)
07:02:54 <lambdabot> forall a b. (a -> b) -> a -> b
07:02:58 <shepheb> I've actually documented C functions with Haskell code. not very useful for others, perhaps, but handy for my understanding.
07:03:34 <yitz_> > nub "mississippi"
07:03:34 <lambdabot>  "misp"
07:03:45 <shepheb> Deewiant: see clusterBy from some blog post? it needs a "signature function", and the signatures need to be Ord.
07:03:48 <wli> Deewiant: With Eq use nub, then tabulate occurrences.
07:04:14 <Deewiant> shepheb: without Ord at all. And I've got an implementation already, just wondering if it can be done 'cleanly'.
07:04:46 <Saizan_> ?src nubBy
07:04:46 <lambdabot> nubBy eq []             =  []
07:04:46 <lambdabot> nubBy eq (x:xs)         =  x : nubBy eq (filter (\ y -> not (eq x y)) xs)
07:06:15 <Deewiant> > let f x = map (\a -> filter (==a) x) $ nubBy (==) x in f [1,2,3,1,4,5,2]
07:06:16 <lambdabot>  [[1,1],[2,2],[3],[4],[5]]
07:06:36 <Deewiant> that works, I guess.
07:06:38 <wli> let f [] = [] ; f (x:ys) = let (xs, zs) = partition (== x) in (x, length xs + 1) : f zs
07:07:06 <wli> > let f [] = [] ; f (x:ys) = let (xs, zs) = partition (== x) in (x, length xs + 1) : f zs in f "mississippi"
07:07:07 <lambdabot>  Couldn't match expected type `(t, t1)'
07:07:25 <Saizan_> > let f [] = [] ; f (x:ys) = let (xs, zs) = partition (== x) ys in (x, length xs + 1) : f zs in f "mississippi"
07:07:25 <lambdabot>  [('m',1),('i',4),('s',4),('p',2)]
07:07:44 <s710b> > (.) group  sort "mississippi"
07:07:45 <lambdabot>  ["iiii","m","pp","ssss"]
07:07:49 <Deewiant> > let fBy f x = map (\a -> filter (f a) x) $ nubBy f x in fBy (==) "mississippi"
07:07:49 <lambdabot>  ["m","iiii","ssss","pp"]
07:08:21 <s710b> > group .  sort "mississippi"
07:08:22 <lambdabot>  Couldn't match expected type `[a]' against inferred type `Char'
07:08:35 <Deewiant> > group . sort $ "mississippi"
07:08:36 <lambdabot>  ["iiii","m","pp","ssss"]
07:09:00 <s710b> this seems strange to me
07:09:31 <shepheb> s710b: that's equivalent to "group . (sort "mississippi")"
07:09:58 <s710b> > group . (sort "mississippi")
07:09:59 <lambdabot>  Couldn't match expected type `[a]' against inferred type `Char'
07:10:02 <shepheb> > group . sort $ "mississippi"
07:10:03 <lambdabot>  ["iiii","m","pp","ssss"]
07:10:27 <wli> > map (head &&& length) (group (sort "mississippi"))
07:10:27 <lambdabot>  [('i',4),('m',1),('p',2),('s',4)]
07:11:53 <yuriyp> guys, could somebody explain what the purpose of $ is?
07:12:11 <allbery_b> parenthesis alternative
07:12:32 <idnar> "blah blah $ blah blah" is easier to write than "(blah blah) (blah blah)"
07:12:40 <scook0> > (head "hello", head $ "hello")
07:12:40 <lambdabot>  ('h','h')
07:13:01 <scook0> normally function application has higher precedence than anything
07:13:17 <yuriyp> allbery_b: scook0 Ok, I see. Thanks
07:13:24 <shepheb> @src ($)
07:13:24 <scook0> but using ($), you can make use of application with lower precedence
07:13:24 <lambdabot> f $ x = f x
07:13:50 <scook0> the cool thing is that ($) is actually just id with a restricted type
07:13:53 <allbery_b> there's a side usage which lets you treat function application as a section
07:13:57 <scook0> > (+1) `id` 3
07:13:58 <lambdabot>  4
07:14:23 <scook0> and yes, the section thing is handy too
07:14:38 <yitz_> yuriyp: $ is an operator. But all it does is apply the function on the left to the value on the right. ie almost nothing, just handy for removing parens in some situations.
07:14:51 <scook0> makes it easy to e.g. apply a list of functions to a single argument, rather than the other way around
07:15:07 <scook0> map ($ x) [f, g, h]
07:15:20 <yuriyp> I have understood the idea, thanks guys
07:15:27 <yitz_> > map ($ pi / 4) [sin, cos, tan]
07:15:28 <lambdabot>  [0.7071067811865475,0.7071067811865476,0.9999999999999999]
07:15:29 <ZimaN> f1 . f2 . f3 . f4 $ something
07:15:42 <yitz_> > map ($ pi / 3) [sin, cos, tan]
07:15:43 <lambdabot>  [0.8660254037844386,0.5000000000000001,1.7320508075688767]
07:16:08 <quicksilver> > sequence [sin,cos,tan] (pi/3)
07:16:08 <lambdabot>  [0.8660254037844386,0.5000000000000001,1.7320508075688767]
07:16:48 <idnar> heh
07:16:52 <scook0> showoff :)
07:16:57 <idnar> hooray (->r) monad
07:17:02 <idnar> er (r->)
07:18:16 <shepheb> > map ($ x) [f,g,h] :: [Expr]
07:18:17 <lambdabot>  [f x,g x,h x]
07:18:58 <blahblah> ok thanks guys done :)
07:19:00 <yuriyp> WHich of the Haskell books gives the best examples of using $ ?
07:19:28 <yitz_> blahblah: good!
07:19:44 <quicksilver> I don't know. Why not read them all and prepare a report on the subject, yuriyp :P
07:20:01 <yuriyp> quicksilver: I am doing that right now
07:20:02 <blahblah> yitz_ and quicksilver thanks for all ur help
07:20:03 <yuriyp> :-)
07:20:10 <quicksilver> blahblah: welcome.
07:21:03 <yitz_> > sequence [f,g,h] x :: [Expr]
07:21:03 <lambdabot>  [f x,g x,h x]
07:21:05 <yuriyp> quicksilver: almost finished Hutton - no mentioning of $
07:21:21 <quicksilver> interesting.
07:21:26 <quicksilver> maybe it's just not hutton's style.
07:21:37 <quicksilver> anyway, I don't think $ is very interesting.
07:21:48 <quicksilver> there isn't really anything much to say about it.
07:21:49 <shepheb> it's not really an essential feature, it's just handy
07:21:50 <quicksilver> ;)
07:21:58 <yitz_> yuriyp: it's the latest fad. Actually, lately you see people writing . . . $ instead of $ $ $ $
07:22:18 <quicksilver> f $ g $ h $ x is strongly frowned upon.
07:22:23 <quicksilver> by me and Cale anyway.
07:22:26 <shepheb> @src (.)
07:22:26 <lambdabot> (f . g) x = f (g x)
07:22:46 <s710b> quicksilver, why?
07:22:49 <Saizan_> i wonder why Cale hasn't changed $ associativity yet
07:22:58 <quicksilver> because (.) is associative
07:23:01 <quicksilver> I think that's the core of it
07:23:12 <quicksilver> associative binary operators are much more pleasant to work with.
07:23:17 <yitz_> quicksilver: there actually could be a difference, depending on the compiler. For more naive compilers, $ $ $ could be better.
07:23:26 <quicksilver> given "f . g . h . i . j $ k"
07:23:37 <quicksilver> "g . h" is a sub expression.
07:23:45 <quicksilver> and can be abstracted out if you want.
07:23:56 <s710b> ah i see
07:23:58 <sw17ch> was any one around last night about 9:00PM EST?
07:23:59 <quicksilver> given "f $ g $ h $ i $ j $ k"
07:24:06 <quicksilver> g$h is not a sub expression
07:24:09 <sw17ch> ah, any one here now around then
07:24:11 <quicksilver> ($ not being associatve)
07:24:19 <quicksilver> sw17ch: I am always here.
07:24:26 <quicksilver> what happened at 9pm est which was so interesting?
07:24:53 <yitz> quicksilver, i think the main issue is that with . . . $ it is easier to switch between function and value when you refactor. Also, . is less noisy.
07:24:55 <sw17ch> well, i was there asking questions before i was side tracked... i was hoping to pick up where i left off ... :)
07:25:40 <Modius> In haskell, it's my understanding that there are "immutable" containers that you can "modify" by requesting Original + Delta (and under the hood, the structure happens to change and the holders of hte old reference get a delta + new) - if there is one that is a lookup or hashtable, can you give me some function names so I can go look it up?
07:25:41 <quicksilver> yitz: I think the things you describe are consequences of . being associative.
07:26:02 <quicksilver> which is why I chose that as the base reason.
07:26:04 <yitz> . less noisy?
07:26:13 <quicksilver> Modius: Only DiffArray behaves that way AFAIK.
07:26:48 <sw17ch> i'm looking around and finding variations of repeatM (let repeatM = sequence . repeat), but i need it slightly different and i'm not entirely sure how the syntax goes
07:27:14 <quicksilver> sw17ch: what do you need, in fact?
07:27:22 <sw17ch> i want a repeatWithForwardM that .. is there a foldM?
07:27:30 <quicksilver> there is a foldM
07:27:31 <EvilTerran> ?type foldM
07:27:31 <allbery_b> :t foldM
07:27:31 <Modius> quicksilver:  Thanks
07:27:33 <lambdabot> forall a b (m :: * -> *). (Monad m) => (a -> b -> m a) -> a -> [b] -> m a
07:27:33 <lambdabot> forall a b (m :: * -> *). (Monad m) => (a -> b -> m a) -> a -> [b] -> m a
07:27:39 <EvilTerran> ?quote fugue
07:27:39 <lambdabot> monochron says:  "Welcome to #haskell, where your questions are answered in contrapuntal fugues."
07:27:44 <quicksilver> it won't be very useful on infinite lists.
07:27:56 <quicksilver> repeat generates infinite lists
07:27:59 <quicksilver> is that what you're after?
07:28:00 <sw17ch> yes
07:28:03 <EvilTerran> it can short-circuit out on infinite lists, can't it?
07:28:04 <sw17ch> well,
07:28:04 <sw17ch> hmm
07:28:14 <shepheb> :t repeat
07:28:15 <lambdabot> forall a. a -> [a]
07:28:17 <EvilTerran> ... or not. nvm.
07:28:24 <sw17ch> the project involves performing a transformation over an IOArray n times
07:28:27 <quicksilver> EvilTerran: don't think so, no.
07:28:35 <shepheb> why can I never keep repeat and replicate straight in my head?
07:28:37 <quicksilver> sw17ch: ah, that's not infinite. that's 'n' times.
07:28:39 <EvilTerran> sw17ch, might you want replicate
07:28:40 <EvilTerran> ?
07:28:40 <quicksilver> I think you mean replicate :)
07:28:43 <quicksilver> replicateM, too
07:28:49 <quicksilver> is replicateM close to what you need?
07:28:57 <quicksilver> in what respect is it lacking?
07:29:08 <sw17ch> ah, i was thinking along the lines of "drop 10 $ repeatM m"
07:29:09 <yitz> how about iterate?
07:29:12 <yitz> @type iterate
07:29:13 <lambdabot> forall a. (a -> a) -> a -> [a]
07:29:24 <sw17ch> can i combine iterate with sequence?
07:29:32 <sw17ch> is therea monadic form?
07:29:34 <yitz> sure
07:29:43 <quicksilver> :t iterateM
07:29:44 <lambdabot> Not in scope: `iterateM'
07:29:46 <quicksilver> no.
07:29:52 <quicksilver> you could do it yourself.
07:29:57 <quicksilver> you'd end up with replicateM
07:30:06 <sw17ch> hehe, let me look at that quick
07:30:11 <sw17ch> it probably does exactly what i need
07:30:11 <quicksilver> but replicateM in a StateT transformed version of your original monad.
07:30:17 <sw17ch> ... is there a replicateWhileM?
07:30:18 <quicksilver> if I've understood you correctly.
07:30:21 <sw17ch> :t replicateWhileM
07:30:22 <lambdabot> Not in scope: `replicateWhileM'
07:30:29 <quicksilver> no, there aren't any built in 'While-like' loops
07:30:36 <quicksilver> they're easy enough to write, obviously.
07:30:48 <quicksilver> (there aren't any monadic onces, I should say)
07:30:50 <sw17ch> hmm.. i guess in my circumstance, i really only want to look at set repetitions
07:30:53 <yitz> usually takeWhile
07:31:33 <sw17ch> i'm still a little hazy about constructing monads and how they interact with the normal stuff :)
07:31:53 <quicksilver> that's fine.
07:32:06 <quicksilver> if you explain a little more about what you're actaully doing, our answers may be more focussed.
07:32:12 <quicksilver> whatever it is, it's possible :)
07:32:20 <quicksilver> @faq Can haskell solve sw17ch's problem?
07:32:20 <lambdabot> The answer is: Yes! Haskell can do that.
07:32:24 <yitz> ok, here comes...
07:32:25 <sw17ch> does replicateM send the output of the (n - 1) pass to the nth pass?
07:32:31 <quicksilver> sw17ch: no.
07:32:40 <quicksilver> sw17ch: but replicateM in a state monad does, effectively.
07:32:57 <EvilTerran> (or any MonadState)
07:33:10 <quicksilver> that's what I meant by state monad with a small 's'.
07:33:11 <quicksilver> FWIW.
07:33:16 <sw17ch> okay, here's the problem. :) I have a large IOArray (large being 1x1 to (>1)x(>1))
07:33:22 <EvilTerran> oh, yeah, "a state monad"
07:33:35 <quicksilver> you could also use foldM with a list [1..n] where you ignore the actual list value.
07:33:42 <quicksilver> slightly perverse but would work.
07:33:43 <sw17ch> i want to perform specific operations on it (similar to the game of life) n times
07:34:00 <EvilTerran> ?type \f e n -> foldr (=<<) e (replicate n f)
07:34:02 <lambdabot> forall (m :: * -> *) b. (Monad m) => (b -> m b) -> m b -> Int -> m b
07:34:13 <EvilTerran> owzat?
07:34:14 <byorgey> sw17ch: sounds like replicateM to me, if you know n in advance
07:34:27 <sw17ch> yes, it is, except for the monad forwarding part
07:34:39 <sw17ch> i want the action performed over the output of the previous action
07:34:43 <sw17ch> not over the same action every time
07:34:45 <quicksilver> there isn't a forwarding part.
07:34:48 <quicksilver> you're using IOArrays
07:34:52 <quicksilver> those are ref-type.
07:34:52 <yitz> @type repeatM
07:34:53 <lambdabot> Not in scope: `repeatM'
07:34:57 <sw17ch> mm... good point
07:35:01 <EvilTerran> oh yeah
07:35:04 <quicksilver> you automatically have the new value :)
07:35:08 <EvilTerran> that makes things easier :D
07:35:11 <quicksilver> in that respect, IO *is* a state monad.
07:35:20 <quicksilver> it carries all sorts of state, including the values of mutable arrays :)
07:35:22 <sw17ch> so, replicateM would re-use the old values then
07:35:32 <quicksilver> replicateM would automatically get the new value
07:35:37 <quicksilver> the old value wouldn't even exist any more
07:35:39 <sw17ch> oh, marvelous
07:35:42 <quicksilver> that's the point of mutability.
07:35:44 <sw17ch> :)
07:35:48 <quicksilver> if you were doing this with immutable arrays
07:35:52 <quicksilver> then you would have to pass it on
07:35:58 <quicksilver> which you could do with an explicit state monad :)
07:36:07 <sw17ch> hhe
07:36:13 <sw17ch> alright, i'll give this a shot quick then
07:36:24 <quicksilver> in terms of expressiveness, an algorithm using a mutable array in IO is equivalent to an algorithm using an immutable one in a state monad.
07:36:31 <quicksilver> of course, they compile to rather different code.
07:36:42 <quicksilver> although many people would like a compiler which could optimise them to the same code.
07:37:28 <Saizan_> in some cases with array fusion they can get quite close
07:37:52 <sw17ch> =)
07:38:47 <EvilTerran> "array fusion"... sounds like something the LHC will be able to do :P
07:39:38 <yitz> lhc?
07:39:50 <quicksilver> large hadron collider
07:40:06 <quicksilver> the science project in CERN to create strange matter and destroy the planet.
07:40:13 <sw17ch> also to destroy the planet :P
07:40:18 <sw17ch> (or so some would like to think)
07:40:25 <sw17ch> oh wow
07:40:33 <sw17ch> i wish i read faster than i typed
07:40:38 <yitz> got it :) yes, some people are afraid of Haskell compilers in the same way
07:41:14 <Saizan_> so we've anti, dark and strange matter?
07:41:14 <sw17ch> so, on an unrelated note... can haskell programs really be thought of as pure mathematical expressions?
07:41:23 <quicksilver> yes.
07:41:26 <quicksilver> next question?
07:41:37 <sw17ch> the ambiguity is killing me :)
07:41:42 <allbery_b> there also used to be the feat of muon matter, iirc
07:41:44 <allbery_b> fear
07:41:48 <yitz> (see landoflisp)
07:42:00 <byorgey> @faq can Haskell compilers destroy the planet?
07:42:00 <lambdabot> The answer is: Yes! Haskell can do that.
07:42:05 <sw17ch> hahha
07:42:13 * quicksilver puts on a muon matter mask and goes "BOO" to allbery_b.
07:42:38 <allbery_b> (muons are, in a sense, "heavy electrons")
07:42:44 <sw17ch> so, if haskell programs are purely math, then wouldn't any haskell implementation of a US software patent go to show the inherent mathyness of it?
07:43:54 <byorgey> but, uh, in that sense, programs in any language can be thought of as pure mathematical expressions.
07:44:10 <blahblah> ok thanks again guys, alls done!
07:44:17 <Maddas> sw17ch: Why would that matter?
07:44:22 <byorgey> if people are not already convinced, a Haskell implementation will not convince them.
07:44:30 <sw17ch> Mathematical algorithms are not patentable
07:44:36 <Maddas> sw17ch: I suggest you read http://www.amazon.com/Math-You-Cant-Use-Copyright/dp/0815749422 :-)
07:44:47 <sw17ch> unless i'm confused :)
07:44:49 <Maddas> (IANAL, and that's the only related book I read, so you may know more than me.)
07:45:01 <sw17ch> have a quick 10 word summary?
07:45:03 <yitz> ((pssst ... sw17ch ... haskell programs can only represent general recursive functions, just like any other programming language ...))
07:45:15 <idnar> if software is patentable, I don't see why mathematical algorithms wouldn't be
07:45:21 <Maddas> sw17ch: It's been a while, but I believe applied math can be pattented.
07:45:26 <idnar> they probably *shouldn't* be, but hey...
07:45:35 <yitz> genes are patentable
07:45:44 <sw17ch> 1/5th of your DNA is patented :)
07:45:46 <Maddas> patented, even. And whatever 'applied math' means in this context, I'm not knoweldgeable enough to offer an explanation :-)
07:46:09 <sw17ch> well, it seems that Software Patents were first allowed based on their machine-like structure
07:46:30 <yitz> remember when it was illegal to export RSA, so someone tattooed it on his leg in one line of Perl and presented himself to authorities at the border?
07:47:01 * Maddas really recommends the book for a readable explanation of what was originally allowed and how that changed. (Both in theory and in practice)
07:47:05 <sw17ch> if they can be shown as the pure algorithms they really are, then i'd think you could make a better case to un-patent them. often times it's just a matter of representing something differently to convince people
07:47:20 * sw17ch will add it to his to-read: list
07:47:37 * Maddas got his university library to order it :]
07:47:40 <quicksilver> software patents, are, to a greater or a lesser extent, 'clearly' invalid according to the guidelines the patent agencies claim to work by.
07:47:45 <quicksilver> nonetheless, they continue to award them.
07:47:54 <quicksilver> they award them now because they awarded them in the past.
07:48:01 * quicksilver shrugs
07:48:03 <quicksilver> it's all broken.
07:48:08 <Maddas> Well, you can patent a 'process'.
07:48:08 <sw17ch> i hate that president
07:48:17 <yitz> quicksilver: because the courts decided to uphold them
07:48:26 <quicksilver> yitz: not much, no
07:48:31 <sw17ch> i think the biggest problem is the obviousness part... what was not obvious 3 years ago... is now
07:48:32 <quicksilver> (there have been a few isolated cases)
07:48:43 <quicksilver> but the vast majority of patent suits never ever ever reach court
07:48:51 <sw17ch> RIM vs. NTP for example...
07:48:51 <quicksilver> very very very few patents have been tested in a court of law.
07:49:25 <yitz> you don't need many to get to court - a handful of landmark decisions in the 90s were enough.
07:49:27 <Maddas> sw17ch: IIRC part of the problem is overworked patent office employees with unreasonable burdens on them and a broken reward system.
07:49:58 <yitz> Maddas: no, its not mistaken, its official policy
07:50:12 <Maddas> What is?
07:50:18 <sw17ch> mmm... yes
07:50:19 <yitz> s/w patents
07:50:26 * Maddas never claimed it was a mistake :-)
07:51:11 <sw17ch> replicateM does exactly what i want, thanks all
07:52:49 <nomeata> Hi. Does Text.XHtml have a website, a darcs repository or a mailing list
07:52:55 <nomeata> s//?/
07:54:46 <Zao> There's a real Text.XHtml now?
07:54:55 * Zao renames his mutilated wash/html
07:55:54 <nomeata> there is http://hackage.haskell.org/cgi-bin/hackage-scripts/package/xhtml
07:56:03 <nomeata> not sure how real you need it :-)
07:59:55 <sw17ch> wow, replicateM is all sorts of fun :)
08:00:00 <byorgey> =D
08:00:35 <sw17ch> byorgey, really, thanks. i've spent too much time looking for exactly what that does :)
08:00:45 <byorgey> just wait 'till you get to use triplicateM ;)
08:01:12 <sw17ch> :P
08:01:23 <byorgey> hehe, glad I could help =)
08:01:34 <sw17ch> really though, replicateM_ is exactly what i need :)
08:01:43 <sw17ch> as long as it's with an IOArray
08:01:55 <byorgey> yup, cool
08:03:00 <sw17ch> alright, i suppose i need to get ready for class now
08:03:02 <sw17ch> thanks again, ttyl
08:03:06 * shepheb reads old HWN quotes for a) a hearty laugh and b) blog name ideas
08:05:28 <sethk> I'm getting an Ambiguous occurrence message after adding import Data.Map.  So, I changed the import to import Data.Map as DM, but the message is unchanged
08:05:47 <shepheb> byorgey: I've got part of the Meijer91 translation sorted, but I'm hung up on "in" and "out" in the later sections. the former appears to be somewhat related to pattern matching, the latter to return. I'm not sure how to work with them in Haskell.
08:06:00 <sethk> so that just changes the name to DM.filter.
08:06:11 <shepheb> sethk: "import qualified Data.Map as DM"
08:06:21 <sethk> shelarcy, ah, ok, forgot the qualified
08:06:39 <sethk> shepheb, shepheb, that is.  thanks
08:06:40 <byorgey> shepheb: did you look at that link I sent you yesterday?  The "evolution of a Haskell programmer" thing?
08:06:54 <byorgey> shepheb: it has an example I think
08:07:06 <shepheb> byorgey: yeah, I did. oh? let me take a look
08:09:02 <byorgey> shepheb: I think the basic idea is that in order to create this sort of recursive data type, you need a newtype to break the recursion.
08:09:49 <byorgey> shepheb: i.e. in Haskell (and, more fundamentally, in the typed lambda-calculus I think)  you can't just say  X = 1 | A||X
08:09:53 <shepheb> byorgey: brain exploding, stand by. in seems to turn an ordinary container into its fixpoint, as yesterday; and out the opposite
08:10:13 <shepheb> right.
08:10:31 <byorgey> shepheb: er, hmm, I don't understand this quite as well as I'd like
08:10:46 <quicksilver> byorgey: you can say X = 1 | A || X
08:10:57 <byorgey> quicksilver: yes, I just realized that
08:11:11 <byorgey> quicksilver: if you understand this better, feel free to take over =)
08:12:29 <shepheb> we do that all the time. but given a general "data Box c a = Empty | c a", in applies it to itself, Box (Box (Box ... and the newtype breaks that cascade
08:12:48 <byorgey> ah, yes, that's it =)
08:12:58 <shepheb> (did I miss anything?)
08:13:25 <ttt--> @go evolution of a Haskell programmer
08:13:27 <lambdabot> http://www.willamette.edu/~fruehr/haskell/evolution.html
08:13:27 <lambdabot> Title: The Evolution of a Haskell Programmer
08:14:03 <byorgey> so, 'in' just applies the Mu constructor, and 'out' removes it
08:14:27 <mrd> isorecursive types?
08:14:40 <shepheb> well, yes.
08:14:48 <shepheb> I'm going for a more intuitive view of Mu
08:14:52 <quicksilver> it's also interesting not to feed the datatype back in.
08:14:58 <quicksilver> or to feed a slight variation of it.
08:15:25 <quicksilver> data ProtoList a x = Elt a | More (x a)
08:15:46 <byorgey> shepheb: well, the type Mu F  contains values which have the shape  Mu (F (Mu (F (Mu (F ....
08:15:50 <quicksilver> bah, not quite what I meant.
08:15:51 <quicksilver> try again
08:16:01 <shepheb> for lists, it seems in = (:[]) and out = ... head?
08:16:11 <quicksilver> data ProtoList x a = Elt a | More (x (ProtoList a))
08:16:18 <quicksilver> guard the recursion.
08:16:26 <byorgey> shepheb: and there is an isomorphism between those and values of shape  F (Mu (F (Mu (F ...
08:16:28 <quicksilver> now ProtoList Identity is isomorphic to streams.
08:16:35 <byorgey> shepheb: in and out simply express that isomorphism.
08:16:38 <quicksilver> ProtoList Maybe is lists.
08:16:48 <quicksilver> and ProtoList IORef is 'lists mutable all the way down'
08:18:12 <byorgey> shepheb: no, in and out don't change values like that, from a semantic point of view
08:19:04 <byorgey> shepheb: so if you had  data L a b = Empty | Cons a b,  then Mu (L a) would represent lists, right?
08:19:17 <shepheb> byorgey: yes
08:19:31 <byorgey> shepheb: so what would a value of type Mu (L a) look like?
08:19:59 <byorgey> for example, the list [1,2,3] would be represented as  Mu (Cons 1 (Mu (Cons 2 (Mu (Cons 3 (Mu Empty))))))
08:20:24 <shepheb> out strips the Mu, leaving Cons 1 (Mu (Cons 2 (...
08:20:28 <byorgey> exactly
08:21:02 <byorgey> in and out just let you deal with this extra layer of 'Mu's in between everything
08:21:23 <byorgey> at least, that's how I think about it, there are probably deeper connections to be made as well
08:21:57 <EvilTerran> i've seen mu defined as newtype Mu f = In { out :: f (Mu f) }, iirc
08:22:34 <byorgey> ah, that's slick =)
08:25:10 <shepheb> so I think an explanation of how Mu f is the fixpoint of f, and the Cons example for in/out gives the right feel from a Haskell programmer's POV.
08:26:01 <byorgey> agreed.  Making that connection certainly helped me to understand it.
08:27:19 <shepheb> and it also demonstrates how in and out are artifacts of the general Mu, and they wouldn't exist for explicit recursive datatypes, like native lists. I think that in and out can just be dropped at all points when specializing to a primitive recursive data type?
08:31:08 <quicksilver> personally I think of the explicit in and out as being artifacts of haskell.
08:31:19 <quicksilver> they're there because the parser needs to be able to 'see' the transitions between types
08:31:24 <quicksilver> so it can infer correctly
08:31:30 <quicksilver> (like the constructor in an existential)
08:31:40 <shepheb> quicksilver: they exist in the paper, though.
08:31:55 <quicksilver> for analogous reasons, I presume.
08:31:57 <mrd> parser?
08:32:08 <quicksilver> It is sometimes convenient to distinguish between isomorphic things
08:32:19 <quicksilver> at other times it is convenient not to distinguish
08:32:28 <mrd> the reason is that equirecursive types make life much more difficult for the typechecker
08:32:35 <shepheb> well, they're hardly artifacts of Haskell if they exist in the category theory of the paper.
08:32:37 <quicksilver> mathematicians spent a lot of time messing about withh notation between the one and the other.
08:33:10 <quicksilver> shepheb: true. But that's not surprsing because the categorical construction they make is closely related to haskell core?
08:33:20 <quicksilver> shepheb: so again the reasons are closely related?
08:33:38 <nomeata> I have a patch against http://darcs.haskell.org/packages/xhtml/, where should I submit that?
08:33:38 <lambdabot> Title: Index of /packages/xhtml
08:35:23 <nomeata> A new ticket on the ghc trac? Or by private mail to the maintainer bjorn bringert?
08:35:25 <shepheb> quicksilver: well, I agree. but saying it's an artifact of Haskell makes it sound like something on the order of "newtype", or using "Mu (L a)" instead of \mu L
08:35:33 <nomeata> bringert: heh, Im just talking about you.
08:35:52 <nomeata> bringert: I have a small patch for xhtml, where can I submit that? directly to you or via trac?
08:35:54 <bringert> hi nomeata
08:36:08 <bringert> nomeata: does it change the API?
08:36:32 <nomeata> bringert: it adds an instance ADDATTRS HotLink
08:36:50 <bringert> nomeata: hmm, that was missing?
08:36:54 <bringert> nomeata: send it to me
08:37:00 <nomeata> ok, on its way in a second
08:38:31 <nomeata> sent
09:06:17 <shepheb> "<SamB> [on the subject of fromJust] thinks that unJust would be a more fun name for that"  -- that would be awesome, like "pointless"
09:06:52 <quicksilver> data Maybe a = Nothing | Just { unJust :: a }
09:10:38 <edwinb> I always liked: unjust Nothing = error "unjust unjustified"
09:13:55 <EvilTerran> reminds me of betweEnum = toEnum.fromEnum
09:14:37 <EvilTerran> also, my thought that the IsString class could be called Stringy
09:20:37 <mm_freak> it happens to me a lot of times that i import a lot of modules, which aren't actually needed anymore (e.g. because of code changes)
09:20:46 <mm_freak> is there any easy way to purge unneeded imports?
09:22:06 <gnuvince> Could anybody comment on my code in this small script: http://gnuvince.wordpress.com/2008/04/01/haskell-programs-for-the-beastie-boys-fans/
09:22:08 <lambdabot> Title: Haskell programs for the Beastie Boys fans  Occasionally sane, http://tinyurl.com/2wger4
09:24:11 <Beelsebob> gnuvince: I'd suggest that you want to take the IO monad out as much as possible... let me have a hack for a min
09:24:25 <dons> gnuvince: i hear you gave up on xmonad?
09:24:42 <ddarius> gnuvince: findWords can be reduced to a single use of liftM
09:24:46 <quicksilver> mm_freak: -fwarn-unused-imports, but is is buggy.
09:24:51 <dons> oh yeah, there's no need for most of that IO
09:25:09 <dons> get a randomGen in main, and open the file in main too
09:25:15 <dons> then everything else can be nicely pure
09:25:46 <ddarius> Hell, findWords = liftM (filter (isPrefixOf "pa" . map toLower) . lines) . readFile
09:26:32 <mm_freak> quicksilver: i'll do it by hand then, thank you
09:26:54 <quicksilver> mm_freak: -fwarn-unused-imports, but is is buggy.
09:26:58 <quicksilver> bah
09:27:02 <quicksilver> didn't mean to say it twice
09:27:11 <quicksilver> mm_freak: it's better than nothing but get confused by qualified identifiers.
09:27:12 <quicksilver> I believe.
09:28:10 <mm_freak> quicksilver: does it give false "unneeded" positives?
09:28:24 <mm_freak> or does it miss some unneeded imports?
09:28:39 <allbery_b> in my experience it whines about indirect imports
09:29:08 <gnuvince> dons: Indeed.  Not per xmonad's fault though, more per my own working habits.  I'm definitely keep xmonad installed and it's commented in my .xinitrc.  Basically, I need to relearn to work with a tiled window manager.
09:29:26 <allbery_b> e.g. I import Control.Monad which just gets me the core stuff you're not supposed to import directly, ghc whines about it being unused, remove it, all the monad instances are gone
09:30:43 <mm_freak> how do you explain a monad informally to a newbie?
09:31:00 <allbery_b> "poorly"
09:31:10 <quicksilver> an annotation which indicates that a function can have side-effects.
09:31:17 <mm_freak> i always tell them, "monads are an abstract sequence", sort of
09:31:18 <gnuvince> :t liftM
09:31:21 <lambdabot> forall a1 r (m :: * -> *). (Monad m) => (a1 -> r) -> m a1 -> m r
09:31:46 <allbery_b> "something happens bnehind the scenes"
09:31:55 <quicksilver> mm_freak: I don't find that a useful starting point. But YMMV.
09:32:04 <quicksilver> if it seems to work for you then I'm sure you're right :)
09:32:39 <mm_freak> quicksilver: well, that's what has helped me understand them  i view a monad as a more general 'sequence'
09:33:03 <warmenhoven> quicksilver: i don't particularly like the "side effects" description either, because that's not strictly true and can be confusing, especially later.
09:33:19 <mm_freak> where each element/step builds upon information from the predecessor
09:33:35 <mm_freak> warmenhoven: that's the point
09:34:08 <quicksilver> warmenhoven: it is strictly true.
09:34:17 <quicksilver> warmenhoven: for an appropriate definition of 'side effect'
09:34:21 <quicksilver> wherein lies the rub ;)
09:34:38 <quicksilver> monads annote that a function can have computational effects
09:34:59 <quicksilver> I wouldn't use that phrase though, because I wouldn't expect a newbie to know what a computational effect is.
09:35:09 <EvilTerran> hehe
09:35:12 <quicksilver> however, side-effects are simple enough to explain.
09:35:18 <quicksilver> and serve in my view as a useful guide.
09:35:42 <mm_freak> quicksilver: i don't think about side effects anymore  i have 'out-pured' them by my view of monads =)
09:36:12 <mm_freak> i view the whole haskell program as a function of the state of the universe
09:36:35 <mm_freak> and each step in the IO "sequence" builds upon information from previous steps
09:36:56 <gnuvince> What does the "lift" part mean in liftM?  That it "removes" the monad temporarily?
09:37:53 <roconnor> @type flip liftM
09:37:54 <lambdabot> forall a1 r (m :: * -> *). (Monad m) => m a1 -> (a1 -> r) -> m r
09:38:09 <byorgey> no, it 'lifts' a function  a -> b  into the monadic world,  m a -> m b
09:38:28 <byorgey> @type liftM
09:38:29 <lambdabot> forall a1 r (m :: * -> *). (Monad m) => (a1 -> r) -> m a1 -> m r
09:38:48 <gnuvince> byorgey: ok.
09:38:49 <gnuvince> thanks
09:38:50 <byorgey> read that as  (a1 -> r) -> (m a1 -> m r)
09:40:47 <quicksilver> mm_freak: that's all true.
09:40:53 <quicksilver> mm_freak: but it's also true that the effects happen.
09:41:03 <quicksilver> mm_freak: arguably they're not "side" any more, though.
09:41:05 <gnuvince> byorgey: gotcha
09:41:21 <quicksilver> mm_freak: (>>) builds sequences, and do {} notation builds sequences
09:41:30 <quicksilver> mm_freak: that doesn't mean monads *are* sequences, though.
09:41:39 <quicksilver> and what is actualy being sequenced?
09:41:42 <quicksilver> the effects.
09:41:46 <quicksilver> (not the computation, particularly)
09:41:58 <sethk> what's the inverse of liftM (if it exists)
09:42:16 <quicksilver> sethk: there isn't one, and indeed cannot be.
09:42:18 <quicksilver> in general.
09:42:51 <sethk> quicksilver, why?  I can call non-monad code from within a monad context, so why is it in general not possible?
09:43:14 <quicksilver> sethk: that's what liftM allows.
09:43:25 <quicksilver> the opposite of liftM would be calling monad code in a non-monad context.
09:44:09 <atsampson> whatever function runs the monad is the inverse of liftM, I guess -- runStateT, etc.
09:44:27 <quicksilver> atsampson: that's more like the inverse of return
09:44:32 <quicksilver> or at least, the complement to return.
09:44:43 <quicksilver> atsampson: the inverse of liftM would pull a function out of a monad
09:44:47 <quicksilver> not just evaluate an action
09:44:53 <quicksilver> (m a -> m b) -> (a -> b)
09:45:14 <Jaak> you can prolly get "m a -> a" out of that
09:46:02 <PeakerWork> do some functional programs make use of IORef's or other monadic references in order to build data structures such as doubly-linked lists?
09:46:22 <quicksilver> PeakerWork: very seldom.
09:46:39 <ddarius> You can make an (immutable) doubly-linked list purely (with laziness).
09:46:42 <quicksilver> it exposes you to all the pain that makes such programming inferior to FP.
09:47:05 <mm_freak> quicksilver: that's really a matter of interpretation  monads are so abstract, that you can view them in a hundred ways
09:47:28 <quicksilver> PeakerWork: but for certain structures you need something like references, yes
09:47:31 <quicksilver> cyclic graphs.
09:47:45 <quicksilver> (you dont' need IORefs, but you need someway to refer to a node)
09:48:40 <PeakerWork> quicksilver, I am discussing pure FP with Jonathan Edwards, and he has concerns that you would have to design your data structures to support cyclic graphs and certain kinds of mutations - and that a pure FP data structure is completely different to one designed with monadic refs
09:49:01 <Jaak> he's correct
09:49:42 <PeakerWork> In other words: An implementation of a pure FP data structure is less flexible to handle new kinds of operations than an imperatively-described data structure
09:49:55 <Jaak> why is that?
09:50:11 <PeakerWork> well, you might have to "rewrite" the whole thing with IORefs to support new kinds of operations
09:50:15 <PeakerWork> s/IORefs/some refs
09:50:16 <ddarius> PeakerWork: They allow flexibility in different ways.
09:50:20 <Jaak> different doesn't imply worse
09:50:56 <ddarius> PeakerWork: To write a "persistent" data structure imperatively requires "rewriting"
09:50:57 <PeakerWork> that's my main black hole in FP/Haskell -- I don't know much about FP data structures
09:51:39 <PeakerWork> ddarius, Serialization can often be automatically derived from the structure, is that what you mean?
09:51:56 <pejo> Peaker, Okasaki has written a great book about it. It's fairly cheap. If you don't want to pay you can read his dissertation which is much of the same material.
09:52:08 <ddarius> PeakerWork: No.  I'm not using "persistent" in a way related to serialization.
09:52:20 <PeakerWork> ddarius, what kind of "persistency" do you mean?
09:52:44 <shepheb> implementing 'undo' functionality is all but trivial when you're using a non-IO-based state monad, for example.
09:52:49 <PeakerWork> pejo, Yeah, I intend to read it. But until then, I'm going to have to answer that I don't have enough knowledge about that point :)
09:52:53 <ddarius> PeakerWork: When you have a singly-linked list and you add an element to the end (in an imperative language) you no longer have the "old" singly-linked list.  Such data structures are called ephemeral.
09:53:06 <ddarius> PeakerWork: Most of the "imperative" data structures people are familiar with are ephemeral.
09:53:16 <ddarius> (or imperative implementations of common ones)
09:53:46 <PeakerWork> ddarius, ah. So you mean that there's a tradeoff where imperative data structures allow some sorts of flexibility, and FP data structures allow access to previous values of the data structure?
09:55:01 <ddarius> PeakerWork: Not exactly.  "pure" data structures are (almost) inherently persistent.  Writing an "imperative" version that also has that property is not as simple as simply making an imperative version.
09:55:55 <ddarius> PeakerWork: I'm not sure what you mean by "flexibility" or "new kinds of operations"
09:56:59 <matthew_-> only 3 hours to go people!!
09:57:01 <matthew_-> get typing!
09:57:10 <PeakerWork> ddarius, well, I am not sure what limitations FP data structures have compared to imperative ones
09:57:45 <roconnor> what happends in 3 hours?
09:58:19 <edwinb> That's the ICFP deadline. There's still time!
09:58:24 <ddarius> PeakerWork: "FP" data structures and "imperative" ones are -different- data structures.  The programming paradigm of the language, however, is not particularly relevant.
09:58:34 <atsampson> matthew_-: we're working on it ;)
09:58:54 * edwinb is about to astonish himself by submitting with 3 hours to spare
09:59:02 <matthew_-> edwinb: snap ;)
09:59:09 <ddarius> PeakerWork: There are reasons to use persistent or ephemeral data structures in both paradigms.  You, as always, use the appropriate data structure for the job.
09:59:16 <matthew_-> but don't worry, you've still got time to endlessly piss around with it
09:59:58 <edwinb> oh, I submitted a version 2 hours ago, I've been poking since...
10:00:08 <edwinb> I think it's gone write-only now though
10:00:33 <PeakerWork> ddarius, Would it not be harder to convert between those two in an FP language than in an imperative one?  Would it be necessary to convert purely functional code to live in a monad, for example?
10:00:34 <matthew_-> edwinb: what? mentally?
10:00:34 <swiert> edwinb: I think it's down to having deadlines that aren't in the middle of the night.
10:01:01 <edwinb> swiert: yeah, no fun that way. Also, someone in our department is having a party starting in half an hour ;)
10:01:30 <edwinb> matthew_-: I mean it's got to the stage where I'm just changing things for the sake of it
10:01:59 <byorgey> changing things just for the sake of changing things can't be good =)
10:02:09 <ddarius> PeakerWork: Probably, but that's because by not using a monad you are making assumptions about the properties of the data structure and your code.  However, such assumptions can (and often are) easily present in imperative APIs too.
10:02:44 <matthew_-> edwinb : ahh
10:03:16 <PeakerWork> ddarius, is it possible to avoid that assumption? For example, writing all data-structure code monadically in anticipation of potential changes?
10:03:44 <pejo> edwinb, what are you submitting?
10:05:28 <PeakerWork> ddarius, and can you elaborate on that kind of assumption in an imperative API?
10:05:30 <edwinb> pejo: it's a DSEL for deadlock-free concurrent programming. with dependent types.
10:06:42 <pejo> edwinb, is the typesystem used to guarantee the absence of deadlocks?
10:08:02 * byorgey scratches head in puzzlement
10:08:13 <PeakerWork> ddarius, I'm going home - and will log in from there,  please keep the message around until I'm there? :)
10:08:18 <matthew_-> edwinb: do you just love how the website shite passes around your passcode in GETs so it appears in the URL and logs?!
10:08:33 <byorgey> I'm testing a property with QuickCheck that keeps coming up with counterexamples.  But when I try them, they seem to make the property true.
10:08:44 <byorgey> I can't figure out what the heck is going on.
10:08:50 <EvilTerran> defaulting?
10:08:54 <EvilTerran> @check (==)
10:08:57 <lambdabot>  OK, passed 500 tests.
10:09:11 <EvilTerran> well, that's the opposite problem...
10:09:28 <byorgey> no, I gave the property an explicit type signature
10:09:38 <byorgey> an explicit non-polymorphic type signature I should say
10:09:57 <byorgey> let me paste something.
10:11:01 <edwinb> matthew_-: I've just had the 'revise' page open for a couple of hours. I hope it's still valid...
10:11:39 <edwinb> pejo: it requires you to show that there is no circular wait by ordering the resources
10:13:59 <byorgey> http://hpaste.org/6785  -- mysterious QuickCheck failure that isn't
10:14:23 <haskeller01> cake??
10:14:45 <tromp> @check (<)
10:14:46 <lambdabot>  Falsifiable, after 0 tests: (), ()
10:15:10 <tromp> @check (<=)
10:15:10 <lambdabot>  OK, passed 500 tests.
10:15:21 <swiert> byorgey: does it have to do with precedence of ==> and ==?
10:15:43 <tromp> @check \a b->a <= b
10:15:44 <lambdabot>  OK, passed 500 tests.
10:16:09 <byorgey> swiert: no, I don't think so,  ==> has precedence 0  and == 4
10:16:24 <haskeller01> i think he may be right
10:16:28 <haskeller01> because...
10:16:31 <tromp> @check \(a::Int) b->a <= b
10:16:31 <lambdabot>  Parse error in pattern at "b->a" (column 11)
10:16:31 <pejo> edwinb, ah, nifty.
10:16:46 <byorgey> and anyway it wouldn't type correctly if it was the other way round
10:16:55 <haskeller01> why
10:16:55 <swiert> yeah, you're right.
10:17:32 <tromp> @check \a ->a <= 100
10:17:33 <lambdabot>  Falsifiable, after 276 tests: 101
10:18:17 <byorgey> @check \a -> a <= 54000
10:18:17 <lambdabot>  OK, passed 500 tests.
10:18:21 <tromp> @check (<= 180)
10:18:22 <lambdabot>  Falsifiable, after 413 tests: 192
10:18:35 <byorgey> what do you know, all numbers are no larger than 54000! =)
10:19:18 <tromp> @check (<256)
10:19:18 <lambdabot>  OK, passed 500 tests.
10:19:37 <tromp> @check (>=0)
10:19:38 <lambdabot>  Falsifiable, after 1 tests: -2
10:21:42 <glen_quagmire> @check (\a b -> compare a b `elem` [LT,EQ])
10:21:42 <lambdabot>  OK, passed 500 tests.
10:22:14 <desegnis> Aren't the Arbitrary instances for numbers explicitly tuned to produce mostly small values?
10:22:20 <matthew_-> edwinb: the resources are statically known?
10:22:22 <tromp> it seems to test only over type () ?~
10:23:33 <glen_quagmire> > mmmmmmmmmmmmmmmmmmmmmmmm :: Expr
10:23:34 <lambdabot>   Not in scope: `mmmmmmmmmmmmmmmmmmmmmmmm'
10:23:56 <forkiliens> hi everyone
10:24:21 <mauke> > var (repeat 'i')
10:24:21 <lambdabot>  iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii...
10:25:00 <mauke> > var ""
10:25:00 <lambdabot>  
10:25:12 <vincenz> > var "\0"
10:25:13 <lambdabot>  
10:25:14 <lament> > var (repeat 'i') + 42
10:25:15 <lambdabot>  42+iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii...
10:25:19 <vincenz> > var "\27ab"
10:25:20 <lambdabot>  ab
10:25:22 <edwinb> matthew_-: if they're statically known it's easy, if they're dynamic, you can either provide proofs, or the system makes sure you do the relevant dynamic checks
10:25:52 <vincenz> > var "\8ab"
10:25:52 <lambdabot>  ab
10:26:02 <edwinb> right, I've had enough ;)
10:26:07 * edwinb presses the submit button and disappears
10:26:29 <lament> > var "a\8b"
10:26:30 <lambdabot>  ab
10:26:34 <lament> blah
10:26:37 <glen_quagmire> :t var
10:26:38 <lambdabot> forall a. String -> Sym a
10:26:48 <vincenz> > var
10:26:48 <lambdabot>   add an instance declaration for (Typeable1 Sym)
10:27:08 <glen_quagmire> > var ""
10:27:08 <byorgey> hi forkiliens =)
10:27:08 <vincenz> > var "\10ab"
10:27:09 <lambdabot> Terminated
10:27:09 <lambdabot>  ab
10:27:16 <vincenz> > var "\10\10ab"
10:27:17 <lambdabot>  ab
10:27:28 <vincenz> > var "\7ab"
10:27:29 <lambdabot>  ab
10:27:48 <vincenz> I'm sure that can be used somehow, the question is how.
10:27:59 <lament> phishing
10:28:18 <vincenz> lament: how so?
10:28:52 <forkiliens> hello byorgey
10:29:55 <forkiliens> i was asking myself how to "handle" calls to the outside world in such way that the side effects are minimized...
10:30:14 <forkiliens> am i stupid, or is this impossible?
10:31:13 <Baughn> forkiliens: In general? Yes.
10:31:22 <byorgey> it depends what you mean.  In the IO monad, there is no way to guarantee any sort of restrictions on the type or number of effects
10:31:28 <Baughn> forkiliens: In particular? Depends on the particulars. Lots of operations can be made safe.
10:31:55 <byorgey> but you can easily imagine other frameworks where the effects are controlled or restricted in some way.
10:32:19 <byorgey> forkiliens: were you thinking about this in some particular context, or just general musings?
10:35:39 <forkiliens> particular: see http://hpaste.org/6783
10:36:01 <forkiliens> i changed the output type to IO String
10:36:19 <forkiliens> so now i don't get any error message.
10:39:11 <forkiliens> so my question is: from your experience, would you say, that it is "safe" to do it this (hpaste.org/6783) way?
10:39:17 <forkiliens> or should i modify it?
10:41:16 * shepheb is going to challenge his blub-programming "I don't see where I would use map, filter, etc." roommate to send him some C# code, and annotate it with where HOFs are win.
10:43:58 <mrd> maybe all the dude writes is println "Hello world"
10:45:05 <lament> shepheb: map and filter are gaining popularity (and usefulness) in C#
10:46:21 <lament> they're not called exactly that, but they're there
10:46:26 <Peaker> Linq?
10:46:38 <lament> listOfFoo.Where(x => x.size > 10)
10:46:46 <shepheb> well, it has nothing to do with C# in particular (though I'm glad to see they're wising up). my friend certainly writes good C# code. the question is just about perception. he sees no utility in map and filter, he's always written the for loops and such.
10:46:57 <Peaker> IIRC, Linq does something similar to map/reduce/filter, but creates a view that's usable to modify the source too
10:47:01 <lament> (x => x.size > 10) being a lambda function
10:47:08 <shepheb> so instead of trying to show them inside Haskell's context, I'll try to show them pseudocode-ly in his  C# context.
10:47:20 <lament> no, don't show them pseudocodely, show them for real
10:47:20 <mrd> doesn't need to be pseudo
10:47:31 <lament> they're language features in C# 3.0
10:47:50 <lament> "Where" is filter
10:47:50 <shepheb> well, the problem with that is my knowledge of C# is essentially Java and C(++) transferral.
10:48:26 <lament> not sure what map is
10:48:27 <mrd> Microsoft is actually doing a decent <gasp> job of making C# a lot better than Java
10:48:57 <lament> c# now has some type inference, lambdas, it's getting cooler every version
10:50:44 <Peaker> C# will have local type inference (only inferring the types of auto variables, iirc), and will still require defining them explicitly
10:51:00 <mrd> full type inference in C# would be undecidable anyhow
10:51:06 <Peaker> My friend uses C# heavily, and maintains a list of problems with its design and its standard libs
10:51:08 <mrd> subtyping causes that
10:51:14 <mrd> is it on the web?
10:51:29 <Peaker> nope, I'll mail him about it and get it on the web
10:52:47 <lament> i think c# is pretty awesome when combined with its dev.environment
10:53:17 <lament> (.net libs and VS)
10:54:12 <Peaker> just talked to him on the phone... hopefully I'll have it in a few days :)
10:54:53 <mrd> few days? that's like a lifetime!
10:54:57 <mrd> internet time!
10:55:13 <cjb> it's like 1.5 memes
10:55:27 <Peaker> yeah, unfortunately his girlfriend's sister is getting married, and he's helping them with arrangement, so he's stuck away in "real world time" for a few days :)
10:55:38 <mrd> ouch
10:55:48 <lament> ouch. so much of his life is lost!
10:57:02 <lament> (like being turned into a dog for year... makes you lose 7 years real time)
10:58:07 <forkiliens> where do i learn best about the library text.html? Is there any introduction except the tutorial practical - programming in haskell @ haskell.org, or do i have to work my way through the library-haddock-definitions?
10:58:35 <forkiliens> i mean module, not library.
10:59:00 <CosmicRay> any of you running GHC on windows?  I'm getting a ton of errors processing mingw/*.h on lines that seem to reference types like ULONG
10:59:12 <CosmicRay> all I can figure out is that it down't know what ULONG is
10:59:17 <CosmicRay> where do I find the files to include for it?
11:05:05 <s|k_> hi
11:05:23 <s|k_> should I get a book or is the web good enough to learn haskell?
11:05:26 <roconnor> hi
11:08:19 <mauke> s|k_: it's a good place to ask questions :-)
11:08:26 <s|k_> ok :)
11:08:33 <Vq^> s|k_: the web-material is good enough, but then there are some good books as well
11:08:35 <mauke> @where tutorial
11:08:35 <lambdabot> http://www.haskell.org/tutorial/
11:08:36 <OnionKnight> YAHT is good enough
11:08:40 <s|k_> thanks
11:08:41 <mauke> @where yaht
11:08:41 <lambdabot> PDF: http://darcs.haskell.org/yaht/yaht.pdf Wikibook: http://en.wikibooks.org/wiki/Haskell/YAHT
11:08:47 <shachaf> OnionKnight: Not by itself, I think.
11:09:00 <svat> can I make ghci show me only 100 elements of an infinite list by default?
11:09:03 * shachaf did not like YAHT (at least the early parts) that much.
11:09:12 <shachaf> svat: "take 100 $"? :-)
11:09:23 <Vq^> the wikibook is also worth looking at
11:09:37 <s|k_> is Haskell low level memory managment like C or is it more high level?
11:09:41 <Vq^> http://en.wikibooks.org/wiki/Haskell
11:09:42 <lambdabot> Title: Haskell - Wikibooks, collection of open-content textbooks
11:09:46 <Botje> haskell is garbage collected
11:09:48 <Vq^> s|k_: high level
11:09:48 <shachaf> Perhaps the RWH alpha/beta wouold work nicely, too.
11:09:49 <mauke> s|k_: very, very high level
11:09:50 <s|k_> does it have garbage collection I guess is what I'm asking
11:09:51 <s|k_> oh ok
11:10:01 <svat> shachaf: yeah, but I got tired of typing that :)
11:10:01 <s|k_> it has hash tables and stuff?
11:10:12 <mauke> yes, but we usually use Data.Map instead
11:10:17 <mauke> (that's a balanced search tree)
11:10:17 <s|k_> ok
11:10:17 <Vq^> s|k_: yes, but hash tables aren't used much
11:10:22 <Botje> @faq it has hash tables and stuff?
11:10:22 <lambdabot> The answer is: Yes! Haskell can do that.
11:10:24 <shachaf> svat: Quick fingers to ^C, then? :-)
11:10:31 <s|k_> haha
11:10:40 <Vq^> s|k_: it certainly has stuff :)
11:10:46 <s|k_> anonymous functions?
11:10:52 <mauke> hahaha
11:10:57 <Botje> s|k_: dude. go look at a web site.
11:11:00 <s|k_> ok
11:11:02 <s|k_> I will :P
11:11:04 <shachaf> @faq Can Haskell astound s|k_ with its beauty and power?
11:11:04 <lambdabot> The answer is: Yes! Haskell can do that.
11:11:09 <s|k_> haha
11:11:10 <Botje> hell, even the wikipedia article on haskell mentions this stuff
11:11:17 <s|k_> ok I'm looking!
11:11:26 <svat> shachaf: is it possible to redefine the "show" for list types? (Or... redefine anything. I'm new-ish to Haskell.)
11:11:46 <shachaf> svat: Probably not easily.
11:11:46 <mauke> svat: not really
11:11:54 <Botje> svat: you could make a Show2
11:12:07 <Botje> which shares Show for all instances, except for [a]
11:12:26 <mauke> but how do you make ghci use it?
11:13:06 <Botje> I .. have no idea. dammit!
11:13:23 <shachaf> You could define a newtype to wrap lists in.
11:13:27 <roconnor> maybe one should define one's own list type?
11:13:48 <roconnor> shachaf's idea is even better
11:13:57 <mauke> then you'd have to wrap everything at the top level
11:14:02 <mauke> might just write take 100 $ instead
11:14:28 <roconnor> > take $100 from mauke
11:14:28 <lambdabot>   Not in scope: `mauke'
11:15:08 <shachaf> You could make a "flip ($)" and say "... `op` take 100". :-)
11:19:50 <Peaker> shachaf, are you from Israel?
11:23:07 <edwardk> @paste
11:23:07 <lambdabot> Haskell pastebin: http://hpaste.org/new
11:25:15 <inimino> how do you get a string from UTF-8 data in a ByteString?
11:25:41 <mrd> inimino: see encoding package
11:25:51 <mrd> ?go hackage encoding
11:25:59 <lambdabot> http://hackage.haskell.org/cgi-bin/hackage-scripts/package/encoding-0.4
11:26:19 <mrd> Data.Encoding.decode in particular, I think
11:26:36 <inimino> ok, thanks
11:26:45 <inimino> I was looking here: http://haskell.org/ghc/docs/latest/html/libraries/index.html
11:26:54 <mrd> it's not in the std libraries
11:26:58 <inimino> ok
11:28:19 <edwardk>  I just tried to build GHC head, using ./configure and got: http://hpaste.org/6786 is the elf binary in there for 'pwd' 64 bit or something?
11:29:52 <shachaf> edwardk: ./configure compiles that file, doesn't it?
11:30:08 <edwardk> shachaf: i have no idea, i just started trying to figure out whats going on
11:30:53 <shachaf> edwardk: What happens if you compile pwd yourself?
11:30:56 <Zao> edwardk: Check config.log?
11:30:58 <pejo> edwardk, what does file on that file report?
11:31:30 <edwardk> sjachaf: trying that now
11:31:46 <edwardk> Apr 1, 15:05, so its not getting rebuilt
11:31:54 <Cale> vegai: Damn it, you got me interested enough to click on your link (on reddit), and then I discovered the meaning of "integration" was entirely different from what I expected.
11:32:16 <shachaf> edwardk: Try making clean in util/pwd/?
11:33:24 <edwardk> heh, well, chicken and the egg gets me there i haven't been able to ./configure so ../../mk/config.mk isn't there... i'll just hand compile it and see how far i get i guess
11:33:56 <shachaf> Oh, right. :-)
11:34:13 <shachaf> Well, that should work, anyway.
11:34:24 * shachaf has different compilation errors problems with GHC.
11:34:36 <shachaf> Something with Unicode, I think.
11:34:40 <shachaf> I should solve that sometime.
11:35:45 <edwardk> that seems to have cleared the hurdle
11:38:17 <vegai> Cale: hmm, what did you expect?
11:39:34 <vegai> oh, you commented
11:39:51 <vegai> ahh. You silly mathematicians :)
11:47:25 <shachaf> Did someone here @localtime me?
11:57:56 <jgrimes> In both the 0.4.0 release and darcs version of AutoForms, my build is hanging on Compiling Graphics.UI.AF.General.CustomTypes. I get some failed assertions and then nothing happens, other than CPU usages going to 100%. Anyone else experience this?
12:01:30 <tromp> @instances Num
12:01:31 <lambdabot> Double, Float, Int, Integer
12:01:43 <EvilTerran> instance Cold toes => Num toes -- :P
12:02:05 <tromp> @src Num
12:02:05 <lambdabot> class  (Eq a, Show a) => Num a  where
12:02:05 <lambdabot>     (+), (-), (*)           :: a -> a -> a
12:02:05 <lambdabot>     negate, abs, signum     :: a -> a
12:02:05 <lambdabot>     fromInteger             :: Integer -> a
12:17:10 <fons> hi all
12:19:43 <fons> I have a function which takes a value and computes a result of the same time in a monad (e.g. f n = return $ n+1) can you think of a way to apply f twice which is more elegant than the obvious (twice n = do {n' <- f n; f n} ?
12:20:17 <fons> sorry: twice n = do {n' <- f n; f n'}
12:21:14 <fons> I've been looking for combinators in Control.Monad but didn't sem to find anything useful
12:21:37 <idnar> @undo \n -> do {n' <- f n; f n'}
12:21:37 <lambdabot> \ n -> f n >>= \ n' -> f n'
12:21:45 <idnar> @pl \ n -> f n >>= \ n' -> f n'
12:21:45 <lambdabot> (f =<<) . f
12:21:49 <mrd> your first function is called liftM
12:21:59 <mrd> ?
12:22:18 <fons> mrd: f you mean?
12:23:06 <fons> idnar: (f =<<) . f it is
12:23:20 <mrd> finding your description confusing
12:23:21 <Heffalump> fons: well, you could at least make a twiceM :: (a -> m a) -> a -> m a
12:23:22 <Heffalump> and apply that
12:23:34 <Saizan_> @pl \f -> (f =<<) . f
12:23:34 <lambdabot> (.) =<< (=<<)
12:23:34 <Heffalump> I think (f =<<) . f is rather obfuscated TBH
12:23:53 <mauke> f x >>= f
12:23:53 <fons> Heffalump: yep
12:23:55 <mrd> > sequence [f,f] a
12:23:56 <lambdabot>  Add a type signature
12:24:02 <mrd> > sequence [f,f] a :: Expr
12:24:02 <lambdabot>  Couldn't match expected type `Expr' against inferred type `[a]'
12:24:21 <Heffalump> mrd: sequence doesn't work like that
12:24:26 <mrd> > sequence [f,f] a :: [Expr]
12:24:28 <lambdabot>  [f a,f a]
12:24:35 <Heffalump> it doesn't thread outputs to inputs etc
12:24:39 <mrd> hmm yea
12:24:52 <fons> mauke: yep, that's simply desugaring the do notation
12:25:13 <fons> (\x -> f x >>= f)
12:28:02 <fons> Heffalump: sure, I was just trying to find a solution using library functions
12:28:20 <fons> Heffalump: an elegant combinator
12:29:23 <Heffalump> fons: yeah
12:29:34 <Saizan_> ?type \f n -> last <$> replicateM 2 f $ n
12:29:34 <lambdabot> forall a a1. (a1 -> a) -> a1 -> a
12:29:47 <Heffalump> ewww!
12:29:49 <monochrom> since "twice" isn't a very general or elegant thing, it isn't going to be in the library.
12:29:55 <Saizan_> not working :)
12:30:11 <Heffalump> replicateM desn't thread either
12:30:17 <monochrom> f n >>= f  is simple and elegant and practical.
12:30:27 <fons> monochrom: of course, I wasn't expecting a concrete implementation of the concrete function
12:33:05 <monochrom> mapping monads to arrows,  Kleisli f >>> Kleisli f
12:33:40 <Heffalump> oh, cunning
12:33:45 <stefast> Beginners question: I am trying to implement a function that flattens a list (of atoms and possibly lists). Now I know how to do this in Prolog (working through the 99 Prolog problems in Haskell) but I have a hit a silly snag in Haskell. I am trying to do: flatten ((y:ys):xs) = ...; flatten (x:xs) = ...; ... So that the first expression matches a list that has a list as head and the second expression matches a non-list. This is of course not valid
12:33:51 <Heffalump> is there an unKleisli?
12:33:53 <monochrom> Then you can talk about foldr1 or foldl1 (>>>) (iterate Kleisli f)
12:34:03 <monochrom> Yes, please s/un/run/
12:34:10 <Heffalump> ah, yes, I remember
12:34:44 <monochrom> oops, s/iterate/repeat/  also missed some parens.
12:35:25 <fons> stefast: heterogeneous lists are not directly supported by the language so you cannot have a list of both lists and atoms (assuming atoms are not represented as lists)
12:35:25 <Heffalump> stefast: you realise Haskell is statically typed?
12:35:28 <monochrom> More than 50% of the time I find (a -> m b) more practical than (m b).
12:36:01 <stefast> yes I know that :) and suspect that therein lies my problem
12:36:17 <Lemmih> stefast: type NestedList x = [ListItem x]; data ListItem x = Item x | Nest (NestedList x)
12:36:28 <stefast> I have tried data Mixed a = MixedList a | MixedValue a
12:36:28 <Heffalump> yes, as fons says you can't trivially mix atoms and lists as you want.
12:36:37 <fons> stefast: first, what is the (simplified) definition of the data structure you are trying to flatten?
12:36:38 <Heffalump> you want MixedList [a]
12:36:49 <Heffalump> do you want arbitrarily nested lists, or just one level of nesting?
12:36:52 <Cale> stefast: So really you have a tree datatype :)
12:37:00 <Cale> stefast: and you're flattening that into a list :)
12:37:04 <monochrom> Data.Tree may help you.
12:37:13 <stefast> Heffalump: yes sorry that was a typo
12:37:32 <stefast> Cale: exactly
12:37:37 <olsner> Heffalump: wouldn't that be MixedList [Mixed a] for an arbitrarily nested list?
12:37:45 <Heffalump> olsner: yes, that's why I asked.
12:38:07 <Heffalump> though as others have said that's a tree
12:38:35 <stefast> olsner, heffalump: Yes sorry: I did data Mixed a = MixedList [Mixed a] | MixedValue a
12:38:43 <Cale> data Tree a = Empty | Item a | Branch [Tree a]
12:39:25 <EvilTerran> Cale, well, you could represent Empty as Branch []
12:39:25 <Cale> (or you can skip the Empty constructor
12:39:27 <Cale> )
12:39:28 <Cale> yes
12:39:30 <EvilTerran> :D
12:39:51 <Cale> data Tree a = Item a | Branch [Tree a]
12:40:10 <Cale> So that's a tree with values on its leaves, which is what lisp lists are like.
12:40:44 <EvilTerran> type Tree a = Mu (Either a `O` [])
12:40:45 <EvilTerran> ;)
12:41:07 <Cale> To flatten it in a naive way, we can just pattern match
12:41:19 <Cale> flatten (Item x) = [x]
12:41:32 <stefast> The problem is I can't seem to be able to do this: flatten ((x::MixedList):xs).
12:41:43 <Cale> flatten (Branch ts) = concat (map flatten ts)
12:42:03 <EvilTerran> stefast, indeed, you can't mix lists and elements thereof in another list without using constructors to tell the different item types apart
12:42:15 <EvilTerran> > [True, [False, True]]
12:42:15 <lambdabot>  Couldn't match expected type `Bool' against inferred type `[a]'
12:42:21 <EvilTerran> > [Left True, Right [False, True]]
12:42:21 <lambdabot>  [Left True,Right [False,True]]
12:42:29 <EvilTerran> ?type [Left True, Right [False, True]]
12:42:29 <lambdabot> [Either Bool [Bool]]
12:42:31 <stefast> I don't know how to differentiate between  the head of the first list (:: MixedList) in the first expression and the head of the list in the second expression (::MixedValue)
12:42:44 <EvilTerran> constructors!
12:43:06 <Cale> You pattern match
12:43:14 <monochrom> You could use Branch[0] instead of Item[0].
12:43:14 <Cale> See my example for flatten there :)
12:43:27 <monochrom> err Branch[0] instead of Item 0.
12:43:33 <Cale> errr
12:43:46 <monochrom> Thus data Tree a = Branch [Tree a]  is all you ever need.
12:43:47 <Cale> monochrom: Wouldn't that still be ill-typed?
12:43:57 <monochrom> See Data.Tree
12:43:58 <Cale> monochrom: huh?
12:44:00 * EvilTerran thinks so too
12:44:09 <Cale> monochrom: You mean storing the values in the branches?
12:44:15 <monochrom> Oops.
12:44:16 <Cale> Branch 0 [] ?
12:44:32 <monochrom> I erred.  But see Data.Tree anyway.
12:47:03 <Cale> That's not really the sort of tree we want though... there's an isomorphism of course, but it's a little awkward :)
12:47:22 <JohGro_> Question: Can I (on ubuntu) compile a program for OS X to give to a friend running OS X 10.4 ?
12:47:59 <Heffalump> JohGro_: no.
12:48:17 <JohGro_> Heffalump: Ok, thanks.
12:48:50 <Heffalump> GHC doesn't cross-compile and it'd be quite hard to make it do so.
12:50:22 <JohGro_> Heffalump: I was hoping to emit standard c-code and cross compile that or send it.
12:50:46 <JohGro_> I failed.
12:52:28 <monochrom> each ghc-produced executable is statically linked with a machine code runtime. that is the obstacle.
12:52:49 <monochrom> (the runtime is tailored for the OS)
12:52:57 <JohGro_> Ok.
12:53:03 <roconnor> idnar: are you going to join #gurgling-blah or what?
12:53:08 <Deewiant> one can't dynamically link the runtime?
12:53:20 <shachaf> roconnor: Wrong channel.
12:53:26 <Japsu> @index fromHex
12:53:26 <lambdabot> bzzt
12:53:32 <thoughtpolice> ghc currently doesn't do dynamic linking
12:53:32 <Japsu> @hoogle hex
12:53:32 <lambdabot> Text.ParserCombinators.Parsec.Char.hexDigit :: CharParser st Char
12:53:32 <lambdabot> Text.ParserCombinators.Parsec.Token.hexadecimal :: TokenParser st -> CharParser st Integer
12:53:32 <lambdabot> Numeric.showHex :: Integral a => a -> ShowS
12:53:34 <Deewiant> @index readHex
12:53:35 <lambdabot> Numeric
12:53:40 <Japsu> Deewiant: thanks
12:53:46 <roconnor> shachaf: oops, I should have asked in #haskell-blah
12:53:47 <Deewiant> Japsu: no problem
12:53:51 <thoughtpolice> i believe there is some initial support and it is in fact supported on OS X (dynlib's)
12:54:04 <thoughtpolice> but outside of that, no
12:54:35 <Japsu> @index unfoldr
12:54:35 <lambdabot> Data.List
12:54:49 <Heffalump> JohGro_: oh. That might just be possible, with some pain. Look at the instructions for bootstrapping GHC on a new platform/
12:55:25 <JohGro_> Ok. I do not want pain.
12:55:52 <JohGro_> I will find some other way to do what I want. Thanks.
12:55:56 <Heffalump> np
12:56:34 <Japsu> is there an easier way to this:
12:57:01 <Japsu> > let f [] = Nothing; f xs = Just (take 2 xs, drop 2 xs) in unfoldr f "foobar"
12:57:02 <lambdabot>  ["fo","ob","ar"]
12:57:42 <Cale> JohGro_: It's probably just easiest to install GHC on OS X and recompile the Haskell program there :)
12:57:46 <mauke> > let f xs = zipWith (\x y -> [x,y]) xs (tail xs) in f "foobar"
12:57:46 <lambdabot>  ["fo","oo","ob","ba","ar"]
12:58:04 <desegnis> > (map (take 2) . takeWhile (not.null) . iterate (drop 2)) "foobar"
12:58:05 <lambdabot>  ["fo","ob","ar"]
12:58:17 <mauke> Japsu: splitAt
12:58:19 <monochrom> It's easiest to silently install ubuntu on friend's computer.
12:58:25 <Japsu> ooh splitAt
12:58:31 <Japsu> :t splitAt
12:58:31 <lambdabot> forall a. Int -> [a] -> ([a], [a])
12:58:41 <Deewiant> > let f [] _ = Nothing; f xs n = Just . takeWhile (not.null) . map fst . tail . iterate (splitAt n . snd) $ ([], xs) in f "foobar" 2
12:58:41 <stefast> flatten List = flttn List []
12:58:41 <stefast>   where
12:58:41 <stefast>     flttn [] acc = reverse acc
12:58:41 <stefast>     flttn ( (x::MixedList) ::xs) acc = flttn xs ((flttn x):acc)
12:58:41 <stefast>     flttn (x::xs) acc = flttn xs (x:acc)
12:58:42 <lambdabot>  Just ["fo","ob","ar"]
12:58:59 <stefast> this is what I am trying to do if you are still interested :)
12:59:07 <Japsu> Deewiant: fails to match my definition for "easier" ;)
12:59:18 <Japsu> hmm
12:59:23 <Deewiant> Japsu: it's generic though :-)
12:59:24 <Cale> stefast: List cons is a single colon
12:59:31 <mauke> > evalState (replicateM 3 (State (splitAt 2))) "foobar"
12:59:31 <Japsu> we need an unfoldr that works with lists
12:59:31 <lambdabot>  ["fo","ob","ar"]
12:59:33 <Cale> stefast: :: is used for type signatures
12:59:36 <Japsu> no, wait
12:59:39 <Japsu> :t unfoldrM
12:59:40 <lambdabot> Not in scope: `unfoldrM'
12:59:51 <Japsu> monadic unfoldr, hmm
13:00:12 <Cale> stefast: and you should basically never have to put a type signature inside a pattern
13:00:19 <Cale> (though there's a GHC option to allow that)
13:00:48 <Japsu> :t unfoldM
13:00:49 <lambdabot> Not in scope: `unfoldM'
13:00:55 <Cale> stefast: also, using an accumulator for this is a strange way to go
13:01:09 <stefast> q
13:01:36 <Cale> hmm
13:01:50 <desegnis> Type signatures being not allowed in patterns can make you look funny if you're coming from, say, the simply-typed lambda calculus
13:01:56 <stefast> flatten List = flttn List []
13:01:57 <stefast>   where
13:01:57 <stefast>     flttn [] acc = reverse acc
13:01:57 <stefast>     flttn (x:xs) acc = flttn xs ((flttn x):acc)
13:01:57 <stefast>     flttn (x:xs) acc = flttn xs (x:acc)
13:02:14 <mauke> eww, tail recursion
13:02:18 <stefast> hehe the problem is I have no way of differentiating between the two expressions
13:02:27 <Cale> oh, and variables are not allowed to start with uppercase letters
13:02:39 <Cale> (those are reserved for constructors)
13:02:55 <wli> \n -> unfoldr (\xs -> fmap (splitAt n . const xs) $ listToMaybe xs)
13:03:00 <stefast> how do I say that x in the first expression should match a list and the x in the last expression should match an atom
13:03:17 <Cale> stefast: Well, what type is x?
13:03:18 <Japsu> vr kanava
13:03:22 <Japsu> ...gah
13:03:24 <Japsu> fail
13:03:26 <Cale> stefast: You'd typically pattern match
13:03:27 <Japsu> in fact, multifail
13:03:29 <Japsu> @index intercalate
13:03:29 <lambdabot> bzzt
13:03:39 <mauke> vd kedavra
13:03:48 <Cale> http://loadingreadyrun.com/videos/view/316/unnatural_resources
13:03:49 <lambdabot> Title: Loading.Ready.Run. - Unnatural Resources
13:03:52 <monochrom> Cale: what is the adopted data declaration now?
13:03:57 <Cale> Japsu: that video is for you :)
13:04:09 <Cale> monochrom: I don't know.
13:04:14 <desegnis> stefast: I'm actually wondering where the flattening shall occur. If you use (:) on the right-hand side like that, you're just reconstructing the list (in reverse order)
13:05:07 <Japsu> oof
13:05:11 <Japsu> :t readHex
13:05:12 <lambdabot> forall a. (Num a) => String -> [(a, String)]
13:05:25 <Japsu> I just could have looked at that first.
13:05:45 <Cale> stefast: start out with this data declaration:
13:05:45 <Japsu> but why does it work like that
13:05:52 <Syzygy-> > readHex "0xdeadbeef" :: Integer
13:05:52 <lambdabot>  Couldn't match expected type `Integer'
13:06:01 <Cale> data Tree a = Item a | Branch [Tree a]
13:06:04 <monochrom> need "initial value"
13:06:08 <Syzygy-> > (fst . readHex "0xdeadbeef") :: Integer
13:06:08 <lambdabot>  Couldn't match expected type `Integer' against inferred type `[a]'
13:06:17 <Syzygy-> > (fst . readHex "0xdeadbeef")
13:06:18 <lambdabot>  [0]
13:06:24 <Syzygy-> > readHex "0xdeadbeef"
13:06:25 <lambdabot>  [(0,"xdeadbeef")]
13:06:27 <stefast> well I am going to read the gentle introduction very carefully :) Thanks for your help
13:06:29 <Syzygy-> > readHex "deadbeef"
13:06:30 <lambdabot>  [(3735928559,"")]
13:06:30 <Japsu> bah
13:06:38 <Japsu> > let f "" = Nothing; f x = Just (take 2 x, drop 2 x) in concat . intersperse "." . map (show . fst . head . readHex) $ unfoldr f "c2fbf072"
13:06:39 <monochrom> Oh! haha
13:06:39 <lambdabot>  "194.251.240.114"
13:06:45 <mauke> > read "0xdeadbeef"
13:06:45 <lambdabot>  Exception: Prelude.read: no parse
13:06:55 <mauke> > read "0xdeadbeef" :: Integer
13:06:56 <lambdabot>  3735928559
13:07:01 <mauke> :-)
13:07:02 <Deewiant> > readHex "ff"
13:07:03 <lambdabot>  [(255,"")]
13:07:10 <Deewiant> > read "ff" :: Integer
13:07:10 <lambdabot>  Exception: Prelude.read: no parse
13:07:22 <Japsu> > readHex "ff ff s"
13:07:22 <lambdabot>  [(255," ff s")]
13:07:36 <monochrom> 3735928559 = 11 * 257 * 1321517
13:07:43 <monochrom> 257 is a nice prime.
13:07:44 <Deewiant> the list, I think, is the retro version of Maybe
13:08:22 <Syzygy-> And Maybe is the limited version of Either, isn't it?
13:08:39 <Syzygy-> Maybe a `iso` Either a ()
13:08:41 <Deewiant> in a way.
13:08:48 <monochrom> Yes.
13:08:49 <Cale> Deewiant: The point is that various parsers might want to return multiple possible parses.
13:09:06 <desegnis> Is there _any_ Read instance out there that may actually produce ambigous parse results?
13:09:09 <Deewiant> Cale: ah. Do any in the Prelude do that, though?
13:09:15 <Cale> So reads is the retro version of parsec :)
13:09:20 <Deewiant> >_<
13:09:48 <Syzygy-> So how do you write > read "0xdeadbeef" :: Integer using parsec?
13:09:53 <Syzygy-> curious
13:10:51 <monochrom> It would take a while.
13:11:15 <yitz> @let groupsOf n = taleWhile (not.null) . map (take n) . iterate (drop n)
13:11:15 <lambdabot> <local>:1:13: Not in scope: `taleWhile'
13:11:26 <Cale> I'm not sure that Parsec actually comes with the appropriate number parsers.
13:11:27 <yitz> @let groupsOf n = takeWhile (not.null) . map (take n) . iterate (drop n)
13:11:28 <lambdabot> Defined.
13:12:22 <Deewiant> @let groupsOf' n = map (take n). takeWhile (not.null) . iterate (drop n)
13:12:23 <lambdabot> Defined.
13:12:26 <Syzygy-> > map (take n) . iterate (drop n) $ [1,2,3,4,5,6,7,8,9,0] where n = 3
13:12:26 <lambdabot>  Parse error at "where" (column 57)
13:12:30 <desegnis> The best hex digit parser we have in Parsec is (many hexDigit) :: Parser String
13:12:37 <Syzygy-> > let n = 3 in map (take n) . iterate (drop n) $ [1,2,3,4,5,6,7,8,9,0]
13:12:38 <lambdabot>  [[1,2,3],[4,5,6],[7,8,9],[0],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]...
13:12:40 <monochrom> Somewhere under Text.ParserCombinators.Parsec.Token there is one.
13:12:46 <desegnis> er, many1, rather
13:12:48 <Syzygy-> > let n = 3 in map (take n) . iterate (drop n) $ [1..]
13:12:49 <lambdabot>  [[1,2,3],[4,5,6],[7,8,9],[10,11,12],[13,14,15],[16,17,18],[19,20,21],[22,23,...
13:13:01 <Deewiant> @check \n x -> groupsOf n x == groupsOf' n (x::[Int])
13:13:01 <lambdabot>  Falsifiable, after 7 tests: -3, [5,-1,4,2,0]
13:13:03 <yitz> > concat . intersperse "." . map (show . fst . head . readHex) . groupsOf 2 $ "c2fbf072"
13:13:04 <lambdabot>  "194.251.240.114"
13:13:32 <yitz> Jaspu: ^
13:13:36 <Deewiant> @check \n x -> n > 0 ==> groupsOf n x == groupsOf' n (x::[Int])
13:13:36 <lambdabot>  OK, passed 500 tests.
13:13:47 <EvilTerran> @seen idnar
13:13:47 <lambdabot> idnar is in #darcs and #haskell. I last heard idnar speak 52m 2s ago.
13:14:01 <monochrom> So, two common approaches:  many1 hexDigit >>= read, or do some setup then Text.ParserCombinators.Parsec.Token.
13:14:17 <Japsu> yitz: bah, your groupsOf function is longer than my f!
13:14:32 <yitz> But its more useful.
13:14:39 <Deewiant> Japsu: but it's general. :-)
13:14:48 <Japsu> so is unfoldr
13:14:53 <yitz> I have it in my dot ghci
13:15:13 <Cale> fmap (foldl (\x y -> 16 * x + digitToInt y) 0) (string "0x" >> many1 hexDigit)
13:16:22 <Cale> oh, d'oh :)
13:16:40 <Cale> fmap (foldl (\x y -> 16 * x + fromIntegral (digitToInt y)) 0) (string "0x" >> many1 hexDigit)
13:16:51 <Cale> Stupid Int
13:17:14 <Cale> But that works.
13:17:35 <yitz> desegnis: I've never seen a built-in Read instance return more than one parse. But the class allows you define instances like that for your own types.
13:17:57 <yitz> oops, desegnis is gone.
13:20:53 <Cale> choice [fmap (foldl (\x y -> n * x + fromIntegral (digitToInt y)) 0) (try (string prefix) >> many1 digitP) | (n,prefix,digitP) <- [(16,"0x",hexDigit), (8,"0",octDigit), (10,"",digit)]]
13:21:38 <yitz> um. Cale. It's starting to get a little hairy.
13:21:47 <Cale> hm?
13:22:09 <Deewiant> s/foldl/foldl'/
13:22:13 <Cale> That's a parser for parsing C-style integers.
13:22:26 <Saizan_> uhm, how hard is to write an [X]HTML renderer?
13:22:30 <yitz> unsigned
13:22:35 <Deewiant> Saizan_: hard.
13:22:42 <Cale> right, unsigned :)
13:23:01 <Cale> - is a unary operator symbol anyway, isn't it?
13:23:06 <Cale> :)
13:23:38 <monochrom> It's easy. Use gtk2hs, make sure it calls up gecko.
13:23:59 <mrd> monochrom: how about a correct [X]HTML renderer?
13:24:19 <yitz> so is this supposed to prove that you wouldn't want to write that in Parsec?
13:24:36 <monochrom> gecko isn't correct?  I suppose.  but it would take 1000 man-years to write a correct one.
13:24:40 * EvilTerran suddenly gets to thinking about a class Minus m where type On a; (-) :: a -> m
13:24:59 <Peaker> monochrom, or a purely functional language :-)
13:25:07 <Saizan_> monochrom: do you know if gtk2hs includes a binding to gecko?
13:25:21 <EvilTerran> instance Num a => Minus a where type On a = a; (-) = negate
13:25:22 <monochrom> Yes it does.
13:25:47 <yitz> monochrom: you mean it would take less man years to write a correct one than it did to write the incorrect one?
13:25:53 <EvilTerran> instance Num a => Minus (a -> a) where type On (a -> a) = a; (-) = flip subtract
13:26:07 <mrd> if a light-year is the distance light travels in a year, is a man-year the distance man travels in a year?
13:26:12 <monochrom> Programming language is a relatively minor issue in this case.  The HTML+CSS spec is simply lengthy.  And BTW I mean 1000 man-years using a purely functional language.
13:26:17 <shachaf> Saizan_: libmozembed, I think.
13:26:24 <Deewiant> EvilTerran: Num requires (*), that's too restrictive. ;-)
13:26:57 <EvilTerran> ?type subtract -- don't blame me
13:26:58 <lambdabot> forall a. (Num a) => a -> a -> a
13:27:22 <Peaker> monochrom, ah, that sounds horrible. Luckily I avoid the web :)
13:27:37 <mrd> the gopher underground
13:27:39 <Saizan_> shachaf: in gtk2hs? i know of that lib, i wonder if there are already haskell bindings
13:27:39 <yitz> monochrom: the problem is not the spec. the problem is matching user expectation when rendering all the forms of non-confromant junk that is out there.
13:27:48 <unenough> like dijkstra used to say, it's much easier to write without mistakes than to write with mistakes and correct them later, so why do people make mistakes?
13:27:52 <monochrom> gecko was certainly constructed with a huge effort. but it is already done. it is available here and now.  if you dream on making your own correct renderer, it is pure speculation.
13:27:53 * EvilTerran notes that that won't work, anyway, as the unary - is a right-section
13:27:59 <shachaf> Saizan_: There seem to be -- I remember it being mentioned when I compiled gtk2hs.
13:28:04 <mrd> unenough: they make mistakes because they are not dijkstra
13:28:13 <unenough> i was just posing a question
13:28:23 <EvilTerran> Saizan_, i've seen it in my local copy of the gtk2hs docs, so i guess so
13:28:26 <mrd> a rhetorical question?
13:28:38 <Peaker> unenough, people don't know and agree with dijkstra
13:28:38 <unenough> a diffeomorphic question
13:28:51 <mrd> that's a new one
13:28:53 <Peaker> unenough, at least those we worked with mostly :)
13:29:05 <unenough> yes .. :(
13:30:46 <roconnor> unenough: because it is even easier to write with mistakes and ship it.
13:30:47 <unenough> i'll define it then, a diffeomorphic question is a one-to-one and onto differential transformation of the unknown answer
13:31:05 <unenough> so if the answer was "yes", it may become "no
13:31:29 <Peaker> unenough, kishkush balabush!
13:31:37 <unenough> roconnor, i've experienced that on my own blood and sweat as Peaker pointed out
13:32:04 <unenough> "let's do everything 60% quick & dirty, and then spend 6 months fixing it"
13:32:50 <Peaker> Various people have varying degrees of thresholds for what corners can be cut for time's sake.. we had to constantly argue that in the long run, corner cutting does not save time
13:33:43 <unenough> in my case it was 3 years fixing!
13:33:48 <unenough> for 2 months of crappy work
13:34:07 <Peaker> unenough, I was about to say you were exaggarating, but then I realized what you were talking about :)
13:34:15 <quicksilver> dijkstra had it easy. His surname included two triads of consecutive letters, so he had a natural instinct for mathematical symmetries and sorting.
13:34:53 <roconnor> unenough: Hmm, in my limited experience I estimated that it takes about 10x the effort to write code without bugs.
13:35:25 <roconnor> that isn't so far off what you said
13:35:36 <Peaker> roconnor, well, if you want code without bugs, its better to put in the 10x effort, then put in just 5x the effort, and then spend 10x the effort fixing it incrementally
13:35:49 <EvilTerran> fnord?
13:36:48 <roconnor> Peaker by definition it is putting in 1x the effort and then spend ... fixinging it incrementally
13:37:35 <roconnor> I suppose I should say it was 10x the effort to prove the code has no bugs, rather than to write code without bugs.
13:37:39 <Peaker> roconnor, No, you can spend more time working the specifications, and perhaps prove with various degrees of formality and detail the correctness of the code. or you can hack something together and hope to "test it until its free of bugs"
13:38:31 <unenough> what really angers me is the fear of "starting over"
13:38:43 <Peaker> roconnor, often full-scale formal verified proofs are impossible, but even undetailed informal documented proofs are a good thing towards reliability
13:38:52 <unenough> it's MUCH better to start over when what you "already have done" is ...crap
13:39:04 <stefast> take haskell for example I have written the flatten function in Prolog and Erlang but haskell won't allow me to ship flatten until it is 100% perfect. Might take som time tough but it will ship when ready. Seems to me that Dijkstra would love Haskell :)
13:39:21 <dons> stefast: just put 'undefined' in for the bits you don't know yet
13:39:39 <Peaker> stefast, Haskell indeed forces you to prove that some aspects of your program are correct..
13:39:45 <dons> haskell does try to guide you to perfection
13:39:55 <EvilTerran> flatten _ = [] -- it ships, but it's wrong!
13:40:06 <dons> flatten = undefined -- also shippable
13:40:19 <unenough> flatten = flatten
13:40:24 <roconnor> flatten = undefined is at least approximately right.
13:40:30 <dons> ?djinn a -> b -> Maybe b
13:40:31 <lambdabot> f _ a = Just a
13:40:33 <unenough> so is my version
13:40:36 <wli> What's flatten supposed to do?
13:40:44 <dons> note that the proof engine is strong enough to derive some implementations for you
13:40:45 <roconnor> although it is the crappiest approximation you can get.
13:40:55 <dons> since the type specifies the behaviour so well
13:41:00 <dons> this ability
13:41:09 <dons> to generate programs from types isn't widely appreciated
13:41:15 <stefast> EvilTerran: ahh you just wrote a generalized version of flatten
13:41:23 <mrd> why to people say "one-to-one and onto" instead of bijective?
13:41:30 <mrd> do*
13:41:57 <Peaker> stefast, flatten = join ?
13:42:02 <earthy> mrd: because the term bijective does not carry any direct meaning to many people
13:42:06 <byorgey> mrd: it uses fewer letters?
13:42:08 <unenough> mrd: because i don't remember it
13:42:15 <earthy> mrd: whereas one-to-one and onto does
13:42:19 <byorgey> > length . nub $ "one to one and onto"
13:42:19 <lambdabot>  7
13:42:22 <Peaker> dons, But types are not complete enough a specification?  I haven't seen many useful @djinn results
13:42:23 <stefast> data Mixed a = MixedList a | MixedValue a
13:42:23 <stefast> flatten List = flttn List []
13:42:23 <stefast>   where
13:42:23 <stefast>     flttn [] acc = reverse acc
13:42:23 <stefast>     flttn (x:xs) acc = flttn xs ((flttn x):acc)
13:42:26 <mrd> I actually didn't learn "one-to-one" and "onto" until much more recently than "bijective"
13:42:27 <byorgey> > length . nub $ "bijective "
13:42:27 <lambdabot>  8
13:42:43 <stefast> data Mixed a = MixedList a | MixedValue a
13:42:43 <stefast> flatten List = flttn List []
13:42:43 <stefast>   where
13:42:43 <stefast>     flttn [] acc = reverse acc
13:42:43 <stefast>     flttn (x:xs) acc = flttn xs ((flttn x):acc)
13:42:45 <mrd> I always wondered why mathematicians used "onto" in such an grammatically incorrect fashion
13:42:46 <stefast>     flttn (x:xs) acc = flttn xs (x:acc)
13:42:49 <dons> Peaker: see 'theorems for free'
13:42:53 <dons> for what functions are specified enough
13:43:13 <dons> ?free sortBy
13:43:14 <lambdabot> (forall x. g x = h (f x) . f) => $map f . sortBy g = sortBy h . $map f
13:43:29 <dons> the type alone gives you also sorts of free rules about what works
13:43:36 * __pao__ is proud to have come to his first uncomprehensible ghc error (... use -fno-monomorphism-restriction ... )
13:43:39 <stefast> Peaker: sorry trying to get this to work. I am a complete newcomer to haskell but have some experience with prolog
13:43:41 <dons> this has consequences for refacotring and optimisation, among other things
13:44:09 <Peaker> stefast, Isn't "join" a flatten already?
13:44:37 <stefast> trying to find out the equivalent of a when statement in Erlang (guard statement
13:44:37 <bd_> dons: what's $map?
13:45:00 <dons> oh, that's map for some type.
13:45:10 <dons> since we're in lists,  $map == map
13:45:12 <bd_> not fmap?
13:45:17 <stefast> Peaker: I am going through 99 Prolog problems and implementing a flatten function
13:45:21 <dons> yeah, its fmap, effectively
13:45:36 <bd_> why doesn't it juse say fmap then?
13:45:38 <Peaker> stefast, and I ask, isn't "join" what you're wanting to implement?
13:45:44 <Peaker> > join [[1,2,3],[4,5,6]]
13:45:45 <lambdabot>  [1,2,3,4,5,6]
13:46:16 <dons> :t concat
13:46:16 <lambdabot> forall a. [[a]] -> [a]
13:46:20 <dons> ?src concat
13:46:20 <lambdabot> concat = foldr (++) []
13:46:42 <byorgey> stefast: by "flatten" do you mean something that would take a value like  [[1,2,3], 4, [[5,6], [7]]]  and produce [1,2,3,4,5,6,7] ?
13:46:56 <EvilTerran> Peaker, i think he's flattening a tree rather than a list-of-lists
13:46:58 <chuckbuck> > join [[1 2 [3]] [4 5]
13:46:59 <lambdabot>  Parse error at end of input
13:47:07 <byorgey> stefast: the problem (as you may know) is that you cannot have lists like that in Haskell.  lists must be homogeneous.
13:47:11 <Peaker> dons: isn't "concat" redundant to the more general join?
13:47:17 <dons> yeah, its the usual conflation of lisp trees to haskell lists
13:47:20 <byorgey> stefast: but really what you are doing is flattening trees.
13:47:20 <dons> trees /= lists!
13:47:21 <chuckbuck> > join [[1, 2, [3]], [4, 5]
13:47:21 <lambdabot>  Parse error at end of input
13:47:30 <stefast> Peaker: flatten is a recursive function, i'm not sure the join function you mentioned is recursive, e.g. [ [1,2], [3,[4]] ] == [1,2,3,4]
13:47:49 <dons> flatten :: Tree a -> [a]
13:47:52 <byorgey> right, I don't think join is what you want stefast
13:47:53 <dons> so that function?
13:47:53 <Peaker> stefast, that list is not homogenous
13:48:11 <dons> stefast: that's a tree, not a list
13:48:12 <byorgey> it is not a list at all =)
13:48:34 <dons> so maybe look at Data.Tree.flatten
13:48:37 <stefast> data Mixed a = MixedList (Mixed a) | MixedValue a
13:48:37 <stefast> flatten List = flttn List []
13:48:37 <stefast>   where
13:48:37 <stefast>     flttn [] acc = reverse acc
13:48:37 <stefast>     flttn (x:xs) acc = flttn xs ((flttn x):acc)
13:48:39 <stefast>     flttn (x:xs) acc = flttn xs (x:acc)
13:48:56 <dons> has anyone written about the conflation of lists and trees in lisp-like langauges being harmful?
13:49:28 <byorgey> stefast: by the way, posting chunks of code into the channel like that is considered rude, that's what we have hpaste.org for =)
13:49:28 <stefast> I have just been doing this for two days now
13:49:35 <dons> ?docs Data.Tree
13:49:35 <lambdabot> http://haskell.org/ghc/docs/latest/html/libraries/4/Data-Tree.html
13:49:51 <byorgey> stefast: welcome! you're doing well, I think =)
13:49:56 <dons> stefast: its cool. maybe poke around the Tree library for ideas on how to define and implement tree transformations?
13:50:26 <stefast> ahh sorry
13:50:47 <byorgey> stefast: no problem
13:51:00 <stefast> sorry people. I should have read the FAQ more closely
13:51:24 <byorgey> stefast: ok, first let's start with an appropriate data type.
13:51:40 <byorgey> data Mixed a = Node a [Mixed a]
13:52:10 <byorgey> a node contains a single element, along with a list of child nodes
13:53:15 <byorgey> hm, actually, that's what Data.Tree is, but maybe that's not the most appropriate
13:53:32 <byorgey> stefast: let's do it like this instead, which is closer to what you had before:
13:53:50 <byorgey> data Mixed a = MixedValue a | MixedList [Mixed a]
13:54:01 <stefast> byorgey: so [1,2,3] is :: [Int] but  [1,2,3,[4,5]] is ill typed
13:54:08 <byorgey> actually, that's almost exactly what you had before, except you need [Mixed a]
13:54:18 <byorgey> stefast: yes, exactly.  because the elements don't all have the same type.
13:54:59 <byorgey> stefast: but using the data type I just gave above, you could represent that as  MixedList [ MixedValue 1, MixedValue 2, MixedValue 3, MixedList [ MixedValue 4, MixedValue 5 ] ]
13:55:14 <byorgey> stefast: does that make sense?
13:55:41 <stefast> byorgey: yes exactly
13:55:57 <byorgey> ok, so now let's write flatten.
13:56:03 <byorgey> first, what type does flatten have?
13:56:26 <byorgey> once you write down the correct type, you're halfway done =)
13:56:27 <cadabra> I have two nearly identical expressions. One's fine, the other just freezes ghci (at 0% cpu). http://hpaste.org/6787
13:56:32 <stefast> byorgey: flatten :: [Mixed a] -> [a]
13:56:49 <byorgey> stefast: almost, you actually don't need the brackets around [Mixed a]
13:56:52 <stefast> byorgey: flatten :: [Mixed a] -> [Mixed a]
13:57:08 <byorgey> remember, Mixed a  already represents a  mixed-depth list/tree
13:57:37 <stefast> byorgey: flatten :: Mixed a -> Mixed a
13:58:00 <EvilTerran> *head -> desk*
13:58:03 <byorgey> I guess you could do that but I would rather have it return [a] like you had at first
13:58:04 <czakey> http://www.lisperati.com/landoflisp/
13:58:11 <quicksilver> cadabra: that's because it's recursive.
13:58:13 <czakey> side effects comic strip
13:58:19 <EvilTerran> czakey, we've had that at least three times already
13:58:20 <mrd> this reminds me of my friend who would program by random permutation
13:58:33 <byorgey> stefast: with a return type of [a], you are guaranteed that the result will actually be flattened =)
13:58:36 <quicksilver> cadabra: your expression for c' depends on the bounds of c', which it tries to calculate...
13:58:40 <czakey> EvilTerran: sorry
13:58:58 <cadabra> quicksilver: but b' depends on the bounds of b, which works
13:59:03 <cadabra> i mean bounds of b'
13:59:11 <quicksilver> oh
13:59:18 <quicksilver> hmm. I didn't read carefully enough.
13:59:25 <byorgey> stefast: you follow?
13:59:28 <EvilTerran> quicksilver, i think it can tell the bounds of an array without forcing any of the list parameter to (array)
13:59:38 <quicksilver> EvilTerran: you're right.
13:59:46 <cadabra> and it seems like you can gee bounds of an array that isn't complete.. bounds $ array (0,3) []
13:59:51 <EvilTerran> > bounds $ array (0,10) undefined
13:59:51 <lambdabot>  Undefined
13:59:59 <EvilTerran> > bounds $ array (0,10) [undefined]
13:59:59 <lambdabot>  Undefined
14:00:07 <EvilTerran> > bounds $ array (0,10) [(undefined,undefined)]
14:00:08 <lambdabot>  Undefined
14:00:13 <EvilTerran> hm. apparently not.
14:00:19 <quicksilver> LB's undefined checking is a bit broken.
14:00:26 <cadabra> Prelude Array> bounds $ array (0,2) []
14:00:27 <cadabra> (0,2)
14:00:30 <EvilTerran> only when it tries to show an undefined, tho
14:00:30 <stefast> mrd: I have posted the few lines of code that I am having trouble with. I have done them in Erlang and Prolog. I initially asked how to discriminate between two expressions. It is a basic question of haskell syntax. Do you know the answer?
14:01:13 <dons> stefast: what expressions do you want to discriminate between?
14:01:28 <dons> in haskell, you'd use pattern matching, or a case statement, or a guard, usually
14:01:31 <Baughn> @instances Binary
14:01:32 <lambdabot> Couldn't find class `Binary'. Try @instances-importing
14:01:33 <stefast> mrd: two expressions: flatten (x:xs) = ...   where   I must indicate that x is of type MixedList and the other expression flatten (x:xs) where x must be of type MixedValue.
14:01:44 <quicksilver> cadabra: I find that intriguing. I don't understand it though.
14:01:52 <dons> stefast: right, you dispatch on the constructor
14:01:57 <mrd> byorgey seems to be helping you quite well
14:02:09 <dons> f (MixedValue a:xs ) = ...
14:02:18 <dons> f (MixedList as : ys ) = ...
14:02:23 <cadabra> It's strange.. and it's not using any cpu, so I'm guessing there is no loop
14:02:26 <Baughn> cadabra: I'm looking hard, but I can't see /any/ difference between b' and c'. Could you point it out?
14:02:36 <EvilTerran> cadabra, from my experimentations, neither of them should work
14:02:37 <Baughn> cadabra: No, it's a deadlock. What happens if you compile with -threaded?
14:03:01 <shachaf> Baughn: One is using b, one is using c? :-)
14:03:25 <Baughn> shachaf: Yes, okay, but those should be interchangable
14:03:29 <stefast> byorgey: yes I follow: thanks
14:04:22 <Baughn> Unless the representation of an array suddenly changes when it passes four elements.. that sounds bizarre
14:04:40 <stefast> mrd: you where just a bit condescending there :)  I know perfectly well how I am to implement this and even how to prove this but I do not know aboutt haskell syntax. I will learn it but i decided to try here also :)
14:04:53 <stefast> byorgey: sorry
14:05:01 <cadabra> In http://www.cs.auckland.ac.nz/references/haskell/haskell-intro-html/arrays.html
14:05:03 <lambdabot> Title: A Gentle Introduction to Haskell: Arrays, http://tinyurl.com/2k9qy5
14:05:07 <stefast> byorgey: please continue if you are not to busy
14:05:15 <byorgey> stefast: ok, sure
14:05:17 <cadabra> wavefront n uses "a" recursively
14:05:36 <byorgey> stefast: so the type of flatten should be   flatten :: Mixed a -> [a], right?
14:05:37 <Baughn> cadabra: "n"?
14:05:48 <stefast> byorgey: yes
14:05:50 <Baughn> cadabra: You're not showing the code that constructs n and m. I assumed those were independent.
14:06:19 <byorgey> stefast: ok.  so the value of type Mixed a  that the function receives can be in one of two forms.  we just do a case analysis on it.
14:06:22 <cadabra> I annotated the paste: http://hpaste.org/6787#a1
14:06:28 <byorgey> flatten (MixedValue v) = ...
14:06:38 <byorgey> flatten (MixedList l) = ...
14:06:58 <Baughn> cadabra: Oh, wonderful. They're truly isomorphic.
14:07:02 <sfultong> there should be a deepConcat in prelude... something to flatten a list of lists of lists... of an arbitrary depth to a single list
14:07:26 <Baughn> cadabra: Gremlins. Must be gremlins.
14:07:32 <byorgey> stefast: I note you were using an accumulator before, which might make it more efficient, but I wouldn't worry about it at first, just get a basic version working
14:07:37 <roconnor> sfultong: there are polymorphism issues with such a function
14:07:53 <dons> sfultong: what's the type of that?
14:07:59 <dons> :t Data.Tree.flatten -- this type?
14:08:01 <lambdabot> forall a. Tree a -> [a]
14:08:22 * dons grumbles about conflating trees with 'lists of arbitrary nesting'
14:08:40 <stefast> byorgey, dons: http://hpaste.org/6788
14:08:56 <stefast> byorgey, dons: this is what I had originally
14:08:57 <Baughn> dons: Why? That's a perfectly good way to look at it..
14:09:19 <stefast> byorgey: with changes from byorgey. the implementation is not the problem it is the syntax
14:09:21 <Baughn> Well, not /haskell/ lists
14:09:24 <cadabra> I'm trying to compile with -threaded. I'm kinda new to Haskell so I've been running in ghci and runhaskell so far. When I try "ghc *.hs" the linker can't find the Data.Set functions. :s
14:09:25 <dons> just that you can't use lists of arbitrary nesting as a type.
14:09:32 <roconnor> dons: Tree a is different from a specific number of nested lists.
14:09:37 <byorgey> stefast: ok, for one, variables must be lowercase
14:09:47 <stefast> byorgey: ahh prolog habit :)
14:09:48 <byorgey> stefast: so you must use 'list' instead of 'List' there
14:09:50 <Baughn> cadabra: Try ghc --make -threaded main.hs for now
14:09:50 <dons> roconnor: sure, but they're asking for arbitrary nesting
14:10:18 <dons> stefast: i'd look at the 'flatten' function for trees. its close to what you have
14:10:30 <byorgey> stefast: and you can pattern-match on the values of 'x' as dons showed, like flttn (MixedValue a : xs) = ...
14:10:34 <roconnor> @src flatten
14:10:34 <lambdabot> Source not found. Are you on drugs?
14:10:36 <cadabra> Baughn: thanks, that worked. When I run the exec it says: runner: <<loop>>
14:10:51 <cadabra> "runner" is the name of the executable.
14:11:07 <Baughn> cadabra: Do you get loop for both, or only if you evaluate one of them?
14:11:18 <stefast> ahhh ok now we are getting somewhere :). I tried similar to yours in GHX but got a indecipherable error message :)
14:11:32 <cadabra> If I go to ghci and run the function, it returns b' correctly, but stops on c'
14:11:37 <sfultong> dons: yeah, you're right, I guess I accept your complaint
14:11:40 <roconnor> stefast: you don't want to write tail recursive fuctions in haskell. ... sorry if someone already mentioned this.
14:11:46 <Baughn> cadabra: (The loop-detection logic is intended to debug deadlocks in multithreaded programs, but it's handy for this too)
14:11:58 <stefast> roconnor: why not?
14:11:58 <byorgey> stefast: but I would just like to point out that handling a [Mixed a] like this is unnecessarily complex, since a single 'Mixed a' already can store a list =)
14:12:13 <sfultong> but... actually... why have lists at all?  Might as well have binary trees as the default data structure
14:12:19 <Peaker> roconnor, don't want as in "want not to", or just "don't want to, don't want not to"?
14:12:30 <sfultong> binary trees can do *anything* :-P
14:12:36 <lament> sfultong: so, like Lisp? :)
14:12:40 <roconnor> stefast: because it is only optimal for strict evaluation, and haskell uses lazy evaluation.
14:12:42 <dons> lists are useful, they act like sequential control
14:12:47 <cadabra> Baughn: in ghci, now it says: fromList [4,5,6,7],array *** Exception: stack overflow
14:12:50 <dons> trees are for other kinds of control flow :)
14:12:52 <roconnor> stefast: we use co-recursion instead.
14:13:26 <Baughn> cadabra: Very odd. What happens if you say c = b?
14:13:30 <sfultong> well, you can do sequential control with binary trees... in fact, you can do different ways of sequential control with trees
14:13:43 <Baughn> cadabra: Or, better yet, just swap c and b
14:13:45 <roconnor> stefast: see http://reddit.com/info/6as3z/comments/c03cgjss
14:13:45 <stefast> roconnor: ok interesting
14:13:48 <sfultong> no need to reverse the datastructure, either
14:14:10 <cadabra> b' completes and c' overflows
14:14:22 <Baughn> cadabra: Even with b=c?
14:14:40 <sfultong> lament: hmm, maybe trees are a more natural way to think of lisp syntax than lists
14:14:43 <Baughn> ..c=b, I should say. Not that there's a difference.
14:14:53 <dons> it seems useful to distinguish unary trees from arbitrary trees
14:15:37 <lament> a cons cell is a binary tree
14:15:44 <sfultong> well, perhaps... but I often find out that I've been using a list and really what I want is a tree or map
14:15:57 <stefast> byorgey: (MixedList x:xs) doesn't work as a discriminator
14:16:14 <sfultong> of course, that's somewhat to do with the fact that I'm not a very experienced haskell programmer...
14:16:20 <Botje> .. yet!
14:16:24 <lament> (a binary tree where only the leaves can store data)
14:16:24 <byorgey> stefast: what do you mean, it doesn't work?  it should
14:16:36 <brad_larsen> question here.  i need to write a simple command-line interpreter for a project...
14:16:56 <roconnor> ((MixedList x):xs)
14:16:59 <roconnor> ?
14:17:02 <brad_larsen> I was thinking of using parsec for this, and having the top-level parser return a list of some datatype corresponding to the commands entered...
14:17:19 <roconnor> hmm
14:17:22 <Baughn> cadabra: By the way, I think you should be able to say just "within = (<)". It'd be faster, too.
14:17:29 <Baughn> @type (<)
14:17:30 <lambdabot> forall a. (Ord a) => a -> a -> Bool
14:17:30 <roconnor> I guess that is the same
14:17:31 <brad_larsen> will parsec return these results as-needed, or will it wait until all the input is consumed?
14:17:41 <byorgey> roconnor: shouldn't need the parens, I don't think
14:17:47 <roconnor> yeah
14:17:48 <cadabra> Yeah my within is pretty naive.
14:17:55 <Baughn> cadabra: Okay. Not on arbitrary bounds, but it'd be fine on these bounds.
14:18:08 <roconnor> byorgey: I'm so used it it being the problem with (:) patterns
14:18:25 <byorgey> roconnor: it's true, it's the first thing I considered too =)
14:18:35 <sfultong> lambdabot should have a function that tells you the worst-case bounds of a function
14:18:51 <byorgey> brad_larsen: it will return them as-needed.
14:19:09 <Baughn> sfultong: It'd be catastrophically worse than the worst-case bounds even a mediocre programmer could figure out
14:19:29 <brad_larsen> byorgey: thank you.  i'm in the process of writing the parsing stuff, and that worry just occurred to me.
14:19:40 <Baughn> sfultong: You can't solve it in general, of course. Halting problem. ;)
14:19:54 <ceibe> hi, someone knows if it is possible to read RTS options in your source?
14:19:54 <lambdabot> ceibe: You have 1 new message. '/msg lambdabot @messages' to read it.
14:20:03 <byorgey> brad_larsen: yup. Parsec is lazy, just like everything else =)
14:20:11 <stefast> byorgey, rconnor: Couldn't match expected type `Mixed t' against inferred type `[Mixed t]'
14:20:11 <sfultong> Baughn: not necessarily an arbitrary function, just having that for builtin functions would be quite helpful
14:20:27 <Baughn> sfultong: I see. Like @src, but for complexity?
14:20:34 <sfultong> Baughn: exactly
14:20:38 <byorgey> stefast: can you annotate your paste with what you have now?
14:20:42 <Baughn> sfultong: Write a patch. ;)
14:21:04 <byorgey> stefast: that doesn't mean using (MixedList x : xs) doesn't work, just that you have a type mismatch somewhere.
14:21:09 <sfultong> oh, I'm not in charge of writing things, I'm just in charge of demanding ideas be implemented :-P
14:21:40 <stefast> byorgey: done
14:21:49 <sfultong> actually, that might be a simple-enough function that I could manage...
14:21:52 <sfultong> but I have other stuff to do
14:22:23 <Botje> @pl \x y -> show x ++ show y
14:22:23 <lambdabot> (. show) . (++) . show
14:22:53 <byorgey> stefast: you did not change the definition of Mixed a.  remember, it needs to be MixedList [Mixed a]  not MixedList (Mixed a)
14:23:32 <TheLorax> how can i do this but with two variables?
14:23:32 <TheLorax> http://www.haskell.org/haskellwiki/Lambda_abstraction
14:23:33 <lambdabot> Title: Lambda abstraction - HaskellWiki
14:24:21 <byorgey> stefast: if you make that change, I think it should work
14:24:26 <Botje> TheLorax: \x y -> ...
14:24:31 <Botje> or \x -> \y -> ...
14:24:54 <byorgey> > (\x y -> [x,y,y,x]) 1 2
14:24:54 <lambdabot>  [1,2,2,1]
14:25:10 <stefast> byorgey: hmmmm
14:25:34 <sfultong> hmm.... the library reference doesn't show the behavior of concat... I assume it's linear bounded
14:25:38 <TheLorax> Botje, ah, I was going (\x y -> x + y) (3, 4)
14:25:45 <TheLorax> thanks
14:25:53 <Botje> TheLorax: no, then you need \(x, y)
14:25:53 <mauke> > (\(x, y) -> x + y) (3, 4)
14:25:54 <lambdabot>  7
14:26:24 <Cale> > uncurry (\x y -> x + y) (3,4)
14:26:25 <lambdabot>  7
14:26:32 <Botje> > let { l = 1, e = 3, t = l+e+e } in [l,e,e,t]
14:26:32 <lambdabot>  Parse error at "," (column 12)
14:26:38 <Botje> > let { l = 1; e = 3; t = l+e+e } in [l,e,e,t]
14:26:39 <lambdabot>  [1,3,3,7]
14:26:41 <Botje> stupid keyboard.
14:26:42 <tromp> > uncurry (+) (3,4)
14:26:42 <lambdabot>  7
14:27:14 <mauke> @src uncurry
14:27:14 <lambdabot> uncurry f p = f (fst p) (snd p)
14:27:23 <sfultong> @src concat
14:27:23 <lambdabot> concat = foldr (++) []
14:27:36 <sfultong> concat isn't really like that, is it?
14:27:40 <sfultong> because that's kinda ucky
14:27:45 <Cale> huh?
14:27:52 <Botje> sfultong: what's icky about it?
14:27:52 <Cale> No, that's really the definition of concat
14:28:00 <byorgey> stefast: are you confused?
14:28:04 <Cale> It's the best possible definition of concat
14:28:13 <byorgey> now, foldl (++) []  would be ucky =)
14:28:28 <sfultong> well, yeah...
14:28:36 <Cale> byorgey: sfultong rather ;)
14:28:36 <lament> what about concat = join
14:28:39 <Peaker> trying to install guitv, but it depends on "haskell-src", what's that?
14:28:56 <pjd> lament: depends whether join = concat
14:29:03 <mauke> join = (>>= id)
14:29:04 <stefast> byorgey: http://hpaste.org/6791#a3, there is a typing error
14:29:05 <byorgey> foldl (++) []  would be O(n^2), as opposed to foldr (++) [] which is O(n)
14:29:08 <Cale> @hackage haskell-src
14:29:08 <lambdabot> http://hackage.haskell.org/cgi-bin/hackage-scripts/package/haskell-src
14:29:10 <mauke> which in the case of lists is concatMap id
14:29:58 <stefast> byorgey: I am reading the gentle introduction now :)
14:30:11 <sfultong> @src concatMap
14:30:11 <lambdabot> concatMap f = foldr ((++) . f) []
14:30:14 <byorgey> stefast: ah, I think you want  (flttn x) ++ acc,  not (flttn x):acc
14:30:28 <Cale> stefast: If I can make a recommendation, write flatten :: Mixed a -> [a] first
14:30:34 <Peaker> trying to build a guitv dependency: "Setup.lhs:30:43: Not in scope: `buildVerbose'" -- I think this is from ghc 6.6, any idea what to convert this to?
14:30:37 <Cale> stefast: and don't use an accumulating parameter
14:30:43 <byorgey> stefast: colon is like cons, if you know lisp, it doesn't concatenate two lists
14:30:50 <byorgey> Cale: I already tried making that recommendation =)
14:31:07 <stefast> Cale: yes I, flttn has type [Mixed a] -> [Mixed a] :)
14:31:17 <Cale> stefast: hm?
14:31:41 <Cale> stefast: I mean, instead of trying to deal with a list of trees, just deal with flattening one tree.
14:31:49 <stefast> cale, byorgey: of course you are right :)
14:31:51 <Cale> stefast: then to get a list of trees, you can use concat and map
14:32:00 <Peaker> stefast, wouldn't it be: [Mixed a]->[a] ?
14:32:32 * byorgey heads home, see you all later
14:32:47 <stefast> Cale: Yes I am doing http://www.hta-bi.bfh.ch/~hew/informatik3/prolog/p-99/ in Haskell in order to learn the language
14:32:53 * byorgey leaves stefast in the care of Cale =)
14:32:57 <stefast> byorgey: thanks for your help
14:33:07 <stefast> Peaker: yes
14:33:11 <Peaker> cabal packages don't build that cleanly on either 6.6 or 6.8 (some cleanly on this, some on the other)
14:33:16 <byorgey> stefast: you're welcome =)
14:41:38 <dmwit> boink!
14:42:31 * ehird_ is defining an IOT. Mwahaha!
14:43:35 <dmwit> I seem to recall there were some reasons that IOT is a bad idea.
14:43:47 <ehird_> dmwit: I'm just playing :D
14:44:16 <ehird_> hard to get 'putStr' etc to be IOT m a though
14:44:29 <ehird_> putStr :: String -> IOT m ()      -- the goal
14:44:57 <stefast> Cale: http://hpaste.org/6791#a4   It works now :)
14:45:09 <dons> http://www.valuedlessons.com/2008/04/you-could-have-invented-monadic-parsing.html
14:45:10 <lambdabot> Title: Valued Lessons: You Could Have Invented Monadic Parsing, http://tinyurl.com/3baxhm
14:45:12 <dons> intersting stuff
14:45:28 <dons> a new rule, any sufficiently complicated hand written parser has an ad hoc ill-specified parsec embedded in it
14:45:30 <monochrom> ehird_: import Prelude hiding (putStr)   -- there's more to come
14:45:39 <monochrom> import qualified Prelude as P
14:46:05 <monochrom> putStr s = lift (P.putStr s)
14:46:05 <stefast> Cale: The typing syntax is a bit tricky to get right when you have been doing prolog and erlang :)
14:46:07 <monochrom> or perhaps liftIO
14:46:16 <stefast> Cale: but i hope I am getting there
14:46:50 <Trinithis> :t interact
14:46:51 <lambdabot> (String -> String) -> IO ()
14:47:02 <ehird_> monochrom: heh, well, yes, but
14:47:02 <ehird_> :)
14:47:15 <monochrom> OK, you're doing something deeper.
14:47:41 <Cale> stefast: tail recursion is usually something to be avoided in Haskell.
14:48:19 <dons> we use recursion guarded by a (lazy) constructor
14:48:22 <Trinithis> Cale: why? doesnt it improve performance?
14:48:24 <dons> far more often, i think
14:48:34 <dons> Trinithis: strict tail recursion
14:48:41 <Cale> Trinithis: It's optimised into a loop, but usually all that loop does is builds a huge expression.
14:48:49 <Trinithis> ok
14:48:49 <Cale> Trinithis: which is only evaluated at the end
14:48:56 <Cale> For example:
14:48:56 <Peaker> arrg, non-qualified imports mean that trying to debug missing symbols, I have *no idea* where they were supposed to come from
14:49:01 <Cale> foldl f z [] = z
14:49:10 <Cale> foldl f z (x:xs) = foldl f (f z x) xs
14:49:11 <roconnor> Trinithis: see http://reddit.com/info/6as3z/comments/c03cgjss
14:49:20 <Cale> Let's look at foldl (+) 0 [1,2,3]
14:49:27 <Cale> foldl (+) 0 [1,2,3]
14:49:35 <Cale> = foldl (+) (0 + 1) [2,3]
14:49:37 <Trinithis> roconnor: page not there
14:49:41 <Cale> = foldl (+) ((0 + 1) + 2) [3]
14:49:48 <Cale> = foldl (+) (((0 + 1) + 2) + 3) []
14:49:54 <Cale> = ((0 + 1) + 2) + 3
14:49:56 <roconnor> crap
14:49:58 <Cale> = (1 + 2) + 3
14:50:01 <Cale> = 3 + 3
14:50:02 <Cale> = 6
14:50:31 <Trinithis> Cale, so the idea is to not do it because of the thunks involved?
14:50:34 <Cale> So even though that first part is implemented by a tight loop, it doesn't do much but build up an expression which is the size of the original list.
14:50:44 <roconnor> Trinithis: http://reddit.com/info/6as3z/comments/c03cgjs
14:50:46 <stefast> Cale: ahh ok very interesting
14:50:46 <dmwit> Trinithis: right
14:51:15 <Cale> So there's a strict left fold which avoids this problem
14:51:26 <Cale> (by forcing the evaluation of the accumulator on each step)
14:51:53 <Cale> But what's usually better is to avoid tail recursion altogether whenever possible, and simply return a constructor applied to some expressions.
14:52:31 <Cale> That way, if the expressions which the constructor is applied to are not pattern matched against, the recursion just stops right there.
14:52:53 <Cale> For example, with  concat = foldr (++) []
14:52:58 <Cale> @src foldr
14:52:58 <lambdabot> foldr f z []     = z
14:52:58 <lambdabot> foldr f z (x:xs) = f x (foldr f z xs)
14:52:59 <ddarius> Sounds like Cale's explaining laziness again
14:53:25 <ceibe> Cale, I receive your message each time that I join here :S
14:53:29 <brad_larsen> parsec question:  is there a built-in way of parsing floats that may begin with a minus sign?  I'm using Text.ParserCombinators.Parsec.Token.float, on a TokenParser created from the empty language definition, and it doesn't like the minus...
14:53:35 <Peaker> Simon says laziness is not that important
14:54:07 <dons> somethings are more important
14:54:20 <Cale> Peaker: I would disagree with him ;)
14:54:37 <dons> i think the idea is that purity and types are more important
14:54:48 <dons> but lazines keeps us honest
14:55:02 <Cale> Not only that, of course.
14:55:02 <Trinithis> best of all worlds!
14:55:09 <Peaker> Cale, well, given that the alternative to laziness in a strict language wasn't figured out yet (he says its not clear if a new type is needed for that, for example), its hard to tell
14:55:26 <Cale> Laziness is also an incredibly useful glue which makes it possible to put programs together in more ways.
14:55:28 <dons> and means haskellers tend to have better understanding of evaluation strategies than typical strict language people -- in particular, mixed lazy/strict data types are well understood here, but rarely seen elsewhere
14:56:04 <\z> Is there some way to run QuickCheck as an HUnit test?
14:56:07 <lament> how come strictness annotations for _expressions_ are not in the language?
14:56:12 <Cale> Laziness makes an excellent default, because it's actually *usually* what you want.
15:00:25 <dons> mmap has an interesting ad-hoc implementation of laziness
15:00:25 <Cale> lament: are you referring to seq?
15:00:25 <dons> lament: seq?
15:00:25 <roconnor> lament: seq ?
15:00:25 <audreyt> $!
15:00:25 <roconnor> !
15:00:25 <dons> \`, yeah. quickCheck
15:00:25 <dons> rnf!
15:00:25 <lament> seq isn't it
15:00:25 <dons> seq is the bang!
15:00:25 <monochrom> What would you like?
15:00:25 <dons> i mean bomb.
15:00:25 <Cale> lament: (seq x y) is an expression which, when evaluated, will cause x to be evaluated into WHNF before resulting in y
15:00:25 <ddarius> dons: Indeed, lazy languages benefit strict ones by providing analyses and idioms that don't seem to come up in strict languages.
15:00:25 <dons> yeah
15:00:25 <lament> as i understand, seq just evaluates enough of the first expression to see that it's not bottom.
15:00:25 <ceibe> brad_larsen, sign could have problems
15:00:25 <roconnor> deepseq ?
15:00:25 <dons> rnf
15:00:25 <Cale> lament: Right, which is the least amount of evaluation you can do. In terms of seq, you can always write deeper evaluations.
15:00:25 <dons> :t Control.Parallel.Strategies.rnf
15:00:25 <lambdabot> forall a. (NFData a) => a -> Done
15:00:25 <\z> dons: how does quickCheck return a pass/fail back to HUnit?
15:00:25 <Cale> Of course, instances of NFData are written in terms of seq
15:00:25 <brad_larsen> ceibe: what do you mean?  I could write my own float parser that accepts unary minus, but don't want to duplicate something that is provided already.
15:00:25 <lament> Cale: right, but why not have something like !(foo) meaning "evaluate foo right now"
15:00:25 <dons> \z, i've used my own wrappers for that.
15:00:25 <dons> its pretty easy to turn quickcheck into a unit tester
15:00:25 <audreyt> :t (id$!)
15:00:25 <lambdabot> forall a. a -> a
15:00:25 <Cale> lament: When is "right now"?
15:00:25 <\z> dons: do you have examples in xmonad?
15:00:25 <dmwit> :t join seq
15:00:25 <lambdabot> forall a. a -> a
15:00:25 <\z> or lambdabot?
15:00:25 <ceibe> brad_larsen, could be a problem to know when - is the binart operator or when is the unary negate operator
15:00:25 <Cale> lament: If !(foo) is being evaluated, then surely if foo was in its place, foo would have been evaluated.
15:00:25 <ceibe> binary*
15:00:25 <ehird_> hee
15:00:25 <ehird_> I defined IOT
15:00:25 <brad_larsen> ceibe: roger that.
15:00:25 <lament> Cale: true!
15:00:25 <ehird_> http://hpaste.org/6792
15:00:25 <ddarius> lament: seq must inherently be (at least) binary
15:00:41 <Cale> lament: (seq x x) is always a waste of time :)
15:00:46 <lament> seq is more or less >>, isn't it
15:00:55 <Cale> No, they're completely unrelated.
15:00:59 <dmwit> brad_larsen: See also the Language module in Parsec.
15:01:12 <TheLorax> is there a built in way to do a factorial operation in haskell?
15:01:20 <TheLorax> (yes I did google)
15:01:30 <dmwit> > product [1..10]
15:01:31 <lambdabot>  3628800
15:01:33 <lament> Cale: unsafePerformIO (return x >> return y)
15:01:35 <ceibe> brad_larsen, I don't know what is "roger" but any way you can define your own float parser without duplicate code
15:01:42 <Cale> lament: x wouldn't be evaluated there
15:01:50 <ehird_> seq :: a -> b -> b
15:02:02 <Cale> lament: seq has more in common with case expressions
15:02:08 <ceibe> brad_larsen, my_float = do s <- sign; f <- float; return $ s f
15:02:16 <monochrom> unsafePerformIO doesn't add strictness.
15:02:29 * roconnor kinda wants the Seq class back
15:02:29 <brad_larsen> ceibe: simpler than what I was thinking!  thanks.
15:02:33 <Cale> For instance, at the list type,  seq x y  is equivalent to  case x of [] -> y; _ -> y
15:02:33 <dons> \z the xmonad Properties.hs has a driver
15:02:45 <dons> roconnor: NFData and WHNFData ?
15:02:51 <\z> dons: thanks.
15:02:56 <roconnor> probably
15:03:11 <lament> Cale: okay, then i just mean conceptually: IO monad is for sequencing stuff to make sure some stuff is done before other stuff.
15:03:23 <roconnor> @hoogle NFData
15:03:23 <lambdabot> Control.Parallel.Strategies.NFData :: class NFData a
15:03:46 <Cale> lament: But in fact, seq x y doesn't even ensure that x is evaluated before y
15:03:58 <dmwit> > let fact x = length . sequence . takeWhile (not . null) . iterate (drop 1) . flip replicate ' ' in fact 5
15:03:59 <lambdabot>  <Int -> Int>
15:04:05 <dmwit> > let fact = length . sequence . takeWhile (not . null) . iterate (drop 1) . flip replicate ' ' in fact 5
15:04:06 <lambdabot>  120
15:04:06 <Cale> lament: It just ensures that before the result of evaluating y is made available, x has been evaluated.
15:04:27 <Cale> lament: However, you can usually assume that x will be evaluated first.
15:04:33 <dmwit> > let fact = length . sequence . takeWhile (not . null) . iterate (drop 1) . flip replicate ' ' in fact 30 -- how fast?
15:04:37 <lambdabot> Terminated
15:04:47 <dmwit> Not very fast, is the answer. =)
15:04:48 <Cale> grumble, stupid freenode rerouting nonsense
15:05:11 <ehird_> lament: 'seq x' wouldn't work because it would be 'x'. This is because that the seq wouldn't kick in until the 'seq x' decided it needs evaluation, so it would have no effect until 'x' would need to evaluate anyway
15:05:14 <ceibe> brad_larsen, I don't know if Parsec exports sign, but in the Token module the author define it like: sign  =  (char '-' >> return negate)  <|> (char '+' >> return id)  <|> return id
15:05:18 <monochrom> The announcement was "about just 7K users will be affected" :)
15:05:18 <ehird_> 'seq x y' means that whenever *Y* needs to evaluate, x must
15:05:22 <ehird_> so it makes sense
15:05:36 <roconnor> dons: so I can use $| from now on?
15:06:06 <Cale> lament: In a very very loose sense, seq is similar to >>
15:06:17 <Cale> lament: But >> doesn't have anything at all to do with evaluation order
15:06:18 <brad_larsen> ceibe: thank you again.  I was just looking for that.
15:06:20 <lament> so, to summarize: switching from lazy evaluation to strict evaluation is nearly impossible :)
15:06:45 <ehird_> lament: no. why do you always twist things into 'haskell sucks'?
15:06:48 <Trinithis> for the most part, you want to code lazily in the fist place right?
15:06:49 <Cale> lament: which you should always keep in mind. If you have x >> y, it's not necessarily the case that x will be evaluated first.
15:06:51 <ehird_> its very possible
15:07:04 <monochrom> "nearly impossible" is a hyperbole.
15:07:23 <ehird_> monochrom: yeah, but lament has said that haskell sucks in #esoteric quite a lot
15:07:24 <monochrom> Satisfying lament is nearly impossible. :)
15:07:32 <Cale> lament: You mean as a mindset? Or taking lazy programs and running them in a strict evaluator?
15:07:32 <dons> roconnor: or $||
15:07:37 <Cale> lament: or what?
15:07:57 <Cale> lament: Annotating lazy programs to do some things strictly is easy.
15:08:06 <stefast> Cale: I think that tail recursion still applies in some situations but lazyness is very nice when you are working with stuff that has some infinite aspect :) But the lazy aspect does break some of the pure functional/mathematical aspects of the programming experience. For instance when you write fibonacci as a self-recursive list you need to take some imperative steps in order to jump start the stream, making sure the stream begins with 1:1:[...]
15:08:34 <Cale> stefast: imperative?
15:08:40 <stefast> or maybe I am just talking nonsense :) It is after all a bit late
15:08:41 <yitz> stefast: why is that imperative?
15:08:48 <ddarius> stefast: When you define the Fibonacci sequence mathematically, you still have to write the starting conditions.
15:08:49 <monochrom> That is hardly imperative, but why am I arguing.
15:08:51 <Cale> > let fibs = 0 : 1 : zipWith (+) fibs (tail fibs) in fibs
15:08:51 <roconnor> stefast: tail recursion can be useful when we use co-recursion.
15:08:52 <lambdabot>  [0,1,1,2,3,5,8,13,21,34,55,89,144,233,377,610,987,1597,2584,4181,6765,10946,...
15:09:04 <dmwit> stefast: That doesn't break anything; in mathematics, you also need a base case for induction.
15:09:05 <dmwit> stefast: Those are the base cases.
15:09:09 <Cale> stefast: That looks *extremely* declarative to me.
15:09:11 <roconnor> stefast: blah, let me try again
15:09:20 <roconnor> stefast: tail recursion can be useful when we cannot use co-recursion.
15:09:23 <roconnor> better
15:09:37 <ceibe> Cale, do you know if there is a way to read RTS options in your source code? I was working more in the code of the last day and I see that to get a good performance using parListChunk I must define chunk = length_of_list `div` numberOfThreads, the length of the list is a parameter that I know... the problem is the number of threads...
15:09:39 <tromp> @let fibab a b = fibs where fibs = a : scanl (+) b fibs
15:09:39 <lambdabot> Defined.
15:09:42 <ddarius> roconnor: Booleans are so complicated.
15:09:45 <monochrom> If you mean "const True (1/0) = True  breaks mathematics", that's another story.
15:09:45 <tromp> > fibab 0 1
15:09:46 <lambdabot>  [0,1,1,2,3,5,8,13,21,34,55,89,144,233,377,610,987,1597,2584,4181,6765,10946,...
15:09:56 <Cale> ceibe: Unfortunately, I don't know of a way to do it.
15:10:04 <ceibe> ;/
15:10:15 <Cale> ceibe: Really, the best solution to your problem is coming in the next version or two of GHC.
15:10:21 <roconnor> fix id breaks mathematics
15:10:24 <stefast> Cale: I mean in the technical sense. When you need to think about thunks and such :)
15:10:25 <Trinithis> what exactly is corecursion?
15:10:27 <ceibe> Cale, data parallism, no?
15:10:38 <ceibe> parallelism*
15:10:41 <stefast> but I am a beginner
15:10:42 <roconnor> Trinithis: see http://reddit.com/info/6as3z/comments/c03cgjs
15:10:42 <lament> Cale: "Annotating lazy programs to do some things strictly" seems kinda difficult
15:10:48 <Cale> ceibe: Yeah, you just use arrays of a certain type and it parallelises and load balances automatically.
15:11:02 <roconnor> ``For co-recursion what we do is make the recursive calls guarded. Guarded recursive call are recursive calls that appear inside a constructor.
15:11:12 <Cale> stefast: Don't think about thunks, think about expressions :)
15:11:21 <ceibe> Cale, ok thanks, so I only can wait :P
15:11:22 <Trinithis> roconnor: not to use it. I want a defn
15:11:26 <lament> Cale: after all, DeepSeq exists :)
15:11:28 <monochrom> Denotational semantics allows you to reason about lazily evaluated things with math and without thunks.
15:11:40 <yitz> > fix $ (0:) . scanl (+) 1
15:11:41 <Cale> lament: DeepSeq is rarely what you want.
15:11:41 <lambdabot>  [0,1,1,2,3,5,8,13,21,34,55,89,144,233,377,610,987,1597,2584,4181,6765,10946,...
15:11:42 <dons> git + darcs in one, http://labs.trolltech.com/blogs/2008/03/30/sourcecode-collaboration/
15:11:47 <lambdabot> Title: Trolltech Labs Blogs  Sourcecode collaboration, http://tinyurl.com/2w6sl6
15:11:48 <roconnor> Trinithis: ``For co-recursion what we do is make the recursive calls guarded. Guarded recursive call are recursive calls that appear inside a constructor.
15:11:50 <Cale> lament: and even so, how is that evidence?
15:12:22 <roconnor> Trinithis: ah, perhaps I was unclear that that was the definition.
15:12:28 <Cale> lament: In fact, it's evidence that seq is the right primitive, since stronger forms of evaluation are easy to write in terms of it.
15:12:32 <stefast> but I was referring to the ues of the strictness and lazyness syntax. are you reasoning about the semantics of the program or the implementation
15:12:48 <monochrom> stefast: Denotational semantics allows you to reason about lazily evaluated things with math and without thunks.
15:12:55 <roconnor> Trinithis: a recursive function is corecursive when all recursive calls appear inside contructors.
15:13:04 <ddarius> stefast: Haskell is non-strict, not lazy.  Strictness is strictly a semantic notion.
15:13:09 <lament> Cale: "Annotating lazy programs to do some things strictly" is what deepseq does, no? and deepseq is not particularly easy to write - you need to write instance declarations for your types
15:13:18 <Trinithis> roconnor: ok
15:13:32 <roconnor> ddarius: it is more than non-strict
15:13:35 <ddarius> lament: No. And yes and no.
15:13:39 <dmwit> dons: neat!
15:13:40 <Cale> lament: Uh, deepSeq is just a deeper version of seq
15:13:41 <monochrom> In fact the semantics of strict/eager languages are much uglier.  I've seen it.
15:13:58 <ddarius> roconnor: In what way?
15:14:02 <stefast> isn't strict like    f (1/0) = "Hello"
15:14:10 <lament> from deepseq source: instance  (DeepSeq a,DeepSeq b,DeepSeq c,DeepSeq d,DeepSeq e,DeepSeq f,DeepSeq g) => DeepSeq (a,b,c,d,e,f,g)
15:14:12 <roconnor> ddarius: evaluation must be equivalent to normal order
15:14:22 <Cale> lament: It's basically always really easy to write DeepSeq instances.
15:14:24 <ddarius> roconnor: Where does it say that?
15:14:24 <roconnor> ddarius: maybe anti-strict is a better word?
15:14:25 <stefast> returns "Hello" because 1/0 is not needed
15:14:28 <dons> > let x = 1/0 in "hello"
15:14:29 <lambdabot>  "hello"
15:14:33 <Cale> lament: What's wrong with that?
15:14:34 <roconnor> ddarius: it is implied by the semantics
15:14:41 <ddarius> roconnor: No it isn't.
15:14:49 <roconnor> ddarius: sure it is
15:14:58 <Cale> lament: (other than the fact that 7-tuples are dumb)
15:15:14 <ddarius> Unless by "equivalent" you mean they give the same answer, but that's a pointless statement.
15:15:17 <dmwit> stefast: That's non-strict.
15:15:38 <stefast> dmwit: yes, sorry I meant non-strict
15:15:48 <lament> Cale: a horrible lack of language support for optional strictness? You can't possibly annotate all tuples with deepseq instances.
15:15:54 <Cale> lament: I have never, not even once, needed DeepSeq
15:16:18 <Cale> It usually does far more evaluation than is necessary, which can often even hurt performance.
15:16:28 <dmwit> stefast: I guess the actual definition is that f is strict if f _|_ = _|_.
15:16:33 <dons> lament: wouldn't you just use a strict tuple type?
15:16:47 <dons> something from under Data.Strict.*
15:16:58 <monochrom> Here is why semantics of strict/eager languages must be uglier.  Write code "f x = ...".  What is the semantics of "f E"?  In non-strict languages, "oh, just macro-expand f and see".  In strict languages, "oh, if E terminates, then so-and-so; if E doesn't terminate, then so-and-so".  Two cases.  Combinatorical explosion.  Completely impractical.
15:17:04 <roconnor> ddarius: there is a whole range of dentations between strict and ``anti-strict''
15:17:13 <resiak> so, id is strict, right?
15:17:18 <ceibe> lament, I think that it's very strange use 7-tuples
15:17:24 <Cale> resiak: technically, yes
15:17:24 <roconnor> ddarius: It's just all the one's in between are useless.
15:17:46 <Cale> resiak: But it doesn't evaluate its parameter ;)
15:17:48 <resiak> sure
15:17:53 <ddarius> roconnor: Actually, you do have a point.  Non-strict doesn't mean that something unused isn't evaluated.  I don't think the Report specifies clearly enough.  Potentially even a strict implementation of Haskell may well satisfy it.
15:18:21 <lament> dons: so if sometimes you want strictness and other times you don't, you need two mostly identical versions of your data type?
15:18:33 <Cale> I think non-strict means that the semantics are identical to normal-order evaluation.
15:18:37 <lament> there's "tuples" and there's "strict tuples" and they're very different things
15:18:39 <pjd> cue Eager Haskell
15:18:42 <monochrom> ddarius: I suggest you change it to "even an eager implementation may satisfy it".
15:19:00 <ddarius> monochrom: Point.
15:19:17 <roconnor> Cale: that is a odd definition of non-, but if that is how it is used, I guess I can live with it.
15:20:03 <Cale> roconnor: Basically, if you choose to optimistically evaluate something, you have to ensure that the termination behaviour doesn't change.
15:20:56 <monochrom> As always, I am a bit puzzled by the way this community says "lazy vs strict".  It should be "lazy vs eager vs other evaluation orders", "non-strict vs strict".
15:21:15 <dons> lament: i don't think that's an issue in practice. if you know its always strict, use a strict type, if its always lazy, use a lazy type, otherwise use something in between, via annotations or bang patterns
15:21:19 <dons> and let the compiler do the restr
15:21:29 <Cale> lament: They're very different types
15:21:30 <roconnor> monochrom: right
15:21:51 <monochrom> It's like, how should I say it, comparing machines with ghosts.  They aren't even on the same plane of existence. :)
15:21:52 <roconnor> monochrom: eager : lazy :: strict : ?
15:21:55 <pjd> monochrom: possibly because "non-strict" is a bother to say
15:22:00 <roconnor> monochrom: what goes in the ?
15:22:05 <monochrom> non-strict
15:22:20 <Trinithis> can you have a lazy language that is strict?
15:22:22 <ddarius> pjd: Yep.  Optimization of syllables.
15:22:24 <roconnor> am I wrong in thinking there are more than two denotational semantics?
15:22:28 <ddarius> Trinithis: No.
15:22:42 <Trinithis> but you can have a nonstrict language that is eager?
15:22:49 <lament> Cale: they seem rather similar, conceptually
15:22:54 <dons> eager haskell is a nonstrict language
15:22:57 * pjd proposes "slack" as a synonym for "non-strict"
15:22:58 <ddarius> Trinithis: Yes.
15:23:04 <dons> bytestrings are eager, and part of haskell
15:23:06 <Trinithis> k
15:23:07 <Cale> It depends on what you mean by eager.
15:23:12 <dons> haskell's quite eager these days
15:23:22 <dons> lots of not-maximally lazy stuff going on
15:23:23 <ddarius> Cale: To me, it more depends on what you mean by non-strict.
15:23:30 <lament> Cale: which is why everybody talks about strictess "annotations" - you don't change the intrinsic nature of something by "annotating" it as strict
15:23:33 <Cale> If by 'eager', you mean 'evaluates all expressions innermost-leftmost first', then no.
15:23:50 <Cale> lament: Yes you do.
15:24:07 <dons> lament: you can radically change the runtime representation..
15:24:27 <Cale> and the semantics
15:24:34 <Trinithis> how long does haskell memorize things for callby need?
15:24:34 <monochrom> just 7K users!
15:24:45 <Cale> Trinithis: It doesn't.
15:24:46 <monochrom> For as long as you have references to it.
15:24:49 <ddarius> Trinithis: Haskell doesn't "memorize" or memoize things at all.
15:24:54 <lament> Cale: i guess we mean different things by "conceptually"
15:24:57 <pjd> Trinithis: it shares them, not memorizes them
15:25:01 <dons> ?userrs
15:25:01 <lambdabot> Maximum users seen in #haskell: 467, currently: 454 (97.2%), active: 20 (4.4%)
15:25:02 <Trinithis> then how is call by need done?
15:25:20 <dons> thunks don't recompute
15:25:21 <Cale> Trinithis: hehe, let me pull out my favourite example again :)
15:25:23 <pjd> think graph reduction
15:25:25 <Cale> double x = x + x
15:25:38 <ddarius> Trinithis: Memoization is not particularly related to call by need at all.
15:25:42 <Cale> We'll evaluate double (double 5) in a few evaluation strategies.
15:25:48 <Cale> First, strict evaluation:
15:25:52 <Cale> double (double 5)
15:25:56 <Cale> -> double (5 + 5)
15:25:59 <Cale> -> double 10
15:26:01 <Cale> -> 10 + 10
15:26:03 <Cale> -> 20
15:26:08 <lament> Cale: conceptually, lists of integers are very similar to lists of strings, even though they have different semantics and are just plain different. Nevertheless, haskell is nice enough to allow us to reify this conceptual similarity by having polymorphic types.
15:26:20 <Cale> Strict evaluation is innermost-first
15:26:31 <monochrom> I guess this justifies "lazy vs strict":  First you use ghc, so you know it's lazy.  Then you add ! and $! and seq somewhere.  Not totally eager now, but somewhere between eager and lazy, but you are sure it's strict in any case.
15:26:31 <lament> similarly, conceptually, strict data is similar to lazy data with the exact same datatype structure
15:26:33 <Trinithis> mm
15:26:36 <Cale> We could also evaluate it outermost first
15:26:41 <Arnfreth> Is there anybody who would know how to interpret haskell from another program?
15:26:42 <Cale> double (double 5)
15:26:48 <Cale> -> double 5 + double 5
15:26:49 <Arnfreth> say I wanted to make an irc client with haskell as a scripting language
15:26:53 <Cale> -> (5 + 5) + double 5
15:26:53 <Arnfreth> how would I interpret the scripts at runtime?
15:26:57 <Cale> -> 10 + double 5
15:27:01 <Cale> -> 10 + (5 + 5)
15:27:03 <Cale> -> 10 + 10
15:27:05 <Cale> -> 20
15:27:10 <dons> Arnfreth: you'd either call out to say, hugs, or call the ghc-api
15:27:17 <Cale> Notice that we did however, waste time evaluating double 5 twice.
15:27:20 <dons> or follow lamdabot's plugin loading ideas
15:27:25 <Trinithis> yep
15:27:32 <Arnfreth> dons, do you know if something like it has been done before?
15:27:33 <dons> maybe follow xmonad's dynamic code loading ideas
15:27:44 <dons> Arnfreth: yeah, several apps use haskell as an extension language.
15:27:53 <Cale> So lazy evaluation saves this effort by sharing the result of the evaluation of a parameter to a function which is duplicated in the body of the function.
15:27:56 <dons> there's no clear best solution, but rather a variety of techniques
15:28:02 <Cale> This looks like:
15:28:05 <Cale> double (double 5)
15:28:12 <Cale> -> let x = double 5 in x + x
15:28:17 <Cale> -> let x = 5 + 5 in x + x
15:28:22 <Cale> -> let x = 10 in x + x
15:28:25 <Arnfreth> is there anywhere I can read up on it, or can you name some of these programs, so I can take a look at the source?
15:28:26 <Cale> -> 20
15:28:44 <Trinithis> ah thanks
15:28:48 <dons> Arnfreth: i'd look at xmonad, yi and lambdabot. also , perhaps read about hs-plugins
15:28:51 <dons> and the ghc-api
15:28:57 <Cale> In a physical implementation, the 'let' here would be replaced by having x be a pointer to either an expression or a value.
15:29:08 <ddarius> Trinithis: In a nutshell, memoization is caching based on value, call-by-need is sharing based on name
15:29:24 <Arnfreth> alright, thanks a lot :)
15:29:42 <Trinithis> ok. good example
15:30:16 <Cale> lament: A strict list of integers is more different from a lazy list of integers than from a strict list of strings.
15:30:19 <dons> the sharing is greatly helped by purity
15:30:27 <Trinithis> Cale: is it arbitray (to the user at least) how haskell chooses to evaluate something like: double (double 5)?
15:30:30 <dons> it means a lot more work can avoid being duplicated
15:30:34 <Cale> lament: Lazy data structures can have code embedded in them.
15:30:54 <dons> Trinithis: yeah, you can't see how its evaluated without language extensions
15:31:06 <pjd> Trinithis: Haskell just requires that it be non-strict, which gives implementations some leeway
15:31:07 <Cale> Trinithis: It doesn't matter what the order is, so long as the termination behaviour is the same as what you get with outermost-first evaluation.
15:31:18 <Trinithis> ok
15:31:23 <Cale> Trinithis: You can usually expect implementations to do something fairly close to lazy evaluation though.
15:31:25 <yav> of course, some evaluation strategies might take a lot longer
15:31:39 <lament> Cale: when you just want to get some data from the structure, there's no difference
15:31:46 <Cale> lament: Yes there is!
15:31:52 <lament> how so?
15:32:02 <Cale> lament: Strict data structures can't be infinite, for one.
15:32:12 <TheLorax> can anybody see why I can't go "take 10 expand 1 2 3" where expand is "expand num denum base = (quot denum num) : (expand num ((rem denum num)*base) base)"?
15:32:21 <lament> Cale: "when you just want to get some data from the structure, there's no difference"
15:32:28 <dobblego> take 10 $ expand 1 2 3
15:32:36 <lament> Cale: fibs !! 10 is no different from myStrictList !! 10
15:32:37 <Cale> lament: They also can't be recursively defined.
15:32:43 <Trinithis> @src expand
15:32:43 <lambdabot> Source not found. Maybe if you used more than just two fingers...
15:32:46 <lament> Cale: "when you just want to get some data from the structure, there's no difference"
15:32:57 <TheLorax> dobblego, thanks
15:33:02 <pjd> lament: fibs !! 10 doesn't work when it's strict
15:33:11 <dobblego> TheLorax, np, remember function application comes first
15:33:13 <pjd> ("it"being fibs)
15:33:23 <lament> pjd: you couldn't define a strict fibs list; that's a different issue
15:33:27 <dons> i guess lament means if you know you'll get data
15:33:29 <Cale> TheLorax: the problem is that you were passing all those parameters to take
15:33:34 <pjd> lament: that's the difference
15:33:41 <pjd> you can define it
15:33:43 <TheLorax> oh, so brakets would do it too?
15:33:45 <pjd> it just doesn't terminate
15:33:47 <Cale> TheLorax: yes
15:33:48 <TheLorax> brackets around the expand
15:33:51 <dobblego> TheLorax, yes take 10 (expand 1 2 3)
15:33:52 <TheLorax> cool
15:33:54 <TheLorax> thanks!
15:34:05 <lament> again, i'm not saying strict data and lazy data are equivalent.
15:34:29 <pjd> lament: it's not just infinite structures which illustrate the difference
15:35:04 <orzo> ok
15:35:06 <Cale> If I have a strict list, I know that once I've evaluated one of the elements (or indeed, whether there are any elements), then the entire list is evaluated.
15:35:07 <orzo> i have a puzzle
15:35:33 <yitz> > let expand num denum base = (quot denum num) : (expand num ((rem denum num)*base) base) in expand 1 2 3
15:35:36 <pjd> lament: let loop = loop in [loop, 2, 3, 4] !! 1
15:35:37 <lambdabot>  [2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0...
15:35:41 <orzo> two lazy lists, [ p0, p1, ... ] and [ b0, b1, ... ]
15:35:50 <Cale> If I have a lazy list, then evaluating each additional element might cost more time.
15:35:51 <orzo> p0 is given
15:36:08 <Cale> (but in exchange, I can tell if it's empty more quickly)
15:36:12 <lament> Cale: look, we're having this long discussion, and throughout we're using terms like "strict list" and "lazy list". Clearly the two things are conceptually similar, otherwise why would we name them the same
15:36:23 <yitz> > let expand num denum base = (quot denum num) : (expand num ((rem denum num)*base) base) in expand 2 23 3
15:36:23 <lambdabot>  [11,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,...
15:36:26 <Cale> lament: Because indeed, they're both lists.
15:36:35 <Cale> lament: But they are different types.
15:36:46 <Cale> Just like I can have all different types of tree.
15:36:49 <lament> right, so are [Int] and [String]
15:36:51 <dons> ?src []
15:36:51 <lambdabot> data [] a = [] | a : [a]
15:36:52 <dons> versus
15:37:01 <Cale> I can have binary trees and rose trees, and AVL trees.
15:37:02 <dons> data [] a = [] | a : ![a]
15:37:03 <dons> and
15:37:06 <dons> data [] a = [] | !a : [a]
15:37:10 <dons> or
15:37:11 <orzo> g is a function that takes [ p0, p1, ... ] and returns [b0, b1, ...] and h is a function taht takes [b0, b1, ...] and returns [p1, p2, ...]
15:37:12 <dons> data [] a = [] | !a : ![a]
15:37:14 <pjd> lament:  [Int] and [String] have different elements, same list
15:37:26 <orzo> g and h are IO actions however
15:37:31 <Cale> These are entirely different datastructures.
15:37:35 <orzo> how do I write this up?
15:37:40 <pjd> a lazy-list [Int] and a strict-list [Int] have the same elements, but different lists
15:37:51 <dons> they have different inhabitants
15:37:52 <pjd> loosely speaking
15:37:59 <Cale> Sure, they're both lists, but that doesn't say much apart from the fact that they're used to linearly arrange some data.
15:38:10 <lament> conceptual similarities should be reflected in the programming language :)
15:38:21 <Cale> and they are!
15:38:36 <tromp> orzo: ps = p0 : h ( g ps)
15:38:39 <yitz> orzo what are the types of the bs and ps?
15:38:40 <Cale> You'll have Functor and Foldable and Traversable instances for each!
15:38:44 <lament> if we define lists, and then define strict lists, the definitions are very similar, like dons just showed.
15:38:47 <byorgey> orzo: I think you need to use unsafeInterleaveIO.
15:38:49 <lament> it's pretty much copy-pasting code.
15:38:51 <Cale> lament: similar is not the same
15:38:54 <lament> copy-pasting code is bad.
15:38:59 <Cale> It's not copy-pasting.
15:39:02 <dons> or use annotations.
15:39:38 <dons> which is why we rarely see copy pasting, instead annotations are used to encourage the compiler to generate both types
15:39:57 <Philippa> Cale: polymorphism exists to express similarity, not sameness
15:40:11 <Cale> Philippa: sure
15:40:20 <ndm> @tell dons Thanks for the XMonad quotes, I shoved them in my paper
15:40:20 <lambdabot> Consider it noted.
15:40:22 <orzo> ps :: [P] and bs :: [B] and g :: [P] -> IO [B] and h :: [B] -> IO [P]
15:40:28 <dons> ndm, sure.
15:40:28 <Philippa> and a lot of functions on lazy and strict lists are the same, modulo strictness and constructor names
15:40:28 <lambdabot> dons: You have 6 new messages. '/msg lambdabot @messages' to read them.
15:40:39 <dons> ndm, you own me an online hoogle for xmonad now
15:40:49 <dons> ndm, which i can integrate with the custom search on xmonad.org
15:41:03 <ndm> dons: if i get a GSoC position, i guarantee it, otherwise you'll get it sometime after my thesis
15:41:07 <dons> all those user extensions need a way to search the api
15:41:21 <dons> ndm, btw, wasn't there a js-based haddock search you wrote?
15:41:36 <ndm> dons: yeah, its in haddock, but the base libraries got too large and now its way too slow...
15:41:48 <dons> hmm is it on by default for haddock?
15:42:04 <Cale> Philippa: but they're semantically different
15:42:17 <dons> ndm, i couldn't see how to get to it, http://xmonad.org/xmonad-docs/xmonad-contrib/
15:42:38 <Igloo> It's the default if your haddock is new enough
15:42:39 <ndm> dons: upgrade your haddock
15:43:21 <dons> ah, i see, http://hackage.haskell.org/packages/archive/xmonad-contrib/0.7/doc/html/doc-index.html
15:43:23 <yitz> orzo: do { bs <- g ps; ps' <- h bs; return (ps' == tail ps) }
15:43:23 <lambdabot> http://tinyurl.com/38mgom
15:43:37 <yitz> orzo: but that won't work if the lists are infinite though
15:44:20 <monochrom> This channel has now 450 users.
15:44:27 <orzo> the lists are ininite and made lazy via unsafeInterleaveIO
15:44:27 <nornagon> @users
15:44:27 <lambdabot> Maximum users seen in #haskell: 467, currently: 450 (96.4%), active: 23 (5.1%)
15:44:29 <dobblego> I wrote a strict list and a lazy list in an imperative language and very few of the functions were the same
15:44:36 <dons> 479 is the high score, fwiw
15:44:38 <yitz> orzo: what I wrote is of type [P] -> IO Bool. When you run it in the IO monad, it tells you whether the property holds. Is that what you want?
15:44:54 <lament> dobblego: that just means the language was not expressive enough.
15:45:10 <Trinithis> assembly is expressive enough!!!
15:45:12 <lament> programming languages should reflect conceptual similarity
15:45:12 <dobblego> lament, I disagree; the functions were *fundamentally different*
15:45:35 <yitz> orzo: then there is no way to check that property by computing the functions, it would take forever.
15:45:41 <dons> its all just strategies people
15:45:44 <roconnor> wholey crap!  a new release of wxHaskell?
15:45:48 <dons> algorithm + strategry
15:45:51 <roconnor> this must be an april fools joke.
15:45:53 <orzo> yits, i mainly want to invoke h on every element of bs for the side effects.
15:45:55 <yitz> so what do you mean by "write it up"?
15:46:05 <orzo> er
15:46:17 <tromp> orzo: that's ill defined
15:47:00 <lament> dobblego: return for lists is fundamentally different from return for IO. They're nothing at all alike. Yet we have one function to express both concepts, because somebody discovered a similarity and Haskell is awesome enough to express it.
15:47:19 <orzo> well, i want to evaluate every element of ps and bs
15:47:27 <orzo> and side effects will occur
15:47:34 <dobblego> lament, that's because they *are* alike at a higher level of specialisation; where the strict list and lazy list are not
15:47:47 <tromp> consider g ps = return ps
15:48:11 <tromp> and h bs = tail . map (+1) $ bs
15:48:21 <dobblego> the only likeness is that we - humans - happen to call both structures 'list'
15:48:33 <tromp> i mean h bs = return . tail . map (+1) $ bs
15:48:55 <Philippa> <Cale> Philippa: but they're semantically different <- instances of a polymorphic value generally are. This doesn't mean you can't be polymorphic in strictness
15:49:20 <wli> My neuropathy is vastly worse than usual today. Even my lower lip is hurting and it's usually just numb.
15:49:34 <Philippa> what /is/ problematic is that Haskell couldn't do anything productive with the info if it had it
15:49:58 <yitz> orzo: are you trying to write a program that does something? or to prove something about an existing program? or something else?
15:50:12 <lament> dobblego: so we just call them the same for no reason at all?
15:50:17 <Philippa> but you can express precisely that variety of polymorphism via an appropriate substructural type system
15:50:19 <lament> dobblego: we're stupid that way?
15:50:31 <dobblego> lament, yes
15:50:36 <lament> dobblego: interesting.
15:51:04 <orzo> i'm trying to write a program that does something but i have a bunch of code and i was hoping not to redesign, so i have g and h written
15:52:00 <yitz> ok. you also told us something about what g and h do, and their types. now what do you need it to do?
15:52:26 <Philippa> (to put it another way: yes, they're different - precisely the difference hidden by "modulo strictness")
15:52:28 <orzo> the functions g and h are written
15:52:39 <orzo> their inputs are not constructedd
15:52:46 <orzo> except p0
15:53:17 <yitz> ok. what do you know about the inputs?
15:53:23 <orzo> h is intended to do some action for every element in its input list, but it also generates the rest of the ps
15:54:27 <orzo> the flow would be like this p0 -> g -> b0 -> h -> p1 -> g -> b1 -> ...
15:54:53 <orzo> but g and h are written to take infinite lists
15:55:17 <yitz> one second, you said that the input to g needs to be of type [P], a list of ps, not just one.
15:55:39 <yitz> ah wait, is this something like the server simulation example in "gentle introduction"?
15:55:59 <orzo> yes, thats right yits.  my flow there is showing logically what is happening when laziness is taken into account
15:56:09 <orzo> don't remember the server simulation example
15:56:13 <yitz> so then you need an irrefutable pattern to get it started
15:57:41 <Trinithis> well thx everybody for le help. cya
16:01:02 <orzo> is it doable, what i want to do?
16:01:23 <yitz> so sthing like let g ~(x:xs) = ...; h (y:ys) = ...;  ps = p0 : h (g ps) in ps
16:02:11 <orzo> i cant use ps = p0 : h ( g ps) though
16:02:19 <orzo> because of the pasky IO
16:02:25 <orzo> pesky
16:02:45 <yitz> ok
16:06:01 <orzo> maybe its just impossible
16:07:30 <orzo> why should it be impossible though
16:07:41 <orzo> i can probably do it if i use unsafePerformIO or something
16:08:23 <yitz> how about let g ~(x:xs) = ...; h (y:ys) = ...;  ps = p0 : ps'; ps' = g (p0:ps') >>= h in ps
16:08:41 <yitz> wait
16:08:52 <byorgey> but you need ps :: IO [a]
16:08:57 <byorgey> ps can't be :: [a]
16:09:03 <orzo> nevermind defining g and h
16:09:07 <orzo> they are globals
16:10:24 <BMeph> Eww!
16:10:25 <byorgey> orzo: I don't think what you want to do is possible.
16:10:53 <byorgey> orzo: well, actually, I take that back, I don't know enough about how unsafePerformIO works to know for sure
16:11:10 <brad_larsen> this problem may be related to irrefutable patterns...
16:11:15 <byorgey> er, I meant unsafeInterleaveIO
16:11:22 <brad_larsen> i'm using parsec to parse some interactive user input
16:11:29 <byorgey> :t unsafeInterleaveIO
16:11:30 <lambdabot> Not in scope: `unsafeInterleaveIO'
16:11:32 <brad_larsen> i want it to processed as it is entered
16:11:39 <orzo> i did some attempts with unsafeInterleaveIO
16:11:43 <orzo> i got it to compmile
16:11:50 <orzo> but it ended up making an endless loop
16:12:00 <brad_larsen> but the way it is now, nothing is output until the entire stream is parsed.
16:12:02 <MyCatVerbs> unsafeInterleaveIO = return . unsafePerformIO, right?
16:12:27 <brad_larsen> i.e. i see no output until end-of-stream
16:12:38 <byorgey> brad_larsen: have you turned off output buffering?
16:12:49 <byorgey> hSetBuffering stdout NoBuffering
16:12:52 <brad_larsen> byorgey: not on purpose
16:12:56 <brad_larsen> byorgey: I will try that
16:13:08 <orzo> h outputs as it goes, and i can use last and seq or something to get it to evaluate everything
16:14:21 <ehird_> MyCatVerbs: Only in type.
16:14:29 <ehird_> Its behaviour is in fact primitive
16:14:34 <ehird_> I think.
16:15:19 <ddarius> byorgey: Parsec is not incremental.
16:15:41 <ddarius> byorgey: You may be able to hack something up with parsec3, but it would require a bit of creativity.
16:16:03 <byorgey> ddarius: oh, really?
16:16:05 <brad_larsen> byorgey: yes, that didn't work.
16:16:12 <ddarius> Er, I meant that more for brad_larsen
16:16:31 <brad_larsen> I thought I had read that about parsec somewhere.  ::sigh::
16:16:34 <byorgey> ddarius: so you're telling me that if I have a function  :: String -> [Foo], implemented with Parsec, that I can't even get the first Foo until the whole String has been processed?
16:16:56 <ddarius> byorgey: Correct.  Just look at the return type of runParser.
16:17:00 <MyCatVerbs> ehird_: oh, fair enough.
16:17:21 <brad_larsen> ddarius: how do you determine that it is non-incremental from its type?
16:17:27 <brad_larsen> :i runParser
16:17:37 <brad_larsen> :t runParser
16:17:37 <lambdabot> Not in scope: `runParser'
16:17:38 <ddarius> :t Text.ParserCombinators.Parsec.runParser
16:17:39 <lambdabot> forall tok st a. Text.ParserCombinators.Parsec.Prim.GenParser tok st a -> st -> Text.ParserCombinators.Parsec.Pos.SourceName -> [tok] -> Either Text.ParserCombinators.Parsec.Error.ParseError
16:17:39 <lambdabot> a
16:17:45 <byorgey> ddarius: ah, yes, I see
16:17:53 <byorgey> brad_larsen: sorry, it seems I have misled you =(
16:18:18 <ddarius> brad_larsen: The type is either Left <some parse error> or Right <the result>, therefore it can't return any results until it is certain that there are no parse errors.
16:18:39 <ddarius> On the old wiki there was a good discussion of this sort of thing.
16:18:49 <ddarius> (Not in the context of parsec)
16:18:58 <brad_larsen> that makes sense.
16:19:30 <kaustuv> Has anyone worked on adding SML-style modules to Haskell (essentially the reverse of Dreyer, Harper and Chakravarty's POPL'07 paper "Modular Type Classes")?
16:20:08 <Lycurgus> sounds more ocamlly
16:21:03 <dolio> Do you mean mimicing modules using type classes, or genuinely adding them and working out all the consequences?
16:21:16 <ddarius> kaustuv: No, or not exactly.  Haskell can already (hackishly) express much of ML-style modules/functors.  Most work that might lead somewhere though was interested in something beyond ML-style modules.
16:21:17 <kaustuv> Genuinely adding them etc.
16:21:43 <kaustuv> I am aware of Wehr and Chakravarty's work on hacking modules with type classes with existential types. Not interested in that.
16:22:03 <dons> kaustuv: in the late 90s there were some papers
16:22:30 <dolio> There's also Oleg's applicative translucent functors, but it's been a while since I looked at those.
16:22:38 <yitz> orzo: right, this whole laziness thing to begin with is the fault of unsafeInterleaveIO. So it makes sense that you would need it to do this. If g and h were implemented without the fake laziness  - call per element, or return a ListT Done Right which is truly lazy - then you could talk sensibly about reading one item from one function and feeding it to the next.
16:22:39 <dolio> But they fall into the first category.
16:23:32 <EvilTerran> couldn't you call everything Oleg does "hacking ... with type classes with existential types"?
16:23:52 <EvilTerran> s/with/and|or/
16:24:01 <orzo> yitz, i don't follow you
16:24:02 <EvilTerran> :P
16:24:26 * EvilTerran follows SPJ. all hail! :D
16:25:42 <orzo> i tried adding unsafePerformIO
16:25:49 <yitz> A "lazy list in IO" is not safe, so g and h can't exist to begin with as you describe them without unsafeInterleaveIO
16:25:51 <matthew-_> well that's a bad idea
16:26:01 <orzo> it caused an endless loop like before
16:26:23 <orzo> g and h use unsafeInterleaveIO
16:26:48 <brad_larsen> is there some way to get the unconsumed input out of a parsec parser?
16:27:04 <dolio> Does Oleg use existential types that much? :)
16:27:18 <yitz> orzo: unsafeInterleaveIO is evil. are you required to do it this way?
16:27:49 <orzo> i dont want to do a major rewrite
16:28:00 <orzo> but why is it so evil?
16:28:47 <Lycurgus> because it violates the IO monad model?
16:28:48 <yitz> the order of your side effects become dependent on the whims of what order the runtime decides to do things in.
16:28:58 <sckot> dons I have a Haskell / OpenBSD question
16:30:43 <yitz> so like any unsafe function, when you use it you need to be able to prove on your own that things will happen in the right order - using data dependencies, or because you have inside knowledge about the compiler.
16:31:05 <orzo> data dependencies is fine
16:31:39 <orzo> its ok in this circumstance to use unsafePerformIO
16:31:54 <orzo> except that i cant seem to do up this portion of the program
16:32:36 <orzo> the design was written in scheme using lazy lists there
16:32:43 <orzo> and is now in haskell
16:33:02 <orzo> unsafeInterleaveIO seemed an apt substitute for scheme's delay/force
16:33:04 <yitz> so for example the usual usage of readFile/writeFile works only if you do almost nothing in the IO monad; otherwise, it becomes difficult to know what will happen first. You are intertwining different interleave calcualtions here.
16:34:43 <orzo> well you all can condemn unsafeInterleaveIO but its draw backs have little to do with my current trouble
16:35:18 <yitz> (they didn't all, just I did. some people like it. :)
16:35:33 <ddarius> brad_larsen: Yes.
16:37:07 <brad_larsen> ddarius: how's that?
16:37:24 <ddarius> :t Text.ParserCombinators.Parsec.getInput
16:37:24 <lambdabot> forall tok st. Text.ParserCombinators.Parsec.Prim.GenParser tok st [tok]
16:37:37 <orzo> how come when i forced it with unsafePerformIO, i got the same infinite loop i had with the unsafeInterleaveIO stratagy
16:37:52 <brad_larsen> aha
16:37:56 <brad_larsen> that might help me out
16:39:50 <glen_quagmire> how can I implement let block properly?
16:39:56 <brad_larsen> i'm thinking i can break up the parsing into a sequence of parses
16:40:01 <glen_quagmire> that doesn't make up to a question
16:40:27 <glen_quagmire> with parsec, you have have separate lexer
16:42:56 <brad_larsen> is this easily done with parsec?  That is, I want to read stdin, run it through one parser, take the output of that parse, process it, then run a different parser on the unconsumed input.
16:43:22 <ddarius> That should be fine.
16:43:25 <orzo> does the compiler treat IO lazy lists a lot different than real lazy lists?
16:44:13 <ehird_> orzo: no
16:44:28 <yitz> orzo: I don't think it's so different; just that thunks can do things that have side effects
16:45:09 <yitz> orzo: can you paste your code?
16:45:39 <brad_larsen> ddarius:  would you give me a hint as to how I might do that?  I'm having trouble imagining how to use getInput to do so. =\
16:47:11 <glen_quagmire> how can I implement/simulate closures in haskell?  implementing let block consumes my soul
16:47:42 <ddarius> case runParser ... (liftM2 (,) firstParser getInput) initialInput of Right (result,rest) -> ... runParser secondParser rest
16:48:16 <brad_larsen> ddarius: thank you, i'll work with that.
16:48:18 <glen_quagmire> Closure (Map String Expr) Expr
16:48:37 <ehird_> glen_quagmire: You just answered your own question
16:48:56 <ehird_> But let has got nothing to do with closures.
16:48:58 <ehird_> Only scoping.
16:49:10 <brad_larsen> glen_quagmire: I'm not sure what you are asking
16:49:11 <glen_quagmire> ehird_: but then the Map is wierd
16:49:22 <glen_quagmire> ehird_: oh how can I scope properly?
16:49:57 <glen_quagmire> I'm going for something like this: (lambda (l) (let { x = (head l), xs = (map (lambda (a) (+ a x)) (tail l)) } (cons x xs))
16:50:15 <glen_quagmire> where lambda is keyword and map is library function (implemented in the language itself)
16:50:29 <ehird_> glen_quagmire: Just read up on scoping..
16:50:44 <ehird_> Basically: capture closure at lambda-time, replace closure with lambda's at call-time
16:51:36 <EvilTerran> glen_quagmire, have you read TaPL? it's very good for this sort of thing
16:51:53 <EvilTerran> albeit abstract, rather than haskell-oriented
16:52:32 <glen_quagmire> x = (head l).. it seems like (head l) is not evaluated until l changes to something other than original list
16:53:11 <EvilTerran> ... are we talking about your language, rather than haskell?
16:54:10 <glen_quagmire> i mean in haskell. I traverse the Map with eval function. but after the traversal, the haskell map doesn't seem to have x = 1  (given l was [1,2,3])
16:54:29 <EvilTerran> l can't change in haskell...
16:55:02 <orzo> !paste
16:55:05 <orzo> where is the paste bin?
16:55:09 <glen_quagmire> i think i should slow down and tackle it again
16:55:13 <EvilTerran> ?paste
16:55:13 <lambdabot> Haskell pastebin: http://hpaste.org/new
16:55:14 <glen_quagmire> orzo: hpaste.org
16:55:56 <yitz> > let {f l = let {x = head l; xs = map (\a -> a + x) (tail l)} in x : xs} in f [1..5]
16:55:56 <lambdabot>  [1,3,4,5,6]
16:55:59 <EvilTerran> scoping names in a language you're interpreting sounds like a good place to use the Reader monad
16:56:08 <yitz> > let {f l = let {x = head l; xs = map (\a -> a + x) (tail l)} in x : xs} in f [3..7]
16:56:09 <lambdabot>  [3,7,8,9,10]
16:56:37 <EvilTerran> or ReaderT Parser, or whatever. altho i'd be inclined to separate parsing and evaluation with a concrete parse tree.
16:56:46 <orzo> http://hpaste.org/6793
16:57:37 <glen_quagmire> I use State monad that has [Map String Expr]  for evaluation. I basically pushEnv and popEnv myself inside the monad
16:57:52 <EvilTerran> i see
16:58:00 <glen_quagmire> there can be a bug too haha
16:58:25 <hircus> lambdabot: @help
16:58:25 <lambdabot> help <command>. Ask for help for <command>. Try 'list' for all commands
16:58:33 <hircus> lambdabot: @list
16:58:33 <lambdabot> http://www.cse.unsw.edu.au/~dons/lambdabot/COMMANDS
16:58:50 <dmwit> hircus: You can also drop the "lambdabot: " bit.
16:58:51 <dmwit> ?list
16:58:51 <lambdabot> http://www.cse.unsw.edu.au/~dons/lambdabot/COMMANDS
16:58:52 <dmwit> @list
16:58:53 <lambdabot> http://www.cse.unsw.edu.au/~dons/lambdabot/COMMANDS
16:59:08 <hircus> dmwit: ah, thanks
16:59:23 <hircus> let x=5 in x
16:59:43 <hircus> @help eval
16:59:44 <lambdabot> eval. Do nothing (perversely)
16:59:46 <dmwit> That's the "eval" plugin; "> " is also short for "?eval ".
16:59:48 <hircus> @help let
16:59:48 <lambdabot> let <x> = <e>. Add a binding
16:59:50 <dmwit> > let x = 5 in x
16:59:51 <lambdabot>  5
16:59:58 <hircus> cool
17:00:08 <dmwit> > product [2..10]
17:00:09 <lambdabot>  3628800
17:00:35 <hircus> > let fac 0 = 1; fac n = n * fac (n-1) in fac 10
17:00:36 <lambdabot>  3628800
17:00:50 <dmwit> Sorry, that's not "eval", that's "run" or something.  But anyway, everybody just uses "> ". =)
17:01:04 <wagle> > [2..1]
17:01:04 <lambdabot>  []
17:01:11 <hircus> dmwit: yup. eval is just the module that provides run, if I understand it
17:01:14 <dmwit> > [5,4..1]
17:01:14 <lambdabot>  [5,4,3,2,1]
17:01:21 <hircus> > [1..]
17:01:22 <lambdabot>  [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,...
17:01:32 <dmwit> hircus: I think eval used to be a brainfuck interpreter, but it was buggy and got removed.
17:01:37 <dmwit> ?eval do something!
17:01:37 <wagle> > let fac n = product [2..n] in fac 10
17:01:38 <lambdabot>  3628800
17:01:38 <hircus> no kidding!
17:02:01 <hircus> now if they have a SKI or X combinator interpreter, that'd make my day
17:02:06 <ddarius> dmwit: No, @bf was.
17:02:09 <gwern> hm. so Hope wasn't installing because of its libraries were linking against different versions...
17:02:14 <dmwit> hircus: See also unlambda. ;-)
17:02:28 <hircus> ?unlambda
17:02:28 <lambdabot> Done.
17:02:28 <ddarius> dmwit: @eval was an insanely implemented mostly pure lambda calculus implementation.
17:02:30 <dmwit> ddarius: Ah, my bad.
17:02:34 <hircus> aha
17:02:38 <hircus> ?help unlambda
17:02:38 <lambdabot> unlambda <expr>. Evaluate an unlambda expression
17:02:38 <gwern> speaking of which, does anyone know why there's no ghc  6.8.3? I'd like to have ghc linked against the latest bytestring
17:03:09 <hircus> oh, it has s and k
17:03:18 <gwern> @where hope
17:03:18 <lambdabot> http://hope.bringert.net/
17:03:46 <gwern> darn. I was going to make a 'lambdabot is hopeless' crack
17:03:57 <gwern> http://hope.bringert.net/about 'Hope is a system for web applications written in Haskell. This is a live demo of Hope with the default set of modules.'
17:04:37 <gwern> near as I can figure. Hope is supposed to be a more humane way to write blogs in haskell than HAppS :)
17:05:31 <kaustuv> dolio: (belated) thanks, Oleg's description of applicative functors in Haskell seems to satisfy my proximate needs.
17:05:35 <ddarius> There have been various "web application" frameworks throughout the years.
17:06:58 <Twey> @pl \c -> a $ b ++ c !! d
17:06:58 <lambdabot> a . (b ++) . (!! d)
17:07:06 <ddarius> @where halipeto
17:07:06 <lambdabot> I know nothing about halipeto.
17:07:08 <yitz> http://en.wikipedia.org/wiki/Hope_programming_language
17:07:10 <mm_freak> @pl \x -> x /= '"' && ord x >= 32
17:07:10 <lambdabot> liftM2 (&&) ('"' /=) ((>= 32) . ord)
17:07:14 <dolio> kaustuv: Heh, wasn't expecting that. Glad to help, though.
17:07:16 <ddarius> @google halipeto haskell
17:07:17 <lambdabot> http://www.haskell.org/communities/05-2004/html/report.html
17:07:17 <lambdabot> Title: Haskell Communities and Activities Report
17:07:45 <gwern> 'Halipeto generates web pages from templates (much like JSP, Zope TAL etc). It is written in Haskell (with a ghc extension) and is available from http://www.acooke.org/jara/halipeto'
17:07:49 <gwern> 'Project Status: completed &released, not currently being developed further'
17:07:52 <yitz> 'Hope is a small functional programming language developed in the early 1980s prior to Miranda and Haskell. '
17:08:24 <ddarius> @google Charity programming language
17:08:25 <lambdabot> http://en.wikipedia.org/wiki/Charity_(programming_language)
17:09:02 <glen_quagmire> http://trac2.assembla.com/cs731/wiki  my programming language
17:09:03 <lambdabot> Title: cs731 - Trac
17:10:27 <lament> cs731, cool name
17:12:31 <wli> Looks ML-ish.
17:13:59 <ehird_>  it always terminates
17:19:25 <roconnor> Charity looks kinda 10 years old
17:21:08 <yitz> orzo, sorry, i can't figure out much more about what you're trying to do, i think my scheme just isn't good enough to reconstruct what you had in mind.
17:21:22 <MyCatVerbs> roconnor: lots of good things are 10 years old. :P
17:21:48 <roconnor> MyCatVerbs: name 3
17:22:29 <ddarius> roconnor: Haskell 98 is 10 years old...
17:22:40 <roconnor> 1
17:22:55 <orzo> thanks for trying, yitz
17:23:06 <TomMD> Do you count people?
17:23:11 <MyCatVerbs> roconnor: awk, the Dragon book, H98. :)
17:23:33 <roconnor> I'm not sure awk counts
17:24:00 <MyCatVerbs> roconnor: NetHack is 20-ish, TeX is over 30...
17:24:17 <TomMD> SPJ is 50.
17:24:19 <MyCatVerbs> roconnor: not very many good things are *exactly* ten years old.
17:24:43 <roconnor> I meant that awk doesn't count as a good thing
17:25:07 <MyCatVerbs> roconnor: well, actually, if you ask their mothers, you'll find that there are plenty of things who are ten years old to within 24 hours' margin, and it'll be claimed that they're all angels.
17:25:42 <MyCatVerbs> roconnor: heheheh, eh. It beats not having awk. It also inspired a lot of useful things elsewhere.
17:25:52 <lament> like what, perl?
17:26:11 <glen_quagmire> haskell is directly inspired from awk
17:26:32 <glen_quagmire> @google haskell vs. awk
17:26:33 <lambdabot> http://www.haskell.org/papers/NSWC/jfp.ps
17:26:42 <MyCatVerbs> lament: precisely!
17:27:03 <roconnor> perl definitely doesn't count as good thing. :P
17:27:15 <MyCatVerbs> lament: or, more precisely, the idea that you really can do interesting and useful things to strings by abusing regular expressions an awful lot.
17:27:18 <TomMD> No, but MyCatVerbs only implied it was 'useful'
17:28:17 <MyCatVerbs> roconnor: I'd argue in its favour, in the grand scheme of things. I can't honestly think of many outright *bad* things happening as a result of Perl (other than Perl itself), but it did have many positive consequences.
17:29:26 <lament> MyCatVerbs: Perl legacy software?
17:29:42 <dolio> slashdot is written in perl.
17:30:00 <lament> slashdot is notorious for being horribly sucky.
17:30:05 <MyCatVerbs> roconnor: Heck, just the fact that it's a widely-used interpreted language at least slightly reduced the world population of people trying to shave microseconds of CPU time from operations that are bounded by disk actions taking milliseconds.
17:30:09 <cjb> therefore perl sucks
17:30:11 <lament> (in design and in technical implementation)
17:30:16 <cjb> (yay for logic.)
17:30:23 <roconnor> MyCatVerbs: I still don't think Perl or awk counts a good.
17:30:25 <glen_quagmire> the world is written in perl
17:30:33 <MyCatVerbs> lament: sure, except for Perl hackers' willingness to throw out and rewrite doggedly bletcherous crap.
17:30:36 <roconnor> as good
17:31:38 <TomMD> @tell andyjgill good work on landing the position at KU!  I just found out from your site.
17:31:38 <lambdabot> Consider it noted.
17:31:48 <MyCatVerbs> roconnor: if Perl and Awk hadn't come by, do you think we'd now have regex APIs even *half* as good as we do now?
17:32:38 <roconnor> no, but we might have monadic parsers that are twice as good as what we have now.
17:33:19 <MyCatVerbs> We *have* monadic parsers that are twice as good as what we have now.
17:33:22 <MyCatVerbs> So to speak. >_>
17:33:49 <MyCatVerbs> They're still prone to slowness and to being cranky.
17:34:17 <dolio> Things are more like they are now than they ever were before.
17:35:12 <roconnor> MyCatVerbs: I'm still waiting for the slides for the online constant space monadic parsing method presented at FPdag 2008
17:35:52 <ddarius> dolio: We live at such a remarkable moment!
17:37:18 <dolio> Provably terminating parser combinators in Agda should be pretty cool, too.
17:37:23 <dolio> Whenever that paper gets released.
17:40:11 <MyCatVerbs> roconnor: I'd like to see a parser combinator that warned me when I did stupid things like accidentally gave it a non-LL(1) grammar.
17:41:12 <roconnor> MyCatVerbs: why?
17:42:14 <MyCatVerbs> roconnor: because I'm prone to fucking up, of course.
17:42:51 <roconnor> I didn't realize that LL(1) was so important.
17:42:52 <ehird_> describe arrows to me in words of 4 letters or less
17:43:07 <MyCatVerbs> roconnor: for Parsec it is.
17:43:15 <ddarius> ehird_: CT
17:43:20 <mrd> well in parsec it's simple.  don't use try.
17:43:30 <roconnor> LL(2) ?
17:43:49 <MyCatVerbs> roconnor: or LR(k) or whatever else particular subset of the set of context-free grammars one might write a parser combinator library targetting.
17:44:09 <roconnor> oh, okay
17:44:10 <ddarius> GLR
17:44:37 <roconnor> MyCatVerbs: if only awk had never been invented, then your dreams may have been realized by now. :P
17:44:48 <ehird_> ddarius: Words that make sense
17:44:48 <MyCatVerbs> roconnor: who can say?
17:44:56 <MyCatVerbs> roconnor: wait, we can. We just can't guarantee correctness. ;)
17:44:59 <wli> LR(k) and/or GLR are not entirely easy to do combinatorially.
17:45:26 <ddarius> ehird_: Oh, I misread you comment.  I thought you were asking for a description in 4 letters or less.
17:45:54 <mrd> i'm not sure if what you ask for is possible, MyCatVerbs
17:45:54 <MyCatVerbs> wli: this is true. Y'know what would be awesome, though? LALR parser generator in Template Haskell.
17:46:03 * MyCatVerbs is tempted to have a go at doing that. :)
17:46:16 <mrd> for example, determining whether a context free grammar is ambiguous is undecidable
17:46:23 <ddarius> MyCatVerbs: No one (except perhaps you) would use it, but have fun.
17:46:40 <MyCatVerbs> mrd: it is? This has been proved?
17:46:41 <ehird_> ddarius: No. Just that the words in the description must be 4 letters or less.
17:46:56 <ddarius> ehird_: Yeah, I reread it and caught the part that I missed before.
17:47:03 <mrd> MyCatVerbs: yes
17:47:17 <wli> MyCatVerbs: I'd like GLR parser combinators.
17:47:22 <mrd> MyCatVerbs: you can reduce the Post Correspondence Problem to it
17:47:33 * dmwit grep '^.\{1,4\}$' /usr/share/dict/words
17:47:33 <ddarius> @google "GLR parser combinator"
17:47:34 <lambdabot> No Result Found.
17:47:37 <ddarius> @google "GLR parser combinators"
17:47:38 <lambdabot> No Result Found.
17:47:57 <wli> MyCatVerbs: It likely means an interface slightly different from most parser combinators, but I'm willing to live without it.
17:47:59 <gwern> @hoogle Element
17:48:00 <lambdabot> Control.Exception.UndefinedElement :: String -> ArrayException
17:48:17 <wli> MyCatVerbs: It likely means an interface slightly different from most parser combinators, but I'm willing to live with differences and/or slightly unusual API's.
17:48:43 <dmwit> Arrows trap the idea that what came out can vary by what came in.
17:49:36 <MyCatVerbs> ddarius: well, it'd have the upside that it would only require a normal Haskell library installation, and not the addition of an extra preprocessing stage.
17:50:15 <ddarius> MyCatVerbs: Having upsides doesn't mean anyone will use it.
17:50:20 <dmwit> ehird_: What came out can also vary by what you did a long time ago.  This is also a part of Arrows.
17:50:28 <MyCatVerbs> ddarius: it'd be fun. :P
17:50:43 <ddarius> MyCatVerbs: Probably.  I told you to have fun doing it.
17:50:44 <MyCatVerbs> mrd: is the question of whether a particular grammar can be recognised by a LALR parser decidable?
17:50:47 <MyCatVerbs> ddarius: :)
17:51:00 <dmwit> ehird_: Okay, I had to resort to using the word Arrows, which technically is longer than four letters. =P
17:51:32 <ddarius> dmwit: Now do Quantum Field Theory.
17:51:39 <MyCatVerbs> wli: GLR sounds similar to ReadP to me.
17:51:39 <dmwit> heh
17:52:27 <lispy> dmwit: nice
17:52:37 <wli> MyCatVerbs: It is not.
17:53:11 <MyCatVerbs> wli: how so?
17:54:12 <ehird_> dmwit: That made no sense. :D
17:54:16 <MyCatVerbs> wli: both do breadth-first parallel search when presented with nondeterminism. Aside from the fact that ReadP seems to be made for almost-LL(1) grammars instead of almost-LR(1) grammars, is there anything else different?
17:54:52 <dmwit> ehird_: What does "sense" mean?  I don't know such a long word.
17:54:56 <dmwit> ;-)
17:54:58 <wli> MyCatVerbs: LL vs. LR is an enormous difference.
17:55:27 <wli> MyCatVerbs: You need to appreciate that to understand the issues with API/etc. vs. LR -based combinators.
17:55:56 <MyCatVerbs> wli: LL being a pain in the arse, you mean?
17:56:11 * MyCatVerbs cries. Whar is mai leff-recursion? :'(
17:56:14 <wli> LL is a pain in the ass to use; it's much simpler to implement.
17:56:49 <wli> More specifically, LL is a pain in the ass to use compared to LR; however, it's much simpler to implement compared to LR.
17:57:20 <MyCatVerbs> wli: I don't think there's any need for "compared to LR" in either of those clauses.
17:57:27 <roconnor> I understand that parsec isn't the end all of parser combinators, but just the beginning.
17:57:37 <Pseudonym> LL(k) for k > 1 isn't necessarily any easier to implement than LR.
17:57:37 <MyCatVerbs> wli: and LR is the reverse in both situations, aye?
17:58:07 <Pseudonym> However, there's one thing that Parsec explicitly recognises.
17:58:32 <Pseudonym> And that's that the future of parsing technology is to allow ambiguity, and provide mechanisms (some automatic, some manual) for dealing with it.
17:58:39 <MyCatVerbs> Implementing LR just sounds like a lot of graph manipulation. Tedious if you want to do it by hand, but hardly impossible to implement.
17:59:03 <ddarius> roconnor: Parsec is neither the beginning nor the end.
17:59:22 <wli> LL(k) in full generality (i.e. k is input) is still simpler than the corresponding LR(k).
17:59:38 <wli> Granted, not by as much as, say, LL(1) vs. LL(1).
17:59:46 <Pseudonym> wli: Maybe.
17:59:46 <wli> LL(1) vs. LR(1).
17:59:58 <Pseudonym> The LR(k) construction algorithm is really not much harder to implement than LR(1).
18:00:22 <Pseudonym> Having said that, there are ways to construct LR(k) parsers (not just LALR) which don't require full LR(k).
18:00:27 <MyCatVerbs> Oh hey. GNU Bison supports GLR. Nice.
18:00:33 <Pseudonym> Admittedly, their automata are slightly different.
18:00:50 <roconnor> I can't wait until we grow out of the string-age of computing and stop caring so much about parsing.
18:01:21 <MyCatVerbs> roconnor: pray tell how else you propose to get data into the machine in order to manipulate it?
18:02:15 <wli> Parsing's a fundamental problem; it's not going to go away.
18:02:18 <ddarius> roconnor: It'll be a long, long, long time before we care much less about parsing and it doesn't really have much to do with strings.
18:02:19 <roconnor> MyCatVerbs: s-expressions, xml, etc.
18:02:38 <roconnor> ddarius: parsing has everything to do with strings.
18:03:01 <mauke> if xml is the answer, I don't want to see the question
18:03:26 <roconnor> mauke: the question is how to serialize structured data.
18:03:28 <ddarius> roconnor: It depends on how you mean "strings"
18:03:28 <wli> Pseudonym: Well, doing the core merging affair as per LALR(k) vs. LR(k) in such a fashion as to avoid doing so where it would create shift/reduce and/or reduce/reduce conflicts (I forget which one is the only one that can happen from it) is one way.
18:03:41 <ddarius> roconnor: Deserialization is parsing.
18:03:46 <Pseudonym> Yes, though that's tricky for k > 1.
18:04:08 <roconnor> ddarius: right, and we don't have to serizialize into difficult to parse notations.
18:04:19 * wli chafes at the use of "serialization" for marshalling as opposed to mutual exclusion.
18:04:36 <roconnor> ddarius: the main problem is that we are currently deseraizing stuff written by humans.
18:04:58 <roconnor> wli: marshalling is probably a better word.
18:05:14 <ddarius> roconnor: Handling network protocols is mostly parsing as well.  The human oriented ones seem easier to "parse" to me in that area.
18:06:49 <roconnor> ddarius: well, there is a diffrence between human readable protocols and protocols designed for interfacing with humans.
18:08:12 <MyCatVerbs> ddarius: human-oriented? Like what, SMTP?
18:08:25 <ddarius> That would be one example.
18:10:40 <MyCatVerbs> I note that there's a strong temptation, when writing protocols that you want to be able to test by hand with nc(1), to make them regular, where possible.
18:11:24 <MyCatVerbs> I note, also, that that sentence contained commas, too many of them, more than any one sentence should be able to justify, on its own, at least.
18:11:30 <roconnor> wli: normally I don't think of saving and loading data later as marshalling.  do you think it counts?
18:12:23 <ddarius> Marshalling, for me, has the connotation of crossing some interface.  E.g. one language to another or COM/SOAP or some such.
18:12:47 <MyCatVerbs> roconnor: hrmn. I'd always taken it that that was precisely what marshalling was.
18:13:09 <ddarius> @hoogle marshal
18:13:09 <lambdabot> Foreign.Marshal :: module
18:13:09 <lambdabot> GHC.Dotnet.marshalObject :: Object a -> (Addr# -> IO b) -> IO b
18:13:09 <lambdabot> GHC.Dotnet.marshalString :: String -> (Addr# -> IO a) -> IO a
18:13:40 <MyCatVerbs> Putting data into the form of an unambiguous stream of bytes that can be put on a socket or written to a file.
18:13:52 <roconnor> oh
18:15:07 <MyCatVerbs> roconnor: I could be wrong, but IIRC that's how the OCaML people use the term too, so...
18:15:51 <ddarius> MyCatVerbs: Marshalling doesn't have to be in a context independent format, though usually there needs to be some agreed upon format.  E.g. those .NET marshalling routines probably don't produce anything that could be "put on a socket or written to a file"
18:16:25 <ddarius> MyCatVerbs: On the flip side, serialization does mean something that can be "put on a socket or written to a file", but it doesn't have to be in any agreed upon format, e.g. Data.Binary.Binary
18:17:18 <MyCatVerbs> Wikipedia defines it as being ridiculously close to serialization, too.
18:17:37 <roconnor> ddarius: intresting distinction
18:17:55 <MyCatVerbs> ddarius: hrmn, point. Though I disagree with the second, in that if there's *no* agreed-upon format then nobody will be able to read it anyway, not even you.
18:18:16 <MyCatVerbs> ddarius: I mean, if you can't even agree with yourself on what the format out to be... ;)
18:19:55 <ddarius> I usually assume "agreeing" requires more than one party...
18:25:03 <MyCatVerbs> ddarius: you sound like a person who has never lost an argument with themself.
18:25:23 <MyCatVerbs> Er, theirself? Dear grammar gremlins, go to hek.
18:25:27 <MyCatVerbs> *heck, dammit.
18:25:36 <dolio> Himself.
18:25:40 <ddarius> MyCatVerbs: I never said disagreeing required more than one party.
18:26:12 <MyCatVerbs> dolio: ladies are allowed to argue with themselves just as often as gentlemen are. :P
18:26:31 <dolio> Is ddarius a lady?
18:27:10 <cjs> "himself" can refer to a lady, too, in the general case. It's polymorphic. :-)
18:27:19 <cjs> But if it feels less awkward, use "oneself."
18:28:22 <dolio> Yeah, he/him is the traditional pronoun when you don't know gender, although I've heard of proposals to make new, more gender neutral ones.
18:28:35 <Adamant> and they're mostly awful
18:28:49 <dolio> They/them technically isn't correct, though it's popular.
18:29:04 <Adamant> it's technically correct in older English, IIRC
18:29:12 <dolio> Adamant: Yeah, I think the one I've heard of is "hirm" or something like that.
18:29:21 <cjs> If we're going to move away from he/him, they/them seems to be the least offensive option these days, mostly via extensive use.
18:29:25 <dolio> Or maybe it's "herm", it's hard to say.
18:29:27 * ddarius proposes they/them as new gender neutral pronouns.
18:29:33 <Adamant> that works
18:30:05 <ddarius> Or we could be like the Japanese and effectively get rid of pronouns.
18:30:19 <cjs> These days I get about as excited over it as I do about wars about whether Python or Ruby is better.
18:30:27 <cjs> Japanese has lots of pronouns.
18:30:44 <ddarius> Okay, "personal pronouns"
18:30:53 <cjs> I can think of six or seven ways to say "I/me" off the top of my head.
18:31:24 <cjs> "Kare" (him, boyfriend) and "kanajo" (her, girlfriend) are in quite common use.
18:31:41 <cjs> Pop songs use "anata" (you) all the time.
18:32:29 <ddarius> cjs: The connotations of anata, kare, kanajo make them much less useful/used than the putative English equivalents.
18:33:19 <cjs> Then again, it still isn't terribly unusual to hear girls use their own names to refer to themselves, which to an English speaker sounds like they're referring to themselves in the third person. It's a bit cutesy, though.
18:33:45 <ddarius> Combined with the omission of the subject, you can have entire conversations about some person in Japanese and never know if you were talking about a male or a female
18:33:55 <cjs> Depends on the level of Japanese. For younger people in non-formal situations, kare and kanajo are used all the time.
18:34:15 <cjs> Even with the subject: Is Suzuki-san male or female? No way to tell.
18:34:29 <ddarius> cjs: Exactly.
18:34:48 <cjs> ddarius: have you lived in Japan?
18:35:01 <ddarius> No.  I have a friend who does.
18:35:39 <tromp_> why is it that ghci says :t 0 is 0 :: (Num t) => t but if you load a file with zero = 0 then zero :: Integer  ?
18:35:52 <ddarius> tromp_: Monomorphism restriction.
18:35:54 <mauke> monomorphism restriction
18:36:10 <cjs> And that mouthful is?
18:36:22 <ddarius> @google Haskell "monomorphism restriction"
18:36:23 <lambdabot> http://www.haskell.org/haskellwiki/Monomorphism_restriction
18:36:23 <lambdabot> Title: Monomorphism restriction - HaskellWiki
18:36:26 <gwern>  >:t 0
18:36:37 <gwern> >:t 0
18:36:45 <mauke> >:O
18:36:53 <TomMD> @type 0
18:36:54 <gwern> > let zero = 0
18:36:54 <lambdabot>  Parse error at end of input
18:36:56 * ddarius assumes that isn't angry gwern with a scimitar in his mouth.
18:36:56 <lambdabot> forall t. (Num t) => t
18:37:01 <gwern> :t zero
18:37:02 <lambdabot> forall m. (Monoid m) => m
18:37:11 * gwern slashes ddarius. die infidel!
18:37:13 <dmwit> ?yow
18:37:13 <lambdabot> I'm also pre-POURED pre-MEDITATED and pre-RAPHAELITE!!
18:37:19 <tromp_> but you can annotate zero :: (Num t) => t and get the polymorphic one
18:37:23 <dmwit> *poke*
18:38:02 <dmwit> yes
18:38:10 <tromp_> wonder why ghc voluntarily chooses a more restricted type than i can annotate
18:38:21 <dmwit> tromp_: It prevents some surprises.
18:38:39 <ddarius> The wiki page explains it.  But GHC does it because the Report tells it to.
18:38:41 <dmwit> tromp_: If the constant is computationally-intensive, a monomorphic value is calculated only once.
18:39:03 <dmwit> tromp_: But a polymorphic value might have to be calculated many times, if it is used with many different instances.
18:39:50 <tromp_> does this affect other things than numeric literals?
18:40:04 <ddarius> It doesn't effect numeric literals.
18:40:23 <ddarius> It effects certain bindings.
18:40:41 <dons> so Heffalump, when's the Credit Suisse haskell blog going to start?
18:41:51 <cjs> People don't seem to use Maps much in Haskell.
18:41:56 <ddarius> "One day there was this Haskell script.  Some people came up and though, 'What does every Haskell script need? A bank!'  The end."
18:42:05 <ddarius> cjs: People use them quite a bit.
18:42:13 <dons> cjs, Data.Maps?
18:42:21 <gwern> ddarius: why does a haskell script need a bank?
18:42:22 <cjs> Really? I guess I'm just not looking at the right code.
18:42:24 <dons> its the single most used structure after lists and tuples
18:42:46 <EvilTerran> more generally useful than Data.Arrays
18:42:54 <dons> yeah
18:43:06 <dons> they're our %hash structure, and just as useful
18:43:11 <ddarius> gwern: What kind of silly question is that?!
18:43:12 <cjs> Huh.
18:43:13 <EvilTerran> and nicer to use, given that you don't have to faff around with bounds
18:43:17 <dons> (well, more so, since they're persistent)
18:43:38 <cjs> Looking back on the code I've written over the last couple of weeks, I find myself declaring structures using data instead.
18:43:46 <dons> yeah, that's also common
18:43:57 <cjs> (In places where I'd be using a Hash in Ruby.)
18:44:02 <dons> but generally not complex structures that need rebalancing and so forth
18:44:03 <ddarius> cjs: I will agree that Haskellers don't -abuse- Maps.
18:44:14 <dons> oh, so then probably you should be using a Map...
18:44:15 <ddarius> (so much...)
18:44:17 <gwern> ddarius: one that demands a silly reply?
18:44:18 <dolio> Heh.
18:44:24 <Pseudonym> I do think that not many people use Data.Set.
18:44:34 <Pseudonym> Nowhere near as many as Data.Map, for sue.
18:44:36 <Pseudonym> sure
18:44:39 <ddarius> Pseudonym: Probably not as many as should.
18:44:43 <Pseudonym> Indeed.
18:44:44 <ddarius> Sets have always been underappreciated.
18:44:47 <dmwit> I like Set.
18:45:05 <dons> yeah, Sets are underappreciasted. good point
18:45:10 <EvilTerran> well, a Set's just a (Flip Map ()) :P
18:45:31 <ddarius> EvilTerran is one of the counterexamples to my earlier statement.
18:45:32 <dons> IntMap is also very nice, but a bit rare (for different reasons)
18:45:48 <dons> and [(k,v)] is also kinda common, due to List.lookup
18:46:11 <EvilTerran> what, the one abusing Maps? don't worry, that was just an aside, not an insight into my programming style
18:46:23 <ddarius> I don't see too many people inappropriately using assoc lists.
18:46:25 <EvilTerran> i should've included "isomorphic" in there somewhere, i guess
18:47:35 <dons> its a bit of a newbie thing, but i think much less common than say 4 years ago
18:48:11 <ddarius> dons: I think mostly people coming from Scheme/CL would do that.  I don't think many people coming from elsewhere would go for it readily.
18:48:40 <ddarius> Certainly when there was no Map like data structure in the standard library it was more common
18:48:44 <ddarius> (not unreasonably)
18:49:07 <OceanSpray> None of the Liskell sample code I've seen shows any type annotations.
18:49:21 <OceanSpray> Is the guy responsible in here?
18:49:26 <gwern> @seen lispy
18:49:26 <lambdabot> lispy is in ##logic, #haskell-blah, #darcs, #ghc and #haskell. I last heard lispy speak 55m 25s ago.
18:49:34 <OceanSpray> that's him?
18:49:54 <ddarius> No
18:50:12 <gwern> huh? no
18:50:19 * gwern was thinking something entirely else, lispy's helisp think
18:50:22 <gwern> *thing
18:50:27 <OceanSpray> ah.
18:50:28 <gwern> supposed to be some sort of elisp compiler
18:51:00 <ddarius> OceanSpray: He's here sometimes but I don't recall his nick off-hand.
18:51:49 <gwern> 'Can't make a derived instance of `MonadError (m a) (UniqueT m)' (even with cunning newtype deriving: the eta-reduction property does not hold) In the newtype declaration for `UniqueT''?
18:52:20 <ddarius> gwern: You mean that error message doesn't make the problem completely clear to you?
18:52:40 <gwern> ddarius: clear as mud
18:52:54 <dmwit> "even with cunning newtype deriving"
18:52:54 <gwern> it seemed to compile through the Makefile...
18:52:54 <dmwit> heh
18:53:27 <gwern> even cunning deriving has failed me! I am truly forlorn
18:53:40 <dmwit> How can it fail!  Such cunning!
18:54:16 <gwern> the plan was foolproof
18:58:42 <gwern> what the heck. so a ghc --make in src/ works, but even turning on -fglasgow-exts in cabal doesn't work?
18:58:48 <gwern> @seen dcoutts_
18:58:48 <lambdabot> dcoutts_ is in #ghc, #haskell-overflow, #haskell, #gentoo-haskell and #haskell-soc. I don't know when dcoutts_ last spoke.
19:10:56 <gwern> @ask lispy so I was looking at your Helisp repo; I can only seem to compile Helisp.hs/helisp; my attempts with Main.hs founder thanks to derivation problems in MonadUnique. Is this expected?
19:10:56 <lambdabot> Consider it noted.
19:13:37 <roconnor> gwern: re: MonadError (m a) (UniqueT m)
19:13:51 <roconnor> gwern: why are you using (m a) for the error type?
19:14:13 <roconnor> @type MonadError
19:14:14 <lambdabot> Not in scope: data constructor `MonadError'
19:14:21 <roconnor> @hoogle MonadError
19:14:21 <lambdabot> Control.Monad.Error.Class.MonadError :: class Monad m => MonadError e m
19:14:47 <gwern> roconnor: I'm not, lispy is
19:14:55 <roconnor> oh
19:15:56 <glen_quagmire> > let { f (x:xs) = (x+1):xs ; g = 0 : f g } in take 3 g
19:15:57 <lambdabot>  [0,1,1]
19:16:04 <glen_quagmire> > let { f (x:xs) = (x+1):xs ; g = 0 : f g } in take 10 g
19:16:04 <lambdabot>  [0,1,1,1,1,1,1,1,1,1]
19:18:15 <glen_quagmire> i would expect it to give me [0..]  why is it [0,1,1..] ?
19:20:38 <tromp_> cause f adds 1 only to head
19:21:22 <dmwit> > let f = (+1); g = 0 : map f g in take 10 g
19:21:23 <lambdabot>  [0,1,2,3,4,5,6,7,8,9]
19:22:26 <glen_quagmire> > let f l = 0 : l in f [0]
19:22:27 <lambdabot>  [0,0]
19:24:17 <glen_quagmire> > let { f (x:xs) = (x+1):f xs ; g = 0 : f g } in take 10 g
19:24:18 <lambdabot>  [0,1,2,3,4,5,6,7,8,9]
19:24:23 <glen_quagmire> yay
19:25:36 <dmwit> > iterate (+1) 0
19:25:36 <lambdabot>  [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,2...
19:25:56 <dmwit> > scanl (+) 0 [1,1..]
19:25:57 <lambdabot>  [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,2...
19:26:11 <MyCatVerbs> > fix ("poo! heehee. poo! "++)
19:26:12 <lambdabot>  "poo! heehee. poo! poo! heehee. poo! poo! heehee. poo! poo! heehee. poo! poo...
19:26:42 * MyCatVerbs is -so- immature. ^_^
19:26:52 <dons> http://reddit.com/info/6eear/comments/
19:26:57 <dons> get yr type level functions!
19:27:55 <dmwit> > fix (\xs -> xs ++ [last xs + 1])
19:27:56 <lambdabot>  Exception: <<loop>>
19:28:16 <dmwit> > fix ((0:) . map (+1))
19:28:16 <lambdabot>  [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,2...
19:34:27 <tromp_> > inits [0..]
19:34:28 <lambdabot>  [[],[0],[0,1],[0,1,2],[0,1,2,3],[0,1,2,3,4],[0,1,2,3,4,5],[0,1,2,3,4,5,6],[0...
19:35:23 <dmwit> I still find "fix" to be mildly magical.
19:35:24 <tromp_> > fix (map length . inits)
19:35:25 <lambdabot>  Exception: <<loop>>
19:35:55 <ddarius> dmwit: Why?
19:36:42 <tromp_> @src inits
19:36:42 <lambdabot> inits []     =  [[]]
19:36:42 <lambdabot> inits (x:xs) =  [[]] ++ map (x:) (inits xs)
19:37:27 <roconnor> After I understood fix, mfix was able to recreate that magical sensation of bewilderment.
19:37:34 <tromp_> inits cld be written to be less strict
19:38:14 <tromp_> it always starts with []:
19:38:17 <BMeph> Froated fix - it's mildly magically delicious! ;)
19:38:34 <dmwit> tromp_: yes =/
19:38:38 <BMeph> Wow, I'm 0/10 this week... :p
19:39:30 <BMeph> s/Froated/Frosted/
19:42:26 <davidL> Igloo: in your Diki app does each page get a new darcs repo?
19:43:55 <glen_quagmire> @pl let { f a b = g a (h b); g a b = a + b; h a = a + 1; } in f
19:43:55 <lambdabot> (line 1, column 5):
19:43:55 <lambdabot> unexpected "{"
19:43:55 <lambdabot> expecting "()", natural, identifier or "in"
19:44:26 <glen_quagmire> If I have  f a b = g a (anotherFunc b)   , is there a way to do it in point-free/less ?
19:44:58 <BMeph> @type  let { f a b = g a (h b); g a b = a + b; h a = a + 1; } in f
19:44:59 <lambdabot> forall a. (Num a) => a -> a -> a
19:45:19 <dmwit> ?pl \a b -> g a (h b)
19:45:19 <lambdabot> (. h) . g
19:45:41 <glen_quagmire> thanks dmwit
20:01:28 <tromp_> ?pl \a b c -> f a (g b) (h c)
20:01:28 <lambdabot> flip flip h . ((.) .) . (. g) . f
20:02:08 <tromp_> ?pl \a b c -> f a ((g b) (h c))
20:02:08 <lambdabot> (. ((. h) . g)) . (.) . f
20:02:49 <tromp_> ?pl \a b c d -> f a (g b) (h c) (i d)
20:02:50 <lambdabot> flip flip i . ((flip . ((.) .)) .) . flip flip h . ((.) .) . (. g) . f
20:03:11 <tromp_> ?pl \a b c d e -> f a (g b) (h c) (i d) (j e)
20:03:11 <lambdabot> flip flip j . ((flip . ((flip . ((.) .)) .)) .) . flip flip i . ((flip . ((.) .)) .) . flip flip h . ((.) .) . (. g) . f
20:03:22 <tromp_> ok, enough for now:)
20:03:42 <tromp_> this is flipping me out
20:09:23 <glen_quagmire> @hoogle [(a,b)] -> [a]
20:09:23 <lambdabot> No matches, try a more general search
20:09:36 <glen_quagmire> > map fst [(1,2)]
20:09:39 <lambdabot>  [1]
20:10:13 <shachaf> @djinn Maybe (a,b) -> Maybe a
20:10:14 <lambdabot> f a =
20:10:14 <lambdabot>     case a of
20:10:14 <lambdabot>     Nothing -> Nothing
20:10:14 <lambdabot>     Just (b, _) -> Just b
20:11:54 <bd_> @djinn Maybe a -> Either () a
20:11:54 <lambdabot> f a =
20:11:54 <lambdabot>     case a of
20:11:54 <lambdabot>     Nothing -> Left ()
20:11:54 <lambdabot>     Just b -> Right b
20:16:20 <gwern> well, shit.
20:16:32 <gwern> so I have a shell function which is a front to a recursive sed, right?
20:17:00 <gwern> and I punched in "'replace '[Char]' String"
20:17:20 <gwern> any guesses on precisely how that trashed my files and destroyed my darcs repo?
20:18:05 <shachaf> gwern: You shouldn't be changing anything inside .darcs.
20:18:18 <shachaf> gwern: At least that part is still OK?
20:18:28 <shachaf> s/\./_/ , of course.
20:18:43 <scook0> gwern: ouch
20:20:08 <gwern> shachaf: no. I said 'recursive', remember? sed doesn't know to leave _darcs alone
20:20:23 <shachaf> gwern: Well, you should teach it. :-)
20:20:27 <gwern> (one reason I want .darcs instead of _darcs, but whatever)
20:20:39 <shachaf> It doesn't go into .directories?
20:21:11 <gwern> scook0: in the future, I think I shall try to remember not to include []s or {}s, or at least \escape them
20:21:23 <cubix__> why does darcs use_darcs?
20:21:24 <gwern> shachaf: I don't think so, but I haven't tested it. most recursive stuff ignores dot-files
20:22:05 <scook0> using something like ack will at least prevent you from trashing your repo
20:22:22 <gwern> cubix__: history. CVS/ and so on
20:22:35 <gwern> cubix__: I once tried to change that, but _darcs is baked in all over the place as a string literal
20:22:47 <SamB> gwern: I think he meant "as opposed to .darcs"
20:22:48 <gwern> I don't care what people say, darcs' source is ugly and hard to hack
20:23:06 <SamB> is there a VCS that isn't hard to hack?
20:23:27 <cubix__> yeah, I meant as opposed .darcs
20:23:43 <vegai> SamB: mercurial, I'd wager
20:23:46 <shapr> gwern: You called?
20:24:26 <gwern> SamB: git seems to get a lot of development, and I understand linus is a partisan for clean code which is well-separated and easy to mess with
20:25:14 <gwern> shapr: didn't I leave a @message?
20:25:26 <vegai> does Linus lead git anymore?
20:25:39 <gwern> shapr: hm. I'm trying to remember what I wanted to ask. maybe it wwas about uncc
20:26:36 <gwern> afaik
20:27:30 <gwern> shapr: ah, yeah. do you know what license uncc is?
20:27:39 <gwern> shapr: I've tried contacting dark before (over mage) but no dice
20:28:05 <gwern> he mentioned in #haskell that mage was bsd3, so that's fine, but I see no license info for uncc
20:30:27 <dejones> vegai: are you asking if Linus leads the development of git anymore?  No, he doesn't.
20:38:10 <elaforge> is there some reason Data.Map doesn't export toDescList?
20:45:34 <dons> elaforge: i think its a mistake
20:45:40 <dons> to have not exported it
20:45:50 <elaforge> interesting that no one noticed...
20:45:55 <elaforge> It claims O(n)
20:46:23 <dons> toAscList is exported though, right?
20:46:23 <elaforge> how do I send a patch to the library maintainers?
20:46:30 <dons> send to libraries@haskell.org
20:46:43 <elaforge> yeah, I'm doing reverse.toAscList, but I don't think that will generate elts lazily
20:47:01 <elaforge> err, toDescList uses foldl... should that produce elts immediately?
20:47:06 <luqui> elaforge, I don't think your list will be lazy anyway
20:47:10 <luqui> because map is strict in its keys
20:47:18 <luqui> or rather, your list will be lazy, but who cares
20:47:34 <elaforge> well, if I have a huge map, can I say head (toDescList m)?
20:47:56 <elaforge> dons, ok, I'll send off a note, thanks
20:48:16 <luqui> ahh yes, probably.    nevermind.  :-)
20:48:35 <elaforge> good, cuz I'd be disappointed if you couldn't efficiently iterate over a balanced tree :)
20:49:05 <luqui> I was in the laziness-for-memory-usage mindset for some reason
20:49:43 <elaforge> you know, I also noticed they had comments in there like "use foldlString if you want better memory performance"
20:49:54 <elaforge> implication is you should manually hack the library source and recompile?
20:50:16 <elaforge> also, while the docs have big-O for time, they don't have space behaviour annotation
20:51:22 <elaforge> like, is foldl' (\m (k, v) -> Map.insert k v m) going to be efficient?
20:51:53 <elaforge> or would it be faster to say old_map `Map.union` Map.fromAscList kv_pairs?
20:52:00 <gwern> (kee-rist. That's the third time I've started over with hjs. I think I'm gonna upload and send it, and call things quit.)
20:52:22 <elaforge> unfortunately the latter keeps dup keys from old_map, which isn't what I want :(
20:53:32 <luqui> elaforge, so use flip union instead?
20:54:03 <luqui> and its hard for me to imagine an implementation in which the fold performs better...
20:54:25 <elaforge> the docs say big_map `union` small_map is efficient, not the other way around
20:54:33 <elaforge> unfortunately I'm updating a big map with a small one
20:54:53 <elaforge> I guess the fold is gonig to be log n * m
20:55:08 <elaforge> union says it's n+m
20:55:38 <elaforge> I guess even a "less efficient" m+n is still going to beat m*n, unless I only have 1 or 2 keys
20:56:03 <gwern> http://www.cse.unsw.edu.au/~chak/papers/SPCS08.html <-- what is an open type level function and why is that so awesome?
20:56:05 <lambdabot> Title: Research Papers of Manuel Chakravarty
20:56:48 <elaforge> oh, looks like I can change the bias with unionWith, cool
20:57:02 <dolio> A type level function is a function from types to types.
20:57:27 <luqui> and an open function is one that is not all defined in one place
20:57:55 <luqui> (typeclasses are open, for example)
20:58:41 <elaforge> oh yes, and what's this List having different contents from Data.List?  some kinda h98 thing?
20:59:29 <billbill> i want to create a function : calc x n = -(x^(n*2)) / 10      (of type Float->Float->Float), but i get an error "Instance of Integral Float required for definition of calc"
20:59:53 <elaforge> (^) works with integers
20:59:59 <elaforge> you could probably use (**)
21:00:13 <elaforge> or fromIntegral the stuff before (/) maybe
21:00:21 <gwern> elaforge: quite likely. System is different from System.IO, System.Exit, etc
21:00:25 <billbill> let me try
21:01:01 <elaforge> gwern: figured.  Are we going to clean this up when haskell' comes in 2015 and recognizes hierarchical modules?
21:01:31 <elaforge> I guess that's mostly retorical, and also up to the ghc library guys :)
21:05:26 <gwern> elaforge: indeed. right now, the question is when will we get a haskell'. I'd really like one out in 2008, personally. it's not good for the community to have a stagnant standards process
21:05:26 <elaforge> gwern totally agree
21:05:28 <elaforge> is there actually work being done?  people being locked in rooms?
21:05:29 <gwern> I mean, if only for the textbook and tutorial writers - plain haskell98 is kind of hard to use, since 'real' haskell expects so many extensions.
21:05:30 <gwern> my god, just hierarchical modules alone!
21:05:31 <elaforge> ... and then, someone needs to decide what to do with the (.) operator and implement one of the better record systems :)
21:05:31 <gwern> elaforge: I have no idea. I think it's being done on a ml and wiki
21:05:49 <gwern> (I wish I could help out, but I don't understand the issues)
21:06:13 <elaforge> likewise
21:06:39 <elaforge> my impression was "what are we going to do about fundeps" was one of the issues, and the answer was "wait and see if type families work"
21:06:49 <elaforge> but I thought haskell' was supposed to be conservative
21:07:07 <luqui> but also fundeps kinda suck
21:07:29 <luqui> so we would rather not have them if we could afford it :-)
21:08:15 <gwern> elaforge: hard to say. I understand haskell standards are manic-depressive when it comes to conservativeness
21:08:56 * luqui acknowledges the ambiguity of that sentence
21:09:21 <elaforge> sorry, I didn't have incoherent sentence support turned on :P
21:10:38 * luqui gets really frightened at all the threads in haskell-cafe which talk about--even suggest-- -XUndecidableInstances
21:10:46 <RyanT5001> so if "lexical analysis" means the time when the input is tokenized, and "parsing" means the time when it is formed into an AST, what do we call the time when lexically-scoped identifiers are mapped to their bind sites?
21:10:56 <luqui> it represents a grave misunderstanding in how typeclasses work...
21:11:11 <RyanT5001> (if we were to have such a phase in our compiler)
21:11:19 <luqui> usually that's "parsing" :-)
21:11:38 <RyanT5001> luqui: really? that doesn't *have* to be done at parse-time, though, right?
21:11:46 <luqui> no, but it's easy enough..
21:11:53 <SamB> mapped to their bind sites? what do you mean?
21:11:58 <luqui> but if not, I'd call it bind-site-mapping or something
21:12:18 <luqui> \x -> \y -> x  -- this x refers to 2 lambdas up
21:12:23 <RyanT5001> SamB: well at some point you have to determine where each identifier you use was bound, in order to determine what the identifier means
21:12:23 <SamB> why is it necessary to know where they are bound
21:12:27 <luqui> or the equivalent in an imperative setting
21:12:45 <RyanT5001> SamB: otherwise how do you know to which x this refers: \x -> \x -> x
21:12:54 <luqui> er... 1 lambda us by debruijn...
21:12:56 <luqui> *up
21:12:56 <luqui> gah
21:13:15 <gwern> oh my god. Fuck you very much gmail. I'm sending a gzipped tarball, and you're decompressing it, unpacking it, and scanning for files with an executable bit? WTF
21:13:17 <SamB> RyanT5001: well, thatt's wha tthe renamer iss for
21:13:31 <RyanT5001> SamB: interesting, i'll look that up
21:13:37 <RyanT5001> this is for a language spec i'm writing
21:13:40 <SamB> and, of coourse, the linker iss there to make me stutterr
21:13:43 <RyanT5001> also, interesting that i'm showing up as ryant5001
21:14:15 <luqui> ahhh, earthquake!
21:14:20 <glen_quagmire> f :: [(k,v)] -> [(k, m v)]     can I make it return m [(k, v)]  ?
21:15:04 <luqui> glen_quagmire, not totally sure what you're asking
21:15:09 <glen_quagmire> @hoogle  (a -> m b) -> [a]  -> m [b]
21:15:09 <lambdabot> No matches, try a more general search
21:15:20 <glen_quagmire> @hoogle  [a] -> (a -> m b)  -> m [b]
21:15:21 <lambdabot> No matches, try a more general search
21:15:27 <luqui> are you asking whether you can convert [(k,m v)] into m [(k,v)]
21:15:28 <luqui> ?
21:15:33 <luqui> (sequence is the answer)
21:15:36 <luqui> @hoogle sequece
21:15:36 <lambdabot> No matches found
21:15:39 <luqui> @hoogle sequece
21:15:40 <lambdabot> No matches found
21:15:41 <luqui> hagowiejg
21:15:42 <luqui> ewogj
21:15:46 <luqui> @hoogle sequence
21:15:46 <lambdabot> Prelude.sequence :: Monad m => [m a] -> m [a]
21:15:46 <lambdabot> Control.Monad.sequence :: Monad m => [m a] -> m [a]
21:15:46 <lambdabot> Data.Traversable.sequence :: (Traversable t, Monad m) => t (m a) -> m (t a)
21:16:02 <luqui> you'll have to do a little messing around
21:16:18 <gwern> (when did gmail become so fascist? :()
21:16:58 <glen_quagmire> :t mapM
21:17:01 <lambdabot> forall a (m :: * -> *) b. (Monad m) => (a -> m b) -> [a] -> m [b]
21:18:04 <luqui> :t let liftPair (a,b) = liftM ((,) a) b in sequence . map liftPair
21:18:05 <lambdabot> forall t (m :: * -> *) a1. (Monad m) => [(t, m a1)] -> m [(t, a1)]
21:18:50 <luqui> @pl \(a,b) -> liftM ((,) a) b
21:18:50 <lambdabot> uncurry (fmap . (,))
21:18:54 <RyanT5000> sequence . map (\(k, mv) -> mv >>= return . (,) k)
21:19:11 <RyanT5000> @t sequence . map (\(k, mv) -> mv >>= return . (,) k)
21:19:11 <lambdabot> Maybe you meant: tell temp thank you thanks thx time tiny-url todo todo-add todo-delete topic-cons topic-init topic-null topic-snoc topic-tail topic-tell type . ? @ ft v
21:19:16 <RyanT5000> @type sequence . map (\(k, mv) -> mv >>= return . (,) k)
21:19:17 <mrmr> how can i get the remainder of a float division?
21:19:17 <lambdabot> forall t (m :: * -> *) a. (Monad m) => [(t, m a)] -> m [(t, a)]
21:20:47 <scook0> > 7 `mod` 3
21:20:49 <lambdabot>  1
21:20:55 <scook0> > 7 `divmod` 3
21:20:55 <lambdabot>   Not in scope: `divmod'
21:21:00 <scook0> > 7 `divMod` 3
21:21:00 <lambdabot>  (2,1)
21:21:13 <luqui> Data.Fixed.divMod' looks like it does it
21:21:38 <luqui> http://haskell.org/ghc/docs/latest/html/libraries/base/Data-Fixed.html#v%3AdivMod%27
21:21:40 <lambdabot> http://tinyurl.com/yqdcrb
21:22:01 <luqui> :t divMod'
21:22:02 <lambdabot> forall a b. (Integral b, Real a) => a -> a -> (b, a)
21:22:17 <luqui> > 7.48 `divMod'` 2.31
21:22:20 <lambdabot>  (3,0.5500000000000003)
21:29:34 <shapr> Yay, I got a new toy! Nokia N800
21:29:50 <shapr> @users
21:29:50 <lambdabot> Maximum users seen in #haskell: 467, currently: 430 (92.1%), active: 10 (2.3%)
21:41:03 <RyanT5000> there's something seriously wrong with my internet connection
21:41:40 <wagle_home> cant be too serious (unless you sent that message days ago)..
21:41:52 <RyanT5000> i keep getting disconnected from irc
21:42:01 <RyanT5000> every hour or so
21:42:10 <RyanT5000> and i don't think GAIM figures it out for a while
21:42:21 <Twey> Well, it wouldn't
21:42:29 <RyanT5000> of course, this is in addition to BitTorrent not working and such, due to it being Comcast
21:42:32 <Twey> There's no client->server ping
21:42:36 <Twey> Heh
21:42:40 <Twey> Time for a new ISP?
21:42:51 <RyanT5000> Twey: Comcast is basically a monopoly here
21:43:00 <RyanT5000> my choice is Comcast cable or Verizon dsl
21:43:12 <Twey> Really?  No others at all?
21:43:20 <RyanT5000> dialup, cellular
21:43:22 <RyanT5000> nothing wired
21:43:44 <Twey> Surely there must be one somewhere
21:43:52 <Twey> Maybe just not advertised much?
21:44:11 <shapr> RyanT5000: I pay $120 a month for unlimited data on my cellphone. I get 400k down and 200k up.
21:44:18 <shapr> Lucky for me, my company actually pays that.
21:44:26 <RyanT5000> shapr: that's pretty nice; are you in the US?
21:44:31 <shapr> yup
21:44:34 <RyanT5000> cool
21:44:48 <shapr> Scandinavian celldata deals are far better in both price and bandwidth.
21:45:23 <RyanT5000> hm
21:45:36 <RyanT5000> maybe i just haven't looked hard enough at dsl
21:46:12 <RyanT5000> are the copper owners required to lease the copper to competitors?
21:46:24 <RyanT5000> i think that's how certain phone stuff works, but i'm not sure about dsl
21:46:31 <RyanT5000> if so it might be reasonable
21:46:37 <wagle_home> your ip address could be changing every hour (?) due to dhcp?
21:46:46 <Twey> I don't really know US law :-\
21:46:47 <RyanT5000> wagle_home: hm, that might have to do with it
21:47:08 <RyanT5000> Twey: yeah; i'm in law school and i don't really have a clue :P
21:47:12 <Twey> I'd imagine something would happen under the anti-monopoly legislation though...
21:47:15 <Twey> Haha
21:48:53 <RyanT5000> lol so i went to the Boston Lisp Meeting on monday
21:49:04 <RyanT5000> felt a little like Daniel in the lions' den
21:49:45 <Pseudonym> Except that lions are dangerous but not arrogant.
21:49:49 <RyanT5000> hm
21:50:07 <RyanT5000> my *ego* felt like Daniel in a den of lion-esque egos
21:50:09 <RyanT5000> does that work?
21:50:25 <Twey> RyanT5000: Hahaha
21:50:26 <SamB> lisp users are dangerous  and arrogant?
21:50:33 <RyanT5000> lol
21:50:42 <RyanT5000> the arguments were strange
21:50:50 <wagle_home> well, being a (former) scheme guy in a haskell enclave was kinda the same
21:51:01 <RyanT5000> from the same guy, i got "ew, haskell's syntax" and "lisp's syntax is probably a major problem for its adoption"
21:51:12 <johnnowak> RyanT5000: what's confusing about that?
21:51:19 <SamB> yeah really
21:51:19 * Twey comes from a Java background :-(
21:51:21 <Pseudonym> LOL
21:51:22 <RyanT5000> johnnowak: it's not so much confusing as interesting
21:51:23 <elaforge> I like lisp syntax
21:51:27 <elaforge> I just don't like the rest so much :)
21:51:28 <SamB> sometimes *I* go "eww, haskell's syntax!"
21:51:31 <johnnowak> RyanT5000: well he's right
21:51:39 <RyanT5000> johnnowak: right; i agreed with him on both points
21:51:46 <Twey> I like Lisp the language
21:51:47 <johnnowak> RyanT5000: ah. i sensed sarcasm.
21:51:47 <RyanT5000> but what does it mean for the future of programming?
21:51:54 <Twey> I don't like the way it's used so much
21:52:20 <wolverian> the problem to me with the little syntax that haskell has is that I both like and hate it at the same time
21:52:28 <Pseudonym> RyanT5000: Obviously, the future of programming is languages with crappy syntax.
21:52:43 <wolverian> I wish I could pry apart the things that induce the latter
21:52:50 <RyanT5000> if talented/educated programmers (read: know how to program a compiler, write macros in their sleep) prefer scheme syntax, and everyone else prefers C++ (or whatever) syntax
21:52:52 <RyanT5000> that's really bad
21:52:57 <wolverian> but I'm not at all sure those are exclusive subsets :(
21:53:12 <RyanT5000> because we have this deep divide between those who should be leading the community and those who *are* the community
21:53:22 <johnnowak> RyanT5000: that would be great actually; you could simply ask which syntax they prefer as part of the hiring process.
21:53:44 <wagle_home> all syntax is sugar
21:53:58 <RyanT5000> it would be like if politicians spoke latin all the time and only reluctantly spoke english - and vociferously insulted everyone who used english whenever they had to use it
21:54:09 <johnnowak> wagle_home: is all sugar equivalent?
21:54:20 <johnnowak> if not, then "all syntax is sugar" is a pretty meaningless statement
21:54:21 <elaforge> wait, and you're the guy in law school?
21:54:38 <RyanT5000> wagle_home: without syntax you can't represent any information
21:54:56 <RyanT5000> wagle_home: even binary has syntax (there's a bit followed by another bit followed by another bit ...)
21:55:04 <RyanT5000> if you didn't know the syntax of binary, it wouldn't mean anything
21:55:21 <RyanT5000> elaforge: yeah, i am :)
21:55:26 <RyanT5000> elaforge: i hate legalese though
21:55:36 <RyanT5000> elaforge: legalese is a weapon of obfuscation
21:55:41 <wolverian> if syntax is sugar, surely haskell's is aspartame
21:55:51 <wagle_home> ewwwww
21:55:58 <elaforge> RyanT5000, you could try writing a legal document in sexprs and see how that goes over...
21:56:16 <wagle_home> aspartame makes /me ill
21:56:17 <RyanT5000> elaforge: just like any other linguistic style, it should only be used with a particular purpose in mind
21:56:35 <RyanT5000> elaforge: well, i'm actually interested in getting a Ph.D. in CS that has some kind of connection to law
21:56:55 <RyanT5000> elaforge: i figure i could churn out law articles by the dozen if i had the right project
21:57:05 <elaforge> like becoming a language lawyer?
21:57:28 <RyanT5000> elaforge: "language lawyer"?
21:57:34 <Pseudonym> RyanT5000: I know someone who has such a PhD.
21:57:50 <elaforge> then you can say "I have a degree in both law and CS, so when I say the semicolon goes over there..."
21:57:58 <RyanT5000> lol yeah
21:58:10 <RyanT5000> Pseudonym: really?  link?  I'd like to use it as inspiration for my own
21:58:19 <Pseudonym> http://2006.xmlconference.org/programme/people/1933.html <- Potted bio.
21:58:21 <lambdabot> Title: XML 2006: Timothy Arnold-Moore
21:58:26 <Pseudonym> I don't think he has a current personal web site.
21:58:30 <RyanT5000> Pseudonym: i've had trouble finding serious CS topics that have much to do with law
21:58:52 <RyanT5000> there's a myriad of serious legal issues dealing with CS, but the other way around is more sparse
21:59:16 <RyanT5000> and what i *really* want to do is write a programming language - which isn't an academically "cool" thing to do *or* related to law
21:59:54 <Twey> Haha, snap
21:59:55 <hircus> RyanT5000: case-base reasoning is probably relevant to you
22:00:07 <Twey> I just started speccin' it up this morning :-\
22:00:34 <RyanT5000> hircus: you mean cases as in legal precedent? yeah, i could see that being an important area of crossover
22:00:48 <RyanT5000> there are some interesting connections between software versioning and law
22:01:37 <RyanT5000> e.g.: res judicata, in law, is the principle that once a "question" has been decided, it can't be litigated again (except for appeals, which might overturn it)
22:01:43 <johnnowak> RyanT5000: so why don't you write a programming language?
22:01:54 <hircus> yup, have an AI system that would try and retrieve relevant cases.
22:02:17 <RyanT5000> similarly, if you've got a file format with an enum type (for example), you can't change your symbol->value mapping without ruining backwards compatability
22:02:19 <hircus> software versioning.. I vaguely see the connection. elaborate?
22:02:25 <hircus> ah
22:02:33 <elaforge> instead of exceptions, you could have "objections" which are either overruled or sustained
22:02:41 <elaforge> objection oriented programming
22:02:44 <RyanT5000> johnnowak: i probably will at some point (i'm writing a spec right now, actually), but i don't know if it should be my phd thesis
22:02:59 <hircus> and deprecation is the equivalent of a precedent being overturned
22:03:01 <RyanT5000> elaforge: there are tons of stuff like exceptions in law
22:03:09 <johnnowak> RyanT5000: why is it not "academically cool"?
22:03:10 <RyanT5000> elaforge: it's way more elaborate than programming
22:03:11 <hircus> RyanT5000: so you have a law degree?
22:03:13 <sjanssen> @yow
22:03:14 <lambdabot> YOU!!  Give me the CUTEST, PINKEST, most charming little VICTORIAN
22:03:14 <lambdabot> DOLLHOUSE you can find!!  An make it SNAPPY!!
22:03:34 <hircus> sjanssen: lambdabot being imperative! heresy!
22:03:39 <RyanT5000> hircus: i'm in my second year of law school; i graduate may 2009
22:04:00 <hircus> ah. and doing a CS PhD as well? or thinking of it?
22:04:13 <RyanT5000> johnnowak: i dunno - according to the academics i've talked to, there's not that much funding or prestige to be had in creating new languages
22:04:24 <RyanT5000> hircus: yeah, i plan to apply to cs phd programs in 6 months
22:04:28 <johnnowak> that's unfortunate
22:04:41 <RyanT5000> johnnowak: yeah, very
22:04:48 <johnnowak> certainly the goal isn't to create the language; it is to get real experience with new concepts
22:05:02 <wli> Or, perhaps, to create a useful tool.
22:05:11 * johnnowak gasps
22:05:23 <RyanT5000> wli: yeah, that's what i'm really interested in - which, i think, might have something to do with the academic uncoolness
22:05:43 <RyanT5000> when i was using C++ when i was 12, i thought it was the most amazing thing in the world
22:06:02 <RyanT5000> i read Design and Evolution of C++ by Stroustrup and i thought it was this amazingly well-thought-out system
22:06:05 <RyanT5000> etc.
22:06:11 <hircus> C++ has some nice features .it's just so.. unclean, though
22:06:35 <RyanT5000> then i realized that it was actually a horrifying conglomeration of backwards-compatibility and cheap hacks, with a few decent ideas thrown in
22:06:49 <hircus> finally read the 'paper' on why double-check lock pattern is unsafe and.. ugh. hairy mess
22:06:58 <elaforge> it may suck, but I appreciate how quickly it generally sucks
22:07:15 <RyanT5000> this was around the time that i decided referential transparency and higher-order functions were absolutely mandatory
22:07:21 <RyanT5000> which pretty much left me with haskell
22:07:30 <hircus> true on backward compatibility. 'static' in C++ has two meanings!
22:07:45 <hircus> RyanT5000: what language did you switch to next?
22:08:02 <RyanT5000> hircus: well, i was writing in C++ with heavy template programming until i was about 18
22:08:06 <johnnowak> hircus: it has more than two
22:08:07 <RyanT5000> then i tried to write a game for a year
22:08:22 <RyanT5000> and got very sidetracked on writing good libraries for myself
22:08:25 <RyanT5000> which if found to be impossible
22:08:38 <RyanT5000> specifically, i'm pretty sure there is no way to write a "good" serialization library for C++
22:08:54 <johnnowak> hircus: it has four
22:08:59 <hircus> johnnowak: ouch. static function, static member, and.. what other two?
22:09:03 <wli> External requirements have always made C the language for "products" of whatever kind (e.g. programming assignments, code for work, etc.).
22:09:09 <wli> For me, that is.
22:09:23 <johnnowak> hircus: http://www.space.unibe.ch/comp_doc/c_manual/CPLUSPLUS/SYNTAX/static.htm
22:09:25 <lambdabot> Title: static keyword, http://tinyurl.com/2mmepa
22:09:25 <hircus> I instinctively recoil from crazy languages
22:09:38 <RyanT5000> wli: yep, i have a project this summer that will be C, C++, C#, and Objective C
22:09:43 <elaforge> interestingly, python seems to be taking over that role
22:09:44 <hircus> ah yes, static var
22:10:01 <hircus> C# : Java :: C++ : C
22:10:17 <RyanT5000> hm
22:10:33 <hircus> (only partially in jest. again, C# has some really nice features... but.. too many of them?)
22:10:43 <RyanT5000> hircus: yeah, way too many
22:10:58 <hircus> would love to see Lua getting as popular as Python/Ruby.
22:11:02 <wli> I had one class that used C++ (2nd semester of freshman programming) and one using Java (upper/grad -level compilers). Everything else is/was C.
22:11:15 <hircus> the only scripting language I've seen with TCO
22:11:22 <johnnowak> lua's implementation doesn't suck
22:11:31 <hircus> wli: that better be writing a compiler in C++, not writing a C++ compiler :)
22:11:52 <wli> hircus: The source language was called "Tiger."
22:11:52 <RyanT5000> so anyway after deciding i'd have to write O(number_of_classes) code for serialization in C++, and that i'd have to rewrite the serialization code for a class every time i changed its contents, i decided not to use C++
22:11:54 <RyanT5000> so i used C#
22:12:05 <Trinithis> thunks are just wrappers right? ie: class Thunk {public value;}
22:12:08 <RyanT5000> i (mistakenly) had decided that the lack of reflection was causing my problem
22:12:18 * wli 's head spins from the "serialization" terminology abuse.
22:12:26 <RyanT5000> Trinithis: thunks always refer to some kind of additional level of redirection
22:12:40 <RyanT5000> Trinithis: but i've heard the term used in at least two completely different contexts
22:13:10 <Trinithis> mmk
22:13:24 <RyanT5000> Trinithis: in haskell, they refer (roughly - i'm sure someone in here can clarify better than i) to closures that have yet to be forced
22:13:44 <RyanT5000> (forcing is what you call it when you actually evaluate a lazy datum)
22:13:52 <Modius> Thought you guys may find this amusing (note - this is not language trolling I admire haskell or I wouldn't be replicating its concepts) - I published a Common Lisp lazy-list library that attempts to resemble Haskell (lispishness added where relevant) featuring control over the laziness and trying to facilitate interaction with "normal" code.  I just added a diff-hash, kind of like Haskell's diff-array - a hash-table where each m
22:13:52 <Modius> odification relegates prior versions to delta + reference like diff-array
22:14:34 <hircus> wli: sounds familiar
22:15:03 <Trinithis> RyanT5000: ok
22:15:06 <wli> I would strongly advise against viewing Haskell as the "ultimate language" or representing it as such.
22:15:14 <elaforge> Modius but lisp has that, um, stream processing thing where it does fusiony stuff, right?
22:15:20 <hircus> thunk's used the same way in Scheme stream implementations
22:15:25 <RyanT5000> Trinithis: in C-land, thunks are jump subroutines that take the place of functions that are loaded from shared libraries
22:15:31 <Modius> elaforge:  Where should I look that up?
22:15:52 <elaforge> Modius, cltl2, last time I looked (ages ago)
22:16:12 <elaforge> Modius, I forget exactly what it was called
22:16:16 <RyanT5000> Trinithis: when you use stuff from libc, for example, a bunch of thunks are inserted into your program statically; when libc is actually loaded (at runtime), the thunks are updated to actually point at the libc functions
22:16:30 <RyanT5000> Trinithis: it's a way of making code somewhat location-independent
22:16:37 <RyanT5000> Trinithis: but again, i don't really know the details
22:16:38 <hircus> RyanT5000: so... stubs?
22:16:40 <Trinithis> like a filler until it is known?
22:16:53 <wli> Procedure linkage table entries.
22:16:58 <RyanT5000> hircus: yeah, stubs - but i've definitely seen the term thunk used
22:17:11 <RyanT5000> Trinithis: yeah
22:17:12 <SamB> whoa neat
22:17:24 <SamB> aww...
22:18:06 <sjanssen> Trinithis: wikipedia says a thunk is a delayed computation
22:18:24 * SamB thinks Jhc has a lot of newtype issues :-(
22:18:27 <sjanssen> I'd tend to agree with that definition
22:18:31 <sjanssen> SamB: oh?
22:18:44 <RyanT5000> ah, i suppose that the second definition i gave was wrong
22:18:54 <RyanT5000> it was probably some stupid microsoft thing i read it in
22:19:19 <sjanssen> there's also an alternate defn. on wikipedia that I'm not familiar with
22:19:26 <hircus> RyanT5000: looks like both are used interchangeably
22:19:31 <SamB> sjanssen: well, I could easily be wrong, especially with how much I've mucked with my local tree...
22:19:43 <hircus> googling for thunk stub, the second link is from the Mingw mailing list
22:19:43 <Trinithis> sjanssen: would you know (in general) how they are implemented (for pedagogical purposes)?
22:20:04 <SamB> but right now I'm seeing unexpanded Kleislis where there definately shouldn't be any...
22:20:05 <RyanT5000> Trinithis: that depends a *lot* on the implementation
22:20:20 <sjanssen> Trinithis: sure, I could describe roughly how GHC does it
22:20:37 <Trinithis> please
22:21:21 <SamB> sjanssen: possibly that isn't actually the problem -- perhaps it's just a messed up error message...
22:21:30 <sjanssen> Trinithis: all values are structured roughly as "struct { int tag; byte[] more_data; };"
22:22:37 <Trinithis> byte[] or byte* ?
22:22:37 <RyanT5000> is there a useful subset of haskell that doesn't require GC?
22:22:40 <SamB> note, this is a very loose paraphrase of GHC's object layout
22:22:44 <RyanT5000> (or could one be made?)
22:22:48 <Trinithis> nvm
22:22:52 <wli> The GOT is initialized with the PLT's entry for the function. The PLT entry is code to save the arguments and invoke the runtime linker, which upon resolving the symbol updates the GOT's entry. The external procedure is invoked by loading the function's code address from the GOT and performing an indirect call to it.
22:22:59 <sjanssen> Trinithis: whenever it inspects a value, the run time system checks 'tag', if tag has a certain value, it executes the code in more_data
22:23:27 <johnnowak> RyanT5000: you might look at linear lisp for a functional language that doesn't require gc
22:23:29 <SamB> sjanssen: wow that's a loose paraphrase
22:23:39 <sjanssen> Trinithis: neither actually, more_data is just a bunch of bytes of variable length stuck to the end of the struct
22:23:51 <Trinithis> ah
22:23:56 <RyanT5000> johnnowak: cool; i'm asking because i'd like my language to implement its own GC
22:23:57 <elaforge> there was a scheme that put everything on the stack too...
22:24:05 <RyanT5000> i don't want to end up with a ridiculous mess like GHC's GC
22:24:20 <johnnowak> RyanT5000: well you'd not need one in a linear language; no garbage is ever created
22:24:21 <SamB> actually that doesn't look much like GHC's heap layout at all
22:24:25 <sjanssen> Trinithis: oh, and the code in more_data will also update the struct, changing the tag and more_data
22:24:28 <wli> Try mlton.
22:24:30 <SamB> @go ghc commentary heap layout
22:24:31 <lambdabot> No Result Found.
22:24:34 <SamB> @go ghc commentary
22:24:36 <lambdabot> http://hackage.haskell.org/trac/ghc/wiki/Commentary
22:24:36 <lambdabot> Title: Commentary - GHC - Trac
22:24:51 <SamB> now I go sleep
22:24:54 <RyanT5000> johnnowak: cool; i'd need to make it strong enough to *implement* a GC, though
22:25:03 <Trinithis> sjanssen: thx
22:25:07 <wli> RyanT5000: cf. mlton
22:25:13 <RyanT5000> wli: thanks
22:25:21 <sjanssen> Trinithis: if tag is set to some value other than the priviledged "thunk" value, then more_data is already evaluated
22:25:31 <hircus> johnnowak: neat. thanks
22:25:35 <sjanssen> SamB: yes, I took some liberty.  No need to describe all the other stuff
22:25:57 <Trinithis> RyanT5000: what's your language like?
22:26:52 <wli> RyanT5000: That's based on region analysis (something like lifetime analysis?), which AIUI does some kind of inference to determine how/when to stack allocate as opposed to heap allocate.
22:26:59 <RyanT5000> Trinithis: i'm trying to make a language with ease-of-metaprogramming exceeding that of scheme while feeling much more like haskell
22:27:24 <RyanT5000> wli: sounds cool; i'll definitely take a look at it when i'm implementing my GC
22:27:47 <johnnowak> RyanT5000: have you looked at liskell?
22:28:01 <RyanT5000> johnnowak: lol no; i'll look at that right now, it sounds cool
22:28:08 <johnnowak> it's exactly what it sounds like
22:28:11 <glguy_> if it's all region inferenced stack allocation, you wouldn't really need a GC, would you?
22:28:46 <sjanssen> glguy_: I doubt you can region inference everything -- I'd wager one can reduce it to the halting problem
22:28:47 <dobblego> if you were to write quickcheck properties for map, would you write more than the identity and composition laws on a functor?
22:29:07 <wli> RyanT5000: There's also C. Barry Jay's fISH, which did something called "shape analysis," which inferred sizes of various kinds of objects (e.g. lengths of lists) and did things like using arrays for things written in (ocaml-like) code as lists.
22:29:11 <johnnowak> sjanssen: you can if you make your regions large enough.
22:29:39 <RyanT5000> glguy_: my *complete* language will require a GC, but i want to create a reasonably-similar subset of my language that does not, and use it to implement the GC
22:29:43 <wli> RyanT5000: In such a manner groups of objects allocated together can be aggregated, in addition to various other things.
22:29:57 <sjanssen> johnnowak: well, if your regions become "the entire life of the program", you may run into some trouble :)
22:30:25 <johnnowak> sjanssen: pfft, mere details
22:30:50 <RyanT5000> glguy_: particularly, i want that subset to support all the metaprogramming features that the full language supports - which means it might require a GC at compile time, but not at runtime
22:30:52 <sjanssen> dobblego: properties for map, or fmap?
22:30:52 <dons> there are some pathological cases, didn't Appel address that in the mixed GC/region paper?
22:31:01 <dobblego> sjanssen, map, specifically
22:31:15 <dolio> From what little I've read, people haven't gotten region inference to perform a much better than GC in practice. But it's pretty cool, of course.
22:31:20 <johnnowak> RyanT5000: what's the advantage of implementing the gc within the language?
22:31:21 <dobblego> sjanssen, I can think of many properties, but are they implied by the functor laws?
22:31:26 <sjanssen> dobblego: I'd also do \f xs -> length (map f xs) == length xs
22:31:35 <dobblego> sjanssen, ok, I was thinking that too
22:31:36 <RyanT5000> johnnowak: i want it to be parameterizable
22:31:39 <sjanssen> dobblego: just for paranoia's sake
22:31:41 <RyanT5000> johnnowak: er, parameterized
22:31:50 <dobblego> sjanssen, thanks
22:31:54 <johnnowak> RyanT5000: it can be parameterizable without that... most are
22:32:46 <allbery_b> http://maradydd.livejournal.com/293666.html (Haskell
22:32:51 <allbery_b> 's toward the bottom)
22:33:07 <RyanT5000> johnnowak: *very seriously* parameterizable - like i want you to be able to provide alternative heap walking algorithms (if appropriate)
22:33:36 <johnnowak> you can have pluggable collectors as well
22:33:58 <johnnowak> so it still doesn't need to be in the language itself
22:34:16 <RyanT5000> johnnowak: meta-circularity is a pretty important goal to me
22:34:41 <RyanT5000> johnnowak: obviously there will be a GC in the bootstrap interpreter that's written in another language
22:35:05 <RyanT5000> johnnowak: (since, as i said, you can have the full benefit of the GC at compile-time, even in the subset of the language that doesn't require a GC at runtime)
22:35:05 <Trinithis> RyanT5000: devise your own revolutionary GC
22:35:06 <johnnowak> well I suppose i can't argue with that if it's a goal in and of itself
22:35:41 <Trinithis> meta-circularity?
22:35:45 <RyanT5000> johnnowak: well, i wouldn't say it's a fundamental goal, but in my experience meta-circularity is almost always a good thing
22:36:55 <RyanT5000> Trinithis: loosely, meta-circularity is the ability of a language to interpret itself easily
22:37:09 <RyanT5000> is there a good, uber-formal definition of meta-circularity?
22:37:34 <johnnowak> i like the wikipedia definition
22:37:44 <johnnowak> "A meta-circular evaluator is a special case of a self-interpreter in which the existing facilities of the parent interpreter are directly applied to the source code being interpreted, without any need for additional parsing. Meta-circular evaluation is only possible in homoiconic languages."
22:38:13 <RyanT5000> yeah, sounds good, but "directly" is not that precise
22:38:19 <Trinithis> johnnowak: where on wikipedia. i cant find it
22:39:16 <RyanT5000> anyway, even though C++ can (obviously) be used to make a C++ compiler, C++ isn't thought of as meta-circular, because you get virtually no advantage from the fact that the interpreting language is the same as the interpreted language
22:39:19 <sjanssen> allbery_b: the OCaml one is quite funny
22:39:47 <RyanT5000> johnnowak: the liskell stuff seems dead
22:40:06 <johnnowak> RyanT5000: yep.
22:40:21 <RyanT5000> johnnowak: also, i'm much more limited in my definition of the "spirit" of haskell
22:40:29 <johnnowak> "joy in joy" is a good example of metacircularity: http://www.latrobe.edu.au/philosophy/phimvt/joy/jp-joyjoy.html
22:40:32 <lambdabot> Title: A Joy interpreter written in Joy
22:41:43 <RyanT5000> johnnowak: i still haven't decided anything about how my type system will be, for example, and i don't consider that decision to be central to the haskelliness of my language
22:42:02 <elaforge> oh, wiktionary has a def for "combinator"
22:42:04 <RyanT5000> johnnowak: i *definitely* will not be using haskell's class system
22:42:08 <elaforge> never actually seen that defined
22:42:37 <johnnowak> RyanT5000: why?
22:42:48 <RyanT5000> johnnowak: i'm aiming for compile-time referential transparency
22:42:50 <elaforge> "a function that takes functions... which are also combinators"... sounds like it never bottoms out
22:43:01 <RyanT5000> johnnowak: the last thing i need is a bunch of implicit compile-time parameters ruining that
22:43:12 <johnnowak> combinator == higher order function
22:43:15 <RyanT5000> (note: i have not yet completely decided what that means)
22:43:37 <RyanT5000> it's definitely required that +, for example, be polymorphic in some way
22:43:38 <johnnowak> RyanT5000: have you any plans for an alternative?
22:43:48 <RyanT5000> johnnowak: not really
22:43:49 <elaforge> but I think of it as something used in a point-free kinda way, defining a kind of DSL
22:43:54 <elaforge> like I wouldn't think of map as a combinator
22:44:19 <RyanT5000> johnnowak: it would be acceptable for classes to be implemented as a standard library of macros
22:44:20 <wli> elaforge: It has no free variables.
22:44:21 <johnnowak> elaforge: the concatenative community tends to call all of their HOFs combinators.. which incidentally are all pointfree
22:44:51 <RyanT5000> johnnowak: but instances *must* be scoped, and there *must* be a way to alter the instance environment in which a callee runs
22:45:01 <RyanT5000> johnnowak: those are my two huge problems with haskell's system
22:45:21 <elaforge> wli, ah yes I read that too... but any function which can be applied must not have free variables, right?
22:45:34 <elaforge> johnnowak, who is "concatenative"?
22:45:46 <johnnowak> my biggest problem with type classes is simply that there's no way of understanding what it does except in terms of types; there's no dynamic semantics
22:46:14 <RyanT5000> johnnowak: another huge problem; i've tentatively listed as an axiom of my language that values may never depend on types
22:46:44 <johnnowak> elaforge: generally speaking, languages in which concatenation denotes function composition... more usefully, it generally means stack-based pointfree languages
22:46:48 <elaforge> oh, you should make it mandatory that values depend on types
22:46:54 <elaforge> then you could call it a co-dependent type system
22:46:56 <RyanT5000> johnnowak: which conveniently allows me to dispense with the type system for the purposes of my bootstrap interpreter, provided it is only given input that actually does type-check
22:47:06 <johnnowak> RyanT5000: indeed
22:47:34 <RyanT5000> johnnowak: and since _*I'm*_ programming it, of *course* it will be well-typed :P
22:47:37 <elaforge> johnnowak, ah so the syntax and the kind of thinking that syntax encourages
22:47:39 <RyanT5000> lol
22:48:21 <johnnowak> ertai_: it's more than syntax. if all you have is composition, all of your functions need to be unary. the unary functions tend to be of the type Stack -> Stack. this gives you a very nice way of having functions produce and consume multiple values
22:48:50 <elaforge> johnnowak I see, so only composition, no application
22:48:57 <johnnowak> exactly
22:49:15 <RyanT5000> wow, that sounds kind of annoying
22:49:19 <RyanT5000> i mean, i love point-free
22:49:25 <elaforge> ummm.. joy?  does forth count?  there was a language called FPL or something
22:49:28 <RyanT5000> it's always a great puzzle, and it's often more expressive (imho)
22:49:39 <johnnowak> point-free in haskell is much uglier than in concatenative languages
22:49:39 <RyanT5000> but being required to use it would be rather limiting, i think
22:49:41 <elaforge> a pedagical one in a functional programming book
22:49:58 <johnnowak> RyanT5000: most concatenative languages offer sugar that translates lambdas and let expressions to pointfree code
22:50:06 <johnnowak> for the rare cases where you need it
22:50:08 <RyanT5000> johnnowak: oh, well that's probably alright then
22:50:15 <RyanT5000> johnnowak: but i hate sugar
22:50:21 <johnnowak> elaforge: it's debatable if forth counts
22:50:32 <sjanssen> RyanT5000: what is wrong with sugar?
22:50:32 <RyanT5000> johnnowak: either give me macros or give me kernel-syntax, or give me death
22:50:34 <johnnowak> RyanT5000: they're typically macros
22:50:52 <RyanT5000> sjanssen: they're built-in macros; i hate built-in
22:51:03 <RyanT5000> sjanssen: my language isn't going to have built-in integers :P
22:51:14 <RyanT5000> (they'll be a standard library)
22:51:27 <RyanT5000> and i'm pretty sure i've worked out how to make that not be annoying
22:51:31 <elaforge> no literals, huh?
22:51:42 <sjanssen> RyanT5000: actually implemented in the language?
22:51:48 <johnnowak> RyanT5000: at some point, removing things makes things more complicated as you need to support additional mechanisms for extension.
22:52:08 <RyanT5000> elaforge: there'll be pair literals and symbol literals
22:52:17 <RyanT5000> elaforge: basically, quoted code
22:52:30 <RyanT5000> sjanssen: they'll have some FFI stuff going on to get a binary representation
22:52:41 <johnnowak> elaforge: as for FPL, you're probably thinking of FP and FL... backus's work. they're not concatenative as there are other "functional forms" besides composition and quotation... although they're certainly related
22:52:44 <RyanT5000> sjanssen: FFI-like, i should say
22:53:00 <johnnowak> elaforge: to be honest, the definition of "concatenative" is quite fuzzy still
22:53:34 <RyanT5000> sjanssen: but it'll be entirely possible to not import integers and thereby lack all ability to deal with them
22:53:55 <lament> useful!
22:54:03 <lament> it's very useful not to have that ability. Somehow.
22:54:16 <sjanssen> I don't think that is useful at all
22:54:39 <elaforge> johnnowak interesting.  is there a stand-out "main" representative?
22:54:44 <RyanT5000> it's useful because you'll be able to substitute modules with equivalent interfaces
22:54:46 <johnnowak> elaforge: joy
22:54:51 <sjanssen> it isn't a bad thing either, but I don't see much of a use
22:55:08 <vegai> elaforge: factor
22:55:14 <RyanT5000> sjanssen: it's mostly useful because it forces me to make the library system extremely powerful
22:55:41 <RyanT5000> sjanssen: i've always found the set of built in types to be exceedingly arbitrary
22:55:48 <johnnowak> vegai: joy is probably a better example than factor; factor has parse words, macros, it's functional, etc
22:55:53 <elaforge> johnnowak thanks, I'll take a look
22:55:53 <johnnowak> it's -> ISN'T
22:56:10 <Trinithis> know what you are going to call the language?
22:56:26 <vegai> the definition of "functional" is quite fuzzy still...
22:56:35 <RyanT5000> sjanssen: at best, the ideal set of builtin types is implementation-dependent (what's your platform's word size? cache line size? is it a uniform memory architecture?)
22:56:48 <johnnowak> vegai: more clearly, factor relies heavily on shared mutable state
22:57:17 <RyanT5000> sjanssen: at worst, it's a completely meaningless decision made by the implementers ("i think bools would be good; and 32-bit integers; and 16-bit integers; and arrays; and lists")
22:57:25 <Twey> johnnowak: Heh, bad mistake to make
22:57:39 <vegai> johnnowak: heavily? Not the most factor code I've seen
22:57:52 <RyanT5000> sjanssen: this is one of my many "wearing the hair shirt" things
22:57:53 <Twey> 'Would you like to sell your house?' 'Yes!  Uh, I mean, no, no!'
22:57:58 <johnnowak> vegai: well this is #haskell. it's heavy, relatively speaking. :)
23:00:03 <vegai> ok :)
23:00:16 <RyanT5000> sjanssen: loosely speaking, literals in my language will all be governed by a macro system
23:00:16 <bockmabe_> Hey, are there priority queue implementations that are part of standard haskell libraries these days?
23:00:44 <vegai> johnnowak: I'd guess their code is about as functional as typical scheme code
23:00:56 <sjanssen> bockmabe_: not in the standard library, but I recommend you check out edison
23:01:13 <johnnowak> vegai: that's not an unreasonable generalization
23:01:29 <sjanssen> bockmabe_: and Data.Map can be used as a priority queue, in a pinch
23:02:50 <dolio> Not even directly, really, though.
23:03:03 <dolio> Assuming you might want multiple values with the same priority.
23:03:13 <bockmabe_> sjanssen, I'm actually reading through the "genuine sieve" paper, and the author makes use of Data.Map, but then goes to a Priority Queue implementation as the next optimization
23:03:44 <bockmabe_> cites another paper for the implemenation :-)
23:04:04 <bockmabe_> I figured there might be a more standard way to do it now.
23:04:04 <RyanT5000> sjanssen: if you got a chance to read all the stuff i just said, let me know if it sounds reasonable - although i was arguing against you, i'm interested in your opinion now that you know more of my thought process
23:04:15 <RyanT5000> sjanssen: if you don't feel like reading it, that's ok too :)
23:04:29 <johnnowak> RyanT5000: you don't sound unreasonable.
23:04:38 <sjanssen> RyanT5000: perfectly reasonable
23:04:42 <johnnowak> not not reasonable
23:04:43 <RyanT5000> cool, thanks :)
23:04:45 <RyanT5000> lol
23:05:05 <RyanT5000> also, the issue of literals relates to the fact that my language will not specify an input format
23:05:05 <sjanssen> I just challenge the idea that not importing integer support could ever be "useful" :P
23:05:07 <johnnowak> just don't kill yourself with such things if there are more interesting problems to tackle
23:05:21 <Twey> Heh
23:06:02 <sjanssen> but the ability to do that is nice
23:06:35 <RyanT5000> i want the core of my language to be ridiculously simple
23:07:08 <RyanT5000> maybe not as simple as brainfuck, but far simpler than r6rs or the haskell report or whatever
23:07:19 <wli> I'd go the other direction and try to incorporate CAS -related functionality into the language.
23:07:32 <RyanT5000> wli: CAS
23:07:33 <RyanT5000> ?
23:07:47 <johnnowak> RyanT5000: if you want simple, look at joy. it's simpler than brainfuck.
23:07:58 <RyanT5000> johnnowak: hm, i will
23:08:01 <wli> RyanT5000: Computer Algebra System
23:08:02 <johnnowak> RyanT5000: also look at http://tunes.org/~iepos/joy.html
23:08:05 <lambdabot> Title: The Theory of Concatenative Combinators
23:08:48 <wli> RyanT5000: Basically so the type system can represent things like finite fields completely statically.
23:08:54 <wli> Or other such things.
23:09:15 <wli> RyanT5000: Also library design work so things like the numeric hierarchy are faithful to the mathematics.
23:09:36 <RyanT5000> wli: i still haven't decided on my type system, but i've decided that it won't influence program value, so i can proceed with my interpreter using a tagged system
23:09:58 <RyanT5000> that rules out ad-hoc polymorphism and such, but i'm hoping i can replace those with macro systems
23:10:04 <RyanT5000> or something else exotic i think up
23:10:27 <sjanssen> RyanT5000: "won't influence program value"?
23:10:35 <johnnowak> RyanT5000: it doesn't rule out ad hoc polymorphism
23:10:47 <johnnowak> RyanT5000: see http://citeseer.ist.psu.edu/172417.html
23:10:47 <lambdabot> Title: A Second Look at Overloading - Odersky, Wadler, Wehr (ResearchIndex)
23:10:56 <wli> There are definitely things I'd leave out vs. Haskell, e.g. implicit parameters and other things that have long-since been decided to be cruft.
23:11:01 <RyanT5000> sjanssen: there will never be any time to transfer information from the type system to a value
23:11:15 <RyanT5000> sjanssen: so, for example, class Typeable couldn't exist
23:11:23 <sjanssen> RyanT5000: doesn't that remove most of the useful benefits of a type system?
23:11:42 <RyanT5000> sjanssen: there's still type-checking and efficiency
23:11:44 <sjanssen> RyanT5000: it goes further -- Haskell-like class overloading also becomes impossible
23:12:07 <RyanT5000> sjanssen: yeah; i haven't completely decided how to deal with that yet
23:12:45 <johnnowak> RyanT5000: read that paper!
23:13:03 <RyanT5000> johnnowak: thanks :)
23:14:22 <johnnowak> you may also want to look at http://www.cse.unsw.edu.au/~chak/papers/mtc-tr-long.pdf
23:14:24 <RyanT5000> sjanssen: it might be possible to add an overloading system to my language as a macro library
23:15:07 <johnnowak> RyanT5000: the first problem that comes into my mind when considering doing it with macros is that type error reporting becomes difficult to do in a way that helps the programmer
23:15:11 <RyanT5000> sjanssen: or there might be some parallel type system - e.g.: variables might have a set of classes to which they belong, with some sort of propagation system
23:15:42 <sjanssen> to borrow a phrase: "you have a problem, and you decide to use macros.  Now you have two problems" :)
23:15:43 <RyanT5000> johnnowak: error reporting seems ridiculously hard to do no matter what; i'll probably have to add some system for extending the error reporting system
23:16:00 <RyanT5000> sjanssen: lol yeah; but it seems to be working out OK for the lisp people
23:16:03 <johnnowak> sjanssen: that's just something wimps say!
23:16:16 <RyanT5000> sjanssen: my goal is really just to kick as much stuff as possible out of the language kernel
23:16:31 <RyanT5000> sjanssen: i'm actually going to write a meta-specification for this language
23:16:35 <johnnowak> RyanT5000: i think you'll find that, when dealing with truly minimal languages, the type system ends up resulting in 95% of the complexity
23:16:51 <RyanT5000> johnnowak: that sounds about right :)
23:17:12 <johnnowak> if you truly want a minimal system, you may consider making the type system a separate component and using softer types
23:17:22 <RyanT5000> johnnowak: i'm definitely considering that sort of thing
23:17:31 <johnnowak> that's the approach i'm taking anyway
23:18:17 <RyanT5000> johnnowak: it'll be possible to build a complete interpreter of my kernel language with absolutely no static type system
23:18:26 <johnnowak> just keep in mind, of course, that soft type systems tend to be quite complex unless the language is very carefully designed to integrate with them
23:18:42 <runar> > []
23:19:01 <Cale> @bot
23:19:04 <RyanT5000> johnnowak: yeah; i love them as an idea, and i think they're necessary to some of the stuff i want to do (dependent types), but they do seem like a lot of trouble
23:19:08 <runar> :-(
23:19:09 <Cale> hmm
23:19:17 <johnnowak> RyanT5000: that's not a particularly unique feature though, is it? a straightfoward HM language would have the same property
23:19:26 <RyanT5000> HM?
23:19:31 <johnnowak> hindley-milner
23:19:39 <runar> her majesty?
23:19:40 <RyanT5000> what feature?
23:19:42 <RyanT5000> lol
23:19:49 <RyanT5000> HMR Type System
23:19:59 <johnnowak> the feature of not requiring types to make sense of the problem
23:20:03 <johnnowak> problem->program
23:20:07 <RyanT5000> "it's provably unsinkable!"
23:20:15 <RyanT5000> er, HMS
23:20:24 <runar> HML Haskell
23:20:32 <runar> Her majesty's language
23:20:35 <RyanT5000> johnnowak: oh, yeah, it's not a particularly unique feature
23:21:02 <runar> Purveyors of lambdas, by order of HRM the queen.
23:21:30 <RyanT5000> johnnowak: it's just a really *useful* one; but since i don't want to have to have a separate + for each type of integer, it gives me some trouble
23:21:40 <RyanT5000> (which are presumably obviated to some extent by that paper you linked me to :P)
23:21:45 <RyanT5000> (which i am currently reading)
23:22:07 <johnnowak> RyanT5000: have you looked at FL at all? it's another example of a very simple and clear language (simple/clear and minimal are not the same thing of course) that's quite interesting
23:22:28 <RyanT5000> it sounds familiar
23:23:04 <RyanT5000> ew, i hate exceptions
23:23:07 <RyanT5000> :P
23:23:32 <johnnowak> you can avoid having a separate + for each type of number via simple parameterized types... + :: Num a -> Num a -> Num a, and just restrict instantiations to Num Int, Num Float, etc.
23:23:48 <johnnowak> you don't need a full class system
23:23:52 <RyanT5000> johnnowak: yeah
23:24:10 <johnnowak> of course you won't be able to extend the system with new numbers through the language itself
23:24:34 <RyanT5000> johnnowak: well that's unacceptable, but i'm sure i'll come up with a solution at some point
23:24:47 <johnnowak> and it makes sense that FL has exceptions; it has no type system, so you need to catch type errors somehow... :)
23:25:10 <johnnowak> or it has a type system, but not one the user needs to care about i should say
23:25:35 <RyanT5000> johnnowak: yeah, that's fine; then it's just another return type with some sugar to cause it to cause the caller to also return unless further sugar is used
23:25:53 <RyanT5000> really i'm ok with exceptions so long as they're integrated into the type system
23:26:12 <johnnowak> aye
23:26:15 <RyanT5000> haskell's exception system is approximately the worst i can imagine
23:26:31 <RyanT5000> algebraic types are just not meant for that sort of thing
23:26:52 <quicksilver> you don't have a very good imagination then
23:26:58 <quicksilver> I can imagine far worse :P
23:27:18 <quicksilver> (without claiming the haskel system is perfect by any means)
23:27:30 <RyanT5000> quicksilver: ok ok, i'm sure i could think up some malbodge-esque stuff, but i think haskell's system may be the worst i *have* imagined
23:27:53 <sjanssen> C's exception system is worse
23:27:57 <dolio> How about C's exception system?
23:28:04 <sjanssen> dolio: high five!
23:28:10 <dolio> Heh.
23:28:11 <johnnowak> pfft
23:28:12 <vegai> respect the longjmp!
23:28:13 <RyanT5000> do you guys mean longjmps, or "nothing"
23:28:16 <BMeph> How about VB's one? ;p
23:28:24 <RyanT5000> i don't know VB at all
23:28:33 <BMeph> Count yourself lucky. ;)
23:28:35 <RyanT5000> i'm totally ok with the "nothing" exception-throwing system
23:28:42 <dolio> I mean, checking global error code variables.
23:28:48 <sjanssen> RyanT5000: errno, checking return codes, etc.
23:28:53 <BMeph> ON ERROR GOTO 10
23:29:07 <RyanT5000> dolio: oh yeah, that's about as bad as haskell's system
23:29:10 * Twey giggles madly.
23:29:11 <RyanT5000> i'm not sure it's worse
23:29:14 <wli> Thread-local, vaguely, but anyhow.
23:29:28 <RyanT5000> it's very difficult to assign a total ordering of goodness to exception systems :P
23:29:43 <johnnowak> RyanT5000: you need a separate mechanism in the type/effect system to handle exceptions properly
23:30:00 <RyanT5000> johnnowak: yeah; i've had to implement some (trivial) ones of those for classes
23:34:11 <RyanT5000> fantastic
23:34:19 <RyanT5000> i've been offline for 4.5 minutes
23:34:29 <johnnowak> no one said anything.
23:34:33 <RyanT5000> cool
23:37:06 <johnnowak> RyanT5000: where do you attend school?
23:37:11 <RyanT5000> harvard
23:38:47 <jberryman> I'm working through the 'Haskell School of Expression' book which is about 8 yrs old. Anything I should watch out for that might be outdated?
23:38:56 <jberryman> I assumed the fundamentals would all be fine
23:39:27 <RyanT5000> jberryman: as long as it's got monads and not that crazy lazy list stuff that was used way back in the day
23:39:37 <RyanT5000> (i'm probably not the most qualified to tell you, though)
23:39:39 <nornagon> did do notation exist back then?
23:39:53 <sjanssen> nornagon: yes
23:39:55 <jberryman> nornagon: yeah
23:40:17 <sjanssen> nornagon: the current language spec was published in 1998, so all the language information should be fine
23:40:27 <sjanssen> erm, s/nornagon/jberryman
23:41:15 <nornagon> oh, right. duh :)
23:41:37 <jberryman> right, thanks. I'm having fun learning haskell and lurking around here.
23:41:46 <jberryman> bed time
23:45:34 <Twey> Is there a new spec intended any time soon?
23:45:46 <nornagon> there's haskell'
23:45:54 <nornagon> but trends show it will never be finalised.
23:46:58 <Twey> Oh :-P
23:47:07 <lament> its page says it's becoming more active
23:47:11 <lament> by the decade
23:47:14 <Twey> Hahaha
23:47:29 <lament> (actually, last month)
23:48:14 <RyanT5000> johnnowak: one interesting thing that i'm changing from lisp is that i'm parsing everything left-associative, so i have reverse lists terminated with their operator rather than forward lists terminated with nil
23:48:21 <lament> they have some bizarre list of implemented features which itself is a haskell program
23:48:47 <RyanT5000> johnnowak: combined with my restriction that the meaning of a particular pair/cons may never depend on the meaning of its parent, this makes for some interesting situations
23:48:57 <johnnowak> RyanT5000: such as?
23:49:11 <johnnowak> er -- and i don't understand your restriction
23:49:22 <Twey> RyanT5000: Er... all lists are reversed?
23:49:46 <RyanT5000> johnnowak: e.g.: it's possible for me to construct a macro 'letrec' such that (letrec x (f x) x) works
23:50:20 <lament> what does that mean?
23:50:20 <RyanT5000> (the first x is the binding, the second and third x are identifiers that refer to it)
23:50:46 <lament> so you define x to be (f x) and then return it?
23:50:48 <RyanT5000> but i can't construct it such that (letrec (f x) x x) works (flipping the first and second)
23:50:50 <RyanT5000> yeah
23:51:28 <RyanT5000> the reason i can't construct the latter is because the macro's required to complete its processing of (f x) before it's allowed to see the next x
23:51:29 <johnnowak> RyanT5000: why can't you have it work the other way?
23:51:42 <johnnowak> why?
23:51:55 <RyanT5000> well, the source tree is built like this
23:52:18 <RyanT5000> (((letrec, (f, x)), x), x)
23:52:37 <johnnowak> isn't the whole point of sexprs that the source tree looks like the source code?
23:53:05 <RyanT5000> yeah, mine does - it just looks like curried haskell source code rather than varargs lisp source code
23:53:25 <RyanT5000> the differenc eis
23:53:26 <johnnowak> you don't need to do that to have currying
23:53:39 <RyanT5000> i know you don't *need* to
23:53:42 <johnnowak> and currying makes no sense with respect to special forms
23:53:57 <RyanT5000> why not?
23:54:15 <RyanT5000> it seems perfectly doable to me
23:54:26 <johnnowak> it seems useless and complicated to be honest
23:54:42 <johnnowak> but i'm open to seeing a use case
23:54:58 <RyanT5000> i'm not sure yet; i'm thinking it might be interesting when you have macros combining other macros
23:55:33 <RyanT5000> but mostly i didn't want the "meaning" of a cons cell to depend on its parent
23:55:52 <johnnowak> that's the whole point though of special forms
23:56:07 <RyanT5000> really?
23:56:14 <RyanT5000> how so?
23:56:23 <runar> head (reverse xs ++ [x]) = head (reverse xs)
23:56:37 <johnnowak> in (let ((foo 5)) foo), FOO is not being applied to 5.
23:56:54 <RyanT5000> oh true, but that's different
23:56:55 <runar> seems like it should hold for all partial lists xs. But how to prove this?
23:56:57 <johnnowak> in (require (lib "32.ss" "srfi")), "lib" is not a precedure
23:57:03 <johnnowak> *procedure
23:57:25 <RyanT5000> the difference there is that you've got things which are being provided as source to the special form
23:57:43 <johnnowak> well do macros not have access to the source tree?
23:57:52 <RyanT5000> i don't want them to have access to the entire one
23:57:56 <RyanT5000> just the ones of their arguments
23:58:21 <johnnowak> so you can't even have LET then
23:58:27 <RyanT5000> why not?
23:58:47 <RyanT5000> (let _ _ _) gets full access to all those 3 source trees
23:58:54 <RyanT5000> it can traverse them, manipulate them, whatever
23:59:04 <RyanT5000> it just doesn't get to know what's above it in the source tree
23:59:14 <johnnowak> oh!
23:59:19 <johnnowak> you do realize that's already the case, yes?
23:59:23 <RyanT5000> yes
23:59:31 <johnnowak> so i'm confused then as to your restriction
23:59:49 <RyanT5000> my restriction is pretty mundane - it's the same one as lisp
23:59:50 <RyanT5000> but
23:59:55 <RyanT5000> since my tree is tipped the other way
23:59:58 <RyanT5000> it has interesting ramifications
