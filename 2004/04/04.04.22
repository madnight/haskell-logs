00:45:32 <Spark> hmm
02:46:35 <Spark> moosefish
03:22:56 <ski> Spark : you there ?
03:23:25 <Spark> tyo
03:23:29 <Spark> i mean "yes"
03:23:31 <Spark> :)
03:23:39 <ski> ok :)
03:23:43 <ski> hmm
03:24:16 <ski> well, first perhaps : have you looked more at the poly. type inf. paper ?
03:24:28 <Spark> is that the putting.ps one
03:24:39 <Spark> or the one with the 3 page long inferred type
03:25:02 <ski> yes, the latter one
03:25:40 <ski> anyway
03:26:14 <ski> i've read through your paper mostly now ..
03:26:40 <ski> .. and have some questions and comments
03:26:47 <Spark> shoot
03:26:56 <ski> ok
03:27:22 <ski> i'm going to look through the pages again and comment on what i thought about
03:27:35 <Spark> theres a lot of referencs (both explicit and implicit) to chapters i havent written yet :)
03:27:39 <Spark> download it again, ive changed it a bit
03:27:45 <Spark> just typos and a conclusion
03:27:52 <ski> hmm, ok
03:28:03 <Spark> http://info.dcs.warwick.ac.uk/~spark/3yp/writeup.ps
03:28:20 <ski> yah
03:28:39 <Spark> theres some other stuff in the dir below on the same project
03:28:56 <ski> typo on page 1 "(e.g. the set the values"
03:30:18 <ski> also, i guess "the Asylum" is the environment/application in which these connections and stuff are being made/modelled
03:30:18 <Spark> cheers
03:30:20 <ski> is that right ?
03:30:26 <Spark> thats right yeah
03:30:29 <ski> ok
03:30:39 <Spark> the asylum itself is a C API
03:30:46 <ski> mm
03:30:56 <Spark> the hive is another wrapper around it, and 'shiver' is a shell around that
03:30:57 <ski> prolly a gui one, i suspect
03:31:07 <ski> mhmm
03:31:17 <Spark> a gui would be ideal but i never implemented it :)
03:31:28 <ski> ok
03:31:53 <ski> what's an oracle, and an advice ?
03:32:08 <ski> it seems to be sources and targets of data (flow)
03:32:09 <Spark> http://info.dcs.warwick.ac.uk/~spark/3yp/presentation/img9.html
03:32:24 <Spark> theyre like pins on a chip
03:32:42 <Spark> each instance of an entity is like a chip
03:33:00 <ski> ok, so it's relative to an 'instance', then ?
03:33:11 <Spark> and you link the different oracles and advice of the various instances together to make it all work
03:33:30 <Spark> they're defined in the entity (class)
03:33:41 <Spark> but all the instances have them as well of course
03:34:14 <Spark> and the types can differ between instances (although the principal type is the same)
03:34:27 <ski> the 'entity' seems to be like a function (definition) to me, while the 'instance' is like a particular call of that function
03:34:34 <Spark> thats the analogy yeah
03:34:42 <Spark> also like class and instance in OOP
03:35:04 <ski> right (re types can differ between instances)
03:35:14 <ski> a bit, mayhaps
03:35:35 <Spark> i'll say that the idea stemmed from OOP but the implementation looks a lot more like FP :)
03:35:44 <ski> ok :)
03:36:04 <ski> is Eden some earlier implementation of these ideas ?
03:36:16 <Spark> yeah its quite a mature tool, its on sourceforge actually
03:36:23 <ski> ok
03:36:29 <Spark> it has a flat namespace of definitions
03:36:51 <Spark> a definition being of the form "a is f(x,y,z);"
03:37:05 <ski> mhmm
03:37:13 <Spark> its also got a procedural side where you can code your own f
03:37:25 <ski> ok
03:37:39 <Spark> its not as graphically inclined, those verbatim quotes give the game away really :)
03:37:41 <ski> with while-loops and such, i suspect .. :/
03:37:44 <Spark> yeah
03:37:52 <Spark> people use while loops for implementing an animation
03:37:53 <ski> mm
03:38:18 <ski> on page 2
03:38:25 <Spark> and because theres no recursion, you cant do a computation with an unbounded duration
03:38:54 <ski> i was thinking about you not wanting to say Eden has 'dynamic typing'
03:39:03 <ski> mm
03:39:30 <Spark> these environments are much more like spreadsheets than interpreters, nothing is constant
03:39:34 <Spark> you can change the value of pi if you want :)
03:39:42 <ski> can't runtime be said to be whenever new data is calculated from previous one ?
03:39:52 <Spark> yeah thats what i was getting at
03:40:16 <Spark> "when definitive expressions are evaluated, not when definitions are made"
03:40:45 <Spark> calculated might be a more obvious word to describe it, evaluation is more vague
03:40:47 <ski> in particular, what do you mean with '(as we have no notion of "execution")' ?
03:41:03 <Spark> execution implies that you write a script and then execute it
03:41:13 <ski> not in my mind :)
03:41:21 <Spark> how would you define it?
03:41:55 <ski> whenever the program is computing/doing/inferring whatever it's purpose is
03:42:01 <ski> or something like that
03:42:25 <Spark> when the software is active
03:42:34 <Spark> problem is the software is kinda active all the time :)
03:42:51 <ski> where 'the program' is not 'the Asylum' but the particular 'model' used in it (is that correct terminology ?)
03:42:59 <Spark> i guess if you make a big spread sheet with lots of dependency, then change a value and it takes a while to recompute, you could describe that period as 'run time'
03:43:07 <Spark> yeah
03:43:23 <Spark> making the distinction between model activity and environment activity is important actually
03:43:44 <ski> even if there is concurrent definition and evaluation going on, i still think we can discern 'runtime' and 'compiletime'
03:44:18 <ski> no problem if the runtime is fragmented
03:44:23 <Spark> ill ask my supervisor and see what he thinks then :)
03:44:54 <ski> also runtime and compiletime might be both about time and 'place' in such a concurrent setting
03:45:24 <Spark> place?
03:45:28 <ski> yeah
03:45:36 <Spark> position within the model?
03:45:42 <ski> you're defining something in one place of the model
03:45:56 <Spark> ah yeah, while another part is active
03:46:17 <ski> at the same time as it is evaluating something in another part of the model
03:46:23 <ski> yeah
03:46:39 <ski> possible even distributed multi-user models, perhaps
03:46:46 <ski> s/possible/possibly/
03:46:51 <Spark> yeah thats been done
03:46:54 <ski> ok
03:47:12 <Spark> its useful because you can have different human actors playing the roles of different agents in a system
03:47:19 <Spark> and you can model things like train crashes that were due to human error
03:47:29 * ski doesn't really know much about this area, just trying to pick some of it up as i read on ;)
03:47:31 <Spark> each person having a different 'view'
03:47:45 <Spark> :) do you find it interesting?
03:48:02 <ski> mm, yes, actually :)
03:48:11 <Spark> most people that have anything to do with this seem to get attached very quickly :)
03:48:24 * ski smiles
03:48:42 <Spark> i'd recommend toying about with eden, see what you think about its strengths and flaws
03:48:58 <ski> mm, perhaps i'll do that
03:49:24 <ski> ok
03:49:27 <ski> page 3 :)
03:49:56 <Spark> http://www.dcs.warwick.ac.uk/research/modelling/hi/tools/  :)
03:50:12 <ski> ok, thanks
03:50:47 <ski> does the 'array(t)' type have constant element access/lookup ?
03:50:55 <Spark> yeah
03:51:18 <Spark> although that doesnt matter from an EP perspective, the important thing is that its size is dynamic at 'run time'
03:51:27 <ski> hmm
03:51:36 <ski> how's it dynamic ?
03:51:37 <Spark> whether looking up the ith member is O(n) or not doesnt matter as long as its possible :)
03:51:55 <Spark> because you could define an instance that took the previous list and appended an element
03:52:08 <Spark> and if you did a feedback loop it would grow in size
03:52:10 <ski> is there some kind of 'cons' or 'append/concat' operations on them ?
03:52:18 <Spark> yeah there could (should) be
03:52:19 <ski> hmm
03:52:43 <Spark> at the moment theres just arr_compose.c  arr_lookup.c
03:52:56 <Spark> which are equivelant to [] in haskell and [] in C :)
03:53:21 <ski> compose taking two arrays and putting them together to a new one ?
03:53:51 <Spark> nah compose having a dynamic number of oracles (im not sure if this is a good idea yet) and producing an array containing them
03:54:06 <ski> hmm, ok
03:54:11 <Spark> so theres always one empty oracle, if you connect to it, another oracle is made
03:54:28 <ski> so it's basically just an aggregate then ..
03:54:32 <Spark> another option would be specifically spawning a 'i want a list of 4' instance
03:54:46 <Spark> or having an empty list, and allowing people to append stuff to it
03:54:51 <Spark> yeah
03:55:01 <Spark> it allows you to do L is [1,2,3,4]; for example
03:55:05 <ski> hmm, this also reminds me a bit of LabView, actually
03:55:14 <Spark> and change that to L is [1,2,3,4,5];
03:55:23 <ski> ok
03:56:07 <Spark> hmm labview does look quite similar :)
03:56:28 <Spark> its like an interactive dataflow paradigm
03:56:35 <ski> i guess the whole network is changeable, with no apriori stratification into "more changeable" and "less changeable" ..
03:56:59 <Spark> no not really
03:57:06 <ski> yeah, it's a kind of visual (not in microsoft's meaning of the word !) programming language
03:57:07 <Spark> although that might be possible for optimisation reasons
03:57:16 <Spark> use a jit compiler etc
03:57:35 <Spark> yeah i have thought a lot about the role of language in programming, and whether it is suited
03:57:48 <ski> you mean there are parts that are not changable or less changeable ?
03:58:02 <Spark> there are bits you'd expect to change very often
03:58:08 <Spark> and there are bits that the modeller can change
03:58:20 <Spark> like theres an instance that just provides an integer, the modeller can change what integer is provided
03:58:24 <Spark> but its not going to change by itself
03:59:01 <Spark> and since the 'by itself' parts of the model are going to iterate maybe 60 times a second, it might make sense to take advantage of that fact
03:59:04 <ski> but can the human modeller change all instances and connections too ?
03:59:10 <Spark> yeah he can
03:59:34 <Spark> theres as fewer restrictions placed on it as possible
03:59:52 <Spark> (that fact alone causes a lot of impedence mismatch problems when we're integrating this with system libaries)
04:00:01 <ski> and can such a model be put into a new entity, ready to be used for more clarity/overview (and reused) ?
04:00:27 <ski> hmm
04:01:24 <Spark> yeah if what i think you mean is what you do mean :)
04:01:25 <ski> the change by itself parts is then connected to some signal that changes over time by itself (such as temperature or stock market) ?
04:01:36 <ski> heh
04:01:43 <Spark> you can build up a nested hierarchy (which gives you a bit of modularity)
04:01:49 <ski> ok, right
04:01:55 <Spark> yeah thats it
04:02:10 <ski> how's that represented ?
04:02:31 <Spark> the nesting entity is an entity defined in terms of other instances, links, oracles and advice
04:02:32 <ski> with some instance that has no oracles, only an advice ?
04:02:48 <Spark> the predefined entity  is an entity defined with C code, oracles and advice
04:03:05 <ski> (oh, sorry, i was talking about the 'change-by-itself' part :)
04:03:08 <Spark> the only instances that only have advice are things like a literal 3 or "hello world"
04:03:39 <Spark> some instances only have oracles (like the print function in haskell)
04:03:49 <ski> so the temp. couldn't be an entity with only advice ?
04:03:51 <Spark> oh, and predefined instances can have side effects :)
04:03:58 <Spark> no
04:04:01 <Spark> check out this model:
04:04:15 <Spark> http://info.dcs.warwick.ac.uk/~spark/3yp/presentation/img24.html
04:04:16 <ski> Spark : what kind of side-effects ?
04:04:36 <Spark> like printing a line to stdout
04:04:45 <Spark> or changing the contents of a window
04:04:50 <ski> hmm
04:04:55 <Spark> http://info.dcs.warwick.ac.uk/~spark/3yp/presentation/img12.html
04:05:09 <ski> not so fast :)
04:05:19 <Spark> heh :)
04:05:53 <Spark> i get hyperactive very easily, i find it helps me code quickly :)
04:06:03 * ski smiles
04:06:31 <ski> hmm, what's the dotted lines for ?
04:06:56 <Spark> you cant have a cyclic dependency unless its dotted
04:07:09 <ski> but what do they do ?
04:07:19 <ski> delay one cycle or something ?
04:07:23 <Spark> when the clock ticks, those oracles are revaluated
04:07:24 <Spark> yeah sort of
04:07:33 <Spark> stops infinite loops
04:07:33 <ski> or maybe the 'Def' delays ?
04:07:44 <Spark> "Def" introduces a default value to kick the thing off
04:07:53 <Spark> a base case in the iteration
04:08:15 <ski> so the current oracle to Def will be the next advice from it, right ?
04:08:19 <Spark> Def is an entity which advises the default value if the input is undefined, or the input if the input is defined
04:08:26 <ski> hmm, ok
04:08:37 <Spark> the oracle *to* def?
04:08:49 <ski> Def's oracle, yes
04:08:56 <Spark> ah right
04:09:02 <Spark> the oracles are the arrow heads yeah :)
04:09:21 <ski> doesn't every data cycle need at least one Def somewhere, anyway ?
04:09:30 <Spark> yeah thats right
04:09:30 * ski might well be worng about that
04:09:34 <ski> ok
04:09:47 <Spark> normally straight after the instance with dotted lines going into it
04:10:00 <ski> so are there reasons for not putting the delay in Def, then ?
04:10:19 <Spark> theres no delay
04:10:32 <Spark> there is simply new state = f(current state)
04:11:06 <Spark> state in this case is temperature and f is (`+` temp_increase)
04:11:27 <ski> but (in img24.html) the + instance has an oracle which is connected from if's advice
04:11:49 <Spark> yeah that if statement chooses between increasing the temp by 3, or decreasing by 1
04:11:56 <Spark> depending on whether the coil is on
04:12:17 <Spark> that literal 1 going into the & instance is like a power switch
04:12:46 <ski> so that one is definitely intended to be changed during the 'evaluation' of the model ?
04:13:11 <Spark> nah the power switch can be thought of constant, if the modeller is not around
04:13:16 <Spark> whereas the temperature is not
04:13:31 <Spark> also the 3 and the -1 are constant (although they can be changed by the modeller)
04:13:34 <ski> yeah, i meant 'changed by the modeller'
04:13:38 <Spark> yeah thats right then
04:13:39 <ski> hmm
04:13:43 <Spark> the modeller can turn the thing off
04:13:57 <Spark> he can also totally break the model by deleting hte link from 1 to coil_is_on
04:14:34 <ski> i was just thinking that these would perhaps be nice to single out as 'often-changing' and thus being oracles to an entity with this model as definition
04:14:44 <ski> ko
04:14:47 <ski> ok!
04:15:16 <Spark> make the distinction between often changing and modeller changing in the type essay?
04:15:34 <ski> maybe
04:16:04 <ski> this would be something that the modeller decides on, based on particular circumstances, of course
04:16:13 <Spark> hinting at the environment?
04:16:28 <ski> and there could of course be different "degrees" of those, too
04:17:18 <Spark> you could compile that model into about 10 lines of ASM, thats the thing :)
04:17:53 <Spark> obviously you'd ahve to recompile it if you changed any of those literal integers
04:18:00 <ski> (e.g. we might have a big model that just connects the main sub-models/entities and providing them explicitely with the most often changing oracles, the less often changing could be defined inside the smaller entities, and so on ...)
04:18:26 <Spark> yeah i see what you mean
04:18:29 <ski> ok
04:18:41 * ski is not sure a 'Clock me!
04:18:45 <Spark> i think the reuse of models is an important thing to have, and its definitely brand new in this context
04:18:47 <ski> is needed
04:19:49 <Spark> its not an instance as such, its just a hint that we want this to be revaluated on clock ticks
04:20:49 <ski> hmm, maybe there should be some difference between an entity (which might be instantiated in several places, each with different actual oracle and advice data (even types)) and a sub-model, which really just exists in one place, only defined separately, to aid modularity
04:21:05 <Spark> yeah that might be beneficial
04:21:18 <Spark> it might also overcomplicate it a bit
04:21:28 <Spark> its just the special case of 1 instance isnt it
04:21:33 <ski> Spark : yeah, i'm just not sure the system would need it (if constructed in a sufficienty good way :)
04:21:36 <Spark> like theres a 'root' entity and a root isntance
04:21:49 <ski> i'm not sure
04:22:11 <ski> there's a sort of detachment of an entity from it's instance
04:22:42 <ski> with a sub-model, types from it's use could well flow inside into it
04:22:45 <Spark> yeah its hard to understand why you have to put something in an entity to put it in an instance
04:22:56 <ski> you don't have that behaviour with entities, right ?
04:23:03 <Spark> yeah you do
04:23:14 <Spark> there will ahve to be some functioanlity in the UI to trace that
04:23:32 <ski> Spark : hmm, were you agreeing or diagreeing with me ?
04:23:44 <Spark> the external type inferring is present
04:24:10 <ski> s/diagreeing/disagreeing/
04:24:18 <Spark> the detatchment you speak of is a problem with the metaphor
04:24:22 <Spark> thats as i see it anyway
04:24:28 <Spark> its just a usability thintg
04:24:30 <Spark> thing
04:24:38 <ski> hmm
04:24:58 <Spark> you think 'i want to change this instance' but you cant, you have to change the entity :)
04:24:59 <ski> if we have defined some generic (polymorphic) entity
04:25:33 <ski> then we should be allowed to feed an array(integer) to one instance of it, and array(real) to another instance of it, right ?
04:25:53 <ski> hmm
04:25:53 <Spark> yeah
04:25:59 <Spark> you have to make sure it is polymorphic though
04:26:08 <Spark> if its specific to arrays of integers then you cant do that
04:26:14 <Spark> and since you can define its internal nature after instantiating it
04:26:19 <Spark> the types have to flow inwards
04:26:29 <Spark> hmm actually
04:26:29 <ski> so, type info that's surrounding an instance can't easily flow into that instance's entity
04:26:32 <ski> do you agree ?
04:26:48 <Spark> it does flow in
04:27:10 <Spark> in the sense that you cannot link an oracle to an instance inside that entity unless the types match
04:27:17 <ski> Spark : the entity has to be polymorphic for this to be an issue, yes
04:27:20 <Spark> yeah
04:27:31 <Spark> ok we need an example here :)
04:28:25 <Spark> we have a nesting entity with one instance inside it, this instance just re-emits its oracle out as advice
04:28:34 <Spark> the nesting entity also has an oracle and an advice
04:28:35 <Spark> no links yet
04:28:47 <Spark> the nesting entity is instantiated twice in a parent entity
04:29:09 <Spark> so at the moment the types are all *
04:29:29 <Spark> suppose we put a real and an int in the parent entity, connect each to one of the instances
04:29:50 <Spark> now inside that instance, the principal type of that oracle is still *
04:29:51 <ski> but, i meant : just because an instance is fed an array(integer) as oracle (maybe then providing an integer as an advice), this doesn't mean that the *entity* suddenly stops being polymorphic (hindering a potential use of e.g. array(real) with another instance of it), do you agree ?
04:30:12 <Spark> yeah thats right
04:30:16 <Spark> they dont flow in yeah
04:30:53 <ski> right
04:30:55 <ski> ok
04:31:17 <Spark> but if you try and connect the inside to something specific, it will fail, because the type check checks all the instances
04:31:35 <Spark> so it kinda makes sure that the entity *is* polymorphic
04:31:52 <Spark> ie the principal type you put onto that oracle is a supertype or every type that has been imposed upon it
04:31:56 <Spark> s/or/of/
04:32:23 <Spark> ill just build a model in the shell to make sure of that :p
04:33:33 <ski> i'll comment more on *, soon :)
04:34:17 <ski> this "wiring" with possible (delayed/clocked) loops reminds somewhat of FRP/AFRP/Yampa
04:34:36 <Spark> whats that?
04:34:38 <ski> (http://www.haskell.org/yampa/ e.g.)
04:34:53 <ski> FRP = Functional Reactive Programming
04:35:09 <ski> AFRP = ... with Arrows
04:35:12 <Spark> reactive is a word that has been used to describe a model that shapes some input
04:36:48 <Spark> i guess the main problem with Empirical Modelling is in trying to find a programming paradigm that provides what we want
04:36:56 <ski> yeah, i think it (the word reactive) is a bit similar to adaptive/incremental computation, while not being exactly the same (at least not having exactly the same focus)
04:36:59 <Spark> so if there is one lurking about that we havent heard about.. :)
04:37:56 <ski> they have a demo for simulating robots which play soccer :)
04:38:09 <Spark> erm
04:38:41 <ski> reading current position and controlling (independently) the velocities of the two wheels of each robot
04:39:08 <Spark> http://empublic.dcs.warwick.ac.uk/projects/footballTurner2000/doc/lsd.html
04:39:09 <ski> so, they model things that change over time
04:39:26 <Spark> ah not quite the same then
04:39:35 <Spark> is an arrow an entity?
04:39:44 <Lunar^> ski: I would like AFRP with discreet semantinc
04:39:45 <Lunar^> -n
04:39:56 <ski> an arrow is something a bit similar to a monad
04:40:01 <ski> :)
04:40:27 <kosmikus> Lunar^: that might be very nice
04:40:32 <ski> Lunar^ : synchronous ?
04:40:51 <ski> Lunar^ : or at least same-size timne-quantas ?
04:41:09 <ski> Lunar^ : or different-sized ones, depending on events and stuff ?
04:42:30 <Spark> ski: i think its definitely worth me getting fully au fait with functional programming theory if i return to this work :)
04:42:34 <ski> Spark : so maybe a yampa arrow is a bit like an entity
04:42:46 <ski> Spark : (there are other arrows)
04:42:54 <ski> heh :)
04:43:31 <Spark> the overlap is quite astounding really
04:43:41 <ski> mm
04:43:44 <Spark> they used to use a langauge called miranda
04:43:48 <Spark> before they developed Eden
04:43:54 <ski> ah
04:43:55 <Spark> ive only been involved with this about a year :)
04:44:06 <ski> that was one of the precursors to haskell :)
04:44:06 <skew> Spark: miranda(TM)
04:44:17 <Spark> this was in the 80s i think (?) :)
04:44:42 <Spark> but it never worked for them, i cant find the email that explained why
04:45:19 <ski> (mayyybe legal problems ? just guessing)
04:46:00 <ski> anyway, i'm looking in the paper to recall more things to comment on ..
04:46:54 <ski> this tree(really DAG, you say)/hierarchy of types
04:47:19 <Spark> yeah {int,int} inherits from {int,*} and {*,int}
04:47:24 <Spark> that must happen in FP too, right?
04:47:39 <ski> i think the 'sub' relation there corresponds to a type (scheme) being an instance of (another) type scheme
04:47:46 <ski> yeah
04:47:49 <Lunar^> ski: different-sized ones
04:47:49 <skew> what are we talking about?
04:48:02 <ski> Lunar^ : ok
04:48:06 <Spark> skew: my undergraduate project report :)
04:48:07 <Lunar^> ski: That would be for hOp, my idea is that drivers ARE signal transformers
04:48:27 <ski> Lunar^ : hmm, cool
04:48:44 <Lunar^> ski: mouseDriver :: Signal Word8 -> Signal MouseEvent
04:49:00 <ski> anyway, in HM systems, they don't have a single '*' type (scheme)
04:49:10 <Lunar^> ski: But that needs arrow with descreet semantic, otherwise no performance at all
04:49:10 <ski> but instead uses type variables
04:49:15 <Spark> yeah a wildcard type is given an identifier
04:49:21 <Spark> which is referenced from another type
04:49:22 <ski> right
04:49:46 <ski> so {A,B} is not the same as {A,A}
04:49:56 <Spark> thats not necessary for oracles and advice, i think
04:50:00 <Spark> since you only compare two types at once
04:50:01 <ski> (though the latter is an instance of the first)
04:50:10 <ski> i'm not sure ..
04:50:10 <Spark> you dont apply an entire function into an environment, if you get me :)
04:50:29 <Spark> ok if + is ('a,'a) -> 'a
04:50:43 <Spark> thats an entity with two oracles and an advice
04:50:48 <Spark> everything is of type *
04:50:48 <ski> mm, yes
04:51:01 <Spark> you can only link one oracle or advice at a time
04:51:01 <ski> hmm
04:51:16 <Spark> so because the process is interactive, we can adjust the other oracles/advice after the first link
04:51:24 <Spark> and thus reject any further links if theyre not the right type
04:51:52 <skew> so you unify the types incrementally
04:51:59 <Spark> yeah thats it
04:52:09 <Spark> returning control to the modeller after each one
04:52:17 <skew> like the O'Caml types on mutable variables
04:52:19 <ski> what if you want to have an entity that takes an array of pairs of A and B into it's single oracle, and provides one array of A in one advice, and an array of B in the other advice ?
04:52:34 <ski> (e.g.)
04:52:44 <Spark> de-splicing
04:52:49 * Spark draws
04:53:32 <Spark> oracle is of type [{*,*}], advice are [*] and [*]
04:53:48 <Spark> if you connect one of the advice to a [real]
04:53:49 <ski> skew : how does types of mutables differ from nonmutable ones in O'Caml ?
04:54:07 <Spark> then the oracle becomes [{real,*}] or [{*,real}]
04:54:19 <ski> how should it know which ?
04:54:29 <Spark> because its coded in C inside the predefined instance ;)
04:54:35 <ski> (or maybe even [{real,real}])
04:54:37 <skew> in the interpreter at least, a new mutable cell with a polymorphic type, or rather an unconstrained one, gets a type like '_a
04:54:46 <Spark> no, not real,real
04:54:50 <ski> but how should the type-system know that ?
04:55:03 <Spark> because its implemented specifically in each instance
04:55:05 <skew> which will unify the first time you use it
04:55:20 <skew> and then the type is reported as var int, or whatever type it was you put in/took out
04:55:21 <Spark> its not very good, but its what i did :p
04:55:39 <ski> skew : oh, right. thought you might've referred to something different ..
04:55:43 <Spark> we really want some factored out mechansism for doing this, based on a specification with type identifiers, i agree
04:55:54 <skew> the underscore types mean "one particular concrete type, but I don't know which yet"
04:57:04 <ski> the variable type means, "one particular concrete type, but I don't know which yet. also it's definitely going to be the same as all other occurances of the same variable, and not related to other type variables" :)
04:57:29 <skew> yeah, like that
04:58:04 <skew> or just, unify this type with whatever comes along, rather than universally quantifying it
04:58:17 <skew> that sounds like what Spike is doing
04:58:19 <ski> so e.g., the principal type of [1,2,3] is array(integer)
04:58:26 <skew> hi shapr
04:58:36 <ski> also [1,2,3] doesn't have type array('a)
04:58:42 <ski> shapr : hello !
04:58:54 <ski> skew : Spike ?
04:59:03 <skew> Spark I guess
04:59:12 <shapr> hiya
04:59:20 <skew> why have a memory when you have a scrollback buffer?
05:00:17 <skew> That reminds me, I watched about 20 minutes of memento today. It seemed appropriate
05:00:43 <Spark> the first 20 minutes or the last? :)
05:00:52 <skew> neither, 20 minutes in the middle
05:01:03 <Spark> thats just totally confusing
05:01:07 <skew> yep
05:02:32 <ski> Spark : if we had a struct-compose of, say, three oracles, of which the first and third are already determined to be integer, the second still just a variable 'a, then the struct-valued advice would have principal type struct(integer,'a,integer). it would also have e.g. type struct(integer,integer,integer) as well as struct(integer,string,integer), but those of 'more constrained than necessary' :)
05:03:08 <Spark> yeah
05:03:46 <Spark> the mechanism to do that is predefined in C
05:04:04 <ski> (also, the graphical notation in the paper reminded me a bit about http://www.uq.net.au/~zzdkeena/Lambda/ :)
05:04:10 <ski> ok, hm
05:04:42 <Spark> it should be replaced with a general mechanism, really
05:05:09 <Spark> the only interesting entitys ive made (in terms of types) are arr_lookup and arr_compose
05:05:25 <ski> ok
05:05:28 <Spark> since they effectively have (int,['a]) -> 'a or viceversaish
05:05:50 <ski> mm
05:06:18 <Spark> the rest are just 'a -> 'a or even worse, concrete
05:07:23 <ski> i was going to ask about whether an 'dependency' was uni- or bi-directional, but i now seem to understand that it's uni- (for data, bi- for types :), mostly connecting some instance's advice into some other instance's oracle
05:08:56 <Spark> yeah
05:08:58 <ski> page 5. near bottom. s/become more constraining/become more constraind/, i think ..
05:09:25 <Spark> yeah thats better english i think
05:10:21 <skew> Spark: you have nontrivial oracles of type 'a -> 'a?
05:10:29 <skew> Spark: what do they do?
05:10:42 <Spark> there are no higher order entities
05:10:53 <Spark> so "->" is not defined as a type :)
05:11:09 <ski> the 'principal type of an instance's oracles and advices' i think are the types of the oracles and advices of that instance's corresponding entity (which of course has got it's principal type, i.e. most general, given the actual internal type constraints)
05:11:12 <Spark> the closest you get is saying that an entity is a function
05:11:44 <shapr> good morning #haskell!
05:11:46 <skew> Spark: seems more like a stratified system with value types and entity types
05:11:47 * shapr begins to wake up
05:12:02 <skew> Spark: but still, what can a 'a -> 'a oracle do?
05:12:15 <Spark> skew: im not familiar with the term 'stratified' :)
05:12:25 <Spark> an oracle cant be of type 'a -> 'a
05:12:39 <skew> Spark: hmm. An entity then?
05:12:45 <Spark> it can be of type [something] or {something,something,...} or int, real or string
05:12:45 <ski> i think the entity could have some local state, given the possibilities of backlinks, at least  (thus being a bit similar to stram processors e.g a la Yampa)
05:12:55 <skew> Spark: you said you made some kind of 'a -> 'a thing
05:12:59 <earthy> shapr: you're about 6 hours late
05:13:16 <Spark> skew: ah i was referring to the interdependencies implemented by the entities C code
05:13:24 <Spark> where one oracle has the same type as another (always)
05:13:46 <skew> oh, so the -> is about a constraint between oracles
05:14:00 <Spark> ski: yeah that "animation state" is caused when an oracle is satisfied with a dashed link
05:14:02 <skew> I thought you meant there was some sort of object taking in and producing values of the same type
05:14:04 <ski> Spark : stratified (in this case) : a entity type can refer to value types (and maybe other entity types), while a value type can only refer to other value types, not to entity types
05:14:19 <Spark> otherwise it has "derived state" which provides no new information
05:14:25 <Spark> ski: ah its not that then
05:14:29 <Spark> there are no entity types at all
05:14:41 <Spark> there could be, it would be an elegant way of implementing certain things
05:14:47 <Spark> but there arent at the moment :)
05:14:55 <shapr> earthy: dang
05:14:57 <ski> Spark : ok (re animation state)
05:15:11 <ski> shapr : good afternoon :)
05:16:00 <shapr> good afternoon #haskell!
05:16:08 <ski> yay!
05:16:12 <ski> @yow
05:16:13 <lambdabot> I'll clean your ROOM!!  I know some GOOD stories, too!!  All about
05:16:13 <lambdabot>  ROAD Island's, HUSH Puppies, and how LUKE finds GOLD on his LAND!!
05:17:02 <ski> you *might* perhaps need entity types for nested entities
05:17:50 <Spark> mm
05:17:53 <Spark> how so?
05:18:03 <Spark> nested entities are a just an abstraction layer really
05:18:14 <ski> (to describe the potentially polymorphic principal type, i was thinking. not sure if you need them explicit though ..)
05:18:39 <ski> (s/type/type of a nested entity/)
05:18:55 <ski> just a random hunch
05:18:59 <Spark> you would need it just as much for predefined entities though
05:19:10 <Spark> from an external perspective, theres no difference there i think
05:19:11 <ski> hmm
05:19:17 <ski> right
05:19:42 <ski> though predefined entities could have their behaviour more built-in to the type system
05:20:07 <ski> (not saying that that is any better ..)
05:20:17 <Spark> ah but if a nesting entity has some special relationship
05:20:25 <ski> hmm
05:20:34 <Spark> its because its using predefined entities with that behaviour
05:21:42 <ski> hmm, regarding deleting links
05:21:53 <ski> yes, that seems more difficult
05:23:03 <ski> maybe one could do something alike 'truth/reason maintainment' systems (from AI- and knowledge-prepresentation-world)
05:23:50 <ski> somehow not only storing the current know type info, but also the reason why it's like that, right now
05:23:51 <Spark> i have *no idea* what that is :p
05:23:56 <ski> :)
05:24:04 <Spark> ah yeah thats the n^2 storage thing isnt it
05:24:17 <Spark> i am what i am because of these n other nodes
05:24:52 <Spark> you can draw a big grid and unify rows of the grid
05:25:00 <ski> so a delete of a link should prolly have the same effect as deleting *everything* and rebuilding all from scratch in exactly the same way as before *except* refrain from adding the link that should be deleted
05:25:15 <ski> (not suggesting this as an actual implementation, of course ;)
05:25:19 <Spark> yeah thats right
05:25:28 <Spark> i think that might be the best implementation though :/
05:25:47 <ski> maybe
05:25:48 <Spark> unbounded computation seems preferable to unbounded memory allocation
05:26:03 <Spark> especially if we're past O(n) complexity
05:26:21 <Spark> luckily i think most models will never get that large
05:26:31 <ski> possibly we could just rebuild the constraints solution for the current (possibly nested) entity, though
05:26:36 <ski> right
05:27:01 <ski> if they get that large, then parts will probably lie inside nested entities
05:27:01 <Spark> and the time is taken away as the modeller clicks, so it wont add up
05:27:05 <Spark> yeah
05:27:09 <ski> ok
05:27:14 <Spark> i dont think the nesting makes a difference to the spread of type though
05:27:21 <Spark> cos type has to flow through them and out the otherside
05:27:30 <Spark> maybe we can cache the derived behaviour
05:27:52 <Spark> so we dont have to delve inside the entity, we can treat it like a single node
05:27:56 <Spark> yes thats good i like that
05:28:10 <ski> hmm, right
05:28:35 <Spark> that also applies to the dependency maintance thing, you can compile the computational behaviour of an entity into a single bit of code too
05:28:35 <ski> it's inside that doesn't need rebuilding, not outside
05:29:21 <Spark> maybe if we can mark an entity as immutable
05:29:32 <Spark> certain optimisations can be made by the environment involving these algorithms
05:29:42 <Spark> just a whole bunch of preprocessing
05:30:09 <ski> maybe also some ideas from type slicing might be useful here (in determining what places that directly affects (or is affected by) some node)
05:30:21 <Spark> how does that work?
05:30:35 * ski doesn't know exactly :)
05:31:08 <ski> i didn't understand most of that paper, when i browsed it :)
05:31:57 <Spark> i know that feeling :)
05:31:59 <ski> ok, so because type info can flow in both ways, cycles can arise ..
05:32:12 <Spark> yeah, cycles can arise anyway because of feedback
05:32:14 <Spark> so thats kinda wrong
05:32:20 <ski> mhmm ?
05:32:23 <Spark> the thing is feedback is controversial anyway
05:32:29 <ski> it is ?
05:32:52 <Spark> i dont want them thinking that feedback causes unnecessary complexity here, if you see what i mean :)
05:33:25 <Spark> i kinda want to present these new ideas on their own merits, as solving certain problems that eden faces
05:33:42 <ski> hopefully the type system should be more 'static' and hopefully decidable, as opposed to actual evaluation
05:34:05 <Spark> yeah thats my criticism of eden from earler on
05:34:09 <Spark> earlier
05:34:46 <ski> (taking into account that a user-transparent (except on type errors :) type inference should prolly not eat too much resources or even loop)
05:35:06 <ski> mm
05:35:11 <Spark> yeah that would be good too
05:35:20 <Spark> but as in compilers, its not a huge priority to make it ultrafast
05:35:29 <ski> ok
05:35:52 <ski> but we prolly want it decidable, if we can get it to ..
05:35:59 <Spark> that would be useful :)
05:36:13 <Spark> with the current types theres no problem
05:36:23 <ski> (i think it would be decidable)
05:36:25 <Spark> where would problems arise, with new types? with enties-as-values
05:36:51 <Spark> im not keen on introducing recursion, incidently
05:36:56 <ski> i don't think so. but i might be worng
05:36:58 <ski> ok
05:37:10 <Spark> not directly anyway
05:37:34 <Spark> i think maybe having an entity that can run a bit of procedural code might be interesting
05:37:38 <Spark> interface with perl or something
05:37:46 <Spark> for those algorithms that *need* recursion
05:37:48 <ski> mayhaps some of the other features (like the 'changeability' of the model) will interact badly with some of those features, i dunoo
05:38:17 <Spark> you cant change the type except at 'link time' :)
05:38:24 <ski> right ;)
05:38:33 <Spark> do you agree with the lists vs structs and array thing
05:38:46 <Spark> we cant be having lists with arbitrary types in them
05:39:25 <ski> at which point it's like a modified program, to be recompiled. except that we will reuse earlier version in mem and just adapt it (prolly just adapt the running model, too :)
05:39:41 <ski> yeah, definitely
05:39:50 <ski> :)
05:39:55 <Spark> cool then
05:40:05 <Spark> i guess thats about it as far as whats in the essay goes
05:40:42 <ski> page 11 and pics i understand even worse than the earlier pages .. :)
05:40:48 <Spark> lol
05:40:59 <Spark> yeah thats 'my algorithm(tm)' isnt it
05:41:21 <Spark> figure 6 should be ok shouldnt it
05:41:23 <ski> prolly i need more info on the algo to understand that ..
05:41:32 <ski> mm, yeah
05:41:54 <ski> (i was not understanding fig. 7)
05:42:10 <Spark> ah
05:42:17 <Spark> its always the badly drawn ones isnt it :)
05:42:30 <ski> whats 'wm' ?
05:42:41 <Spark> ah that should be u
05:42:47 <Spark> its the function that unifies types
05:42:52 <ski> as in unify ?
05:42:53 <ski> ok
05:43:00 <Spark> http://info.dcs.warwick.ac.uk/~spark/3yp/doxygen_output/html/group__type.html
05:43:01 <Spark> :)
05:43:10 <Spark> i did this before i knew what unify meant
05:43:28 <Spark> i came to the same result about how the inference should work, as i think everyone else has
05:43:47 <Spark> i could probably have saved myself some brain juice by actually reading *anything at all* though
05:43:59 <Spark> although the problem solving aspect is the fun bit :p
05:44:54 <ski> yeah :)
05:46:19 <ski> hmm, comparing with gc, i see
05:46:53 <ski> maybe you could use some mark-and-sweep inspired technique, then. just a though
05:46:55 <Spark> yeah i overheard some discussion on garbage collectors
05:46:58 <ski> t
05:47:06 <Spark> got me thinking about the reachability thing
05:48:45 <ski> (i still feel like some kind of reason maintainance might be helpful here. not that i know very much about that, though :)
05:48:46 <Spark> i like the preprocessing / caching idea
05:49:36 <Spark> yeah i suppose you could say 'i am an int because of this edge and this edge, i am a * because of this edge'
05:49:41 <Spark> that might be very efficient
05:49:54 <Spark> of course you have to propogate that information when you make a link :)
05:49:57 <ski> mm somethink like that ..
05:50:03 <ski> yeah
05:50:07 <ski> somehow ..
05:50:45 <ski> you reference TaPL ? without having looked at it yet ? :)
05:50:56 <Spark> its in the post :)
05:51:01 <ski> ok
05:51:48 <ski> hmm, ok. i've gone through the paper, and commented on what came into my mind
05:52:03 <Spark> yeah, thankyou its been very helpful
05:52:05 <ski> hope some of it might be of any use to you
05:52:27 <Spark> what do you do for a living anyway?
05:52:42 * ski studies at uni
05:53:01 <Spark> some kind of postgrad thing?
05:53:07 <ski> not yet :)
05:53:19 <Spark> i dont know any undergrads here who are as well read
05:53:28 <ski> haven't got that far, yet
05:53:30 <Spark> i think that might be because this is a crap uni though :)
05:53:33 <Spark> ah
05:53:41 <Spark> im just about to finish my ugrad course
05:53:58 <skew> I don't think people read much in CS
05:54:00 <Spark> then im off for a masters at imperial college, then i might come back again to do a phd in empirical modelling
05:54:06 <ski> well, i've interested in many CS things and read many papers on my own :)
05:54:12 <skew> yeah, me too
05:54:22 <Spark> are you not actually CS students then?
05:54:30 <skew> well, I haven't read much recently...
05:54:31 <ski> i'm
05:54:36 <Spark> which uni?
05:54:46 <ski> gothenburg
05:54:50 <Spark> ah right
05:55:26 <Spark> i dont suppose theres a command in irssi to dump the backlog to a file? :)
05:55:59 <skew> I don't know, I just started using it
05:56:02 <ski> http://tunes.org/~nef/logs/haskell/04.04.22
05:56:22 <Spark> lol
05:56:31 <Spark> ok that might do the trick :)
05:56:51 <skew> I had been running xchat, but just recently realized it was broken
05:57:08 <Spark> i think im going to run a copy of that paper to my supervisor now, if hes around :)
05:57:20 <ski> which one ?
05:57:32 <ski> schwartzbach ?
05:57:51 <skew> it was very weird. Checking or unchecking a box in the preference window appeard to instead pick a random xhcat window and toggle that setting (not set or unset).
05:59:33 <Spark> nah, the essay ive written :)
05:59:45 * ski smiles
05:59:47 <Spark> im going to thoroughly digest and reference that one though
05:59:56 <Spark> and check out the book when it gets here
06:01:11 <Spark> right, cya later then, and thanks again :)
06:01:24 <ski> ok
06:01:26 <ski> bye
06:02:12 <Spark> give eden a go too, its got a graphics language and everything
06:02:24 <Spark> and a database back end
06:02:33 <Spark> its all substandard of course but its interesting stuff :)
06:03:05 <ski> i'll see if i can manage to download and install it .. :)
06:03:43 <ski> i have a class in 12 minutes, though
06:04:26 <Spark> oh yeah, it doesnt compile in newer gcc versions because it has some use of multiline string literals :)
06:04:33 <Spark> anyway im off
06:05:12 <ski> ok
06:09:01 <Spark> well, he wasnt there :)
06:09:15 <ski> mhm
06:09:57 <ski> ok, anyway, i gotta leave now
06:10:00 <ski> bye all
06:10:04 <Spark> cya
06:39:54 * jesse just had his monad 'aha'
06:40:55 <kosmikus> congratulations
06:41:00 <jesse> :D
06:46:05 <SyntaxNinja> jesse: you're now qualified to wear this shirt: http://www.cafeshops.com/doitmonad
06:47:21 <jesse> hehe
06:48:29 <earthy> hm. I love the ACM
06:48:41 <earthy> especially their online database of articles. ;)
07:02:28 <saz> hey that's cool!
07:02:39 <saz> they have a women's t-shirt that's not a baby dolls tshirt
07:02:48 <saz> (I don't like BD t-shirts much)
07:48:59 * shapr boings
07:55:45 * SyntaxNinja bounces
07:57:44 <Igloo> Wassup, dudes?
07:58:31 <Igloo> Does anyone know of a multi-format music playling library that isn't libalsaplayer? Should include at least flac, ogg, mp3
08:16:48 <shapr> Igloo: what about JACK?
08:20:09 <Igloo> That looks like a ripper/encoder
08:21:07 <Si\> libavcodec?
08:25:42 <Igloo> Could be interesting, ta
08:26:09 <Si\> or you could use libmplayer if you're adventurous
08:26:54 <Igloo> That has not-in-Debian issues  :-)
08:28:16 <Igloo> Hmm, libavcodec looks a bit lower level than libalsaplayer
08:48:28 <shapr> whee
09:36:33 * shapr boings
09:38:51 <shapr> I'm looking forward to eurohaskell
09:41:13 <bring> i asked peter gammie in TaPL class today about what he wanted to do at eurohaskell
09:41:21 <bring> he said "sleep and drink beer"
09:41:48 <kosmikus> at the same time?
09:42:09 <bring> he wasn't quite clear on that point
09:45:36 * shapr grins
09:45:50 <shapr> sounds like fun
10:15:51 <Spark> i asked my supervisor why they stopped using miranda in EM, he said because it was proprietory, they couldnt bind it to the peripherals that they wanted to
10:16:33 <Spark> obviously thats no longer an issue with open source haskell interpreters around, so im looking forward to going back there
10:56:24 <heatsink> when I import a qualified module, how do I use operators defined in the module?
10:58:20 <bring> 1 Prelude.+ 2
10:58:29 <bring> for example
10:59:14 <heatsink> okay
11:06:29 <SyntaxNinja> (Prelude.+) 1 2
11:07:00 <SyntaxNinja> oops
11:19:44 <jesse> could someone point me somewhere where i can read about the difference between newtype type and data. the link in the gentle introduction is dead
11:21:23 <SyntaxNinja> here's the report: http://www.haskell.org/onlinereport/
11:21:23 <ski> perhaps it's in Yet Another Haskell Tutorial at http://www.isi.edu/~hdaume/htut/
11:21:40 <SyntaxNinja> you might look at chapter 4
11:21:50 <jesse> thanks
11:21:51 <SyntaxNinja> but that's not "Gentle" :)
11:22:06 <SyntaxNinja> basically, type is when you want to create a synonym:
11:22:10 <SyntaxNinja> type Time = Double
11:22:52 <SyntaxNinja> "data" is when you want to create a new type with its own constructors and stuff. "newtype" is when you want a type synonym, but you don't want to be able to substitute the use of one for the other (as with type)
11:23:36 <ski> (restricted type synonyms in hugs can be used for a similar purpose, as well)
11:23:38 <SyntaxNinja> that's probably something that should go on the wiki, actually: http://www.haskell.org/hawiki/FrequentlyAskedQuestions
12:59:17 <Spark> dum de dum
12:59:39 <Spark> ski: you look at eden yet?
15:25:57 <heatsink> How do I use the value of a variable in a pattern match?
15:26:24 <heatsink> startswith c cs = case cs of
15:26:46 <heatsink>                      c:cs' -> True -- Doesn't work
15:27:12 <Riastradh> You can't do that.
15:27:13 <Riastradh> Use guards.
15:27:23 <heatsink> Oh, okay
15:28:30 <heatsink> Can I use guards in a lambda expression?
15:29:25 <heatsink> ...apparently not
15:31:26 <heatsink> I also can't say \c:cs
15:31:36 <Cale> startsWith c cs = c == head cs
15:32:35 <Cale> you can say (\(c:cs) -> c)
15:32:41 <Cale> which is the same as head
15:32:46 <heatsink> Oh, that works
15:32:47 <Shaminotd> 42!
15:33:07 <Cale> 1405006117752879898543142606244511569936384000000000
16:09:22 <BruceQ-1> Hey cool... I didn't know there was an IRC #chan for haskell
16:10:40 <BruceQ-1> Are the people here users or lanuage implementers or both?
16:14:35 <Cale> mostly users
16:15:06 <Cale> there are some people here who use Haskell in their jobs.
16:16:10 <BruceQ-1> I have been playing with Hugs/GHC... Its cool stuff but some things are very frustrating.
16:16:27 <Cale> anything in particular that I might be able to help with?
16:17:22 <BruceQ-1> Its been a few weeks.  The type system seems very confusing.. I had a terible time converting floats to ints and vis versa. 
16:17:49 <heatsink> I haven't had to deal with those in a while
16:18:11 <BruceQ-1> I don't have an interpreter on this machine or I would try to reproduce the problem. 
16:18:31 <heatsink> One thing to remember is that some of the types are actual types, and some "types" are classes
16:19:07 <Cale> You can convert integral types to any other numeric type using fromIntegral
16:19:53 <Cale> and to get back to an integer from, say, a float, use round, floor, or ceiling
17:25:53 <Neme> anyone know how to have multiple modules imported in hugs? I would like to use both functions from List and Char at the interpreter prompt
17:26:43 <Neme> I tried :l List Char
17:26:53 <Neme> but that doesnt work
17:26:59 <Pseudonym> Try :m List Char
17:27:01 <Pseudonym> Or :m List
17:27:02 <Pseudonym> :m Char
17:28:08 <Neme> :m takes only one argument
17:28:13 <Pseudonym> OK
17:28:25 <Pseudonym> Another option is to make a Haskell script which imports both and load that.
17:28:43 <Neme> yeah i know, but i just thought it would be nice to do it directly
17:28:49 <Neme> it should be doable
17:28:56 <Pseudonym> Yes, should be.
17:29:02 <Pseudonym> Well, issue two :m's and see what happens/
17:29:26 <Neme> i tried
17:29:37 <Pseudonym> It overwrites one?
17:30:10 <Neme> it just changes the evaluating module
17:30:29 <Pseudonym> You can do it from the command line:
17:30:32 <Pseudonym> hugs -m List -m Char
17:30:35 <Neme> if the prompt is Char>, I cant use any functions from List
17:30:57 <Neme> or can I give a qualified name?
17:31:08 <Neme> like List.Data.sort?
17:31:15 <Pseudonym> Give it a go and see.
17:31:22 <Neme> doesnt work
17:32:04 <Pseudonym> OK.  Looks like the command line might be your only option.
17:32:17 <Pseudonym> Incidentally, in GHCi, you can do:
17:32:20 <Pseudonym> :module + Data
17:32:26 <Pseudonym> i.e. "load in addition to"
17:32:37 <Pseudonym> Might want to request this as a feature.
17:33:22 <Neme> i can see all the functions are loaded when i do a :names
17:33:30 <Neme> i just cant use em
17:33:41 <Pseudonym> Send mail to hugs-bugs@haskell.org
17:34:04 <Neme> i dont think its a bug, im prolly just thick-headed :)
17:34:22 <Pseudonym> It seems wrong to me, FWIW.
17:35:33 <Neme> if I do a :l List Char the prompt changes to Char>
17:35:54 <Neme> the I can do a :t toLower, but I cant do a :t sort
17:38:10 <Neme> damn 
17:38:20 <Neme> it annoys me :)
17:39:08 <Pseudonym> Whether it's a bug or not, it definitely sounds like a feature you could use.
17:39:14 <Pseudonym> So ask for it.
17:39:45 <Neme> well, its not like it means the world, it would just be nice
17:40:12 <Neme> most serious work arent done in the interpreter anyways
17:40:38 <Pseudonym> The people who write Hugs would disagree with you there.
17:40:54 <Pseudonym> Fact is, the world needs more than one implementation of Haskell 98 + the standard extensions.
17:41:06 <Neme> yeah its nice for testing i admit that
17:41:09 <Pseudonym> Otherwise people will write to an implementation, not a standard.
17:41:45 <Neme> i think its doable in GHCi
20:28:48 <heatsink> Okay, I have a question
20:29:31 <heatsink> I make a type that is an instance of Show, but I don't declare show
20:29:43 <heatsink> In main, I print the object
20:29:50 <heatsink> It compiles in GHC with no errors
20:30:11 <heatsink> (... I don't define show either)
20:30:27 <heatsink> And then when I run it, I get a stack overflow
20:31:12 <heatsink> I can understand why the program is not correct, but...
20:31:32 <heatsink> Why does GHC not complain? And why do I get a stack overflow, rather than a segfault or some other error?
20:34:45 <Etaoin> looking at the prelude, it seems that show and showsPrec are mutually dependent. so you can define either one and then the other will be intelligently defined in terms of it. but if you define neither, then they keep calling each other. that I my guess
20:36:00 <heatsink> oh, okay
20:36:40 <Etaoin> http://www.haskell.org/onlinereport/standard-prelude.html
20:40:20 <heatsink> my nested parenthesis and nested bracket parser works now
20:40:35 <heatsink> In fact, it has been working for several hours
20:41:16 <heatsink> but now that I fixed show, I actually know that it's working
20:42:46 <Etaoin> neat
20:42:59 <Etaoin> now write a lisp interpreter
20:43:10 <Etaoin> :)
20:43:42 <heatsink> *hmph*
20:43:48 <heatsink> I thought you liked haskell!
20:44:45 <Etaoin> well that's the first thing that popped into my head when I heard "nested parenthesis parser"
20:45:05 <Etaoin> what are you using to write the parser?
20:45:13 <heatsink> Haskell
20:45:23 <Etaoin> no parser libraries?
20:45:48 <heatsink> I used the stuff described in http://www.cs.nott.ac.uk/~gmh/pearl.pdf
20:46:37 <Etaoin> ah
20:47:32 <heatsink> Hmm... If I say do {l <- "abcd"} will it bind "abcd" to l, or will it bind the characters 'a', 'b' ... to l sequentially?
20:49:17 <Etaoin> the second, if you're using a list monad
20:50:20 <Etaoin> but in that case you might as well do [...| l<-"abcd"]
20:50:47 <heatsink> right... but I wanted the first
20:51:41 <Etaoin> maybe do {let l = "abcd" in ...} ?
20:52:02 <heatsink> oh, I thought let and while were only for functions
20:52:20 <heatsink> s/while/where/
20:52:57 <Riastradh> No 'in:'   do { let l = "abcd"; ... }
21:03:43 <heatsink> Is there a function to take the nth item of a tuple?
21:06:11 <SamB> heatsink: unfortunately the idea that tuples even have nth items isn't captured by Haskell
21:10:01 <heatsink> well, I'm pleased that's done
21:10:15 <heatsink> now, let's see.. there was some homework I had to do..
21:34:44 <Smerbyakov> I changed my mind about Haskell. C is the the best language out there today.
21:40:17 <SamB> Smerdyakov: you have to be kidding...
21:40:30 <SamB> ...or crazy.
21:40:54 <SamB> oh well. Theres always clog ;-)
22:15:54 <heatsink> That wasn't Smerdyakov, that was Smerbyakov
22:26:41 <SamB> oops
22:27:14 <SamB> that was a very stupid name to pick.
23:03:13 <Riastradh> We already knew he was crazy, or schizoid anyways.
