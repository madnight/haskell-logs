00:17:04 <cmoline> weird. ive never had so many problems with irc before
00:17:59 * cmoline is trying to think of something haskell related to talk about
00:57:29 <Marvin--> woo
03:45:44 <MakeMyDay> what does the "?" operator do? 
03:46:24 <Lor> Uh. In what context?
03:46:46 <MakeMyDay> er.. Filters?
03:47:01 <MakeMyDay> isn't it a standard operator?
03:47:09 <MakeMyDay> bah wait
03:47:12 <MakeMyDay> sorry it isn't
03:47:48 <Lor> The only common usage I can think of is implicit parameters, and even that isn't very common.
03:59:30 <bringert> Spark: did you get any tickets?
04:13:21 <mikef> bringert: Last time he looked they'd shot up in price.
04:14:49 <bringert> and they probably won't be getting cheaper :(
04:15:21 <mikef> :(
04:44:26 <Grind> hello
04:45:23 <Grind> what would be more efficient
04:45:24 <Heffalump> 'lo
04:45:44 <Grind> get a list of elements and then mkSet 
04:45:53 <Grind> or addToSet each one of them
04:46:10 <Grind> I don't know much about Data.Set nor Data.FiniteMap
04:46:12 <Grind> hi Heffalump
04:46:27 <Heffalump> I think Data.Set justs uses Data.FiniteMap
04:46:35 <Heffalump> I doubt mkSet would be _less_ efficient than addToSet
04:46:37 <Grind> I searched for the paper "Efficient sets: a balancing act" but couldn't find it
04:47:02 <Grind> Heffalump: know anyplace describing FiniteMap properties?
04:47:19 <Heffalump> is that Stephen Adams' paper?
04:47:26 <Grind> all heard was that it's supposed to be similar to hashing
04:47:33 <Grind> Heffalump: yeap
04:47:47 <Heffalump> isn't it in JFP? Does your uni have an online subscription?
04:47:54 <Grind> nope it doesn't :-/
04:48:25 <Lor> I still haven't received last year's final issue, and I'm not going to renew before I get it..
04:48:55 <Grind> hmm I think I might have found something
04:50:36 <Grind> all I'd be happy with something listing the main properties of it
04:50:44 <Grind> like complexity of operations and such
04:57:22 <ozone> Heffalump: are you going to ICFP this year?
04:57:37 <Heffalump> nope
04:57:42 <ozone> aww
04:58:15 <Heffalump> and I'm leaving academia at the end of October, so it's quite likely I'll never go again, unless I decide to pay for it and take holiday for it myself
04:59:57 <Grind> so what are you going to do next Heffalump, if I may ask. Got plans?
05:00:59 <Heffalump> working for ARM
05:01:22 <Grind> nice
05:02:19 <Grind> I'm also curious about what would work better for an insertion sort
05:02:38 <Grind> just the usual algorithm with lists
05:03:25 <Grind> or just setToList.mkSet
05:04:13 <Grind> (I know the 2nd would eliminate repetitions but I want that)
05:04:46 <Grind> (brb)
05:18:05 <arjanb> is it possible to make a type class an instance of another type class?
05:19:39 <Heffalump> no
05:19:46 <ibid> arjanb: it does not make sense
05:19:52 <Heffalump> but you can say that one has another as a prerequisite
05:19:59 <Heffalump> and you can say instance Foo a => instance Bar a
05:20:03 <arjanb> class PartialOrder a where
05:20:03 <arjanb>   lt :: a -> a -> Bool
05:20:03 <arjanb> instance Eq PartialOrder where
05:20:03 <arjanb>   x == y = x `lt` y && y `lt` x
05:20:08 <Heffalump> (I think)
05:20:37 <Heffalump> IM instance Foo a => Bar a
05:20:39 <ibid> arjanb: that cannot be done, but you can get the equivalent effect if you can edit PartialOrder
05:20:40 <Heffalump> BICBW
05:20:56 <ibid> BICBW?
05:21:08 <Heffalump> "But I could be wrong"
05:21:23 <ibid> :)
05:23:09 <arjanb> Heffalump: that doesn't seem to work
05:23:44 <Heffalump> ok, then I am wrong :-)
05:23:55 <Igloo> You also need "where" and the actual definition
05:23:58 <Igloo> But it should work
05:24:18 <Heffalump> igloo: oh, I thought it might complain because there's no type constructor at the head of the instance of Bar
05:24:26 <Igloo> Oh, actually, I might be confused
05:24:45 <arjanb> class Eq a => PartialOrder a where
05:24:45 <arjanb>   lt :: a -> a -> Bool
05:24:45 <arjanb>   x == y = x `lt` y && y `lt` x
05:24:57 <Igloo> You can have prerequisites for classes, but only sub-prerequisites for instances I think
05:25:09 <arjanb> > No member "==" in class "PartialOrder"
05:26:02 <arjanb> any idea how to express something like this?
05:26:35 <Igloo> You'd want PartialOrder a => Eq a
05:26:46 <ibid> arjanb: oh yeah, you can't do that; what that class declaration says is that a already is an Eq instance (and has a ==), then it is also a PO
05:28:35 <arjanb> Igloo: i think that requirement is too restrictive
05:30:26 <Igloo> Yeah; I don't think there's a nice H98 solution. GHC might allow the above which, together with overlapping instances, might be able to do what you want
05:33:28 <Marvin--> what you want is  class PartialOrder a where lt :: a -> a -> Bool ; instance PartialOrder a => Eq a where x == y = ..., but this needs overlapping-instances and stuff
05:33:31 <Marvin--> and glasgow-exts, and...
05:33:37 * Marvin-- curses the markov chains and goes to get some lunch
05:54:34 * TheHunter thinks he has just understood Arbitrary-Rank Polymorphism and finds it very useful
05:55:19 <Lor> Do you need it often?
05:55:34 <ozone> Igloo: so how's the implicit parameter support in TH.Syntax coming along? :}
05:55:42 <ozone> (just poking, please don't take it seriously)
05:58:39 <Marvin--> Heffalump: ARM?
05:59:08 <Igloo> I did everything except that last night, but then found I can't commit to CVS. I'll probably do it in the next few days, and will e-mail you if I still can't commit
06:00:07 <ozone> Igloo: wow, cool!  you rock
06:01:16 <Igloo> :-)
06:01:36 <ozone> Igloo: implicit parameters actually work quite well with TH
06:01:57 <ozone> john meacham's regex stuff would go rather well with it, for example.  (e.g. bind ?1 to first regex match, ?2 = second regex match, etc ...)
06:02:12 <Marvin--> yay. perl. :/
06:02:27 <ozone> Marvin-- :)
06:02:32 <Igloo> Hmm, but you'd still need to have the right number (well, enough) of arguments to any functions using it
06:03:34 <ozone> nod.
06:05:31 <Heffalump> Marvin--: the CPU company
06:05:44 <Marvin--> Heffalump: oh
06:05:48 <Marvin--> Heffalump: what will you be doing there?
06:06:04 <Heffalump> compiler group
06:06:18 <Marvin--> nifty
06:24:38 <MakeMyDay> ok this is wierd
06:24:47 <MakeMyDay> I have a list of some data
06:24:57 <MakeMyDay> and I do a classic takeWhile on that list
06:25:28 <MakeMyDay> and it takes a few of my data that it should , but it leaves some behind. When they are identical.
06:25:31 <Marvin--> MakeMyDay: and then the universe explodes?
06:25:38 <Marvin--> oh
06:25:40 <Marvin--> boooooring
06:25:59 <MakeMyDay> Well it has exploded in that past soo..
06:26:04 <Marvin--> oh, okay
06:26:49 <Marvin--> which elements are identical to what?
06:27:23 <MakeMyDay> They are XML data
06:27:38 <MakeMyDay> Tag "SPEAKER" [] is what I am looking to take out
06:27:44 <MakeMyDay> there are around 500 of those.
06:28:01 <Marvin--> so you have something like  takeWhile (==Tag "SPEAKER" [])  ?
06:28:07 <MakeMyDay> When I do takeWhile (== Tag "SPEAKER" [])..
06:28:10 <MakeMyDay> yes
06:28:18 <MakeMyDay> but it only takes out 37 of them?
06:28:39 <Marvin--> and you are sure there is no tiny element in between?
06:28:50 <Marvin--> some forgotten whitespace?
06:29:31 <Marvin--> honestly, I think it's pretty safe to assume that the bug is not in takeWhile :)
06:29:37 <MakeMyDay> yes. And even if there was it should be on all elements because they are beeing produced by the same function
06:29:47 <MakeMyDay> yeah I am pretty sure about that also
06:29:49 <MakeMyDay> :p
06:30:17 <Marvin--> well, it's hard to help without more context
06:38:16 <Grind> MakeMyDay: to make sure, just do a dropWhile and then check the head
06:38:47 <Marvin--> or use span and check both components of the returned pair
06:39:08 <Grind> MakeMyDay: let hd = (head . dropWhile f) yourList
06:39:16 <Grind> MakeMyDay: f hd
06:39:41 <MakeMyDay> hm let me try that
06:40:03 <Grind> if you get a true, then something is very wrong with system :)
06:40:04 <Igloo> Is it just me or has http://www.haskell.org/ghc/ lost its sidebar etc?
06:40:30 <Grind> Igloo: not just you
06:41:13 <Grind> MakeMyDay: if you get a false, then take a close look at that element
06:44:55 <MakeMyDay> wait.. dropWhile goes the whole list through or does it stop the first time the condition doesn't work?
06:45:07 <Heffalump> it stops the first time the condition fails
06:45:14 * MakeMyDay smacks himself
06:45:16 <Heffalump> or rather, it starts then
06:45:32 <Heffalump> since it only gives you the elements at/after the condition fails
06:45:32 * MakeMyDay smacks himself hard
06:45:35 <Grind> MakeMyDay: just like take while...
06:45:55 <ibid> dropWhile does not copy the rest, does it?
06:46:33 <Grind> I guess it dependes on what you mean by copy...
06:46:52 <ibid> Grind: is there more than one possible meaning?
06:47:07 <Igloo> If that's not what you want then you probably want filter
06:47:11 <Grind> it copy as much as tail or any other standard function that returns a list
06:47:25 <Grind> ibid: yes there is ;)
06:47:33 <MakeMyDay> Igloo : I am trying to build a filter :))
06:47:42 <ibid> Grind: tail is similar yes, but not all other standard functions
06:47:49 <ibid> Grind: give one :)
06:48:08 <Grind> ibid: drop
06:48:13 <Grind> ibid: dropAt
06:48:27 <Grind> ibid: no drop lol
06:48:37 <ibid> Grind: i meant, give another meaning of copy
06:48:50 <Marvin--> MakeMyDay: then use filter ;)
06:50:06 <Grind> ibid: can we make it another way
06:50:19 <Grind> ibid: you give me an alternative to returning a copy
06:51:08 <ibid> Grind: okay, id x = x does not copy
06:51:09 <Grind> ibid: "dropWhile does not copy the rest, does it" <- give me an alternative to this
06:51:18 <ibid> Grind: alternative to what?
06:51:21 <Grind> ibid: so what does it do?
06:51:31 <ibid> Grind: it returns what it is given
06:51:39 <ibid> Grind: not a copy of it
06:52:12 <Grind> ibid: what about f x = 2:x ?
06:52:20 <ibid> does not copy x
06:52:47 <ibid> but f (x:y) = 2:x:y copies the argument cell
06:53:12 <ibid> (of course, lazy evaluation makes it more complicated, but i think we tend to abstract that away)
06:53:27 <Grind> ibid: nah ibid, your f and id work the same way
06:53:47 <Heffalump> note that he said the argument cell
06:53:52 <Heffalump> i.e. the one represented by that :
06:54:12 <ibid> Grind: the difference is that f deconstructs the head cell and reconstructs it (ie. copies it), id does not
06:54:24 <ibid> of course, x and y are not copied by f either
06:55:17 <Grind> ibid: ok so there you have what I was looking for, what you meant by copying
06:55:51 <ibid> Grind: ok, to summarize, by copying i mean deconstructing a constructor cell and reconstructing an identical cell
06:56:10 <Grind> ibid: ok right
06:57:01 <Grind> so let me try to get this right
06:57:05 <ibid> Grind: are you thinking of the usual imperative language question of value vs reference parameters?
06:57:21 <Grind> ibid: I was just not understanding your question
06:57:26 <ibid> :)
06:57:36 <Grind> ibid: because copying the list didn't make sense to me...
06:57:41 <Grind> so let me see
06:57:48 <Grind> f (x:y) = x:y
06:57:56 <Grind> what about this f?
06:58:04 <ibid> Grind: copy (x:xs) = x : copy xs ; copy [] = [] :)
06:58:15 <ibid> Grind: it copies the head cell and leaves the tail uncopied
06:58:25 <ibid> a shallow copy, in a sense
06:58:30 <Grind> yes I got it...
06:58:32 <ibid> my copy fucntion is a deep copy
06:58:43 <Grind> but what if you do this:
06:58:46 <Grind> f (x:y) = x:y
06:58:57 <ibid> did i not just answer that?
06:59:09 <ibid> 16:58  <ibid> Grind: it copies the head cell and leaves the tail uncopied
06:59:09 <ibid> 16:58  <ibid> a shallow copy, in a sense
06:59:11 <Grind> ok then the answer is yes
06:59:15 <Grind> :)
06:59:30 <Grind> I was just wondering if it would be smart enough to interpret that as an id
06:59:37 <Grind> so
06:59:44 <Grind> f (x:y) = x:y
06:59:48 <Grind> is different from
06:59:56 <Grind> f (x:y)@l = l
06:59:57 <Grind> ?
07:00:00 <Heffalump> a good compiler could translate that to f l@(x:y) = l
07:00:01 <Heffalump> I think
07:00:08 <Grind> opss
07:00:18 <Grind> I typed it the other way around
07:00:19 <Grind> :)
07:00:33 <Grind> Heffalump: that's exactly where I was trying to get at
07:00:49 <ibid> yeah, since values are immutable, there is no functional difference between copying and not copying
07:00:55 <Grind> because in the dropWhile function you'll have to check the head of the returned list
07:01:14 <ibid> and thus a compiler is allowed to optimize the distinction away
07:01:28 <Grind> so my guess is that it depends on how something like,  f (x:y) = x:y  works
07:01:50 <ibid> however, there are functions where copying cannot be optimized away without unusual runtime representations
07:02:06 <ibid> but i think in these discussions compiler optimizations are best ignored :)
07:02:34 <ibid> a function where copying is necessary is takeWhile
07:02:43 <Grind> ibid: ok so if I understood everything then, ignoring compiler optimizations, it would make a copy in that sense
07:02:52 <ibid> yeah
07:03:03 <Grind> ibid: since it will always check the head of the list it returns
07:03:28 <ibid> Grind: my mental model ignores laziness and so any constructor application translates into an allocation of a new cell
07:03:54 <ibid> Grind: checking is not the problem, the question is whether it returns the original cell or reconstructs an identical cell
07:04:00 <Grind> drpWhile p (x:xs) | not (p x) = x:xs
07:04:13 <ibid> yeah, that copies that cell
07:04:39 <Grind> that's why I was asking about  f (x:xs) = x:xs
07:04:48 <ibid> dropWhile p r@(x:_) | not (p x) = r, however, does not
07:05:20 <Grind> and that was the 2nd part of my question ;)
07:06:10 <Grind> if they actually work differently I'd expect compilers to implement it the 2nd way
07:06:14 <Grind> but I'm not sure...
07:08:02 <ibid> Grind: since haskell is referentially transparent, there is no functional difference
07:08:23 <ibid> Grind: but i usually try to avoid copying where i can, since it affects performance
07:08:42 <Grind> ibid: yes, I had never thought about this
07:13:52 <Marvin--> I'm pretty sure that ghc -O takes care of that for you :)
08:00:00 <Spark> bringert: how about if we wait for last minute tickets now then
08:03:38 <bringert> not sure how that works for this kind of flight, but I guess that's the only chance if they have gotten too expensive now
08:36:29 <Spark> mm
08:36:42 * Spark slaps forhead for not buying them earler
08:37:01 <Spark> i think the website did suggest that the prices would be the same for a fortnight though
08:37:12 <Spark> maybe a fortnight has passed while ive been doing exams, and i havent noticed
08:42:34 <mikef> Spark: Only 13 days.
08:49:40 <Spark> ha
09:25:16 <jadrian> hello
09:25:20 <jadrian> in Data.Set
09:25:22 <jadrian> mapSet :: Ord a => (b -> a) -> Set b -> Set a
09:25:31 <jadrian> isn't "Ord b" missing?
09:25:45 <jadrian> ghc documentation
09:26:16 <Cale> Prelude Data.Set> :t mapSet
09:26:16 <Cale> mapSet :: forall a b. (Ord a) => (b -> a) -> Set b -> Set a
09:26:27 <Marvin--> I'm guessing that mapSet simply traverses the input set and folds it into the output set with some add function
09:26:37 <Marvin--> the traversal shouldn't need Ord, but the add does
09:27:02 <jadrian> but the input Set is a, Set b
09:27:22 <jadrian> and you need b to be an instance of Ord in order to make a set of bs...
09:27:47 <Marvin--> so?
09:28:31 <Marvin--> Ord b still isn't *needed*
09:28:44 <jadrian> let me try something
09:28:49 <Marvin--> and actually, you don't need Ord to create a unit set :)
09:28:56 <Marvin--> (or empty set)
09:29:02 <jadrian> newtype Set a = Data.Set.MkSet (Data.FiniteMap.FiniteMap a ())
09:29:25 <Marvin--> mapSet f (MkSet set) = MkSet (listToFM [ (f key, ()) | key <- keysFM set ])
09:29:38 <jadrian> shouldn't a (Ord a =>) be in the definition of the newtype Set?
09:29:51 <Marvin--> no
09:30:57 <Marvin--> you can probably google for data type definition and class constraints and find a whole lot of debate about whether that's a feature or a misfeature
09:31:06 <jadrian> so when do you use those constrains in newtype/data declarations?
09:31:09 <jadrian> oh
09:31:57 <Marvin--> I don't remember the arguments myself, but I think that the way they work right now, they're kinda useless
09:32:12 <jadrian> hmm I think it would have been handy for me
09:32:21 <jadrian> no big deal but for instance
09:33:19 <jadrian> you just cannot define Sets of b, unless they are an instance of ord right?
09:33:38 <Marvin--> yes you can
09:33:57 <Marvin--> the empty set and unit sets can be defined without Ord
09:33:59 <jadrian> besides  = undefined...
09:34:10 <Marvin--> emptySet = MkSet emptyFM
09:34:15 <Marvin--> unitSet x = MkSet (unitFM x ())
09:34:18 <jadrian> yes and undefined :)
09:34:32 <jadrian> still... that's not really that usefull or is it? ;)
09:34:47 <jadrian> (it's a serious question... can it be usefull?)
09:34:57 <Marvin--> but why would it be useful to restrict the type if you don't have to?
09:35:10 <jadrian> type error messages could be more clear
09:35:40 <Marvin--> keep in mind that even with a class constraint on the newtype, you still have to write the class constraints in the type signatures of functions operating on the type
09:35:44 <jadrian> I was getting an error from the usage of addToSet...
09:36:03 <jadrian> yes I know I do
09:36:33 <jadrian> and if I didn't, I'd get an error msg saying
09:36:42 <jadrian> b is not an instance of Ord
09:37:04 <jadrian> and that I would need it for Set b
09:37:20 <jadrian> seems more clear to me that complaining about 
09:37:34 <jadrian> b is not having an instance for Ord
09:37:41 <jadrian> because of addToSet
09:37:45 <jadrian> or something like that
09:39:13 <jadrian> brb
10:05:45 <jadrian> hmmm a concatMap for sets would be handy :)
10:09:24 <Marvin--> what should it do? Take a function a -> Set b or a function a -> [b]?
10:09:48 <Marvin--> or have two versions?
10:15:25 <jadrian> Marvin--: well for (a -> [b]) I did:
10:15:30 <jadrian> unionManySetsMap   :: (Ord a) => (b->Set a)->[b]->Set a 
10:15:30 <jadrian> unionManySetsMap f = foldr (\t s->union (f t) s) emptySet
10:15:33 <Igloo> a -> Set b would be unionMap
10:15:56 <jadrian> Igloo: yes I just called it that :)
10:16:05 <jadrian> unionMap   :: (Ord a) => (b->Set a)->Set b->Set a 
10:16:07 <jadrian> unionMap f = unionManySetsMap f . setToList
10:20:39 <jadrian> it could probably be done more efficient without using lists as an intermediate structure though...
12:32:48 <bringert> does anyone use Text.Html and Network.CGI?
12:32:54 <bringert> and/or rather
12:33:21 <Lunar^> ... WASHHTML and home made CGI wrapper... sorry
12:33:31 <bringert> :)
12:33:56 <bringert> I'm not asking for support, I'm wondering if anyone has any views one how they should work
12:34:03 <bringert> s/one/on/
12:34:37 <bringert> I have modified Text.Html to produce XHTML 1.0 and am working on making Network.CGI more usable (for me at least)
12:35:15 <bringert> trying to get together a lightweight library for writing web apps
12:36:02 <monotonom> I use Text.Html to produce a very simple html photo gallery.
12:37:27 <bringert> would you care if it changed to produce XHTML instead? keeping the interface the same (except some internal functions which seem to be exported for some reason)
12:37:58 <monotonom> As long as my Mozilla still understands it.
12:40:23 <Marvin--> does it generate doctype tags too?
12:49:56 <bringert> yeah, the current Text.Html already does that
12:52:58 <Hyp3rion> yay
12:53:12 <Hyp3rion> xhtml with doctype is incorrectly rendered by mozilla
12:53:22 <Hyp3rion> doctype is evil anyways
12:54:35 <Marvin--> *html* is evil
12:54:46 <Marvin--> you got it all wrong :)
12:55:39 <Hyp3rion> :-/
12:55:49 <Hyp3rion> sorry just learning haskell [still]
12:55:52 <Hyp3rion> so
12:56:01 <Hyp3rion> what is Text.Html for?
12:56:14 * Hyp3rion likes HTML!
12:57:24 <monotonom> Writing programs that output html
12:58:38 <Hyp3rion> monotonom - ?
12:58:53 <Hyp3rion> monotonom - i always wrote c cgi that output html using printf()
12:59:10 <Hyp3rion> monotonom - i guess haskell cgi can output html with print too, nope?
12:59:15 <jadrian> when using literate haskell, LaTeX kind, can you include code that is not shown on the document?
12:59:38 <monotonom> But hand-coded html doesn't scale.
12:59:48 <Marvin--> jadrian: \begin{comment}\begin{code}  would work I guess
12:59:49 <monotonom> Err, hand-coding html doesn't scale
13:00:38 <jadrian> Marvin--: ah right :) 
13:00:53 <jadrian> Marvin--: by the way \begin{comment} exists or should I 'define it'?
13:02:37 <Hyp3rion> monotonom - doesnt scale?
13:02:48 <Marvin--> jadrian: good question :)
13:03:16 <monotonom> Well, "scale" is subjective I guess.  There are people who claim C++ scales.
13:03:17 <jadrian> Marvin--: anyway I'll try it and it's not really important :)
13:03:47 <Marvin--> jadrian: it works with the docs that LyX produces, but since I include slews of packages, it may come from somewhere else than the basic files
13:05:12 <jadrian> Marvin--: the definition is trivial though. I asked because I thought there could be some other environment like \begin{hiddencode}
13:05:41 <jadrian> Marvin--: but yeah, just wraping it up in another environment seems enough
13:05:42 <Hyp3rion> monotonom - ? c++ doesnt scale if compared to multithreaded haskell of course, but ...
13:05:51 <Hyp3rion> i mean ghc -smp
13:06:26 <Hyp3rion> anyway, why doesnt the ghc manpage document --make?
13:06:34 <monotonom> Oh, and there are people who think "copy con: proggie.exe" scales.
13:06:38 <jadrian> ghc manpage?
13:06:53 <jadrian> is there such a thing?
13:06:58 <Hyp3rion> and why is the ghc homepage so different now? with much fewer stuff...
13:07:08 <Igloo> haskell.org is broken (again)
13:07:22 <Hyp3rion> jadrian - i dunno. man ghc shows a thing here...
13:07:29 <Hyp3rion> Igloo - oh :-(
13:07:47 * jadrian has no manpage...
13:07:57 <Igloo> And the manpage is autogenerated from one of the SGML docs, so either it's not covered in that doc or it's missing. Assuming you're using Debian if you file a bug I'll be more likely to remember it
13:08:03 <Hyp3rion> jadrian?
13:08:33 <Igloo> I think the previous maintainer sent it to SimonM a while ago, but I'm not sure why it didn't make it in. I'll talk to Simon about it again some time
13:09:01 <jadrian> Hyp3rion: nope don't have one, but it might be because I'm using a RH package in SuSE
13:10:17 <Hyp3rion> Igloo - yes debian. so i'll file a bug... tho i am not even sure now... is it --make or --Make?
13:10:29 <monotonom> --make
13:10:32 <Hyp3rion> jadrian - lol
13:10:39 <Hyp3rion> monotonom - okay thanks
13:11:08 <monotonom> --make is covered in section 4.4.1
13:12:32 <monotonom> ghc --help also lists it.
13:12:48 <Hyp3rion> yay
13:12:51 <Hyp3rion> man ghc doesnt
13:13:18 <Hyp3rion> but sphor is down i think [still no power?] so i cannot file a bug yet can i?
13:14:53 <Hyp3rion> heh no its up now...
13:19:22 <Hyp3rion> Igloo - thanks its filed now: 250 OK id=1BUWhj-0005Er-00
13:19:29 <Igloo> Cool, ta
13:39:04 * shapr swears at PalmOS
13:39:24 <Igloo> What's it doing to you, shapr?
13:39:32 <andersca> hey shapr
13:39:55 <shapr> hej andersca 
13:40:07 <Marvin--> ooh, there's an Ender's Game movie in the works
13:40:15 <shapr> Igloo: I've downloaded one too many crippleware language vocabulary training programs
13:40:17 <Marvin--> and Orson Scott Card is writing the screenplay
13:40:37 <Igloo> That name sounds familiar - is he the author?
13:40:39 <shapr> I think this is the fifteenth crippled program I've tried to use to learn swedish vocabulary.
13:40:41 <Heffalump> is it just me, or does the outline in http://perl.plover.com/book/ all seem like fairly obvious stuff to experienced functional programmers
13:40:45 <Heffalump> igloo: yes
13:41:00 <Marvin--> Igloo: yes!
13:41:06 <shapr> I want nhc98 running on PalmOS :-(
13:41:20 <Igloo> Is it aimed at experienced functional programmers?
13:41:22 <shapr> then I can write something that is a) free, libre, gratis, etc and b) doesn't suck
13:41:23 <Heffalump> (I seem to have annoyed the author somewhat by making this comment on a random blog: http://blog.simon-cozens.org/bryar.cgi/id_6738?comments=1 )
13:41:32 <Heffalump> It didn't seem that way to me.
13:41:56 <Heffalump> my initial impression was "yes, tell Perl programmers about interesting FP techniques"
13:42:01 <shapr> it does seem like obvious stuff
13:42:52 <shapr> Marvin-- has a quote about this on HaWiki
13:43:14 <shapr> I agree with you, if you write FP code in Python/Java/etc, your coworkers are completely lost.
13:44:01 <Marvin--> I do?
13:44:31 <shapr> <Marvin--> I often sigh and say "I could do this way faster and way better in Haskell" and then they say "yeah, and we wouldn't understand a thing of it"
13:44:43 <Marvin--> oh
13:44:45 <Marvin--> that one :)
13:44:46 <Heffalump> but presumably he meant actually using Haskell, then...
13:45:07 <shapr> but, if you understand the patterns being used, reading Haskell is easy.
13:45:21 * Heffalump writes some more Java
13:45:31 * shapr feels sorry for Heffalump 
13:45:38 <Marvin--> the point was that when you write stuff where you're not the sole maintainer, you stick to languages that all the maintainers know
13:45:41 * andersca writes some more TeX
13:45:43 <Heffalump> it's quite fun
13:45:46 * shapr writes more Python
13:45:52 <shapr> what's fun about Java?
13:46:00 <Heffalump> it's not a bad language, really
13:46:01 <Heffalump> bit verbose
13:46:08 * Igloo wonders what the image at the top right is meant to be
13:46:19 <Lunar^> shapr: you can do quite FP code with Python, but it is even close with Javascript
13:46:20 <Lor> Yes, it is.
13:46:23 <Lor> (Bad, that is.)
13:46:39 <shapr> I've been using Java since it was in beta
13:46:47 <shapr> I liked it when the only other thing I knew was Visual Basic.
13:46:52 <Marvin--> though admittedly, I once wrote a small XF86Config file parser in Haskell to show some Debian people that parsing isn't hard :-)
13:47:03 <shapr> how'd they respond?
13:47:19 <Heffalump> top right of what?
13:47:26 <Lor> I just heard that our cs department decided to move to java as a teaching language already when all you could do with it was write applets...
13:47:35 <Heffalump> oh, the HOP url
13:47:38 <Heffalump> no idea
13:47:45 <Marvin--> shapr: I don't remember
13:47:45 <shapr> Heffalump: you can write the equivalent Java code in Jython, less verbose, more flexible, less bugs, less time, and your boss still won't let you use Jython ;-)
13:47:47 <Igloo> http://perl.plover.com/book/
13:48:01 <Lunar^> Marvin--: neat :)
13:48:03 <Igloo> Oh, I really should read to the end
13:48:19 <Marvin--> I think the main objection was introducing a build-dep on ghc in the xfree source package :)
13:48:31 <Igloo> That would rule!
13:48:55 <Igloo> Using it in the gcc build process would be cooler, of course
13:49:03 <Marvin--> heh
16:19:37 <jadrian> are there any std functions that return Ordering?
16:19:45 <jadrian> for instance, something like  (<=)
16:20:35 <jadrian> nevermind found it :)
16:20:50 <jadrian> it's "compare"
16:22:20 <Hyp3rion> @test (sorry)
16:22:21 <lambdabot> Sorry, I don't know the command "test", try "lambdabot: @listcommands"
16:36:23 <jadrian> does it make sense to define an Ord instance such that 
16:36:33 <jadrian> x /= y     but
16:37:26 <jadrian> x <= y   and   not x < y 
16:40:08 <jadrian> (and you ask, why would you want that??)
16:40:17 <jadrian> well,
16:40:27 <TheHunter> shouldn't be x <= y be the same as x < y or x == y?
16:40:58 <jadrian> TheHunter: yes... it usually is :-/
16:41:08 <jadrian> TheHunter: that's why I'm asking
16:41:31 <TheHunter> Well, do you have an example where it seems useful?
16:41:37 <jadrian> yeap
16:42:04 <jadrian> lets say you are working with pairs   (element,cost)
16:42:38 <jadrian> equality should be the standard one
16:43:12 <jadrian> when you compare them, only the cost matters
16:43:59 <jadrian> seems to me like this should be the natural definitions
16:44:30 <TheHunter> You better not put them in a FiniteMap or something...
16:45:36 <jadrian> hmmm why?
16:45:50 <jadrian> it probably uses compare
16:46:01 <TheHunter> I'm not sure but I think they assume all the laws hold
16:46:13 <jadrian> it doesn't have to
16:46:34 <jadrian> I think, since it will probably relay on Ord methods
16:47:12 <jadrian> the law that would fail in my case would be  the relation between EQ and (==)
16:47:41 <TheHunter> But (==) is an Ord method either, isn't it?
16:47:52 <jadrian> it's an Eq method
16:48:16 <jadrian> in Ord you have compare which returns EQ, LT or GT
16:49:19 <jadrian> but yeah, it could probably end up all wrong :-/
16:49:31 <TheHunter> well you either define EQ etc. by (==) and (<=) or you define (<=),... by EQ, LT, GT
16:50:41 <jadrian> yeap, better not make this an Ord instance...
16:50:43 <TheHunter> And FiniteMap uses both (<), (<=) and (==) so it assumes the laws hold
16:52:36 <jadrian> at first it just seemed natural to be able to use   <, <=, etc to compare elements by their weight
16:53:03 <jadrian> guess I'll just use   cost e1 <= cost e2
16:53:40 <jadrian> and define another "compare" function etc
16:54:48 <TheHunter> you'll probably have to do that, though it would be nice to somehow use two different compare functions on the same data...
16:57:07 <jadrian> the FiniteMap thing wouldn't be a problem since I don't plan on using it...
16:58:33 <jadrian> anyway I'll just do it the right way :)
16:58:55 <jadrian> brb
16:58:57 <TheHunter> if you just wanna sort it you might be able to use sortBy
17:00:13 * desrt installs ghci on gala
17:07:52 <cedricshock> Anyone have a suggestion for a book on category theory, arrows, monads, etc?
17:10:36 <TheHunter> I'm currently reading Mac Lane's book but I think it's quite hard even for mathematicians
17:16:29 <cedricshock> I think monads and arrows were both invented for the purpouse of job security, and have had their explanations deliberatly obfuscated to prevent competition in the market.
17:16:42 * shapr snickers
17:16:47 <np_hard> HEHEHE\\\\
17:16:54 <np_hard> oops capslock
17:19:31 <cedricshock> Consider this little imperitive code (also easy to do with accumulators):
17:19:32 <TheHunter> Monads seem almost natural if you think about how to handle side effects in a pure language...
17:19:33 <cedricshock> while x != y do
17:19:33 <cedricshock>           if x < y
17:19:33 <cedricshock>           then y := y-x
17:19:33 <cedricshock>           else x := x-y
17:19:33 <cedricshock>      return x
17:19:53 <cedricshock> how is this easier:
17:20:07 <cedricshock> Maybe I shouldn't paste it all here (like 20 lines)
17:21:01 <cedricshock> Oops. I got everything wrong. I give up.
17:21:43 <cedricshock> Ok here's my problem, and it doesn't model well in any language. I have a fealing that these monad and arrow things are like it.
17:22:14 <cedricshock> I want to take data, and take a description of how to modify the dat or calculate something new and put them together.
17:23:43 <cedricshock> And I want to put them together in a manner such that the results of any calculation could be cached, so that small data points could be changed without recalculating the whole thing, so that results can be inspected at any level, and so that multiple sets of data calculated in a previous timeframe can be combined into a calculation in this time frame.
17:25:33 <shapr> you can do it in Haskell, and probably other languages too
17:26:30 <shapr> it sounds a lot like a spreadsheet, have you looked at some of the spreadsheet code written in Haskell?
17:26:34 <cedricshock> I'm sure I can do it in any turring complete language - it's a solvable problem. I just don't want it to be overly complicated.
17:26:38 <cedricshock> It is a spreadsheet.
17:27:29 <cedricshock> I have looked at some of the Haskell spreadsheet stuff (i.e. Haxcel)
17:30:27 <shapr> g'day
17:30:34 <Pseudonym> G'day,
17:30:59 <Pseudonym> It's official: Sorting is a tricker problem than most people realise.
17:31:07 * shapr agrees
17:31:13 * shapr has trouble sorting his socks
17:31:20 <Pseudonym> Socks I can do.
17:31:26 <Pseudonym> Strings are much harder.
17:31:33 <shapr> but socks are made of strings!
17:31:56 * shapr goes back to reading about generic programming
17:32:07 <shapr> Pseudonym: so, do you have an illustrative example?
17:32:16 <Pseudonym> Uhm... not really.
17:32:20 <Pseudonym> It's very complicated.
17:32:32 <Pseudonym> Doing it is easy.  Doing it fast is what's hard.
17:32:49 <Pseudonym> In particular, because sorting in databases requires fetching keys off disk.
17:32:49 <cedricshock> Doing it on already sorted things is even harder.
17:33:04 <Pseudonym> You want to avoid as much disk traffic as you can.
17:33:25 <Pseudonym> It gets even harder when you have primary and secondary keys,
17:33:47 <Pseudonym> And it may be more efficient to sort your primary key using radix sort but your secondary key using quick or merge sort.
17:34:35 <TheHunter> cedricshock: The way I see it, arrows (FRP) are the right tool for tackling this kind of problem, but I'm not sure it's feasable since I've never done anything with it.
17:36:34 <cedricshock> thehunter: Arrows sure sound a lot like this problem and the conceptual solutions for it that I've had floating around in my head. But I'm not really grabbing onto the arrows from the abstract end. Maybe I should try the point end.
17:36:36 <shapr> Haskell has awesome buzzwords like "kind-indexed types"
17:36:46 <shapr> yes! pointy end!
17:37:36 <cedricshock> Pseudonym: Your database problem is in haskell?
17:37:37 <shapr> konichiwa juhp-san
17:37:51 <juhp> morgen, shapr
17:38:03 <Pseudonym> cedricshock: No.
17:43:52 <shapr> are functors the first step towards generic programming?
17:44:04 <Pseudonym> No, higher-order functions are.
17:44:06 <Pseudonym> IMO
17:44:14 <Pseudonym> Hmmm.
17:44:20 <shapr> functors let you 'traverse' a datatype, right?
17:44:20 <Pseudonym> OK, maybe someting like length.
17:44:22 <Pseudonym> Or reverse.
17:44:36 <shapr> @info fmap
17:44:39 <Pseudonym> I think you're thinking of fold.
17:44:41 <lambdabot> -- fmap is a method in class Functor
17:44:41 <lambdabot> fmap :: forall f :: (* -> *). (Functor f) =>
17:44:41 <lambdabot> 	forall a b. (a -> b) -> f a -> f b
17:44:50 <shapr> @info Functor
17:44:52 <lambdabot> -- Functor is a class
17:44:52 <lambdabot> class Functor f :: (* -> *) where {
17:44:52 <lambdabot>     fmap :: forall a b. (a -> b) -> f a -> f b; }
17:45:01 <shapr> well, that was straightforward
17:45:18 <cedricshock> What's the type *?
17:45:23 <shapr> that's a kind
17:45:28 <shapr> it's a higher order type :-)
17:45:30 <Riastradh> Kinds are like metatypes.
17:45:46 <cedricshock> A type of types?
17:45:48 <shapr> cedricshock: and you thought arrows were the only job security we have here ;-)
17:46:37 <cedricshock> shapr: You have no job security I can't crack. Unless of course you encrypt your code with some sort of public key and need the private one to compile it ...
17:46:54 <shapr> cedricshock: I welcome anyone who will learn :-)
17:47:03 <Pseudonym> We use the "Haskell source code" encryption algorithm.
17:47:05 <arjanb> haskell's typesystem looks more complex than perl's syntax
17:47:23 <shapr> arjanb: that's probably true, but guess which one is more regular
17:47:58 <shapr> Hindely-Milner type system with the Damas-Milner inferencing
17:48:01 <shapr> or...
17:48:07 * shapr can't say it
17:48:30 <Pseudonym> What's irregular about regular expressions?
17:48:51 <monotonom> Perl is not a regular language. :D
17:49:23 <Pseudonym> Yes, but every interesting Perl program is a one-line regular expression.
17:49:47 <cedricshock> Here's a challenge: Explain monads in fairly plain language without introducing haskell or category theory.
17:49:56 <monotonom> Yes, but unfortunately many Perl hackers write uninteresting Perl programs.
17:50:16 <Pseudonym> cedricshock: Is abstract algebra okay?
17:50:53 <cedricshock> Pseudonym: Some abstract algebra is Ok, I guess. But it seems it would just make the explination harder.
17:51:05 <shapr> cedricshock: http://okmij.org/ftp/Scheme/monad-in-Scheme.html
17:51:15 <monotonom> The last chapter of Lawrence Paulson's "ML for the working programmer" is full of monads. (There are three.)  It mentions no category; it doesn't even use Haskell.
17:52:00 <monotonom> And most surprisingly, it doesn't even mention the monad word.
17:54:15 <monotonom> But the abstract-algebra idea is good.  A ring is a bunch of operators satisfying some laws, and there is a huge diversity of rings.  A monad is also a bunch of operators satisfying some laws, and there is a huge diversity of monads.
17:55:50 <cedricshock> Surprisingly I understand groups far better than rings, having started abstact algebra in the fall with groups, only to have the professor change, and pick up again in the spring with groups. Havn't even seen the definition of ring or field yet :(
17:56:22 <cedricshock> And the first abstract algebra course next fall is groups too.
17:56:25 <Pseudonym> A ring is pretty much addition + multiplication.
17:56:29 <Pseudonym> A field is that + division.
17:57:06 <Pseudonym> (Well, addition, subtraction and multiplication.)
17:57:25 <Pseudonym> An abelian group you can think of as addition and subtraction.
17:57:39 <Pseudonym> (Note, it's an analogy, not a definition!)
17:59:21 <cedricshock> Yeah, a ring is something like an (abelian?) group plus another operator, where the second operator interacts with the first like addition does with multiplication (or at least thats what I've gathered from scattered references).
17:59:37 <Pseudonym> Monads are not quite objects like you'd trivially see in abstract algebra.
17:59:47 <Pseudonym> Because they're more closely connected with families of algebras.
18:00:19 <monotonom> I now recall a fine treatise from Wadler. It is quite plain.
18:00:26 <Pseudonym> If you think of a set X, then you can construct sets-of-X.
18:00:35 <Pseudonym> monotom: Comprehending Monads, I believe.
18:00:56 <monotonom> Yes, I believe too. I just have to take a look and confirm.
18:01:01 <Pseudonym> And the sets-of- construction will work for any X.
18:01:17 <Pseudonym> The idea is thaqt you can generalise this to monads-of-X for any X.
18:01:31 <Pseudonym> Wadler constructs them by analogy with list comprehensions.
18:01:36 <cedricshock> monotome:  Just found a huge list of his stuff. Thanks
18:01:45 <Pseudonym> i.e. list-of-X
18:02:04 <cedricshock> Ok
18:02:18 <monotonom> Yeah, starting from some concrete things such as list, and then say "look there is this commonality", seems effective to many people.
18:02:23 <Pseudonym> Right.
18:02:34 <Pseudonym> There are basically three important operations:
18:02:49 <Pseudonym> There's map :: (a -> b) -> ([a] -> [b])
18:03:10 <Pseudonym> Basically, if you have a function mapping type a to type b, you can construct a function mapping list-of-a to list-of-b.
18:03:11 <cedricshock> ok
18:03:24 <Pseudonym> Secondly, there's unit :: a -> [a]
18:03:31 <Pseudonym> Which takes an a and produces a list-of-a.
18:03:54 <Pseudonym> Thirdly, there's concat :: [[a]] -> [a]
18:04:04 <Pseudonym> Which takes a list-of-lists-of-a and returns a list-of-a.
18:04:11 <Pseudonym> By concatenating the sublists together.
18:04:15 <Pseudonym> And that's all a monad is.
18:04:28 <Pseudonym> It's a type which supports those three operations.
18:06:29 <cedricshock> Ok
18:06:45 <Pseudonym> The connection to list comprehensions is this:
18:06:59 <Pseudonym> [ A | P, Q ] = concat [ [A | Q] | P ]
18:07:35 <Pseudonym> [ A | x <- xs ] = map (\x -> A) xs
18:07:57 <Pseudonym> [ A | ] = unit A
18:08:13 <Pseudonym> (Though [A | ] isn't valid Haskell.)
18:09:54 <cedricshock> I don't quite get the second one there
18:10:16 <Pseudonym> [ f x | x <- xs ] == map f xs
18:10:20 <Pseudonym> DOes that help?
18:11:08 <cedricshock> yes
18:11:39 <Heffalump> is [A | True] valid Haskell?
18:11:55 <cedricshock> So what are bind and return?
18:12:09 <Riastradh> @type [5 | True]
18:12:12 <lambdabot> [5 | True] :: forall t. (Num t) => [t]
18:12:32 <Pseudonym> return is unit
18:12:36 <Heffalump> return :: a -> [a]
18:12:46 <Heffalump> bind :: [a] -> (a -> [b]) -> [b]
18:12:46 <Riastradh> [A | x] is [A] if x and [] if not x.
18:12:50 <Heffalump> just like any other monad operation
18:13:01 <Heffalump> riastradh: yes, I assumed so, just checking
18:13:08 <Pseudonym> bind is something that turns out to be more convenient than concat
18:13:15 <Pseudonym> But it's equivalent.
18:13:18 <Heffalump> you could write those laws from above in terms of things with True at the end if you really want to keep it all valid Haskell, then
18:13:27 <Pseudonym> One point of order, BTW.
18:13:32 <Pseudonym> "[]" is not defined on all monads.
18:13:41 <cedricshock> It's zero
18:13:50 <Pseudonym> Right, and it's not defined on all monads.
18:14:07 <Pseudonym> So boolean guards don't make sense on all monads either.
18:14:10 <cedricshock> Compined with some notion of plus we get the special "MonadPlus" tings that obey a couple other rules
18:14:19 <Pseudonym> Right.
18:15:24 <cedricshock> So how is bind like concat? Let me try first.
18:15:24 <Heffalump> are there any monads with a notion of plus but no zero?
18:16:13 <Pseudonym> Can't think of any offhand.
18:17:04 <Pseudonym> That is, of course, a semigroup.
18:17:13 <cedricshock> I see that it turns elements of a into lists of b, and then makes one list of type b
18:17:57 <Pseudonym> cedricshock: Right.  You can think of it as:
18:17:59 <cedricshock> Ok, It takes two arguments (if we curry it or whatever)
18:18:09 <Pseudonym> bind m k = concat (map k m)
18:18:19 <cedricshock> It takes a list of a, and a function that goes from a to lists of bs
18:18:22 <Pseudonym> But it's usually more efficient.
18:18:34 <cedricshock> Applies that function on all of a, and concats the results
18:19:48 <Pseudonym> Interestingly, you just worked all that out from the type alone.
18:20:01 <cedricshock> So how could we write concat in terms of bind?
18:20:28 <Pseudonym> Well, you can think of it this way.
18:20:39 <Pseudonym> concat xss = [ x | xs <- xss, x <- xs ]
18:20:41 <cedricshock> I've been going through this exercise myself. This is about the 4th or 5th time through monads in the last few days.
18:21:22 <Pseudonym> So: concat xss = bind xss (\xs -> bind xs (\x -> unit x))
18:21:58 <cedricshock> I'll see if that works:
18:22:02 <Pseudonym> One interesting thing about Haskell is that you can learn a lot about a function from its type alone.
18:22:17 <cedricshock> yeah
18:22:22 <Pseudonym> There's another Wadler paper about this called "Theorems for Free".
18:22:38 <Pseudonym> Given the type, you can extract a theorem which the function must obey.
18:22:54 <cedricshock> Cool
18:23:56 <Heffalump> pity it doesn't work quite that easily for Haskell :-)
18:24:06 <Pseudonym> Yes, pity. :-)
18:24:08 <cedricshock> so return takes something and returns a monad of it.
18:24:28 <Heffalump> (have you seen the POPL 2004 paper about making it work?)
18:24:48 <monotonom> I have just seen the paper. I haven't read it.
18:25:15 <monotonom> Rather, I found the watermarking paper from the Cousots quite amusing.
18:25:36 <cedricshock> and bind takes a monad of something, and a function from that something to a monad of something else and produces a monad of somethings else?
18:25:59 <Pseudonym> No, I haven't.
18:26:28 <Pseudonym> cedricshock: I believe that's the type translated into English, yes.
18:26:57 <cedricshock> pseudonym: that's where I'm getting it from.
18:27:30 <cedricshock> so how do we ever get data out of the monad? Can we?
18:27:40 <cedricshock> Like go from [a] -> a
18:27:51 <monotonom> Some monads allow you to. Some don't.
18:28:21 <cedricshock> I saw as soon as I started asking that much of the time it would be impossible.
18:28:27 <monotonom> For example with the list monad, of course the list datatype is happy to vomit the list content for you any time.
18:28:31 <Riastradh> There is no generalized monadic operation for run.
18:28:45 <Riastradh> Run is precisely the unsafe monadic operation.
18:28:46 <cedricshock> monotome: but only one at a time
18:29:09 <ozone> is there a reason why there isn't an UnsafeMonad class?
18:29:20 <Pseudonym> Or RunnableMonad.
18:29:39 <ozone> right.  for monads like ST and lists, where you can get the value out of it safely
18:29:41 <cedricshock> Ok, so haskell monads have the bind and return operations.
18:29:49 <cedricshock> Does bind serve as map as well?
18:30:38 <Riastradh> ozone, because different monadic computations might take inputs that you can't generalize.
18:30:55 <Heffalump> you can't get a value out of list safely
18:31:06 <Heffalump> though if you consider a Haskell error 'safe' then I guess you can
18:31:25 <Pseudonym> The reason is that it would be too hard to generalise.
18:31:34 <Pseudonym> Right, like Riastradh said.
18:31:38 <Heffalump> and you'd rarely want to, anyway
18:32:01 <cedricshock> map f [a] could be bind [a] (return f)?
18:32:14 <Heffalump> you'd end up with a bunch of code where the code that used a RunnableMonad instance was hidden inside it, so you'd have to use an explicit type signature to pick a monad
18:32:32 <Pseudonym> cedricshock: map f xs = bind xs (unit . f)
18:33:12 <Pseudonym> Where (.) is function composition.
18:33:16 <cedricshock> pseudonym: I should have written it that way. That's exactly what I meant.
18:33:20 <Pseudonym> Right.
18:33:56 <Pseudonym> [ A | x <- xs, Ps ] = bind xs (\x -> [ A | Ps ])
18:34:01 <Pseudonym> where [ A | ] = unit A
18:34:21 <Pseudonym> It's a simpler translation from list comprehensions/do notation.
18:34:28 <Pseudonym> Which is why bind is preferred.
18:34:31 <Pseudonym> Although...
18:34:49 <Pseudonym> The coproducts paper suggests that bind might be harder to implement for coproducts of monads.
18:34:53 <Pseudonym> But that's another story.
18:36:00 <cedricshock> So we need ((map and concat) or bind) and (unit == return)
18:36:16 <Pseudonym> Right.
18:36:16 <cedricshock> What do we do with these, other than lists?
18:36:21 <Pseudonym> Sets.
18:36:29 <Pseudonym> Set comprehensions are like list comprehensions.
18:36:35 <Pseudonym> That's a simple example.
18:36:49 <Pseudonym> And in Haskell, of course, we do I/O.
18:37:28 <cedricshock> How do we carry around an iterator?
18:37:40 <Pseudonym> What's an iterator?
18:37:46 <Pseudonym> It means different things to different people.
18:38:07 <Pseudonym> A C++ STL iterator is very different from a Java iterator, for example.
18:38:14 <cedricshock> Like if we are going to use a monad to keep track of state (state in this case being an incrementing integer)
18:38:26 <cedricshock> Yeah. Sorry. To general a word.
18:39:05 <Pseudonym> @wiki PaulGrahamAccumulatorProblem
18:39:06 <lambdabot> http://www.haskell.org/hawiki/PaulGrahamAccumulatorProblem
18:39:25 <Pseudonym> That may help.
18:39:43 <Pseudonym> Probably easier in this case to use a state monad.
18:40:03 <Pseudonym> @wiki MonadState
18:40:04 <lambdabot> http://www.haskell.org/hawiki/MonadState
18:40:56 <np_hard> what's the canonical pronunciation of "trie?"
18:41:13 <Smerdyakov> I think it's the same as "try."
18:41:42 <Pseudonym> It confusingly comes from the word "retrieve", which is pronounced "tree".
18:41:45 <Pseudonym> But yes, it's "try".
18:43:00 <Riastradh> cedricshock, a much better word for what you were talking about is 'cursor.'
18:43:48 <np_hard> try
18:44:41 <Smerdyakov> A much better word is "dohickitydoodad."
18:44:50 <Pseudonym> That is a good word.
18:44:52 <cedricshock> riastradh: which is slightly more general.
18:44:55 <Pseudonym> @wn dohickitydoodad
18:45:15 <Pseudonym> Actually, we have a good word in Australia: "doover".
18:45:15 <cedricshock> smerdyakov: which is even more general
18:45:25 <Pseudonym> A doover is a thingummy which operates a doohicky.
18:45:26 <np_hard> bullfrog?
18:45:30 <np_hard> that's a funny name
18:45:37 <Smerdyakov> cedricshock, it's better than _!
18:45:47 <Pseudonym> For example, a lever, switch or button can all be a "doover".
18:45:54 <Pseudonym> Because it's a control for a doohicky.
18:46:12 <np_hard> I'd have called it a chazwallop
18:46:47 <Pseudonym> @wn chazwallop
18:46:54 <Pseudonym> Just checking.
18:48:43 <ozone> Pseudonym: or doobie!
18:49:03 <Riastradh> cedricshock, what do you mean 'which is slightly more general?'
18:49:32 <Riastradh> An iterator is just something that iterates.  A cursor always refers to something that keeps track of its location in some object, such as a buffer, a collection, or a screen.
18:49:32 <cedricshock> I proclaim that I now understand monads. Or at least posess the illusion of understanding monads. This makes me elligible to tackle arrows?
18:50:07 <monotonom> You are now ready for arrows.
18:50:34 <stepcut> me arrows
18:50:40 <cedricshock> riatradh: An iterator is a type of cursor. (think cursor keeping track of a position, the position being in the act of iterating). A cursor is a type of "dohickitydoodad", which is a type of _
18:50:46 <stepcut> um, sorry, that didn't come out right ;)
18:51:23 <Riastradh> cedricshock, an iterator is a type of cursor?  That makes no sense; map is an iterator, but definitely not a cursor.
18:51:26 <Pseudonym> Doobie.  Hmmm.  Not entirely sure of the connotation of that one.
18:51:28 <Riastradh> (the map on lists, that is)
18:51:31 <Pseudonym> I know what you mean, though, ozone.
18:52:05 * stepcut wonders if anyone has actually used arrows outside the context of learning about them...
18:52:07 <cedricshock> riastradh: Hmm. Hadn't thought of that sort of iterator. Was thinking STL.
18:52:10 <Riastradh> On the other hand, a cursor is one meaning of 'iterator.'
18:52:27 <Riastradh> cedricshock, had you not noticed Pseudonym pointing out that 'iterator' means many different things depending on whom you speak with?
18:52:51 <Pseudonym> @foldoc cursor
18:52:56 <Riastradh> 'Iterator' might mean 'cursor'; it might also mean 'function that iterates over a collection'; it might mean something else entirely.
18:53:03 <Pseudonym> @eval 1
18:53:09 <Pseudonym> Is lambdabot dead?
18:53:28 <Riastradh> It would seem so.
18:53:37 <cedricshock> @arr
18:53:50 <stepcut> @stepcut
18:53:53 <stepcut> :(
18:56:56 <cedricshock> Pseudonym: A few days ago we talked a bit about a spreadsheet type thing. You were suggesting arrows. I think I now see where they could be useful.
18:57:16 <Pseudonym> Cool.
18:57:44 <cedricshock> However, I'm still not quite certain what they are, or how to put them together.
18:58:20 <Pseudonym> Welcome to our world. :-)
18:58:41 <cedricshock> For example, one of the big spreadsheet problems would be having data in two+ sources, which you want to define a relationship to some new data object.
18:59:14 <cedricshock> How would / could you use arrows to bring the data together? They seem to be very, er, linear.
19:00:53 <Pseudonym> I believe what you want is this:
19:01:03 <Pseudonym> (***) :: a b c -> a b' c' -> a (b, b') (c, c')
19:01:07 <Pseudonym> Which is defined for all arrows.
19:01:08 <Smerdyakov> Wow. What was a javaNewb doing here?
19:01:11 <ozone> cedricshock: monads are linear.  arrows can form a graph.  (or at least a tree)
19:01:35 <Pseudonym> Or maybe this:
19:01:36 <Pseudonym> (&&&) :: a b c -> a b c' -> a b (c, c')
19:02:08 <cedricshock> So that *** is something you get for free after defining the basic arrow operations for your arrow?
19:02:24 <Pseudonym> I think so.
19:02:33 <Pseudonym> Or it may be a basic operation.
19:02:37 <Pseudonym> Not sure.
19:02:44 <Pseudonym> Never actually implemented an arrow before.
19:04:42 <cedricshock> So the arrow wrapping around functions could handle stuff like caching results, inspection, coordinating recomputiation etc? Or am I imagining something else?
19:05:26 <Pseudonym> You're talking about this, right?
19:05:41 <Pseudonym> arr :: (Arrow a) => (b -> c) -> a b c
19:05:52 <Pseudonym> Think of that operation like "map" for lists.
19:07:15 <cedricshock> I'm thinking at a slightly higher level, more abstract still.
19:07:44 <cedricshock> Like what can I accomplish in that mysterious dotted-line box around the arrows.
19:08:23 <Pseudonym> Right.
19:08:41 <Pseudonym> The thing you have to keep in mind is that arrows are generalisations of functions.
19:08:51 <Pseudonym> Kind of like how monads are generalisations of containers.
19:10:13 <Pseudonym> So the _generalisation_ could indeed handle caching results, inspection etc.
19:11:22 <cedricshock> In my head I see nodes that are data objects, and edges that are functions defining new data objects based on old ones. And the arrows are some funny ooze, structured around the edges that hopefully is where the spreadsheet program goes.
19:13:39 <Pseudonym> The way I see it is that the arrows are a network of communication channels.
19:13:57 <Pseudonym> When a data object gets changed, a message flies down the channels.
19:14:05 <Pseudonym> Possibly splitting, possibly merging.
19:14:12 <Pseudonym> Until it finally hits the "outputs".
19:14:53 <cedricshock> That's what Erlang is for. Erlang absolutly shines at message passing.
19:15:11 <cedricshock> And Q absolutley shines at algebraic manipulation / symbolic processing stuff.
19:15:35 <cedricshock> And haskell shines at abstractions / being abstract (or at least seems to)
19:17:15 <cedricshock> What you describe sure makes object oriented programming with signals and slots sound close to the problem domain. But the domain is functional programming, so functional programming seems close to the problem domain.
19:19:19 <cedricshock> Which puts erlang (being a functional language of many processes that can act as uber-objects) really close to the problem domain, I guess. Yet it is eager and not lazy enough for symbol manipulation (at least easily), and doesn't do GUIs very well.
19:23:31 <cedricshock> We have a problem with extensive user interaction (read side effects galore), using dynamicly chosen data types and algorithms, designing a big functional program.
19:24:19 <cedricshock> I guess the message passing will be difficult no matter what language is used. There is no good reason any data object should know everything that depends on it, but when it changes they'd ll need to know.
19:25:05 <Pseudonym> That's one problem with arrows, incidentally.
19:25:25 <Pseudonym> If you change the graph, you can't iteratively rebuild the arrows easily.
19:25:30 <Pseudonym> Sometimes you can, sometimes you can't.
19:25:38 <Pseudonym> You should look at fudgets.
19:26:01 <cedricshock> I've looked at fudgets. Very interesting.
19:27:47 <Pseudonym> Fudgets are kind of like your spreadsheet problem.
19:27:59 <Pseudonym> A user manipulates a control, then stuff has to happen as a result.
19:28:05 <Pseudonym> Which eventually gets to displays.
19:28:18 <cedricshock> Yeah
19:28:28 <Pseudonym> This is why I said "arrows".
19:29:58 <cedricshock> Hmm. I kind of like that.
19:30:37 <cedricshock> Writing a spreadsheet program as fudgets could be interesting.
19:34:41 <Riastradh> Is Fudgets still alive?
19:37:31 <cedricshock> Why not. Does anyone these days not have an X server?
19:37:54 <monotonom> Nooooooooobody
19:39:02 <Riastradh> I try to avoid the X Windows Disaster as much as possible, and I was more wondering if the project was still maintained.
19:39:16 <Riastradh> Or used very much.
19:40:07 <cedricshock> Last release was August, 2003, so no pronouncing it dead yet.
19:43:08 <cedricshock> cedric's theoem of programming: there exists no non-trivial program
19:43:29 <cedricshock> proof: All non-trivial progams never get written
19:43:29 <Smerdyakov> cedricshock, what do you mean by that?
19:44:00 <cedricshock> cedricshock: clean r key.
19:44:00 <Pseudonym> I disagree with that.
19:44:17 <Pseudonym> There exists at least one program which has been written and is non-trivial.
19:44:31 <Pseudonym> Whether or not it could have been written in an alternative, trivial way is another question.
19:44:42 <cedricshock> Yes?
19:45:05 <cedricshock> You mean XFree86?
19:45:14 <Pseudonym> No, I mean your C compiler.
19:45:17 <Pseudonym> And GHC.
19:47:53 <cedricshock> Yeah, but my C compiler never produces what I wanted, so there must be some serious bugs in it still.
19:48:22 <bluejay> or bugs in you ;)
20:04:46 <Pseudonym> Ah, you mean there are not non-trivial bug-free programs.
20:04:49 <Pseudonym> That I'd agree with.
20:05:04 <Pseudonym> But there are plenty of non-trivial buggy programs.
20:08:09 <monotonom> All programs are interesting.  Standard proof by well-foundedness.
20:09:40 <wagle> gcc was not written.  it was evolved, sort of like a frankstein slimemold
20:12:49 <bluejay> Paradox: "The smallest positive integer that cannot be described in less than thirty english words of less than ten letters each"
20:13:47 <bluejay> There are only ~10^30 things describable in that many words, so the number that cannot be described has just been described.
20:18:32 <Riastradh> 'In _fewer_ than thirty English words,' bluejay, not 'less.'
20:18:45 <Riastradh> And 'of _fewer_ than ten letters each.'
20:19:20 <wagle> Riastradh: he did not say grammatical english sentences.
20:19:34 <bluejay> lol
20:19:45 <bluejay> I sit corrected
20:19:54 <Riastradh> wagle, it was not a sentence anyways!
20:20:19 <wagle> s/$/()/
20:20:23 <wagle> s/$/(*)/
20:20:58 <wagle> x = x + 1
20:27:25 <wagle> bluejay: i understand that kolmogorov complexity is the study of objects that don't have descriptions shorter than themselves
20:28:37 <bluejay> I've never heard of kolmogorov complexity. Is it wikipediable?
20:29:05 <Smerdyakov> Almost certainly. It's appropriately geekish.
20:29:08 <wagle> not by me
20:32:25 <wagle> done! http://en.wikipedia.org/wiki/Kolmogorov_complexity
20:32:40 <bluejay> there it is. "Berry paradox"
20:34:24 <Smerdyakov> wagle, you just added a redirect?
20:36:59 <wagle> no i googled for it..
20:37:19 <wagle> then pretended to have done something to cause its existence
20:40:18 <Cale> Is it fewer cake and less currants, or less cake and fewer currants? :)
20:41:39 <desrt> Cale; i'll give you my currants if you let me have your cake
20:42:24 <Cale> I'll stick with what I have. That doesn't sound quite right.
20:42:37 <desrt> i know
20:42:41 <desrt> i was going to say "let me eat your cake"
20:42:55 <desrt> but it would have sounded even more kinky than it already does
20:43:21 <Cale> "Is not expressible in just seven words", is not expressible in just seven words.
20:43:43 <desrt> you just did
20:44:19 <desrt> plus you could quiet easily drop "just" and be down to six
20:44:26 <desrt> *quite
20:44:41 <Cale> :)
20:44:41 <wagle> Maybe
20:45:54 <wagle> Maybe Nothing
20:46:07 <desrt> maybe just
20:46:08 <Cale> heh
20:46:15 <wagle> Just Maybe
20:46:15 <Cale> Maybe Just Nothing
20:46:23 <desrt> woh
20:46:25 <desrt> Maybe Maybe a
20:46:30 <desrt> and a Just Nothing
20:46:31 <desrt> freaky
20:46:36 <wagle> join
20:47:28 <Cale> join should be in the Prelude
20:47:42 <desrt> what does join do?
20:47:43 <Cale> Just out of respect alone :)
20:47:45 <wagle> i thought it was
20:47:50 <Cale> it's out in Monad
20:48:08 <desrt> we should rewrite the prelude
20:48:09 <Cale> join :: forall a m. (Monad m) => m (m a) -> m a
20:48:11 <wagle> join :: Monad m => m (m a) -> a
20:48:18 <desrt> using nothing from the prelude in the process
20:48:21 <desrt> unless we're already written it
20:48:25 <desrt> ie: build from the ground up
20:48:39 <desrt> er.  excepting simple operators
20:48:44 <Cale> There is at least one Prelude replacement out there
20:48:53 <desrt> ie: just the functions
20:49:21 <desrt> you really need some stuff
20:49:24 <desrt> like : and so on
20:49:27 <wagle> join (Just Nothing) :: Maybe ()
20:49:27 <wagle> Nothing
20:50:05 <Cale> join is actually one of the parts of the definition of a monad in its original form from category theory
20:50:34 <desrt> i need to take a math class or something
20:51:39 <desrt> engineering focuses *way* too much on DE
20:51:58 <Cale> Indeed, and the computational aspects of things in general.
20:52:08 <wagle> DQ
20:52:16 <Cale> Dairy Queen.
20:52:32 <wagle> need diffyQ on POs
20:53:03 <Cale> Partial Orders?
20:53:05 <Cale> heh
20:54:45 <wagle> partially ordered "numbers"
20:55:13 <desrt> i love my router
20:55:15 <desrt> *hugs it*
20:55:32 <wagle> woodworking?
20:55:38 <desrt> 802.11g
20:55:52 <desrt> 200mhz linux box
20:58:18 <stepcut> my router is a 600Mhz FreeBSD box ;)
20:58:24 <wagle> i was regretting earlier today not having a gigabit card in my home file server
20:58:32 <desrt> stepcut; mine is completely silent
20:58:53 <desrt> and it says "linksys" on the front
20:59:00 <desrt> and "cisco" on the bottom
20:59:15 <Pseudonym> bluejay: THere is no paradox.
20:59:26 <desrt> my internet router is a 266MHz freebsd box, though :)
20:59:40 <wagle> oh neat..  i need to get a two slot pcmcia sleave for my iPaq, and turn it into an access point
20:59:45 <Pseudonym> The so-called "set" of integers that cannot be so described is not a set.
21:00:16 <Pseudonym> Oh, no it's not.  It is a set.
21:00:18 <Pseudonym> The empty set.
21:00:26 <stepcut> well my *other* router is very noisy and says 'ryobi' on it 
21:00:37 <wagle> Pseudonym: prove it
21:01:01 <Pseudonym> You give me a number and I'll describe it in that many words.
21:01:19 <desrt> zero
21:01:33 <Pseudonym> Zero.
21:01:34 <wagle> i
21:01:37 <Pseudonym> One word less than ten letters.
21:01:37 <bluejay> Did I say "set" in my sentence? ;) Oh, and it can't be empty since there are an infinite number of integers, but only (26^10)^30 can be described like I said.
21:02:05 <desrt> huh?
21:02:11 <desrt> this seems like quite a lot of nonsense
21:02:15 <Pseudonym> Hang on.
21:02:17 <wagle> "least" implies a mathematical object with an ordering
21:02:18 <Pseudonym> Ah, I see.
21:02:18 <desrt> -200
21:02:30 <Pseudonym> Right.  If it's not a set, then it need not have a least member.
21:02:30 <desrt> ... 
21:02:35 <wagle> "zero" is not zero words
21:02:39 <bluejay> yeah, I think I said "positive integer", but if I didn't, then just modify the phrase
21:03:24 <Pseudonym> And I do maintain that there are no positive integers which can be adequately described in a mere 30 words.
21:03:30 <desrt> this reminds me of an argument i had with someone who was convinced that pkzip would never make a file larger
21:03:46 <Pseudonym> And certainly not imprecise English words.
21:04:00 <Cale> It's not really a paradox - the only thing is that there's an option as to which number you let the phrase "The smallest positive integer that cannot be described in less than thirty english words of less than ten letters each" represent.
21:04:22 <wagle> the compressors i've seen just use the identity function when they cant make it smaller
21:04:23 <bluejay> It's a paradox. Look at http://en.wikipedia.org/wiki/Berry_paradox
21:04:28 <Pseudonym> It's like the proof that all numbers are interesting.
21:04:40 <Cale> Well, it's not a strict paradox.
21:04:41 <Pseudonym> All positive integers, anyway.
21:04:48 <Pseudonym> Proof proceeds by induction.
21:04:50 <Cale> i.e. it's not a contradiction
21:04:53 <bluejay> wagle: then they need a bit to say that they've just used the identity function
21:04:56 <desrt> wagle; won't work
21:05:00 <Pseudonym> Base case: 1 is interesting because it's the multiplicative identity.
21:05:32 <wagle> desrt: huh? its what it observably does do
21:05:33 <Pseudonym> Inductive case: Assume all k < n are interesting.  Assume n is not interesting.  Then it is the smallest uninteresting number, which makes it pretty interesting.
21:05:42 <desrt> wagle; there's always something that will be made at least 1 bit larger
21:05:54 <desrt> else the compression algorithm *must* make nothing smaller
21:06:00 <Pseudonym> wagle: Compression algorithms don't compress noise.
21:06:08 <wagle> "tagless"
21:06:08 <Pseudonym> On the whole.
21:06:11 <bluejay> hooray for pigeon holes
21:06:13 <desrt> the identity argument doesn't work
21:06:16 <Pseudonym> Precisely.
21:06:17 <Cale> "the first ordinal that cannot be named in a finite number of words" is more fun :)
21:06:35 <bluejay> Cale: yeah, that one's fun. But harder to understand. ;)
21:06:49 <Cale> The simple explanation being that that's not well-defined.
21:06:54 <Riastradh> The compression algorithm could just spit out an error message saying 'you idiot, I can't make this file smaller' and expect the user to not bother compressing it.
21:07:02 <Pseudonym> True!
21:07:06 <desrt> Riastradh; not valid
21:07:12 <Riastradh> Not valid?
21:07:16 <Pseudonym> Calling users idiots is reasonable behaviour for any reasonable program.
21:07:22 <Riastradh> Of course.
21:07:22 <desrt> it must produce an output that is the new file
21:07:30 <Riastradh> desrt, why must it do that?
21:07:30 <wagle> i think gzip can be told to compress it even if it comes out larger
21:07:48 <Pseudonym> Actually, there's a simple way to do it with pkzip.
21:07:50 <wagle> ... which i think it has to do if the output is to STDOUT
21:07:51 <monotonom> { compress :: String -> String; compress _ = error "idiot!" }
21:07:51 <Pseudonym> Compress the zip.
21:07:52 <desrt> Riastradh; because otherwise it's too easy
21:07:54 <Pseudonym> Then compress that zip.
21:07:59 <Pseudonym> Keep going until you get a larger file.
21:08:09 <wagle> graph it
21:08:11 <desrt> Pseudonym; there's an easier way
21:08:15 <bluejay> Riastradh: I think then you need to count the name of the file as part of the data. Since the compressed file will be tagged with an extension.
21:08:17 <Cale> The compression algorithm which always "returns" bottom is silly.
21:08:22 <desrt> dd if=/dev/random of=myfile bs=1k count=1k
21:08:25 <desrt> pkzip myfile
21:08:34 <Pseudonym> No, because you may get a file it can compress.
21:08:44 <desrt> of the result is smaller than the original file then report it as a bug against either the operating system or pkzip :)
21:08:45 <Pseudonym> Actually, a file of one byte will be expanded.
21:08:50 <Pseudonym> By pkzip's algorithm.
21:08:57 <desrt> always
21:08:58 <wagle> note that i said "the compressors i've seen", and not "pkzip"
21:09:06 <monotonom> { compress :: String -> String; compress _ = [] } \begin{comment}No one says it has to be recoverable\end{comment}
21:09:09 <desrt> not always true for non-pkzip, though
21:09:25 <Pseudonym> wagle: It doesn't matter.
21:09:37 <wagle> Pseudonym: sure it does
21:09:44 <desrt> your compression program could state "all files that begin with the byte "C" are compressed files.  all other files are in their original form"
21:09:53 <Pseudonym> It's the pigeonhole principle.
21:10:05 <desrt> and then any 1-byte file that was not "C" would be compressed to a single byte
21:10:07 <Cale> desrt: heh
21:10:12 <Pseudonym> If you could losslessly compress any file down to a file smaller in size, there wouldn't be enough compressed files to produce all the original files.
21:10:43 <Pseudonym> Oh, if you give me any file, I can produce a program which compresses it to onebit.
21:10:59 <monotonom> I already did 0 bit.
21:10:59 <desrt> you can produce a program that compresses it to 0 bits
21:11:00 <Pseudonym> Disclaimer: All other files are expanded by one bit, and the decompression program may be big.
21:11:06 <wagle> i can compress it to zero bits!  8-ppppp
21:12:08 <desrt> you could make the compression program quite small :)
21:12:08 <wagle> if (-z file) cat ORIGINAL_FILE else panic "duhhh!"
21:12:47 <desrt> all it needs to store is an md5/sha1/whatever of the file in question
21:13:29 <desrt> (i wish you luck in trying to trick it)
21:13:35 <wagle> hmm..
21:13:57 <desrt> it's incorrect
21:14:00 <desrt> but not proveably so
21:14:01 <wagle> if (-z file) cat ORIGINAL_FILE else loop_forever "now decompressing, please wait!"
21:14:05 <desrt> and certainly not demonstratably so
21:14:37 <Pseudonym> Of course there are an infinite number of files which map to the same SHA-1/MD5/whatever signature.
21:14:57 <desrt> Pseudonym; you can't prove that
21:15:09 <Pseudonym> Let me rephrase.
21:15:13 <monotonom> Just pick the smallest, most uninteresting one.
21:15:21 <Pseudonym> There exists an MD5 signature such that an infinite number of files map to that signature.
21:15:32 <wagle> there exists a md5 signature with an infinite number of files mapping to it
21:15:35 <desrt> that's true
21:15:38 <Pseudonym> Given a signature, I can't prove that more than one file maps to it, no.
21:16:01 <desrt> but there is at least one
21:16:04 <monotonom> I can. Just give me some time.
21:16:06 <desrt> and hopefully, it's true for all of them
21:16:16 <desrt> monotonom; you might end up wasting your life
21:16:17 <Pseudonym> monotonom: No you can't.
21:16:28 <desrt> md5 might have a fluke in it
21:16:33 <monotonom> I just ask my computer to look for it.
21:16:43 <desrt> that says that only 1 file maps to the hash value 0x000000000000000000000000
21:16:44 <Pseudonym> If there exists an MD5 signature such that only one file (or no files!) maps to it, you may not be able to prove that.
21:16:49 <desrt> or maybe no files have that signature at all
21:17:49 <monotonom> I may waste electricity but I have a nuclear power plant.  Oops I forgot I am not playing SimCity.
21:18:42 * desrt imagines that finding md5 colissions by brute force is still one of those 'impossible' problems
21:19:22 <desrt> although a good way to make yourself quite famous
21:19:33 <desrt> would be to write a text file with your name in it or whatever
21:19:36 <desrt> take its hash
21:19:44 <desrt> and search for files with that same hash value :)
21:20:06 <Pseudonym> If you have 2^128 files to search.
21:20:22 <desrt> one of the few projects you could do that would be impossible for someone else to take credit for :)
21:20:31 <Pseudonym> Actually, you only need 2^64 or so before there's a reasonable chance.
21:20:36 <desrt> Pseudonym; or more.  or infinite. :)
21:20:44 <desrt> wrong
21:20:46 <monotonom> It may be easier to look for two files that collide. Then go to the government registry and change my name to one of the files.
21:20:48 <desrt> 2^127
21:20:49 <Riastradh> Eh, I've got about that many files lying around.  I'll go try it!
21:21:12 <Pseudonym> You need 2^64 or so files before you have a reasonable chance of two of those files having a hash collide.
21:21:12 <desrt> after searching 2^64 files you've only exausted 1 in 2^64 of the space
21:21:28 <Pseudonym> That's the birthday paradox.
21:21:28 <Riastradh> I'll get back to you guys in a billion years.
21:21:30 <desrt> oh
21:21:33 <desrt> like birthday problem
21:21:37 <Pseudonym> Right.
21:21:37 <desrt> ya
21:21:43 <desrt> i mean to find one equal to myself :)
21:21:47 <wagle> use a quantum computer
21:22:05 <desrt> well
21:22:13 <desrt> good luck with that 2^64 business
21:22:19 <desrt> because the storage requirements are through the roof
21:22:33 <wagle> i wonder if you could solve it in a 128 qubit QC
21:23:56 <desrt> 2^64 * 32
21:24:03 <desrt> 590295810358705651712
21:25:56 <desrt> with something like md5 hash outputs to store it would be pretty easy to store and search the results really quickly
21:26:05 <desrt> because they'd be almost perfectly distributed
21:27:04 <wagle> where are you going to store a hash table that big?
21:27:14 <desrt> that's the problem :)
21:27:41 <desrt> 590 295 810 trillion bytes
21:28:13 <wagle> times $500
21:28:21 <Pseudonym> Implementing MD5 on a quantum computer doesn't sound like fun.
21:28:26 <desrt> it's within the realm of imagination
21:28:47 <wagle> Pseudonym: you have no sense of adventure!  8)
21:28:55 <desrt> if you were buying that much space i think you'd probably get a bit of a volume discount :)
21:29:35 <wagle> not really..  the support hardaware starts jacking the price back up at a terabyte or three
21:29:59 <desrt> well
21:30:01 <desrt> on the bright side
21:30:07 <desrt> the disks could afford to be ridiculous slow
21:30:13 <desrt> and the computer would still run quite rapidly
21:31:12 <desrt> because each disk would see very little activity if it only gets hit 1 time in [however many disks there are]
21:31:39 <wagle> but it would be slow when it was its turn
21:31:53 <Pseudonym> Let's be fair, you could optimise your algorithm.
21:32:04 <Pseudonym> Reads and writes would be pretty coherent.
21:32:35 <monotonom> I have a better use of that computer. Have it write Haskell programs from specifications.
21:32:43 <desrt> if you were spendign that much cash
21:32:56 <desrt> you'd want each drive to support 1 very simple operation
21:32:58 <wagle> the way i'd try it would be to randomly choose a file, take its md5sum, then try to generate another file with the same md5sum..  no particular storage requirements
21:32:59 <desrt> read and set bit
21:33:15 <desrt> wagle; bad call
21:33:30 <wagle> of course, you might get unlucky and choose a onesy, or one with a really large twin
21:33:44 <desrt> wagle; pseudonym's algorithm will hit a result in 9223372036854775808 times less time than yours
21:34:15 <wagle> but i can afford mine
21:34:23 <desrt> you won't live that long
21:34:35 <monotonom> As soon as the ICFP problem is posted, you translate it into the specification language, then let the computer solve it.  You may qualify for the lightning session either.  You will become famous overnight, and the code monkeying industry will never be the same.
21:34:37 <wagle> i dont have a half trillion dollar handy
21:34:49 <monotonom> s/either/even/
21:34:56 <wagle> i didnt say i was going to try brute force
21:35:22 <desrt> wagle; if the entire world came together we could solve it the pseudonym way within a decade or two
21:35:30 <desrt> possibly quite less
21:35:46 <desrt> wagle; if the entire world came together we couldn't solve it your way before the expected end of the world
21:35:56 <wagle> rrrrigght..  that sounds way more realistic than my way
21:35:59 <Pseudonym> I have a suspicion that any reasonable program built automatically from a specification will have a specification which is just as hard to debug as the traditionally-written program would have been.
21:36:22 <desrt> wagle; it is
21:36:30 <desrt> neither way is very realistic
21:36:39 <desrt> but one way *could* happen
21:36:42 <desrt> yours just couldn't
21:36:43 <wagle> i cal start mine now..  race ya
21:36:48 <desrt> be my guest
21:36:49 <wagle> i can start mine now..  race ya
21:36:51 <monotonom> I only say the code monkeying industry will never be the same.  I don't say it will disappear. :)
21:36:58 <desrt> have fun
21:37:03 <wagle> you will never start
21:37:07 <monotonom> Namely, it will become specification monkeying.
21:37:08 <desrt> i won't have to
21:37:10 <desrt> and we'll still tie
21:37:16 <desrt> because neither of us will find a colission
21:37:26 <desrt> and i'll have put much less effort in
21:37:30 <desrt> so really, i'll have won :)
21:37:48 <Pseudonym> And then we'll have to come up with an ad-hoc language to write specifications in, and then it will become Turing-complete, and then we'll be in the same mess, only one meta-level up.
21:38:18 <desrt> we're not talking getting hit by lightning on the same day as winning the lottery here
21:38:35 <desrt> we're talking about doing that every day for a year straight
21:38:38 <wagle> desrt: and i wasnt talking brute force
21:39:04 <wagle> desrt: and you are assuming md5 is perfect
21:39:15 <desrt> wagle; sounded like you were talking brute force to me
21:39:22 <desrt> and i know md5 isn't perfect
21:39:22 <monotonom> Isn't it the same over all human endeavours?  Just twenty years ago accounts were calculator monkeys, now they are spreadsheet monkeys.  But there is an improvement.  Calculator monkeys couldn't afford to investigate several "what-if" scenerios; spreadsheet monkeys can.
21:39:34 <desrt> someone's already found colissions in the digest function
21:39:47 <wagle> Pseudonym: most specifications are "smaller" than the implementation
21:40:03 <desrt> anyway.  this is boring :)
21:40:11 <Riastradh> Don't think; hack!
21:40:13 <Pseudonym> wagle: Not all, unless specification is lossy compression.
21:40:35 <Pseudonym> Besides, it doesn't imply that they're simpler.
21:40:42 <desrt> no
21:40:47 <desrt> implementation is always bigger
21:40:58 <Pseudonym> I have a program I can mail you which initialises a 10,000 element array.  It's simple, but it's fairly big.
21:40:59 <monotonom> Excel is certainly harder to use than calculators.
21:41:01 <desrt> if it properly impliments the specificiation it contains more information
21:41:29 <Pseudonym> Handy for people who get paid by number of lines of code. :-)
21:41:37 <shrimpx> specification doesn't describe machine state changes =)
21:41:52 <wagle> shrimpx: some do
21:42:10 <desrt> specification is what it should do
21:42:12 <desrt> not how it does it
21:42:15 <desrt> implimentation is both
21:42:19 <shrimpx> if that kind of low level data is needed in the problem at hand... yeah
21:42:31 <monotonom> Do accountants today even have the choice to say "I prefer to work low-level, I will stick with my calculator and never touch Excel"?  I think such an accountant would be jobless.  I want to see the same thing happen in the software industry.
21:42:52 <wagle> intel has various specifications of its chip
21:43:03 <Pseudonym> Hang on, isn't that the definition of declarative programming?
21:43:10 <Pseudonym> Say what it should do, not how to do it?
21:43:16 <monotonom> Yes.
21:43:17 <Pseudonym> But we still write algorithms!
21:43:31 <Pseudonym> They're just declarative algorithms.
21:43:32 <desrt> humans are smarter than computers
21:43:42 <monotonom> The current state-of-art of declarative programming is miserable.
21:44:53 <monotonom> Humans are smarter?  Don't kid me.  The America trend and the Moore trend together implies that humans (at least Americans) become more and more stupid, and computers become faster and faster.  There will be a cross-over, if it has not already happened.
21:45:13 <Pseudonym> Stupid is not the antonym of fast.
21:45:32 <Pseudonym> And Blinn's law negates Moore's law.
21:45:32 <wagle> artificial intelligence was a few years away back in the 50's
21:45:47 <monotonom> Creativity can be replaced by a fast enough brute force search.
21:45:52 <wagle> blinn's law?
21:46:14 <Pseudonym> Blinn's law is a rule of thumb in the visual effects/animation industry.
21:46:26 <Pseudonym> Basically, the amount of time it takes to compute a film frame is constant.
21:46:43 <Pseudonym> As machines get faster, audience expectation grows at the same speed.
21:47:08 <wagle> "all disks are full"
21:47:13 <Pseudonym> Right.
21:47:27 <Pseudonym> All CPUs run at a constant utilisation, too.
21:47:48 <Pseudonym> And I disagree that creativity can be replaced by a fast enough brute force search.
21:48:51 <Pseudonym> At least not with a fast enough test to see if a result is clever or not.
21:49:12 <Pseudonym> Cleverness metrics are notoriously tricky.
21:49:44 <monotonom> Cleverness is overrated.
21:50:53 <wagle> interesting that the computer that "deep blue" was implemented on was disassembled immediately after "defeating" Kasparov
21:52:14 <wagle> so it cannot be replicated
21:52:38 <Riastradh> ...well...you could redo it from scratch...
21:53:12 <Pseudonym> If you had an identical pre-game Kasparov.
21:53:14 <wagle> you ever read "The Double Helix" by Watson and Crick?
21:53:53 <Pseudonym> No, I'm waiting until they make it into a film.
21:54:08 <wagle> haha
21:54:54 <wagle> the guy who lead the deep blue team gave a talk..  he convinced me that he'd pulled something over
21:55:40 <wagle> .. but Kasparov was complaining that he had no access to the games that Deep Blue was playing
21:56:25 <wagle> .. a couple months ago i saw that Kasparov is involved i another chess playing game that is "open source" this time
21:57:36 <shrimpx> deep blue had analyzed many/all of kasparov's games prior to the showdown, right?
21:58:09 <wagle> there were rumors of a team of human assistants to deep blue
21:58:32 <shrimpx> heh
21:58:39 <wagle> analyzed all of kasparov's games
21:59:25 <wagle> since deep blue was immediately disassembled (the next day, i think), no way to find out for sure, and no way to replicate
21:59:30 <monotonom> Yes it is quite possible they designed heuristics targetted at him.
21:59:58 <wagle> so if you try now to replicate, and fail, it must be bevcause you werent good enough, not because deep blue didnt either
22:00:37 <wagle> non-replicatibility is not science
22:01:36 <wagle> kasparov intensely studies the games of his opponents.  but was not allowed to do this with deep blue
22:08:26 <vegai> did deep blue have previous matches?
22:08:48 <vegai> officially?
22:08:56 <wagle> a few
22:09:15 <wagle> not many
22:10:01 <wagle> but his attempts to get even them were stonewalled
22:17:30 <wagle> i cant find the cite of what computer program kasparov was helping develop, but he does seem to be playing X3D Fritz a lot
22:23:12 <vegai> I wonder how large the number of possible chess games is
22:23:32 <Smerdyakov> Is there an official limit on number of turns?
22:24:47 <wagle> i read thing that gave numbers, but dont remember
22:25:04 <vegai> oh, indeed. People so inclined could keep playing forever
22:25:07 <Riastradh> No, but I believe there is a limit to the number of times you can have a single configuration on the board.
22:25:15 <wagle> there are disallowed cycles in moves
22:25:17 <Riastradh> So you'd be limited by the number of possible configurations of pieces.
22:25:34 <Pseudonym> There are more chess games than there are particles in the known universe.
22:25:38 <wagle> in practice games are quite finite
22:25:39 <Riastradh> (which would be a pretty hard limit to reach, of course, but it would still be there)
22:26:25 <wagle> http://mathworld.wolfram.com/Chess.html
22:26:45 <wagle> (google number of possible chess games)
22:26:58 <vegai> 10^10^50
22:27:48 <vegai> anyway, a bit more than what fits a computer today
22:28:01 <vegai> no pun there
22:30:06 <vegai> it will be disturbing when after the first move, computer tells you "checkmate in 46"
22:30:17 <monotonom> Hahaha
22:31:18 <wagle> .. after doing a retina scan to find out that you are kasparov
22:31:41 <wagle> ... that'd really send him over the top
22:32:53 <vegai> "checkmate in 46, but since you're you, probably much sooner"
22:33:00 <monotonom> vegai is Kasparov?
22:33:07 <vegai> I don't think so
22:33:17 <Riastradh> Hah!  We've caught you!
22:33:39 <wagle> note that there are far fewer realistic positions (ie from real/serious games) than random positions of pieces on the board
22:34:22 <wagle> to wit: realistic positions are easily remembered by chess players than random positions
22:34:34 <wagle> ... much more easily ...
22:35:25 <wagle> so the number of realistic games is far fewer than all possible games..  and the boundary is fuzzy
22:37:15 <wagle> hmm:
22:37:18 <wagle> Tiny nitpick: Zermelo seems to make a mistake about the rules of chess. Chess can?t, even in theory, go on forever, because of the 50-move rule. (If 50 moves pass without a pawn being moved, or a piece being taken, it?s a draw; in some situations there?s an extension.) So there are only a finite (though impossibly huge) number of possible chess games, which I think entails that best strategies exist for both sides.
22:37:31 <wagle> (from http://www.henryfarrell.net/movabletype/archives/000068.html)
22:41:13 <wagle> here's a fun page: http://home.earthlink.net/~mrob/pub/math/numbers-11.html
22:59:10 <Pseudonym> Oh, it doesn't have Graham's number.
23:02:31 <Pseudonym> Oh, I tell a lie.  There it is.
23:02:35 <Pseudonym> http://home.earthlink.net/~mrob/pub/math/largenum-3.html#graham
