00:00:59 <ozone> i think all that is going to be superceded by the scrap your boilerplate stuff anyway
00:01:02 <urtie> ozone: that's quite right
00:01:15 <ozone> oh, you're urtie
00:01:22 <ozone> boo :)
00:01:41 <shapr> I thought they were different versions of the same thing?
00:02:11 <urtie> yeah, managed to firewall the ssh port of my home server
00:02:11 <ozone> shapr: i dunno, i'm sure kosmikus or one of the generic haskell folks can give you a less hand-wavy answer than me
00:02:21 <shapr> ok
00:03:15 <urtie> The "Scrap your boilerplate" approach to generic programming in Haskell has been further elaborated, see the recently submitted paper "Scrap more boilerplate: reflection, zips, and generalised casts" available from http://www.cs.vu.nl/boilerplate/. This papers shows how to fill some of the gaps (such as generic zips) which previously were difficult to solve in this approach
00:03:33 <shapr> aha
00:06:33 <shapr> at first glance, generics would make it easy to htmlize datatypes
00:07:04 <urtie> xmlize even
00:07:41 <shapr> yup
00:07:54 <urtie> `UUXML: A Type-Preserving XML Schema - Haskell Data Binding' by Frank Atanassow, Dave Clarke and Johan Jeuring (to appear in PADL'04) 
00:07:58 * urtie smiles
00:08:32 <shapr> hm!
00:08:58 <urtie> that uses generic haskell, btw
00:09:15 <shapr> neat
00:10:12 <ozone> shapr: that's what i was playing around with in my demo
00:10:23 <ozone> it seems that they've taken it a bit further than the 'playing around' step. :)
00:14:41 <shapr> yah, XML Schema are complex
00:19:50 <shapr> hm, interesting paper
00:25:10 <Pseudonym> Must away.
00:25:13 <Pseudonym> Nytol!
00:57:27 <shapr> hi jmob 
00:57:36 <jmob> hola
00:58:01 <shapr> man I can't figure out what's up with these boilerplate demos
01:04:30 <shapr> oh, I think I got it
01:10:02 <shapr> how do I make a Data instance for () ?
01:10:11 <shapr> jmob: any haskell questions?
01:10:16 <shapr> hi cuelebre 
01:10:25 <cuelebre> hello 
01:10:35 <jmob> shapr: does haskell have/need macros?
01:10:53 <shapr> there's Template Haskell
01:11:07 <shapr> it's sort of the same thing
01:11:31 <andersca> although way cooler
01:12:12 <shapr> it's staged: generation, type-checking, compilation
01:12:47 <shapr> and the newly compiled code can then generate more
01:13:32 <shapr> Wolfgang Thaller used that to write a neat Objective C binding
01:14:25 <shapr> I've heard that the next version of Greencard uses Template Haskell the same way
01:17:51 <jmob> Greencard?
01:19:18 <juhp> http://www.haskell.org/greencard/
01:19:57 <juhp> it's a binding generator
01:20:16 <juhp> or ffi tool
01:20:50 <jmob> ah, interesting
01:25:25 <juhp> haskell _is_ done, right?
01:25:38 <l^rchkrn> juhp: Hircules RuLZ!
01:25:41 <shapr> huh?
01:25:45 <shapr> is done?
01:26:28 <shapr> oh, haskell.org is down, yes
01:26:44 <juhp> erm, s/done/down/
01:26:52 <juhp> ugh
01:27:04 <juhp> l^rchkrn: well, thank you
01:27:16 <juhp> buggy, but nice ;)
01:27:25 <l^rchkrn> :-D
01:27:34 <shapr> the more I play with galeon et al, the more I want a haskell browser
01:27:39 <l^rchkrn> Is there a command-line Hircules?
01:27:45 <juhp> hopefully there will be another release before the New Year ;-)
01:28:04 <juhp> seriously, I'm hoping to get some more work done on it soon
01:28:08 <shapr> galeon seems to be losing features and stability
01:28:14 <juhp> shapr: me too
01:28:24 <juhp> all the browsers suck
01:28:39 <juhp> from a UI pov I mean
01:28:59 <juhp> l^rchkrn: I'm afraid not
01:28:59 <shapr> yup
01:29:43 <l^rchkrn> elinks and firefox are cool, nope?
01:30:33 <juhp> firefox keeps losing focus for me... I find myself keep having to click somewhere to that I can press C-t, yuck
01:32:07 <juhp> firefox is the nicest browser I've used though - just wish they would make tab placement configurable
01:32:21 <shapr> yes, that happens to me too
01:32:29 <shapr> both of those
01:33:14 <juhp> l^rchkrn: I would like to add tty support one day - but don't hold your breath :)
01:33:37 <juhp> l^rchkrn: any feature requests? :)
01:34:23 <l^rchkrn> :-\ Well in FVWM Firefox cant loose focus so i didnt notice it, but it sounds annoying...
01:34:32 <l^rchkrn> juhp: Nick coloring.
01:35:00 <juhp> shapr: OT, but have you tried "Tabbrowser Extensions"?
01:35:05 <shapr> nobe
01:35:29 <juhp> l^rchkrn: yes, ME Want ;)
01:36:09 <juhp> shapr: it allows one to place tabs on the sides, etc and other stuff
01:37:47 <juhp> hircules could do with a good cleanup though to be honest
01:38:22 <urtie> plus, it could use an update to the entry in the Haskell Communities and Activities Report. ;P
01:38:35 <l^rchkrn> juhp: I am still just learning Haskell... But hacking on Hircules would be cool if once I know this language well enough...
01:39:15 <juhp> urtie: yep - unfortunately I haven't made a new release since the last report :-\
01:39:35 <urtie> *so*? :)
01:40:46 <juhp> Well, I dunno, suppose I could write, "hoping to make a new release before the next H C&A..."?
01:41:13 <urtie> for instance
01:41:31 <juhp> I have a few fixes I didn't check into sf yet...
01:41:58 <juhp> feeling kinda burnout though right now...
01:42:19 <juhp> work has been overclocking me
01:42:36 <kosmikus|away> shapr, ozone, urtie: I am not sure if I have read enough of the backlog, but there might be additional confusion due to the fact that Data.Generics contains also some declarations related to the paper "Derivable Type Classes" by Ralf Hinze and Simon PJ, which is an approach to generic programming that is more directly related to the one that GH follows ... This part is still there, afaik, but no longer developed, and will probably be compl
01:43:19 <urtie> juhp: that happens. ;)
01:43:48 <urtie> I could just take the previous entry, and update it to reflect `a new release is expected before the next release of the HC&AR'
01:43:57 <juhp> shapr: I was thinking earlier IWBNI lambdabot didn't need a @ when you talk to it in private?
01:44:41 <juhp> urtie: ok, thanks for the nudge - will write a few lines then :)
01:45:26 <juhp> I was more thinking about talking about Fedora Extras QA of haskell pkgs, but don't have much to say there either
01:45:53 <juhp> hugs, ghc, gtk2hs are still in the incoming queue at fedora.us...
01:48:06 <urtie> hm. pity. ;)
01:48:19 <urtie> better leave that to the november HC&AR then.
01:48:55 <shapr> juhp: good point
01:49:34 <juhp> urtie: well, on the other hand it would be good to encourage others to join in the effort
01:49:42 <shapr> I think it'd be cool to hack on a Haskell browser
01:50:16 <juhp> gemi has been doing a great job of submitting those packages and updating them, but noone is Qa'ing them
01:50:35 <juhp> well, I started but then ran out of time
01:50:49 <urtie> if you manage to find the time to write that up in a few lines, please do.
01:51:12 <juhp> it is really not that hard though - though the fedora.us people are a bit "fascist"^Wfanatic about packaging
01:51:26 <juhp> urtie: yeah, I will :)
01:51:50 <shapr> kosmikus|away: is the Data.Generics and "deriving Data" the Generic Haskell related one? or other?
01:52:33 <juhp> once gtk2hs is added, we can submit hircules too ;)
01:52:38 <urtie> juhp: being fascist about packaging is a *good* thing.
01:52:38 <shapr> actually, a Self-like code collab tool would be the most fun
01:52:52 <juhp> urtie: agreed
01:53:15 <shapr> something where you get shared collaborative editing and a share GUI of some type
01:53:44 <shapr> is there such a thing as a multi-user shared Xnest that any user can write to?
01:55:42 <shapr> I want graphical irc
01:57:26 * shapr has a large appetite for code
02:03:11 <shapr> in theopen source world, wanting code means write it!
02:04:42 <andersca> or paying someone to write it
02:06:02 <shapr> right
02:06:09 <shapr> speaking of which
02:06:53 <shapr> I wonder how the commercial haskellers page is doin
02:10:00 <flaw> does haskell have generators?(I'm thinking from a python perspective)
02:10:31 * flaw doesn't need one; just curious
02:13:35 <norpan> generators?
02:16:18 <shapr> yes
02:16:37 <shapr> though they look a bit different
02:16:43 <norpan> any function in haskell can be a generator, since haskell has lazy evaluation
02:17:05 <shapr> lazy lists are common
02:17:27 <shapr> iterate and scanr are good for other things
02:18:09 <shapr> @eval scanr (\x -> x + 1) [1,2,3]
02:18:10 <lambdabot> <<EM Dynamic -> EM Dynamic>>
02:18:20 <shapr> @eval scanr (\x -> x + 1) 0 [1,2,3]
02:18:20 <lambdabot> type error
02:18:23 <shapr> hm
02:18:59 <shapr> > scanr1 (+) [1..5]
02:18:59 <shapr> [15,14,12,9,5]
02:19:58 <shapr> @eval foldr (\x y -> x + y) 0 [1,2,3]
02:19:58 <lambdabot> 6
02:20:08 <shapr> @eval scanr (\x y -> x + y) 0 [1,2,3]
02:20:08 <lambdabot> [6, 5, 3, 0]
02:20:24 <shapr> flaw: is that understandable?
02:21:03 <flaw> sorta, I'm still getting a grasp on haskell's syntax
02:21:22 <flaw> it's quite a contrast from what I'm used to..
02:21:40 <shapr> well, map does something once to each item of a sequence
02:22:20 <shapr> fold 'rolls up' a sequence with the something happening with the last result and the next item
02:22:56 <shapr> scan shows you all the intermediate steps of fold
02:23:05 <shapr> is that better?
02:24:57 <flaw> hrm
02:26:33 <shapr> iterate turns a function into a lazy list where each result is the input for the next step
02:26:41 <shapr> > take 5 $ iterate (+ 1) 1
02:26:41 <shapr> [1,2,3,4,5]
02:26:46 <flaw> k
02:27:55 <shapr> flaw: do you know how to use foldr to get the sum of a list of integers?
02:27:59 <andersca> yoyo ski
02:28:44 <flaw> the little @eval foldr you did earlier?
02:29:07 <shapr> yup
02:29:16 <ski> yo #haskell
02:29:24 <shapr> hi ski
02:29:46 <shapr> so, how could you get the length of a list with foldr ?
02:31:29 <flaw> I'm playing with it in ghci to make it soak in 8)
02:31:43 <shapr> @eval foldr (\x -> x) 0 [1,2,3,4,5]
02:31:44 <lambdabot> type error
02:31:57 <urtie> @eval foldr (\x y -> x + 1) 0 [1,2,3,4,5]
02:31:58 <lambdabot> 2
02:32:18 <urtie> owh, duh.
02:32:36 <andersca> fold_r_
02:32:36 <andersca> :)
02:32:54 <urtie> yeah, I'm nowhere near awaak
02:32:55 <shapr> seems to act like foldl
02:32:57 <urtie> awake, even.
02:33:15 <shapr> or maybe I'm asleep too
02:33:34 <simon-> how safe is @eval?
02:33:45 <shapr> very safe
02:33:54 <shapr> it's not a Haskell interpreter
02:34:07 <simon-> oh..
02:34:48 <ski> shapr : apropos, the '->' syntax in lambdas are a recent addition, yes ?
02:35:03 <shapr> it's a lambda calculus interpreter, but it's great for demonstrations
02:35:13 <shapr> yes, they are
02:35:46 <shapr> @get-definition foldr
02:35:46 <lambdabot> foldr = \\c n l.if null l then n else c (head l) (foldr c n (tail l))
02:36:01 <ski> \\ ?
02:36:23 <shapr> I'm not sure
02:36:31 <ski> old-style lambda ?  multi-arg one ?
02:36:42 <shapr> I think my escape code has a bug
02:36:59 * ski tries
02:37:16 <ski> lambdabot:  @eval (\\x.x + 1) 5
02:37:17 <lambdabot> (line 1, column 3):
02:37:17 <lambdabot> unexpected "\\\\"
02:37:17 <lambdabot> expecting identifier
02:37:17 <shapr> @define a \x -> x + 1
02:37:17 <lambdabot> a defined
02:37:28 <shapr> @define a (\x -> x + 1)
02:37:28 <lambdabot> a defined
02:37:34 <shapr> @get-definition a
02:37:35 <lambdabot> a = (\\x -> x + 1)
02:37:37 <shapr> yup
02:37:39 <ski> oh, ok
02:39:58 <shapr> @eval foldr (\x y -> 1 + y) 0 [1,2,3,4,5]
02:39:58 <lambdabot> 5
02:40:21 <urtie> @eval foldl (\x y -> x + 1) 0 [1,2,3,4,5]
02:40:21 <lambdabot> 5
02:41:06 --- topic: '["We put the Funk in Funktion","See logs @ http://tunes.org/~nef/logs/haskell/", "Learning Haskell - http://www.haskell.org/learning.html","deb http://www.syntaxpolice.org/haskell-experimental unstable/","Donate your brain! - hawiki/UsingHaskellWikiMaterial","haskelldb 0.8 released","related channels #haskell.se #haskelldb #darcs","http://www.haskell.org/hawiki/EuroHaskell"]'
02:41:06 --- topic: set by bring on [Tue May 04 12:41:39 2004]
02:41:06 --- names: list (clog ski dennisb isomer ludde Matt-W skylan opet raver gnufan Etaoin Jad viblo Pinnen tic Leimy_zzzz shawn_ blackdog_ bringert themaximus SamB tmoertel gini ssorrow jak l^rchkrn|away ibid Fractal mattam forester Segora flaw Spark shrimpx mwotton tumm cmeme Lemmih shapr smkl urtie Jerub JaffaCake liiwi seafood Cale asmodai Smerdyakov Lunar^ earthy _rubix Hipo desrt tea Taaus Riastradh kosmikus|away Lor wagle jagular Jon emu polli edwinb)
02:41:06 --- names: list (jasonw _Codex otsuka vegai norpan eivuokko zas saz simon- flori gdsx ozone Igloo andersca juhp [dan] lambdabot Maddas chucky Lurc ksandstr neologism keverets)
02:45:05 <norpan> @get-definition head
02:45:06 <lambdabot> head not defined
02:45:21 <norpan> indeed
02:45:32 <norpan> @get-type head
02:45:32 <lambdabot> Sorry, I don't know the command "get-type", try "lambdabot: @listcommands
02:45:39 <norpan> @type head
02:45:41 <lambdabot> head :: forall a. [a] -> a
02:46:00 <ski> that's GHCi
02:46:04 <norpan> @get-definition length
02:46:04 <lambdabot> length = foldl (\\x y.x+1) 0
02:49:50 <ski> @eval foldr (\x y -> \a -> x : y (x : a)) (\a -> a) [1,2,3,4,5] []
02:49:51 <lambdabot> [1, 2, 3, 4, 5, 5, 4, 3, 2, 1]
02:49:57 <ski> @eval foldl (\y x -> \a -> x : y (x : a)) (\a -> a) [1,2,3,4,5] 0
02:49:58 <lambdabot> type error
02:50:02 <ski> @eval foldl (\y x -> \a -> x : y (x : a)) (\a -> a) [1,2,3,4,5] []
02:51:14 <lambdabot> [5, 4, 3, 2, 1, 1, 2, 3, 4, 5]
02:51:17 <ski> (which direction does those lean in ?)
02:52:34 <urtie> inward and outward. >:P
03:48:54 <shapr> @yow
03:48:55 <lambdabot> Ha ha   Ha ha  Ha ha   Ha  Ha  Ha  Ha  -- When will I EVER stop HAVING
03:48:55 <lambdabot>  FUN?!!
03:52:48 <ski> err : "... The programming language that will be our tool for this is Haskell, a member of the Lisp family. ..."
03:53:05 <shapr> where's that?
03:53:13 <ski> http://homepages.cwi.nl/~jve/HR/
03:53:20 <ski> "The Haskell Road to Logic, Maths and Programming"
03:53:25 <ski> (seen on LtU)
03:53:29 <shapr> crazy
03:56:31 <ski> the textbook could still be interesting, of course
04:10:27 <Cale> Perhaps they mean it in the more general sense of "anything functional"
04:11:28 <ski> maybe
04:27:36 <Igloo> Can anyone confirm the cvs.haskell.org key has changed, and what it now is?
04:28:13 <Igloo> Looks like haskell.org is down again too  :-(
04:28:42 <JaffaCake> Igoo: yes the key has changed, and haskell.org is currently down.
04:29:07 <JaffaCake> cvs.haskell.org,129.95.44.145 ssh-rsa AAAAB3NzaC1yc2EAAAABIwAAAIEAxVKxvKsy7T5Yl6t7q+KEJSB/QnlVtHrW+HTCRh5IzjiN8khFKmtG6RqePq7KH12kzrHRiRxdDxofH9P/yms27SaoABuoNT3fhX+Co7bQ9k1iTt3X4mwbBsSbRtnWA+Rr5IY4BhCT/zKtSkg1ufjMi46L6nUHlTuYeEk96Gb/qEU=
04:29:38 <Igloo> Ta
04:29:48 <JaffaCake> cvs.haskell.org was compromised, and had to be reinstalled
04:30:24 <Igloo> Oh, looks like I'll have to resend my SSH key to them
04:30:47 <JaffaCake> if your account doesn't work, you'll have to ask Jeff Lewis <jlewis@galois.com>
04:30:57 <JaffaCake> he's only reinstating accounts on demand
04:31:07 <Igloo> OK, thanks
04:33:58 <Cale> haskell.org has been down a lot recently.
04:36:06 <Cale> Perhaps there should be a backup host for the webserver. Not that I can provide it :)
04:42:37 <JaffaCake> http://www.mirror.ac.uk/sites/www.haskell.org
04:43:08 <JaffaCake> has a full mirror, including GHC distributions!
04:43:39 <ski> nice
04:43:55 <Lunar^> good news
04:44:00 <JaffaCake> I asked them to mirror it ages ago, and completely forgot
04:44:23 <JaffaCake> I'm sure there are other places that would mirror for us if we asked nicely
04:44:32 <Igloo> JaffaCake: Would GHC doing an unaligned memory access be expected or a bug?
04:44:46 <JaffaCake> hmmm, probably a bug
04:45:15 <JaffaCake> misaligned accesses can cause traps on some arch's, so we avoid them
04:45:29 <JaffaCake> but I suspect we haven't run on any such arch for a while (Alpha was one)
04:46:21 <Igloo> IA64 can be told to be one, and the Debian buildds do. I'm just trying to check if that is what is going on
04:51:37 <urtie> hey, an igloo!
04:52:44 <Igloo> JaffaCake: OK, looks like that is the problem. What's the best debug info to give you?
04:54:22 <Igloo> Oh, in fact it even says so - "ghc-6.2(9999): unaligned access to 0x4000000001ef647d, ip=0x4000000000e1d4d1"
04:55:05 <Igloo> urtie: Is there somewhere I can see the previous two reports?
04:55:41 <urtie> igloo: the haskell.org mirror
04:55:53 <Igloo> http://www.mirror.ac.uk/sites/www.haskell.org/001_communities isn't very useful
04:56:25 <urtie> hmmm.
04:56:35 <urtie> just a sec, I'll put a local copy online
04:57:04 <Igloo> ta
05:00:37 <urtie> http://www.cs.uu.nl/~arthurvl/communities/
05:01:17 <ski> 403 Forbidden
05:01:29 <Igloo> Why the nick change, BTW?
05:01:59 <urtie> stupidly managed to lock myself out of my home server after a powerdip reset my ADSL modem
05:02:34 <Igloo> Ah  :-)
05:02:58 <urtie> there, that should be online now
05:03:23 <ski> yes
05:04:14 <urtie> it's only the previous two by the way
05:05:55 <urtie> ah, dropping the _darcs metadata makes it complete. :)
05:06:12 <JaffaCake> Igloo: re the misaligned access problem, can you debug it there?
05:08:00 <Igloo> OK, I'll do my best  :-)
05:08:42 <JaffaCake> compile the program with -debug (make sure you have a debug RTS)
05:10:57 <Igloo> That's there by default in 6.2.1, right? I've just set that building
05:18:23 <Igloo> urtie: I have a mail from you saying "tail +4 mail12"
05:21:11 <urtie> gowd no. *sigh*
05:21:32 <urtie> I never seem to get the hang of this. :(
05:23:40 <urtie> okay, that should be better.
05:24:00 * urtie goes off in a corner to hide
05:24:12 <Igloo> :-)
05:24:22 <Igloo> Yup, that looks good
05:24:35 <urtie> I suddenly feel *very* bad about the talk this evening
05:25:13 * urtie has a feeling he's going to screw up doing the planned Haskell evangelizing as well
05:25:50 <Igloo> I'm sure it'll be fine  :-)
05:31:08 <urtie> I'm not. it is in 5 1/2 hours and I still have to do the slides
05:37:12 <urtie> hey! haskell.org is up again
06:05:32 <jvee> lo
06:05:42 <ski> 'lo jvee
06:16:18 <Leimy> was hakell.org back up earlier?
06:16:33 <urtie> yeah, about 45 mins ago
06:16:39 <Leimy> hmmm
06:16:41 <JaffaCake> seems down again
06:16:43 <urtie> it is.
06:16:44 * Leimy isn't having luck
06:16:46 <urtie> :(
06:16:54 <Leimy> what does it run?
06:16:58 <Leimy> when it's running :)
06:17:03 <urtie> redhat 9
06:17:09 <urtie> IIRC
06:17:10 <Igloo> http://www.mirror.ac.uk/sites/www.haskell.org/ might help you, though
06:17:13 <Leimy> yikes
06:17:16 <ski> <urtie> hey! haskell.org is up again
06:18:40 <urtie> yeah, at 14:37 local time
06:18:44 <urtie> it is now 15:19
06:19:55 <shapr> does this need a lambdabot plugin? :-)
06:19:59 <shapr> @status
06:20:00 <lambdabot> Sorry, I don't know the command "status", try "lambdabot: @listcommands
06:21:08 <ski> status of what ? haskell.org ?
06:22:15 <shapr> sure
06:23:14 <shapr> backup wikis could use darcs for sync among them
06:23:36 <Lunar^> darcs will rule the world
06:23:50 <shapr> yay!
06:24:10 <shapr> once David gets that complexity problem figured out
06:24:20 <Lunar^> ?
06:24:34 <Leimy> I've not decided if I like darcs or not yet
06:25:02 <Lunar^> shapr: I'm currently using darcs to manage /etc on my new server
06:25:14 <Leimy> It seemed like the initial checkout of any project had to go through revision1 to the last by replaying everything that had happened
06:25:23 <Leimy> or maybe I am imagining things
06:25:33 <Lunar^> shapr: It seems to work really well. Being able to include changes to a file piece by piece is well suited here
06:26:00 <JaffaCake> darcs needs branches, IMHO
06:26:26 <urtie> yes, that's one thing I found lacking
06:26:43 <urtie> another thing is that the _darcs subdirectory quickly gets quite large.
06:27:33 <shapr> well, every repo is a branch
06:27:36 <Igloo> darcs' biggest problem is that 1.0 doesn't seem to be getting here, and large new features and redisgn for the complexity issues are having to wait until it does
06:27:58 <shapr> the complexity is the only thing that worries me
06:28:05 <shapr> everything else I can deal with
06:28:29 <shapr> urtie: you can use partial pulls
06:29:03 <shapr> personally, I'm mostly in favor of the branch as separate repo
06:30:11 <JaffaCake> hmm, I 'spose so... how does darcs deal with merging two repos?
06:30:25 <shapr> quite well in general
06:31:20 <Igloo> You pull from one to the other and fix any conflicts as normal
06:31:25 <shapr> hi Heffalump!
06:31:27 * Heffalump replaces Igloo with #haskell
06:32:18 <shapr> darcs is *really* easy to use
06:32:29 <shapr> you don't need user accounts to commit
06:32:40 <shapr> ssh or email works
06:33:11 <shapr> according to the docs, pulling your choice of patches from multiple repos works fine
06:33:19 <shapr> though I haven't tried it
06:33:57 <shapr> I've only used two repos for real production work
06:34:04 <Heffalump> it should do, dependencies notwithstanding
06:34:08 <shapr> as in, one remote and one local
06:34:26 * Heffalump uses three or four habitually for my main project that uses darcs
06:34:35 <Heffalump> sadly the project itself is coded in Perl...
06:34:38 <shapr> any problems?
06:34:41 <shapr> ow
06:34:42 <Heffalump> yes, see above.
06:34:51 <Igloo> Predominantly one developer though, right?
06:34:55 <shapr> what are you working on these days?
06:35:00 <Heffalump> (but yes, some real problems with darcs, mostly posted to the list)
06:35:01 <Igloo> So roughly speaking it'll be conflict-free
06:35:02 <JaffaCake> it would be nice if branching was O(1), though
06:35:17 <Heffalump> shapr: that project is play, not work - http://www.photopub.org
06:35:20 <JaffaCake> for a local repo, of course
06:35:31 <Heffalump> work is still CVS, currently writing a new AspectJ compiler
06:35:45 <Heffalump> jaffacake: cp -al is _very_ fast
06:35:45 <jadrian> hello Heffalump!
06:35:51 <Heffalump> hi jadrian
06:35:58 <shapr> greetz jadrian
06:36:19 <Igloo> I don't think in-repo branches as well would complicate things much, and I think it can make things psychologically seem easier, so maybe it would be a good idea. I imagine David will want to wait until post-1.0 though
06:36:26 <shapr> what's al? is that copy on write?
06:36:32 <Heffalump> hard links
06:36:35 <shapr> ah
06:36:39 <Heffalump> you need to use a copy on write editor for it to be safe to do
06:36:40 <JaffaCake> Heffalump: not on Windows :-P
06:36:44 <Heffalump> (I think emacs is good, but vi is bad)
06:36:53 <jadrian> hi shapr!
06:36:56 <Igloo> I think both are configurable
06:37:01 <Heffalump> JaffaCake: I nearly qualified what I said with that given who I was talking to, but then I thought you might have a sane system to use too :-)
06:37:13 <shapr> ext3 is adding COW calls for fast copying
06:37:16 <Igloo> Anyway, you can cp -al _darcs but cp everything else
06:37:17 <shapr> moo!
06:37:31 <Heffalump> anyway, even -al isn't O(1), obviously.
06:37:39 <Heffalump> COW directory entries would be cool, if rather scary.
06:37:47 <Heffalump> s/directory entries/directories/
06:37:56 <Heffalump> and rather hard to actually implement
06:38:05 * urtie ponders
06:38:07 <shapr> lwn.net has details
06:38:15 <urtie> there is something somewhat like it in the BSD's already
06:38:17 <JaffaCake> there are COW filesystem layers, not sure if Linux has one
06:38:22 <Heffalump> eek, it's a misspelled earthy
06:38:33 <Heffalump> urtie: COW directories?
06:38:37 <shapr> ask him why ;-)
06:38:38 <urtie> where you can mount a r/w fs on top of a r/o fs
06:38:41 <Heffalump> oh, right
06:38:45 <ski> (what's COW ?)
06:38:49 <JaffaCake> urtie: yup
06:38:49 <Heffalump> that's a bit different, but I guess it'd have the same effect
06:38:50 <urtie> and have it do cow
06:38:51 <Heffalump> Copy on Write
06:38:52 <shapr> Copy On Write
06:38:55 <shapr> == COW
06:39:48 <shapr> I wonder what a hOp filesystem would look like
06:40:05 * JaffaCake wonders if you can import the repository into a new repository, and version that
06:40:31 * urtie wonders what a hOp OS would look like
06:40:31 <shapr> you can pull from one dir to another, right?
06:40:50 <Heffalump> JaffaCake: I hope you don't mean what I think you mean.
06:40:51 <urtie> and how much it would differ from Jonathan Rees' scheme based thingamajig. :)
06:41:04 <JaffaCake> as in nested repositories, I mean (silly, but maybe fun...)
06:41:06 <shapr> urtie: what's that?
06:41:20 <Heffalump> you did mean what I thought you meant.
06:41:24 <Heffalump> You have a sick mind :-)
06:41:25 <JaffaCake> sorry :)
06:41:44 <jadrian> is there a more elegant way to write: foldl (>>=) (Just 0) mfs
06:41:50 <Igloo> Ah, so you mean you pull from uberrepo and you get a certain version of the minorrepos inside it?
06:41:57 <urtie> http://mumble.net/~jar/pubs/secureos/
06:42:01 <jadrian> mfs :: [Int -> Maybe Int]
06:42:12 <JaffaCake> Igloo: yes
06:42:30 <jadrian> or more generaly:  foldl (>>=) (return a) mfs
06:42:40 <Igloo> But then uber merges would want you to merge the minor repo metadata, wouldn't they?
06:42:50 <jadrian> mfs :: Monad m => [a-> m a]
06:43:09 <shapr> oh, I had a thought about using GHC safely from IRC
06:43:45 <shapr> is there a top level GHCi monad or something? could that be lifted through an ACL monad?
06:44:07 * shapr is beginning to see the power of monad transformers
06:44:21 <jadrian> I cannot find any function for that in Control.Monad
06:44:26 <shapr> urtie: that's really interesting, I gotta read that
06:44:29 <JaffaCake> Igloo: yeah, maybe it doesn't work
06:44:33 <jadrian> a function for composing monadic functions would be nice too...
06:44:42 <Heffalump> jadrian: like >>= ?
06:44:43 <jadrian> compM fms = \a ->  foldl (>>=) (return a) fms
06:45:24 <jadrian> Heffalump: well I want do   ((f1 a >>= f2) >>= f3) >>= f4...
06:45:33 <Lunar^> urtie: This paper is on my desk for 3 weeks now :(
06:45:45 <jadrian> Heffalump: so what I have is a list of functions
06:45:51 <urtie> and you still haven't had time to read it? :)
06:46:05 <ski> jadrian : foldM (flip ($)) a   mayhaps ?
06:46:15 <jadrian> Heffalump: I can do this using a (fold (>>=) (return a) monadicfs)
06:46:20 <urtie> man, I read that thing out of general interest 2 years ago. :)
06:47:04 <jadrian> ski: nice :) I also thought about defining "compM fms = \a ->  foldl (>>=) (return a) fms"
06:47:23 <shapr> I thought a haskell OS was an unrealistic dream
06:47:38 <jadrian> but I'm really asking if you know any ready to use Monadic function
06:48:02 <jadrian> I can define my own but wanted to avoid reeinventing the wheel as much as possibile
06:48:06 <shapr> hOp surprised me
06:48:06 <ski> jadrian : i would have called it composeM, i guess
06:48:17 <jadrian> I can't find any, but I thought I'd ask anyway just in case
06:48:47 <ski> is there a compose :: [a -> a] -> (a -> a), btw ?
06:49:01 <jadrian> ski: that's what I was going to ask... :)
06:49:10 * ski defined that a couple of times ..
06:49:23 <jadrian> ski: I thought so, but it isn't in Prelude nor Data.List
06:49:27 <shapr> @type foldr1 (>>>)
06:49:28 <lambdabot> bzzt
06:49:42 <shapr> @type (foldr1 (>>>))
06:49:44 <lambdabot> bzzt
06:49:52 * shapr blinks
06:50:05 <Leimy> did it just say bzzt?
06:50:10 <shapr> yes it did
06:50:20 * Leimy rubs his eyes
06:50:59 <jadrian> well guess I'll just define my own Monadic composition... seems to usefull not to be in in Control.Monad :-/
06:51:08 <jadrian> hope I'm not missing something
06:51:16 <jadrian> shapr: (>>>) ?
06:51:31 <shapr> that's just flip (.)
06:51:41 <shapr> it's from Control.Arrow
06:51:41 <urtie> @type 1
06:51:41 <jadrian> oh
06:51:42 <lambdabot> 1 :: forall t. (Num t) => t
06:51:55 <jadrian> ah, I haven't played with arrows yet
06:51:56 <Lunar^> urtie: Unfortunately not :)
06:52:01 <ski> @type (>>>)
06:52:03 <lambdabot> bzzt
06:52:06 <Lunar^> urtie: that was a :(
06:52:09 <shapr> @type Control.Arrow.(>>>)
06:52:09 <lambdabot> bzzt
06:52:17 <shapr> @type Control.Arrow.arr
06:52:18 <lambdabot> Control.Arrow.arr :: forall a c b.
06:52:18 <lambdabot> 		     (Control.Arrow.Arrow a) =>
06:52:18 <lambdabot> 		     (b -> c) -> a b c
06:52:19 <urtie> lunar^:  it's not *that* large a text...
06:52:21 <shapr> huh
06:52:39 <shapr> how do I qualify an operator?
06:52:44 <Lunar^> urtie: Sure, but I need to have enough concentration to read it well. And I want to finish the EROS papers fisrt
06:52:52 <urtie> `ah'
06:53:04 <Lunar^> urtie: And before that, I would like to complete the Tannenbaum book about Minix
06:53:22 <urtie> well, I found the EROS papers to be a lot easier after I read that text by jar
06:53:24 <Igloo> @type (Control.Arrow.>>>)
06:53:26 <lambdabot> (Control.Arrow.>>>) :: forall a d b c.
06:53:26 <lambdabot> 		       (Control.Arrow.Arrow a) =>
06:53:26 <lambdabot> 		       a b c -> a c d -> a b d
06:53:38 <Lunar^> urtie: Are you also interested in OS stuff ?
06:53:39 <shapr> aha
06:53:41 <urtie> and the tanenbaum book on minix is not something I can recommend.
06:54:11 * urtie is indeed interested in OS stuff... very much so even
06:54:21 <jadrian> hmmm
06:54:22 <Lunar^> urtie: It is an old book, but I deeply to get vocabulary and concepts
06:54:31 <Lunar^> urtie: deeply need, sorry
06:54:32 <urtie> I've read the Lions book as bedtime reading.
06:54:43 <shapr> @type foldr1 (Control.Arrow.arr)
06:54:44 <lambdabot> bzzt
06:54:54 <shapr> @type foldr1 (Control.Arrow.>>>)
06:54:55 <lambdabot> foldr1 (Control.Arrow.>>>) :: forall a b.
06:54:55 <lambdabot> 			      (Control.Arrow.Arrow a) =>
06:54:55 <lambdabot> 			      [a b b] -> a b b
06:54:57 <shapr> yeesh
06:55:12 <jadrian> Any ideas for a good simbol for "sequential application of monadic functions"?
06:55:19 <urtie> http://www.peer-to-peer.com/catalog/opsrc/lions.html
06:55:28 <jadrian> a >>== [f1, f2,...,fn]
06:55:44 <jadrian> f1 a >>= f2 >>= ... >>= fn
06:55:50 <jadrian> >>== ?
06:56:01 <ski> do we need an infix op ?
06:56:14 <jadrian> no but it would look nice :)
06:56:48 <ski> what precedence and fixity, then ? (to play nice with >>= and such)
06:57:48 <shapr> does mconcat work here?
06:59:14 <jadrian> ski: my guess would be the same as (>>=), no? 
06:59:28 <ski> jadrian : i suppose so, yes
06:59:43 <ski> shapr : that seems no so related, to me ..
06:59:51 <shapr> ok
07:00:34 <ski> other things would mayhaps be  [m a] -> m a  and such ..
07:00:52 <ski> (e.g. select :: MonadPlus m => [m a] -> m a)
07:02:54 <jadrian> is it a common practice to declare newtypes as instance of Monad to be able to liftM functions?
07:03:21 <ski> lambdabot: @type foldr (Control.Arrow.>>>) (Control.Arrow.arr id)
07:03:23 <lambdabot> foldr (Control.Arrow.>>>) (Control.Arrow.arr id) :: forall a b.
07:03:23 <lambdabot> 						    (Control.Arrow.Arrow a) =>
07:03:23 <lambdabot> 						    [a b b] -> a b b
07:04:24 <shapr> I gotta clean out the whitespace there at some point
07:04:49 <Heffalump> jadrian: making something a monad just for liftM? I hope not...
07:05:08 <jadrian> Heffalump: plain ugly rigth? :-/
07:05:18 <Heffalump> yeah...
07:05:57 <jadrian> Heffalump: so what would you do if you want to re-use functions defined for the "wrapped" type? (was that clear?)
07:07:09 <jadrian> an example would be: newtype ID = ID String
07:07:33 <jadrian> now I want to use string functions on ID
07:07:57 <ski> internally or extrnally ?
07:08:08 <jadrian> Functor (fmap) seems even more ugly than Monad (liftM)
07:08:13 <jadrian> ski: internally
07:08:49 <Heffalump> fmap doesn't seem ugly.
07:08:58 <Heffalump> since a newtype in that sense _is_ a functor, and it isn't a monad
07:09:04 <ski> jadrian : it seems to be a bit easier to use hugs' restricted type synonyms for such ..
07:09:12 <Heffalump> oh, sorry, it is, just the identity monad
07:09:22 <jadrian> Heffalump: yeap :)
07:09:24 <ski> huh ?
07:09:48 <ski> how is ID above a identity monad ?
07:10:04 <jadrian> return str = ID str
07:10:28 <Igloo> It can't be a Monad as it has the wrong kind
07:10:28 <ski> return :: Monad m => a -> m a
07:10:31 <Heffalump> oh, yes.
07:10:32 <Heffalump> duh :-)
07:10:37 <ski> right
07:10:38 <Heffalump> same for functor-ness, of course.
07:10:52 <Heffalump> but there is a constant functor.
07:10:57 <Heffalump> which that is
07:11:40 <jadrian>  oh right
07:11:55 <ski> Heffalump : you mean if we add a nonvariant argument ?
07:12:11 <jadrian> it's not a Monad :-/
07:13:19 <Heffalump> yes
07:18:55 <jadrian> ok now that I just figured my idea was pretty dumb I can't see how can it be a Functor 
07:19:01 <jadrian> what is a nonvariant argument
07:19:02 <jadrian> ?
07:19:15 <ski> an unused (type) argument
07:19:21 <ski> like
07:19:24 <ski> newtype ID a = ID String
07:19:25 <jadrian> you mean ID a = ID STring
07:19:30 <ski> yes
07:19:30 <jadrian> yeap got it
07:19:46 <ski> that would trivially be a functor and a monad, i think
07:19:47 <ski> hmm
07:19:52 <ski> at least functor
07:21:44 <jadrian> I was mistaking the kind of the Type Constructor with the kind of the Value Constructor
07:21:57 <jadrian> the value constructor as kind * -> *
07:22:17 <ski> huh ?
07:22:30 <ski> ID :: String -> ID a
07:22:40 <Igloo> That's kind *
07:22:43 <ski> ID ::: * -> *
07:22:43 <jadrian> newtype ID (<- type constructor) a = ID (<- value constructor) String , right?
07:22:48 <ski> yes
07:22:59 <jadrian> the 1st ID as kind *
07:23:06 <jadrian> the 2nd as kind * -> *
07:23:13 <jadrian> right?
07:23:17 <Igloo> Right
07:23:29 <Igloo> In "ID :: String -> ID a"
07:23:34 <jadrian> but to have a monad or a functor the type constructor needs to be *->*
07:23:34 <ski> the first ID has kind * -> * (after the arg is added)
07:23:43 <jadrian> not the value constructor
07:23:43 <ski> yes
07:24:08 <ski> the value constructor ID :: String -> ID a ::: *
07:24:08 <kosmikus> value/data constructors don't have kinds
07:24:58 <jadrian> hmm guess I don't understand kinds right then...
07:25:08 <kosmikus> values have types, and types have kinds
07:25:24 <kosmikus> kinds are for types what types are for values
07:25:36 <kosmikus> only types of kind * have values
07:25:39 <jadrian> k got it
07:26:07 <urtie> errr....
07:26:21 <kosmikus> (ignoring GHC-specific kinds)
07:32:09 <jadrian> ski: restricted type synonyms would probably do the trick (had never heard of those), maybe a little tedious not to be able to lift functions on the fly...
07:32:16 <jadrian> ski: anyway, I'm using GHC
07:33:35 <ski> jadrian : you looked them up, now ?
07:33:41 <jadrian> ski: yeap
07:33:55 <ski> jadrian : (what do you mean by not be able to lift functions ?)
07:34:07 <shapr> oh, I think we were over 90 for a bit, yay
07:34:50 <ski> jadrian : (yea, sometimes i want GHc to have something like it ..)
07:34:51 <jadrian> ski: for (newtype ID = ID String) I have to declare all functions on string I'd want to use with ID
07:35:11 <shapr> mikef: looking for Haskell info?
07:35:11 <ski> jadrian : in GHC or Hugs ?
07:35:20 <jadrian> ski: in hugs
07:35:27 <jadrian> http://cvs.haskell.org/Hugs/pages/users_guide/restricted-synonyms.html
07:36:02 <jadrian> from what I've read I'd need to do, type ID = ID String  in <functions on string go here>
07:36:32 <jadrian> and I'd only be able to use those listed functions on ID
07:36:37 <ski> jadrian : yes, but not necessarily their signatures
07:36:53 <ski> jadrian : right, not when exporting ..
07:36:54 <Heffalump> sorry shapr :-)
07:37:32 <jadrian> ski: yes but it might end up beeing tedious to list them I guess
07:38:14 <ski> jadrian : would perhaps be nice if we could say somthing to the effect of "in current module"
07:38:48 <jadrian> ski: by beeing able to lift them on the fly I meant something like Monads liftM, to simple lift a String function work 'inside' ID
07:38:55 <jadrian> ski: yeap that could help too
07:39:12 * ski supposes restricted type synonyms came before the module system ..)
07:40:27 <ski> though, the concept seems quite useful, at times
07:41:14 * jadrian would also like to have "implicit values" afecting modules...
07:41:24 <ski> hmm, how ?
07:41:37 <ski> like module parameters
07:41:40 <jadrian> or even better, beeing able to define the "scope" of an implicite value
07:41:41 <ski> only implicit ?
07:42:28 <jadrian> ski: yeap something like it, instead of declaring implicit values explicitly in functions type signatues
07:42:36 <ski> sometimes, it seems 'twould perhaps be good to parameterize a whole module by type variables ..
07:42:45 <ski> jadrian : aha
07:43:12 <jadrian> ski: it'd be just syntatic sugar
07:43:28 <jadrian> defining the scope would probably even be better
07:43:38 <ski> (instead of having a couple of datatypes and functions in it, all parameterized by, say, 3 type variables)
07:45:24 <jadrian> hmm not sure
07:45:37 <ski> jadrian : perhaps one could find a good way to parameterized modules by one or more of : type variables, same with class constraints, implicit variables, *maybe* value variables
07:45:38 <jadrian> "parameterized by, say, 3 type variables"
07:45:39 <jadrian> ?
07:45:42 <jadrian> the datatypes?
07:45:50 <jadrian> and the functions?
07:45:52 <ski> jadrian : perhaps coupled with some kind of local modules
07:45:59 <ski> yes
07:46:13 <ski> say some such modules, i think
07:47:54 <jadrian> what I meant was just that instead of declaring explicitly functions as:
07:47:58 <ski> (hmm e.g. all datatypes and functions in a module has a 's' parameter, for ST state space)
07:47:59 <jadrian> f :: (?a ::Int, ?b:: Double) => Double -> Double
07:48:08 <jadrian> g :: (?a ::Int, ?b:: Double) => Double -> Double -> Double
07:49:02 <jadrian> it'd would be nice to say that all functions in a module have (?a ::Int, ?b:: Double) in their context 
07:49:06 <jadrian> and declare them as
07:49:18 <jadrian> f :: Double -> Double
07:49:25 <jadrian> g :: Double -> Double -> Double
07:49:39 <ski> (http://www.cs.chalmers.se/~peb/papper/logvar-code/LP4Typed.hs have some of this, 'tseems)
07:49:43 <jadrian> that is the module assumes fixed values of ?a and ?b
07:49:53 <ski> right
07:49:56 <jadrian> or maybe better, define the "scope" of ?a and ?b in a module
07:50:02 <jadrian> ski: let me see
07:50:52 <ski> (though, i seem to recall somewhere i've seen it worse. but putting lotsa 's' in all types and signatures can be a bit annoying anyway)
07:52:32 <ski> if we have some things that should not not parameterized, we could perhaps put the stuff that should in a local module, and the other stuff outside that one
07:53:15 <Lunar^> jadrian: It's complicated
07:53:21 <Lunar^> jadrian: think about threads 
07:53:23 <ski> (i've seemed to have wanted local modules for some diverse things, already ..)
07:53:38 <jadrian> ski: what is that url exactly?
07:53:40 <ski> Lunar^ : what's complicated ?
07:53:58 <ski> jadrian : sorry ?  what do you mean ?
07:54:09 <jadrian> ski: the file you sent me? 
07:54:30 <jadrian> ski: how is that related with the "global implicit values" issue?
07:54:36 <Lunar^> ski: thread-safe local module state is not easily achieved
07:54:39 <ski> jadrian : you wonder what the link is ? or where the contents come from ?
07:55:07 <ski> Lunar^ : local module state ? where ?
07:55:13 <jadrian> I'm wondering the connection between it and the global implict parameters...
07:55:41 <ski> jadrian : i was thinking about possible parameterizing a module by type variables. 's' in this case
07:56:00 <jadrian> ski: oh right!
07:56:25 <jadrian> ski: by the way, it is that module about?
07:56:39 * jadrian is also implementing unification right now :)
07:56:53 <jadrian> (simple unification on FOL)
07:57:09 <jadrian> <jadrian> ski: by the way, it is that module about?
07:57:10 <ski> jadrian : the code there comes from the paper "Typed Logical Variables in Haskell", Koen Claessen and Peter Ljunglöf (2000)
07:57:14 <jadrian> s/it/what
07:57:41 <ski> (can e.g. be gotten at http://www.cs.chalmers.se/~peb/papper.html)
07:57:47 <jadrian> thanks
07:58:57 * jadrian needs to hire someone to clean and organize his bookmarks...
08:00:10 * ski 's current bookmark incarnation is already 1315803 bytes, and growing fast :/
08:01:17 <jadrian> I should probably read this paper to see how they go on about unification
08:01:30 <jadrian> right now I'm just implementing a toy theorem prover (Folderol)
08:02:11 <jadrian> and during unification I maintain an evironment with the pairs (meta var, assigned term)
08:03:28 <jadrian> opsss brb
08:16:32 <shapr> yay, haskell.org is back
08:17:56 <Lunar^> shapr: What's happening ?
08:22:34 <shapr> Lunar^: with haskell.org?
08:22:48 <Lunar^> shapr: yes :)
08:22:49 <shapr> I duuno
08:23:59 <shapr> dmesg seems happy enough
08:24:19 <Lunar^> :(
08:27:32 <shapr> I'm tempted to make a readonly backup wiki
08:28:54 <Lunar^> Would be a good idea
08:29:02 <Lunar^> I need to finish my FFI tutorial
08:29:11 <Lunar^> It's missing a callback example
08:36:31 <ShaminoDC|FBK> using winhugs i get an error
08:36:34 <ShaminoDC|FBK> module Blatt2
08:36:34 <ShaminoDC|FBK>  where
08:36:34 <ShaminoDC|FBK>  import Aufg1.hs
08:36:34 <ShaminoDC|FBK>  import Prelude hiding (repeat, head, tail, take, sum, filter, lex)
08:36:34 <ShaminoDC|FBK>  import Char
08:36:43 <ShaminoDC|FBK> module Blatt2
08:36:44 <ShaminoDC|FBK>  where
08:36:44 <ShaminoDC|FBK>  import Aufg1.hs
08:36:44 <ShaminoDC|FBK>  import Prelude hiding (repeat, head, tail, take, sum, filter, lex)
08:36:44 <ShaminoDC|FBK>  import Char
08:37:01 <ShaminoDC|FBK> ERROR "G:\UNI\Informatik\propra\blatt2.hs":4 - Syntax error in import declaration (unexpecte
08:37:01 <ShaminoDC|FBK> d symbol "Aufg1.hs") 
08:37:13 <ShaminoDC|FBK> any ideas ?
08:38:04 <ski> use 'import Aufg1', instead
08:38:16 <kosmikus> you have to use a module name, not a filename
08:38:43 <ShaminoDC|FBK> ERROR "G:\UNI\Informatik\propra\blatt2.hs" - Module "Aufg1" not previously loaded     
08:39:20 <Lunar^> ShaminoDC|FBK: Please avoid to make so long cut/paste. Use this page instead http://haskell.org/hawiki/HaskellIrcPastePage
08:39:20 <kosmikus> do you have a module Aufg1? I mean, is there a line "module Aufg1 where" in your file "Aufg1.hs"?
08:39:28 <ski> ShaminoDC|FBK : have you enabled import chasing ?
08:42:02 <ShaminoDC|FBK> ok now i get anotehr error
08:42:31 <ShaminoDC|FBK>  Syntax error in input (unexpected symbol "ER
08:42:31 <ShaminoDC|FBK> ROR")                                            
08:43:22 <ski> ShaminoDC|FBK : have you pasted an error description in your source file ?
08:44:39 <ShaminoDC|FBK> yes
08:45:12 <ShaminoDC|FBK> but its in {-   -}
08:45:17 <ski> good
08:47:43 <ShaminoDC|FBK> ok found all errors now thx for help
08:47:52 <ski> yw
08:49:33 <ShaminoDC|FBK> whats this haskell pastepage about ? there is alot of code ? for ?
08:50:10 <ski> it's used for pasting code to be discussed here, e.g.
08:51:05 <ski> @wiki HaskellIrcPastePage
08:51:06 <lambdabot> http://www.haskell.org/hawiki/HaskellIrcPastePage
08:51:30 <ShaminoDC|FBK> a very useful
08:51:48 <ski> yes
09:03:03 <ShaminoDC|FBK> ok i entered a stream of nonprimes now first i need the stream of primes to sub from stream of nat num
09:03:25 <ski> mhmm
09:04:22 <Igloo> Don't suppose anyone knows of a GLib Haskell binding?
09:05:09 <andersca> Igloo: that would be weird
09:05:29 <Igloo> How so?
09:06:00 * Igloo is looking at gstreamer which uses g_object_set in its hello world app (which may be in some other bit of GNOME, actually)
09:06:15 <andersca> oh, you mean like that
09:06:15 <andersca> hmm
09:06:35 <Igloo> Ah, yes, GObject in fact
09:10:30 <Igloo> Oh bother, it has a ... type
09:11:01 <ski> huh ?
09:11:37 <Igloo> Like printf - varargs?
09:11:47 <ski> oh
09:11:55 <Igloo> i.e. C wrapper time
09:12:27 <Igloo> I wish they wouldn't give gfoo names to everything
09:12:53 <ski> just remove it in the binding, right ?
09:13:17 <Igloo> Yeah, it just means I have to keep following links to check that gpointer is what I think it is etc
09:13:49 <ski> mhmm
09:24:02 <andersca> @eurohaskell
09:24:02 <lambdabot> less talks, more code!
09:24:02 <lambdabot> http://www.haskell.org/hawiki/EuroHaskell
09:24:02 <lambdabot> EuroHaskell - Haskell Hackfest - June 10-12 - Gothenburg, Sweden
09:24:30 <bringert> @seen lambdabot
09:24:31 <lambdabot> Yes, I'm here
09:24:40 <bringert> well, duh
09:26:11 * shapr grins
09:26:38 <bringert> @join #haskell.se
09:26:39 <lambdabot> not enough privileges
09:26:58 <shapr> @join #haskell.se
09:28:36 <shapr> time to unicycle
09:28:59 <Lunar^> shapr: have fun  !
09:29:12 <shapr> chgrp wheel unicycle
09:29:16 * shapr &
09:51:12 * Igloo gets round to writing the C wrapper and finds it might be impossible. How inconvenient.
10:20:58 <SamB> hmm, how do you spell ", ".join in Haskell?
10:21:51 <Lunar^> concat $ intersperse ", " [..]
10:22:47 <SamB> hmm, that isn't very concise, is it?
10:24:58 <Lunar^> commaJoin = concat . (intersperse ", ")
10:25:00 <Lunar^> use commaJoin then
10:25:34 <SamB> that would be nice if I was writing a program to keep
10:27:06 <Lunar^> let commaJoin =  concat . (intersperse ", ") in <exp>
10:28:57 <SamB> I'll just let join s = concat . (intersperse s)
10:29:27 <Lunar^> You could also make an operator if you prefer
10:29:38 <SamB> hehe, thats fine ;-)
10:30:18 <Lunar^> ", " `join` [...]
10:30:50 <SamB> I don't suppose there is a printf "macro" in GHC's library?
10:35:28 <Lunar^> printf is not something that can easily be done in Haskell
10:36:16 <Lunar^> There's two real implementation AFAIK, the first using type classes in a crazy way, the other by using Template Haskell to create the right function at compile time
10:36:54 <Lunar^> But you can learn 'show' and 'shows' instead
10:45:55 <SamB> yes, I was thinking a template haskell function to use like $(printf "(%d, %d, %d, %s)") 2 1 -1 "\\frac12"
10:46:38 <SamB> but I think I'll just copy and paste into python
10:47:06 <Lunar^> http://web.comlab.ox.ac.uk/oucl/work/ian.lynagh/Printf/
10:58:58 * Igloo suspects that's out of date, and probably won't compile today
10:59:38 <Igloo> I need to run through everything and update it some time
11:10:50 <wagle> if i define f = (\x -> x, (fst f) 1)
11:11:11 <wagle> then f gets the type (Integer -> Integer, Integer)
11:11:24 <wagle> and not type (a -> a, Integer)
11:12:02 <wagle> this is how i predicted (ie, intuitive), but i'm wondering why
11:17:32 <Marvin--> anybody know who Kees Doets and Jan van Eijck are?
11:18:36 <wagle> did you ask google?
11:19:12 <Marvin--> well, yeah, I'm reading a book by them, I was just wondering if someone else knew about them :)
11:19:49 <Marvin--> http://homepages.cwi.nl/~jve/HR/
11:33:47 <ShaminoDC|FBK> i found to define a funktion that gives a stream of prime numbers
11:33:52 <ShaminoDC|FBK> want to
11:34:14 <ShaminoDC|FBK> primes                        :: Stream Integer
11:36:00 <wagle> in haskell?
11:36:04 <ShaminoDC|FBK> yes
11:36:33 <wagle> is Stream Integer the same as [Integer]?
11:36:43 <ShaminoDC|FBK> its a stream of integers
11:37:15 <ShaminoDC|FBK> natural numbers would be something like Cons 0(Cons 1(cons 2 (cons 3 ...........
11:37:20 <wagle> what is the difference between stream of integers and list of integers?
11:37:38 <ShaminoDC|FBK> a list has a certain size a stream is endless
11:37:39 <wagle> oh.. how is your "Stream" defined?
11:38:00 <ShaminoDC|FBK> data Stream a                 =  Cons a (Stream a)
11:38:06 <wagle> lists in haskell are streams (which is why i asked if you meant haskell)
11:38:39 <ShaminoDC|FBK> http://haskell.org/hawiki/HaskellIrcPastePage
11:38:46 <wagle> ahh.. okay, now can you define the Stream of all integers?
11:40:09 <wagle> ... or the Stream of 1's
11:40:10 <ShaminoDC|FBK> already did so
11:40:20 <ShaminoDC|FBK> to much code to pase here
11:41:01 <wagle> uh.. if you made the Stream of integers more than one line, your stream of primes is going to be gross
11:41:35 <ShaminoDC|FBK> its endless
11:41:47 <ShaminoDC|FBK> thats why its a stream :)
11:42:08 <wagle> the value is endless. the code is finite
11:42:42 <ShaminoDC|FBK> well but that doesnt help me
11:43:28 <wagle> i will help you do your homework, but i wont do you homework for you
11:45:04 <wagle> what is your code for the Stream of all positive Integers?
11:45:33 <Marvin--> write the code in Lustre instead, all variables are streams :-)
11:47:34 <ShaminoDC|FBK> hehe its no homework
11:48:06 <ShaminoDC|FBK> i am just asking you how you would hack a stream a primes
11:49:06 <keverets> if you're really interested, you should know that a Haskell List matches your description of Stream.
11:49:26 <keverets> just google for "haskell primes" and you'll get a decent answer.
11:50:22 <wagle> keverets: technically, haskell lists are finite or "infinite".  his streams are only "infinite"
11:51:19 <keverets> wagle: true enough.  I don't know why he would want only infinite lists, though.
11:51:29 <wagle> but it might be less confusing to model streams with the infinite subset of haskell lists
11:51:45 <cgibbard> In the context of type inference rules, does tau < Gamma(x) (where the < is curly) mean anything more than Gamma assigns type tau to x?
11:52:03 <ShaminoDC|FBK> i decided to change my code ill try something else instead
11:52:08 <wagle> using his Stream data structure might force some explicit thinking
11:52:36 <ShaminoDC|FBK> i will use something like prime :: [Integer ] -> Stream Integer so i can call finite or infinite
11:52:44 <wagle> the stream of primes is tricky, and you should build up to it
11:53:31 <ShaminoDC|FBK> its infinite when i call it with the infinite list of natural numbers
11:53:51 <ShaminoDC|FBK> otherwise its not so that solves m yproblem
11:55:24 <ShaminoDC|FBK> the sieve of erasthotenes is not realy helping me i want it infinite
11:55:57 <cgibbard> the sieve of Eratosthenes works on an infinite stream.
11:57:26 <ShaminoDC|FBK> well but it will give me a finite result
11:57:32 <cgibbard> no
11:57:34 <ShaminoDC|FBK> mhh
11:58:28 <desrt> yo
11:58:31 <desrt> what up, g?
11:58:35 <cgibbard> heh
11:58:40 <desrt> this is sooo lame
11:58:43 <desrt> ghazis!!
12:59:05 <tyler> 'soar
13:18:57 <bluejay> How can I know whether I've written a function that is as efficient as (++)? (In that it tends to run in O(1) space due to lazy evaluation)
13:19:32 <bluejay> (I'll have to leave in ~15 minutes, so sorry if I disappear before anyone's answered)
13:23:03 <Smerdyakov> Eyeball it.
13:23:19 <Smerdyakov> Look at which expressions must be evaluated.
13:24:44 <chucky> you would also have to look at typical usage of the function, to see how often the result is evaluated, hence forcing the expressions in the function to be evaluated
13:25:12 <chucky> more or less every function in haskell should be O(1) if you never look at the result
13:26:09 <bluejay> Can I ask ghc for help eyeballing it? Or, is there a web page with pre-eyeballed examples of good and bad functions?
13:26:24 <Smerdyakov> Do you understand how Haskell expressions are evaluated?
13:26:58 <chucky> you could run it through ghc's profiler, although it would probably take more effort than its worth
13:27:01 <bluejay> "They're evaluated only when they need to be." So not really. ;) 
13:27:41 <Smerdyakov> Things are only evaluated when needed to produce visible output, basically.
13:28:19 <Smerdyakov> The semantics are really the standard ones for call-by-name lambda calculus, as far as I know.
13:28:44 <chucky> an example would be producing an infinite list and using "head" on it. Only the first element of the list would ever be evaluated, the rest would be thrown away
13:29:24 <bluejay> If I'm producing a single number as output, that sounds like there's no way short of `seq` to get Haskell to evaluate things more efficiently?
13:30:35 <Smerdyakov> bluejay, I don't follow you. You can always evaluate a complicated expression that always has the value 1 and multiply it by your real answer.
13:31:50 <det> bluejay: evaluate what more efficiently?
13:32:05 <bluejay> I'm writing a factorial function (for homework, which asks for the big-O efficiency), and that should be able to execute in O(1) space. Is there a way to accomplish that without using seq, and how can I know I've found that way?
13:32:53 <det> This is haskell specific homework?
13:33:02 <Smerdyakov> That's a silly question, since if it uses O(1) space, then there must be a bound on input sizes, and you can use a fixed-size table mapping each input to its proper output.
13:33:23 <bluejay> no, it's actually ML/Scheme/Haskell, but I like Haskell
13:33:42 <Smerdyakov> bluejay, do you know what tail recursion is?
13:34:30 <bluejay> yeah. But I was under the impression that a tail-recursive solution in Haskell builds up a big tree of the (*) applications, and then applies them later, which would take O(n) space
13:35:04 <Smerdyakov> I don't know. It shouldn't.
13:35:22 <Smerdyakov> Functional languages need constant space tail recursion to be practical.
13:35:23 <bluejay> So (product [1..4]) puts "1*2*3*4" in memory and then evaluates it, rather than evaluating each sub-* eagerly
13:36:05 <bluejay> I was wondering how to figure out if ghc has done constant space tail recursion
13:36:23 <Smerdyakov> This is one of the reasons I don't use Haskell. ;-)
13:36:33 <bluejay> I have to go. I'll be on later. Thanks for trying. :)
13:36:45 <det> bluejay: maybe you should unburden yourself and go play with SML :)
13:38:29 * Igloo triple checks which channel I'm in
13:41:01 <eivuokko> What is a good way to catch error string from fail without "user error"-part?
13:44:57 <bringert> eivuokko: catchJust userErrors
13:45:17 <bringert> e.g.
13:45:20 <bringert> > catchJust userErrors (fail "foo") putStrLn
13:45:20 <bringert> foo
13:45:31 <eivuokko> Thank you!
13:47:20 <bringert> eivuokko: coming to EuroHaskell (let me know if I should stop bugging you)?
13:48:41 <eivuokko> I honestly don't know!  I'd love to, but I am switching apartements and going to LinuxTag (Germany) next month, so I might not have too much money :\
13:49:09 <eivuokko> Other than that, I'd come for sure :)
13:50:17 <bringert> let us know if you decide to come
13:50:30 <eivuokko> Of course.
13:50:31 <bringert> I think it's going to be a lot of fun
13:51:21 <eivuokko> That userErrors still left me with "", ah well.
13:52:23 <bringert> ?
13:53:05 <eivuokko> I don't understand where that comes from, tho.
13:53:22 <eivuokko> Oh, my bad.
13:53:48 <eivuokko> show on String adds ""
14:38:56 <shapr> hi Fractal 
14:39:56 <Marvin--> shapr: I got an interesting url from Laura today
14:40:24 <shapr> oh, tell me
14:40:48 <Marvin--> http://homepages.cwi.nl/~jve/HR/
14:40:56 <Marvin--> "The Haskell Road to Logic, Math and Programming"
14:41:20 <Marvin--> it looks interesting
14:50:35 <shapr> I started reading that today
14:50:56 <shapr> it was mentioned on lamdba the ultimate
14:53:29 <Marvin--> ah
14:53:47 <Marvin--> that must've been where Laura saw it too, then
14:55:50 <shapr> I thought it was cheesy at first, but I'm becoming interested
14:56:22 <Igloo> Hmm, if I want to be called back by a C library I'm going to need all my state in an IORef, aren't I?
14:58:02 * Igloo wonders if I have to worry about any concurrency issues with (a) the normal RTS and (b) the threaded RTS
15:00:28 <shapr> Marvin--: have you started reading that yet?
15:02:14 <shapr> whoa, 93
15:02:23 <shapr> nifty
15:02:38 <jadrian> hello
15:02:41 <shapr> Logan: hey, long time no see
15:02:48 <shapr> hi jadrian
15:03:13 <shapr> wassup?
15:03:23 <jadrian> hello shapr
15:03:27 <jadrian> playing with monads
15:03:33 <jadrian> again :)
15:03:42 <shapr> found anything nifty?
15:03:53 <jadrian> what are you up to?
15:04:04 <shapr> nothing useful at the moment
15:04:10 <jadrian> well, I'm back to composing monads 
15:04:17 <jadrian> monadic functions that is
15:04:28 <shapr> did you figure that out?
15:05:42 <jadrian> yeap it was easy, I had it figured out, just wondering if there were already std functions for this purpose
15:05:54 <jadrian> and if not, what names/symbols should I use :)
15:06:27 <jadrian> right now I was changing my composition definition and just about to test it
15:06:36 <jadrian> My first definition was:
15:06:45 <jadrian> compM        :: Monad m => [a->m a] -> (a->m a)
15:06:45 <jadrian> compM mfs a  = foldl (>>=) (return a) mfs
15:07:25 <jadrian> compM [f1...fn] a = ((f1 a >>= f2) ... >>= fn)
15:07:41 <jadrian> but I thought that right associativity woudl be better, right?
15:08:22 <jadrian> If I got a "fail" I'd stop instantly with right association and not with left...
15:08:27 <jadrian> am I right?
15:08:39 <jadrian> so now I defined:
15:08:51 <jadrian> rightCompM       :: Monad m => [a->m a] -> (a->m a)
15:08:52 <jadrian> rightCompM  = foldr (\f g-> (\x ->f x >>= g)) return 
15:09:02 <jadrian> which is supposed to give me
15:09:07 <Marvin--> shapr: 93?
15:09:38 <shapr> on the channel
15:09:46 <Marvin--> oh
15:09:58 <shapr> 94 for a bit
15:10:29 <shapr> I'm happy to see the community growing
15:10:59 <jadrian> rightCompM [f1...fn] a = \x->f1 x >>= (\x->f2 x>>=... >>=(\x->fn x)) a
15:11:10 <jadrian> shapr: am I on the right track
15:11:36 <Smerdyakov> jadrian, if you want different associativity, why don't you use foldr instead of foldl?
15:11:53 <jadrian> Smerdyakov: that's what I did...
15:12:01 <jadrian> Smerdyakov: isn't it?
15:12:01 <shapr> I think foldl is necessary is monadic folding
15:12:20 <Logan> shapr: Hi.
15:12:23 <Smerdyakov> jadrian, oops, yeah
15:12:27 <jadrian> Smerdyakov: of course I had to change the function passed to fold... because >>= isn't associative... 
15:12:33 <shapr> though I forget why at the moment
15:12:53 <shapr> Logan: how's code? written anything neat lately?
15:14:09 <jadrian> shapr: why?
15:14:10 <shapr> jadrian: do you want composition of the same monad? or different monads?
15:14:31 <jadrian> shapr: I want to "compose" monadic functions unders the same monad
15:14:44 <jadrian> shapr: it's just a [a -> m a]
15:14:56 <jadrian> sand I want to get a 
15:15:04 <jadrian> a -> [a -> m a] -> m a
15:15:28 <jadrian> it's just ((f1 a >>= f2) ... >>= fn)
15:16:20 <jadrian> I was just changin the definition to a foldr, because that way if I get a fail the computation will stop right away
15:16:24 <jadrian> and not with foldl
15:16:29 <jadrian> am I right?
15:16:34 <shapr> I don't know
15:16:37 <monotonom> @type flip ($)
15:16:38 <lambdabot> flip ($) :: forall b b1. b1 -> (b1 -> b) -> b
15:16:51 <monotonom> Let b1 = a, b = ma. :)
15:17:01 <shapr> I've read about this somewhere, but I don't remember the details
15:17:28 <jadrian> k, I think I'm right though :)
15:17:41 <jadrian> I'll test it and give it some more thought just in case
15:18:00 <shapr> have you tried what monotonom just suggested?
15:18:18 <shapr> y0 Fractal, how's the crypto?
15:18:26 <jadrian> shapr: me?
15:18:31 <Fractal> Hey man... Not bad
15:18:33 <shapr> jadrian: yes
15:18:33 <Fractal> Haven't worked on it in a while
15:18:38 <jadrian> monotonom?
15:19:08 <shapr> Fractal: your code is in the Haskell cryptolib
15:19:38 <jadrian> shapr: do not follow which sugested monotonom, I probably missed it... (I don't even know what that is...)
15:19:54 <Fractal> Ya I talked to the guy (name... ?). Pretty cool eh?
15:20:41 <shapr> was it Alistair Reid?
15:20:46 <shapr> yah, that's nifty
15:21:02 <jadrian> ah!
15:21:05 <shapr> jadrian: the rightCompM looks right to me
15:21:34 <shapr> as in, it looks like it would work, but I have the feeling there's an easier way
15:21:37 <jadrian> monotonom: monotonom! it's you! lol I was thinking about a monotonom function, sorry :)
15:22:31 <jadrian> monotonom: what was your sugestion again?
15:22:56 <jadrian> flip ($)...)
15:22:58 <jadrian> ?
15:23:32 <jadrian> hmmm
15:25:15 <jadrian> brb
15:27:40 <shapr> jadrian: what about foldM ?
15:28:07 <jadrian> If I recall correctly foldM purpose is different
15:28:22 <jadrian> you get one function, and a list of values 
15:28:33 <jadrian> what I'm doing is completely different
15:28:53 <jadrian> I have one starting value and a list of monadic functions
15:29:41 <Logan> shapr: Not really, just work stuff.  I have plans of things to implement after I graduate, though.
15:30:01 <shapr> Logan: revolution?
15:30:36 <Logan> shapr: Revolution?
15:31:20 <shapr> things to implement: 1. The Revolution
15:31:52 <Jerub> shapr: which one, american?
15:32:16 <Jerub> I would appreciate an american revolution, considering their reprehensible acts lately.
15:34:06 <shapr> jadrian: the more I look at this, the more I think monotonom is right
15:36:22 * shapr tries to work it out
15:37:27 <jadrian> shapr: I didn't get monotonoms hint
15:37:35 <jadrian> shapr: what was the idea?
15:39:36 <shapr> basically, fold (\x y -> y x)
15:41:15 <shapr> at least, that looks right to me at the moment
15:41:15 <jadrian> fold flip?
15:41:15 <jadrian> oh... no...
15:41:15 <jadrian> that would change the order...
15:41:16 <jadrian> wouldn't it?
15:41:29 <jadrian> I want the functions to be applied from left to right
15:41:43 <jadrian> both my functions are working fine
15:41:51 <jadrian> I'm positive about that
15:41:53 <shapr> ok
15:41:57 * SamB thinks Lout is incabable of doing literate haskell.
15:42:12 <shapr> I think there's a much simpler implementation though
15:42:20 <jadrian> maybe :)
15:42:25 <shapr> though I haven't figured it out yet
15:42:31 <jadrian> I was just wondering if I was right about the 2nd beeing the best
15:42:41 <jadrian> ist there a way to see the number of reductions in ghci?
15:42:50 <jadrian> :set +s just shows the time...
15:43:24 <jadrian> If I'm correct using the 1st one (left associative) if I get a fail, it will pass the fail to all the other monadic functions
15:43:40 <jadrian> the 2nd one though, should stop as soon as I get a fail
15:44:23 <jadrian> but I wanted to be sure I'm right
15:44:33 <jadrian> right about the 2nd stoping
15:44:55 <jadrian> and not missing anything that will eventually make the 2nd one inefficient
15:45:45 <jadrian> (maybe strictness analyses may be applied on foldl and not on foldr or something like that)
15:45:51 <flaw> what's the operator for string catenation?
15:46:02 <jadrian> ++
15:46:17 <flaw> list cat
15:46:47 <shapr> same one
15:46:51 <shapr> strings are lists
15:46:54 <flaw> yeah
15:48:11 <shapr> jadrian: I'm not awake enough to remeber how, but I'm sure there's a way to use fold with a singje value and a list of functions
15:48:46 <shapr> and I'm almost sure that can be generalized to monadic concatenation as you want
15:49:25 <shapr> I'll think about it tomorrow
15:50:40 <jadrian> well I think that's what I did...
15:55:53 <shapr> yes
15:56:36 <shapr> I think there's a simpler way to do that that I've forgotten
16:04:14 <monotonom> @type foldr
16:04:16 <lambdabot> foldr :: forall b a. (a -> b -> b) -> b -> [a] -> b
16:04:23 <Jon> @type sequence
16:04:24 <lambdabot> sequence :: forall a m. (Monad m) => [m a] -> m [a]
16:04:32 <monotonom> @type foldr ($) 0
16:04:34 <lambdabot> foldr ($) 0 :: forall b. (Num b) => [b -> b] -> b
16:04:55 <monotonom> @type foldr ($) [neg, neg, neg] 0
16:04:56 <lambdabot> bzzt
16:05:08 <monotonom> @type foldr ($) [neg, neg, neg] True
16:05:10 <lambdabot> bzzt
16:05:23 <monotonom> @type foldr ($) [not, not, not] True
16:05:25 <lambdabot> bzzt
16:05:56 <Jon> is it not op, cap, list?
16:05:56 <monotonom> @type foldr ($) [negate, negate, negate] (0::Int)
16:05:58 <SamB> @type (foldr ($) [neg, neg, neg] 0)
16:05:58 <lambdabot> bzzt
16:05:59 <lambdabot> bzzt
16:06:16 <Jon> @type foldr ($) True [False, False, False]
16:06:18 <lambdabot> bzzt
16:06:21 <Jon> :/
16:06:39 <monotonom> Ah I see my mistake
16:06:47 <monotonom> @type foldr ($) (0::Int) [negate, negate, negate]
16:06:49 <lambdabot> foldr ($) (0::Int) [negate, negate, negate] :: Int
16:07:45 <SamB> @type foldr ($) True [not, not, not]
16:07:47 <lambdabot> foldr ($) True [not, not, not] :: Bool
16:08:01 <monotonom> Anyway, that is a fold, a single value, and a list of functions.
16:09:26 * SamB wonders if you can do that with a rank-two list
16:10:39 <jadrian> monotonom: yeap, nice trick ;)
16:10:51 <jadrian> monotonom: doesn't apply in my case though
16:11:04 <jadrian> monotonom: 2 motives
16:11:25 <jadrian> monotonom: the functions are not applied on the returned value of the previous ones
16:11:45 <jadrian> monotonom: their monadic functions, so I need the >>=
16:11:53 <jadrian> monotonom: 2nd
16:12:15 <jadrian> monotonom: when you do a foldr, you're applying the function on the right first
16:12:38 <jadrian> monotonom: not me, I allways apply the one on the left first even with foldr
16:12:52 <jadrian> monotonom: the only difference was the association
16:12:57 <monotonom> Methinks either foldM or mapM.
16:13:07 <jadrian> nope neither :)
16:13:28 <jadrian> foldM takes one monadic function and a list of values
16:13:35 <jadrian> and performs, well... a monadic fold :)
16:13:45 <Jon> why isn't sequence appropriate?
16:14:07 <jadrian> let me think :)
16:14:34 <monotonom> sequence lacks the single value. :)
16:14:47 <jadrian> sequence takes a list of monadic values
16:14:51 <jadrian> I don't even have that...
16:14:58 <jadrian> I have a list of monadic functions
16:15:07 <Jon> ah
16:15:21 <Jon> thx
16:15:26 <jadrian> what I'm doing is:
16:15:55 <jadrian>  compM [f1,..., fn] a = f1 a >>= f2 >>= ... >>= fn
16:16:03 <jadrian> simple
16:16:21 <jadrian> and the definiton is easy I think
16:16:25 <jadrian> compM        :: Monad m => [a->m a] -> (a->m a)
16:16:25 <jadrian> compM mfs a  = foldl (>>=) (return a) mfs
16:16:29 <jadrian> anyway
16:16:33 <monotonom> Yes.
16:16:46 <jadrian> ((f1 a >>= f2) >>= ... >>= fn)
16:16:49 <monotonom> @type foldM
16:16:50 <lambdabot> bzzt
16:16:55 <monotonom> @type Monad.foldM
16:16:57 <lambdabot> Monad.foldM :: forall a m b.
16:16:57 <lambdabot> 	       (Monad m) =>
16:16:57 <lambdabot> 	       (a -> b -> m a) -> a -> [b] -> m a
16:17:11 <jadrian> imagine f1 a returns Nothing
16:17:25 <jadrian> or more generaly
16:17:33 <shapr> @type Control.Monad.foldM
16:17:35 <lambdabot> Control.Monad.foldM :: forall a m b.
16:17:35 <lambdabot> 		       (Monad m) =>
16:17:35 <lambdabot> 		       (a -> b -> m a) -> a -> [b] -> m a
16:17:40 <jadrian> (f1 a) returns fail
16:17:56 <jadrian> ((f1 a >>= f2) >>= ... >>= fn)   <-- fail will be passed to f2... and on to fn
16:18:07 <jadrian> if I had: 
16:18:22 <jadrian> f1 a >>= (....)    -- that is right associativity
16:18:30 <jadrian> and I'd get a fail
16:18:39 <jadrian> it would stop the computation at that moment
16:18:42 <jadrian> right?
16:19:19 <jadrian> so I used the assoc monad law: (f a >>= g) >>= h <=> f a >>= (\a -> g a >>= h)
16:19:26 <jadrian> and came um with:
16:19:30 <jadrian> rightCompM       :: Monad m => [a->m a] -> (a->m a)
16:19:30 <jadrian> rightCompM  = foldr (\f g-> (\x ->f x >>= g)) return
16:19:36 <jadrian> which is right associative
16:19:53 <jadrian> and I expect it to stop as soon as I get a fail
16:20:05 <jadrian> now I'd like to test this... :-/
16:20:15 <jadrian> any easy way to see if its working?
16:20:38 <Pseudonym> You could use the Maybe monad.
16:20:47 <jadrian> Pseudonym: yeap I am :)
16:21:02 <jadrian> Pseudonym: I was wondering if I could see the n. of reductions on ghci
16:21:05 <jadrian> for instance
16:21:13 <Pseudonym> Yes, but I'm not sure it'd help you.
16:21:26 <Pseudonym> Here's a suggestion.
16:21:50 <jadrian> well if I had a huuuuge list of functions and a Nothing returned by the first...
16:21:56 <Pseudonym> rightCompM [\x -> return (1:x), \x -> return (1:y), ...]
16:22:08 <Pseudonym> Sorry, \x -> return (2:x) for the second.
16:22:18 <Pseudonym> Oh, I see.
16:22:19 <Pseudonym> Duh.
16:22:26 <jadrian> ?
16:22:27 <Pseudonym> What you want is a monad transformer.
16:22:32 <jadrian> nope!
16:22:34 <Pseudonym> No?
16:22:40 <jadrian> errr no
16:22:54 <Pseudonym> See, what I _think_ you want is a monad which can fail stacked on top of one that can't.
16:22:56 <jadrian> I know the function is working fine
16:23:04 <jadrian> no :)
16:23:05 <Pseudonym> But you want to test it.
16:23:07 <Pseudonym> That's what you said.
16:23:26 * Pseudonym shuts up and listens
16:23:34 <jadrian> nope I got two HO functions that compose monadic functions
16:23:42 <jadrian> compM and rightCompM
16:23:51 * Pseudonym checks back in the logs
16:23:59 <jadrian> the difference is that 1st is left and 2nd is right associative
16:24:03 <Pseudonym> Right.
16:24:15 <jadrian> I think 1st is better because it should stop the computation as soon as I get a fail
16:24:18 <jadrian> opss
16:24:19 <jadrian> 2nd
16:24:27 <jadrian> right associative : better!
16:24:32 <Pseudonym> Kind of.
16:24:38 <Pseudonym> It's actually not that simple.
16:24:47 <jadrian> that's what I'm afraid of!
16:24:47 <Pseudonym> It depends on how the monad is implemented.
16:24:48 <jadrian> :)
16:25:07 <Pseudonym> See, the actual monad computation will stop as soon as you get a fail.
16:25:11 <Pseudonym> That's the semantics of monads.
16:25:19 <Pseudonym> fail >>= k == fail
16:25:21 <jadrian> yes but!
16:25:36 <jadrian> ((f1 a >>= f2) >>= ... >>= fn)   <-- fail will be passed to f2... and on to fn
16:25:46 <Pseudonym> Maybe.
16:25:57 <jadrian> why maybe...
16:26:02 <Pseudonym> Actually what will happen will be the outermost >>= will get evaluated first.
16:26:07 <Pseudonym> BUT its arguments may not be.
16:26:27 <Pseudonym> It depends on how >>= is implemented.
16:26:37 <jadrian> the outermost is fn
16:26:37 <jadrian> it cannot be evaluated
16:26:42 <Pseudonym> Sure it can.
16:27:19 <Pseudonym> The definition of m >>= k (for some monad) may, for example, turn k into a continuation, then tail-call m.
16:27:21 <jadrian> how? f2 needs the computation f1 a
16:28:15 <Pseudonym> You're not thinking lazily enough.
16:28:30 <jadrian> (that line always sounds funny :))
16:28:33 <Pseudonym> In m >>= k, the >>= may be reduced, but k probably won't be.
16:28:55 <jadrian> I think I see what you mean
16:29:03 * jadrian always skipped continuations...
16:29:30 <jadrian> what if, for now, I just instanciate m as Maybe...
16:29:35 <jadrian> in that case, am I right?
16:32:40 <jadrian> compM [f1,..., fn] a = (f1 a >>= f2) >>= ... >>= fn
16:32:46 <jadrian> rightCompM [f1,..., fn] a = \x->f1 x >>= (\x->f2 x >>= ... >>= \x->fn x) a
16:33:06 <jadrian> by beeing right I mean, 2nd is better with Maybe Monad
16:33:27 <Pseudonym> Let me think.
16:33:37 <Pseudonym> Just x >>= k   = k x
16:33:42 <Pseudonym> Nothing >>= k = Nothing
16:33:43 <Pseudonym> Right?
16:33:48 <jadrian> exactlt
16:33:50 <jadrian> exactly
16:33:58 <Pseudonym> OK, in that case, you may be right.
16:34:19 <Pseudonym> In fact, you are.  Because >>= has to evaluate its first argument.
16:34:31 <jadrian> yeap :)
16:34:53 <jadrian> I didn't even think that might not happen in the general case :-/
16:34:58 <jadrian> now I see I'm wrong
16:35:24 <jadrian> now if I wanted to test such a thing...
16:35:27 <jadrian> how should I do?
16:35:44 <jadrian> I think checking the number of reductions would be ok
16:36:01 <jadrian> but ghci doesn't seem to have that option
16:36:16 <Pseudonym> :set +s
16:36:18 <Pseudonym> I think.
16:36:20 <Pseudonym> Oh, no.
16:36:27 <Pseudonym> No, GHCi doesn't count reductions.
16:36:32 <jadrian> yeap :-/
16:36:36 <Pseudonym> The reason for that is simple: The STG machine doesn't quite work that way.
16:37:24 <jadrian> I'm also having trouble with names for such functions 
16:37:31 <jadrian> composeM ?
16:37:49 <jadrian> classic compose  works the other way around...
16:38:10 <jadrian> that is, the 1st function in the list is the last to be applyed
16:38:24 <jadrian> also where should I put the R and L
16:38:36 <jadrian> composeMr like foldr?
16:38:45 <jadrian> but all monadic functions end in M...
16:38:58 <jadrian> (anyway I should probably shut up and think :))
16:38:59 <Pseudonym> THere's a foldM already, isn't there.
16:39:01 <Pseudonym> @type foldM
16:39:03 <lambdabot> bzzt
16:39:08 <jadrian> yeap
16:39:08 <Pseudonym> Apparently not.
16:39:14 <jadrian> yes, there is one
16:39:14 <Pseudonym> Ah, it's in a module.
16:39:28 <jadrian> @type Control.Monad.foldM
16:39:29 <lambdabot> Control.Monad.foldM :: forall a m b.
16:39:29 <lambdabot> 		       (Monad m) =>
16:39:29 <lambdabot> 		       (a -> b -> m a) -> a -> [b] -> m a
16:39:58 <Pseudonym> Ah, yes.
16:40:11 <jadrian> this isn't exactly a fold either
16:40:13 <Pseudonym> Maybe it's seqM.
16:40:23 <Pseudonym> But that's not quite right either.
16:40:27 <jadrian> oh you're searching for the function?
16:40:29 <jadrian> I did it
16:40:32 <jadrian> it's not there
16:40:36 <jadrian> which I find surprising
16:40:45 <jadrian> because it seems quite usefull... I think
16:40:48 <Jerub> irssi displays \t characters as highlighted 'I's.
16:40:52 <Jerub> its quite annyoing.
16:41:01 <jadrian> I'm just wondering about the name I should choose for my functions
16:41:34 <Pseudonym> No, I'm thinking of names.
16:41:38 <jadrian> oh
16:41:45 <Pseudonym> seqM isn't quite right.
16:41:50 <Pseudonym> Maybe some variation on bind.
16:42:05 <jadrian> not quite either
16:42:13 <Pseudonym> bindFoldM
16:42:15 <jadrian> I was indeed thinking about defining something like
16:42:21 <jadrian> oh
16:42:32 <Pseudonym> Because >>= is pronounced "bind".
16:42:37 <jadrian> by bind I though you meant >>=
16:42:40 <jadrian> oh you do :)
16:42:45 <Pseudonym> Right.
16:42:56 <jadrian> I was thinking about declaring a:
16:43:14 <jadrian> a >>== [f1 ... fn]
16:43:15 <jadrian> too 
16:43:48 <jadrian> but this is slightly different as it already takes a monad as input...
16:44:01 <jadrian> bindFold seems nice but
16:44:16 <jadrian> in the right associative case it may be ,isleading
16:44:19 <jadrian> misleading
16:44:48 <Pseudonym> They should be identical semantically.
16:44:59 <Pseudonym> And you only need the right-associative version.
16:44:59 <jadrian> yes... 
16:45:13 <jadrian> I'd like to keep both just in case
16:45:18 <Pseudonym> Why?
16:45:26 <Pseudonym> What would you need the left-associative version for?
16:45:32 <Pseudonym> We don't need both versions of concat.
16:45:37 <jadrian> maybe it's better in some cases (?)
16:45:53 <jadrian> (some other monads maybe)
16:46:03 <Pseudonym> I don't think so.
16:46:14 <Pseudonym> And the right-associative version is better if the list is infinite.
16:47:33 <jadrian> is there any chance, for instance, that strictness analyses migt be performed on the left assoc version in some cases?
16:48:13 <Pseudonym> Well, with the left associative version, the >>='s are all evaluated, and the strictness analyser will probably know that.
16:48:30 <Pseudonym> With the right associative version, the >>='s may not all be evaluated, and the strictness analyser will find that, too.
16:49:45 <jadrian> may be just whishful thinking, foldl (+) is lazy
16:50:14 <SamB> jadrian: how can that be...?
16:50:44 <jadrian> SamB: just try it out,  foldl (+) 0 [1..100000] on ghc
16:50:50 <Pseudonym> Yes, it is.
16:50:55 <jadrian> it creates a chunk
16:51:02 <SamB> jadrian: oh, that. everything does that!
16:51:04 <jadrian> (0 + 1) + 2)...
16:51:21 <SamB> unless it wouldn't make a difference.
16:51:52 * jadrian is getting tired
16:52:01 <SamB> notice how the whole thing must be evaluated at once.
16:52:48 * jadrian is off to bed... 
16:52:51 <jadrian> see you all!
16:53:17 <Pseudonym> Night (belated).
16:59:59 <bluejay> SamB: Is there a way to know whether ghc is noticing that it doesn't make a difference?
17:05:11 <SamB> bluejay: you could ask it to give you lots of information about what it is doing
17:06:00 <Pseudonym> The thing is, it _might_ make a difference.
17:06:11 <Pseudonym> Suppose the operation wasn't (+), but (/).
17:06:16 <Pseudonym> That might throw an exception.
17:06:51 <Pseudonym> And if you evaluate it eagerly, the wrong exception might be thrown, whereas you might get the right one if you evaluate it lazily.
17:06:54 <bluejay> SamB: do you mean with profiling?
17:09:00 <SamB> bluejay: no. there are a variety of different options for outputting things such as the final core representation of the module, or displaying the contents of the .hi file...
17:14:56 <bluejay> SamB: Is that the -ddump* options? Will I (ignorant newbie) understand their output?
17:17:32 <SamB> bluejay: probably take a while.
17:17:51 <SamB> bluejay: which, exactly, was it you wanted to know if GHC had figured out?
17:17:53 <Pseudonym> You'll probably learn a lot, though.
17:20:52 <bluejay> SamB: whether it needed to be lazy. (I'm specifically wondering if I can write an O(1) space factorial function without using seq)
17:21:15 <Jerub> you can write an O(1) time factorial!
17:21:17 <Jerub> :))
17:21:23 <bluejay> SamB: (And if the answer's yes, how I can know that I've succeeded)
17:21:57 <bluejay> Jerub: :-P yes, but that's not what I'm going for
17:22:55 <SamB> bluejay: O(1) space factorial, you say? that might be easier.
17:24:03 <Pseudonym> bluejay: foldl' is eager.
17:24:04 <Pseudonym> However.
17:24:12 <Pseudonym> Factorial isn't O(1), really.
17:24:16 <Pseudonym> In space.
17:24:32 <Pseudonym> An arbitrary-precision Integer is not constant space.
17:24:39 <bluejay> Pseudonym: foldl' uses seq, iirc
17:24:42 <Pseudonym> Moreover, multiplying two of them is not constant time!
17:24:51 <SamB> Pseudonym: O(log n)?
17:24:52 <Pseudonym> bluejay: Yes, seq or $!.
17:24:54 <bluejay> *sigh*
17:25:12 <bluejay> pedantic people...
17:25:26 <Pseudonym> It really makes a difference when you need to find the factorial of big numbers.
17:25:32 <Pseudonym> As I had to recently.
17:25:38 <Pseudonym> For work, no less.
17:25:43 <Pseudonym> (Haskell rules!)
17:28:42 <bluejay> will "f_l = 1:zipWith (*) f_l [1..]" be efficient? More or less than a tail-recursive solution? What does tail-recursive really mean in a lazy language? (++) doesn't look tail-recursive, but it's nice and efficient anyway, while my tail-recursive fact isn't efficient.
17:29:24 <bluejay> *sigh* Is there a book on writing efficient Haskell?
17:29:51 <Pseudonym> No, there isn't.
17:29:57 <Smerdyakov> There are very few people who _care_ about that subject, bluejay. :)
17:30:17 <bluejay> There's lots of little (contradictory) tips on the subject in the wiki. So somebody must car
17:30:19 <bluejay> care
17:30:24 <SamB> you don't need tail recursion for lazy list operations in haskell... because lazy recursion is just about as good as tail recursion
17:30:33 <Pseudonym> bluejay: If it helps...
17:30:35 <SamB> if not better
17:30:40 <Pseudonym> f_l is a CAF.
17:30:54 <Pseudonym> This means it won't get garbage collected if anyone still refers to it.
17:31:20 <Pseudonym> Even if they only need to compute 3 factorial, the results from that time you computed 1000 factorial will still be around.
17:31:34 <Pseudonym> Is that a concern?
17:31:53 <bluejay> Pseudonym: maybe. It's nice to know. But it does mean that they won't ever need to do the multiplications again
17:32:05 <Pseudonym> That's true.
17:32:25 <Pseudonym> It will take O(n) time to compute n factorial, of course.
17:32:34 <Pseudonym> Because it takes that long to search the list.
17:33:06 <Pseudonym> Have you considered an O(log n) structure?
17:34:14 <Pseudonym> And will your application ever compute the factorial of 1000 or more anyway?
17:34:35 <Pseudonym> If so, the naive factorial algorithm may not be appropriate to begin with.
17:34:47 <Pseudonym> (There are faster algorithms for large N.)
17:34:51 <bluejay> I don't have an application. I'm just writing the functions and then saying what their big-O cost is.
17:34:56 <Pseudonym> Ah, OK.
17:35:07 <Pseudonym> Well it's O(n), because that's how long it takes to search the list.
17:35:23 <bluejay> yeah. But I also need the space cost.
17:35:57 <Pseudonym> Well, you only compute as much of the list as you need.
17:36:04 <Pseudonym> So that should help.
17:36:33 <bluejay> So, if the only reference to f_l is a single call to f_l!!73, I suspect that everything before the current spot in the list will be garbage, and the space cost will be O(1)
17:36:42 <Smerdyakov> I think bluejay is making this harder than needed, since he's said he could also use ML or Scheme for this.
17:37:07 <Pseudonym> bluejay: No.  Everything before the current spot in the list will NOT be garbage.
17:37:09 <Smerdyakov> Thus, the person assigning the question doesn't mean to request an analysis of a lazy program.
17:37:14 <Pseudonym> Ah, I see.
17:37:21 <Pseudonym> Yes, you might want to use a strict language.
17:37:25 <Pseudonym> Much easier to analyse.
17:37:41 <bluejay> I would like to understand a lazy language, even if it's not on the test.
17:37:58 <bluejay> And I hate ML's type system
17:37:59 <Pseudonym> Fair enough.
17:38:15 <Pseudonym> #1: Hugs doesn't garbage collect CAFs.
17:38:34 <Pseudonym> So if you're using hugs, the list will never be collected.
17:39:04 <Pseudonym> #2: GHC only garbage collects CAFs if the CAF can no longer be reached.
17:39:25 <Pseudonym> If anyone AT ALL holds onto a (possibly indirect) reference to it, it is not collected.  Otherwise the whole thing is.
17:39:48 <Pseudonym> The point being that "everything before the current spot in the list" is not garbage.
17:39:53 <bluejay> I see
17:40:04 <Pseudonym> Unless the CAF as a whole is garbage, and then that's not true under Hugs.
17:40:06 <bluejay> This is why someone needs to write a book on efficient Haskell ;)
17:40:23 <Pseudonym> Part of the problem here is that it's so variable.
17:40:30 <Pseudonym> See, that's a difference between GHC and Hugs.
17:40:50 <Pseudonym> I just learned yesterday that GHC doesn't do let-floating when optimisation is turned off.
17:40:58 <bluejay> ??
17:41:07 <Pseudonym> So if this CAF was in a where-clause, whether or not it is garbage would depend on the optimisation level.
17:41:28 <Pseudonym> To quote a movie: "Nobody knows.  It changes practically every day."
17:42:32 <bluejay> I may quote you on the test. ;) But the basic answer seems to be that O(n) space is required, except maybe with seq
17:43:29 <Pseudonym> Yes.
17:44:23 <Pseudonym> @wiki MemoisingCafs
17:44:23 <lambdabot> http://www.haskell.org/hawiki/MemoisingCafs
17:44:26 <Pseudonym> Dunno if you've read that.
17:47:28 <bluejay> I think I skimmed it, but didn't bother to fully understand the O(log n) CAF
17:47:59 <bluejay> Anyway, thanks for the help
17:48:05 <Pseudonym> No problem.
17:57:11 <Spark> is there a name for an asbtract computational machine that is effectively the same as a turing machine, but cannot go backwards along the tape
17:57:23 <Spark> so for example a procedural langauge without "while", but with "if"
17:57:35 <Spark> or something that could be implemented with just foldl
17:58:03 <Pseudonym> Under what conditions could it choose to make a decision about "if"?
17:58:06 <Pseudonym> Tape input?
17:58:41 <Pseudonym> If so, it sounds like a deterministic finite state transducer to me.
17:59:16 <Pseudonym> Kind of like the Haskell scanl function.
18:00:19 <Spark> mm
18:00:48 <Smerdyakov> Spark, sounds like primitive recursion or something equivalent.
18:00:59 <Smerdyakov> Or, actually not. :)
18:01:07 <Smerdyakov> But that is another machine model where the code always terminates.
18:01:10 <Spark> theres no representation of an infinite loop
18:01:14 <Spark> yeah
18:01:44 <Spark> im wondering why it is useful to be able to implement an infinite loop in a language :)
18:01:46 <Pseudonym> However, I suspect that you can detect termination in this machine.
18:02:01 <Spark> you can always detect termination cant you?
18:02:03 <Pseudonym> Spark: Well, most servers are infinite loops. :-)
18:02:13 <Spark> ah yeah, in a closed system you can
18:02:23 <Smerdyakov> Spark, no. You can't detect termination on a Turing machine, for example.
18:02:30 <Pseudonym> Sorry, I mean you may be able to statically analyse the description of this kind of machine and tell whether or not the program will terminate or not.
18:03:10 <Spark> Pseudonym: ah yes, is that what while() or generic recursion prevents
18:03:34 <Pseudonym> It's what heading backwards on the tape also prevents. :-)
18:03:48 <Spark> mm
18:03:52 <Spark> are these things all equivelant then?
18:04:56 <Smerdyakov> No. With just "if's", you get a much weaker language than primitive recursion.
18:05:14 <Spark> what is primitive recursion, precisely?
18:05:26 <Smerdyakov> You'll never be able to have infinitely many different outputs, for example.
18:05:33 <Smerdyakov> (for different inputs)
18:05:40 <Spark> nondeterminism?
18:05:45 <Smerdyakov> No
18:05:53 <seafood> Spark: Very generally, primitive recursion is what you can obtain with bounded loops.
18:06:12 <Spark> bounded loops, i.e. mapping a list or folding
18:06:12 <Spark> ?
18:06:26 <Smerdyakov> It's recursion over inductive structures where you can only recurse with sub-arguments of the current argument.
18:06:34 <Spark> or the sort of thing you can implement by repeating a block of code
18:06:52 <Smerdyakov> When the inductive structure is the natural numbers, you can only make a recursive call with the predecessor of the current (non-0) argument.
18:06:55 <seafood> Smedyakov: textbook answer 
18:06:59 <Spark> ah, recursions where you can construct a function over the parameters, that always decreases, and has a lower bound?
18:07:25 <Smerdyakov> Spark, yes. It's usually done using inductive types where it's very clear what that "function" is.
18:07:36 <Spark> can you not represent that by repeating blocks of code
18:07:46 <Spark> and jumping into the middle of that code
18:07:58 <Smerdyakov> No. You'll never handle an infinite domain of inputs that way.
18:08:03 <Smerdyakov> Since you may only have finitely many blocks of code.
18:08:13 <Spark> ah
18:08:20 <Spark> suppose the code can be infinite?
18:08:37 <Smerdyakov> Then the whole thing is boring....
18:08:46 <Smerdyakov> We've left behind any connection to real computers.
18:08:47 <Spark> is it a kind of syntactic sugar that prevents the necessity of infinite code?
18:09:12 <Smerdyakov> It's no more "syntactic sugar" than Haskell is "syntactic sugar" for machine language.
18:09:36 <Smerdyakov> We just take as a given in CS that code must be finite.
18:09:43 <SamB> what if the code had recursive links in it?
18:09:44 <Smerdyakov> It's part of the folk definition of what a computer is.
18:10:14 <Spark> hm thats a shame
18:10:18 <Smerdyakov> Spark, why?
18:10:35 <Smerdyakov> SamB, what do you mean?
18:10:37 <Spark> because we can abstractly deal with infinite code
18:10:46 <Smerdyakov> Spark, sure, but why would we want to?
18:11:01 <Spark> if finite code can be converted to a simpler langauge, but an infinite amount of code
18:11:05 <SamB> Spark: you probably think it is a shame that you can't write modular bf, too
18:11:07 <Spark> or the other way round, for implemetnation
18:11:20 <Smerdyakov> Spark, all programs can be made into simple infinite tables mapping inputs to outputs if you have "infinite code"....
18:11:30 <Spark> oh yes thats true
18:11:30 <SamB> Smerdyakov: it is easier than abstractly dealing with finite code ;-)
18:11:50 <Spark> hmm but those tables arent countable infinite :)
18:12:01 <Spark> not sure what relevance that might have though
18:12:14 <Smerdyakov> Spark, now you're over the edge in another way: we only deal with countable sets as machine contents in CS.
18:12:31 <Spark> machine contents?
18:12:33 <Smerdyakov> Spark, except for some loonies off in the far side of theory-land. :)
18:12:36 <SamB> if you have infinite programs, they must come from somewhere.
18:13:33 <Smerdyakov> Spark, machines are always modeled as values from countable sets.
18:13:42 <SamB> they must either be generated on the fly or or downloaded from somewhere else, where they have been generated on the fly (possibly by people ;-)
18:13:48 <Spark> what is a machine in this context?
18:13:50 <Spark> a program?
18:13:55 <Spark> or an interpreter of a program
18:13:56 <Smerdyakov> Spark, which is quite reasonable, since we mean to apply this theory to real computers.
18:14:02 <Smerdyakov> Spark, your motherboard, RAM, etc..
18:14:19 <Spark> right so thats an interpreter of a language then, with an internal state :)
18:14:41 <Smerdyakov> Sure
18:14:44 <Spark> so what is the value of a machine?
18:14:53 <Smerdyakov> That depends on the formalism.
18:15:04 <Smerdyakov> For a finite state machine, it's one of a finite set of states.
18:15:16 <Smerdyakov> For a real PC, it's register contents, RAM contents, etc..
18:15:21 <SamB> Spark: nobody worries about what the value is, the thing is that you can pretend there is one
18:15:29 <Spark> ah right
18:15:42 <Smerdyakov> SamB, I worry about what the value is all the time, and so do other peopel working in program verification.
18:15:50 <Spark> so the set of all states must be countable
18:16:15 <Smerdyakov> Spark, yes, since otherwise you can't bring your theory back down to reality and have any useful results.
18:16:15 <SamB> Smerdyakov: what do you mean? you can't point at something and say "this is the value of this computer"
18:16:31 <Smerdyakov> SamB, you can in program verification.
18:16:35 <Spark> surely the set of states must be finite for implementation?
18:16:38 <Smerdyakov> SamB, you create the type of machine states.
18:16:47 <Smerdyakov> SamB, and reason about them.
18:16:58 <SamB> Smerdyakov: don't tell me you must find every bit of state in every piece of hardware on the bus!
18:17:00 <Smerdyakov> Spark, no.
18:17:10 <Smerdyakov> SamB, of course not, but you didn't specify which machine model you meant.
18:17:30 <Smerdyakov> SamB, you don't need much state to handle interpretation of most machine code programs.
18:17:49 <Spark> samb: "value" is of course an abstract concept, but you can still reason about it
18:17:58 <SamB> hmm, yes.
18:18:53 <Smerdyakov> You make them concrete concepts to reason about them.
18:19:15 <Spark> hmm whats the definition of concrete?
18:19:21 <Smerdyakov> And I have to give a (short) talk about this in two days. :)
18:19:22 <Spark> it would need a mapping to reality surely?
18:19:34 <Smerdyakov> Define it in a logical framework, like CIC or LF.
18:19:36 <Spark> of concrete means "well defined" then yeah i guess :)
18:19:41 <Spark> ah right yeah
18:19:47 <SamB> but it would be silly to pretend you can reason mathematically about the value of a concrete machine, I think
18:19:48 <Spark> concrete/abstract are relative terms :p
18:20:04 <SamB> as a whole, I mean.
18:20:14 <Smerdyakov> SamB, then I guess you think the Java Bytecode Verifier for JIT compilation is silly?
18:20:22 <Smerdyakov> SamB, because it sure seems to me to work quite well!
18:20:25 <Spark> samb: have you ever studied formal semantics?
18:20:36 <Spark> (of programming languages)
18:20:39 <SamB> okay, s/concrete/metal-and-silicon/
18:20:50 <Spark> you dont need to verify it at that level
18:21:10 <Smerdyakov> In this conversation, I've meant "abstract" as "complete English language fluff" and "concrete" as "defined in a logical framework."
18:21:18 <SamB> oh
18:21:21 <SamB> hehe
18:21:29 <Spark> :)
18:21:43 <Spark> im not sure thats the conventional definition of "abstract" :p
18:22:14 <Pseudonym> We should make computers out of concrete.
18:22:14 <Smerdyakov> Well, I never really used the word "abstract" to refer to it. It was more "un-concrete." :P
18:23:01 <Smerdyakov> I think in #haskell, it's fair to consider anything formalized in a logical framework to be concrete. Or so would be my impression from what I know of the crowd.
18:23:26 <Pseudonym> Anything you can implement is concrete.
18:23:31 <Spark> i dont think abstract/concrete works in that sense
18:23:40 <SamB> hmm. I don't know. I thought Haskell was abstract ;-)
18:24:11 <Pseudonym> This just goes to show that theory and practice are the same, in theory.
18:24:17 <SamB> hehe
18:24:21 <Spark> right
18:24:32 <Pseudonym> Not necessarily in practice, though.
18:24:43 <SamB> but which is this?
18:24:55 <Pseudonym> Haskell is practice.
18:25:02 <Spark> design of programming languages is theory :)
18:25:02 <Pseudonym> But it's also grounded in good theory.
18:25:08 <Smerdyakov> LF and CIC are practice.
18:25:25 <SamB> maybe haskell is theory in practice 
18:25:39 <Spark> talking about haskell (the langauge) is more abstract than talking about a specific implementation of haskell, or a specific program written in haskell
18:25:44 <Pseudonym> Personally, I think the best work is done in the intersection between good theory and good practice.
18:25:51 <Smerdyakov> I don't buy the idea of a hard-and-fast distinction between "theory" and "practice."
18:25:55 <Pseudonym> That's why I do R&D for a living and not just R.
18:26:00 <SamB> or is it practice in theory?
18:26:10 <Smerdyakov> These terms are not broadly applicable.
18:26:23 <Smerdyakov> I'd have a hard time classifying what I do in either.
18:26:25 <Spark> implementation for me, means proof of concept :)
18:26:33 <Smerdyakov> For me, implementation means proofs!
18:26:36 <flippo> I'd love to see "Essentials of Programming Languages" by Friedman et al translated from scheme to Haskell
18:26:40 <Spark> which is why i suspect i shouldnt work in engineering :)
18:26:42 <Pseudonym> Implementation for me means stuff a customer will pay for.
18:26:55 <Pseudonym> And not know there's good theory behind it. :-)
18:27:09 <Smerdyakov> They don't need to take your word for it when you prove it!
18:27:15 <SamB> it is (theory, practice) where (theory, practice) = (Theory practice, Practice theory)
18:27:27 <SamB> er, I lost a perhaps
18:27:34 <Spark> Smerdyakov, you know the sort of derivations you can do using structured operational semantics...
18:27:50 <Spark> are they not equivelant to implementing the program, and executing it, to see what it does
18:27:59 <Pseudonym> Right. -)
18:28:02 <Smerdyakov> Spark, yes
18:28:04 <Pseudonym> But proofs are often important.
18:28:13 <Spark> Smerdyakov: what about denotational semantics?
18:28:15 <Pseudonym> I don't hack compilers for a living any more, but that's a good example.
18:28:26 <Smerdyakov> Spark, of course, if your program can have infinitely many different behaviors, you'll never "prove" anything useful with brute testing.
18:28:30 <Pseudonym> Your optimisations may not necessarily improve the program, but they'd better not change its meaning.
18:28:36 <Pseudonym> You'd better PROVE that they don't.
18:28:38 <SamB> Smerdyakov: but if they can't understand the proofs, they might have to take your word for it anyway!
18:28:50 <Pseudonym> The customer jsut wants their programs to go fast.
18:28:57 <Spark> Smerdyakov: nondeterminsm can be represented with models!
18:29:37 <SamB> Spark: you mean like /dev/random?
18:29:51 <Spark> thats not nondeterminism
18:29:54 <Smerdyakov> Spark, you can't use "models" with finite numbers of tests.
18:30:09 <Pseudonym> No.  /dev/random is probabalistic, not nondeterministic.
18:30:12 <Smerdyakov> SamB, that's the beauty of proof irrelevance.
18:30:32 <Smerdyakov> SamB, they just need to understand the theorem statement and trust the logical framework implementation's proof checker.
18:31:23 <Spark> ive always considered the beauty of formal derivations, is that they can consider all possible input
18:31:37 <Spark> and that is true of all maths that i have studied
18:31:58 <Spark> you can deal with an infinite number of values, and still come up with the correct solution
18:32:15 <Pseudonym> That's true of programming too.
18:32:18 <Smerdyakov> Spark, yes. It's even better when you have a visual "debugger" that executes all possible program paths using symbolic machine states. :D
18:32:26 <Pseudonym> For some value of "infinite".
18:32:38 <Pseudonym> (Given that all computers are actually finite and all.)
18:33:36 <Spark> Smerdyakov: so a table of all possible input against all possible output, could we not consider that to be an infinite "program" with the most simple language possible
18:33:51 <Smerdyakov> Spark, yes, and useless to computer scientists.
18:33:56 <Spark> ok
18:34:01 <Spark> i think the table is countable
18:34:18 <Spark> since its the set product of other countable sets
18:34:25 <Spark> (cartesian product)
18:34:52 <Spark> as the complexity of the language increases, you can make the table finite, right?
18:35:03 <Spark> there must be several steps along that path
18:35:12 <SamB> Spark: practically uncountable, though, since you'd take forever!
18:35:28 <Smerdyakov> SamB, do you know the technical meaning of "countable"?
18:35:29 <Spark> if you make the language too complex, it becomes impossible to reason about it
18:35:33 <Spark> so where is the "sweet spot"
18:35:41 <SamB> Smerdyakov: I said practically!
18:36:03 <SamB> practically is just about the opposite of technically
18:36:30 <Smerdyakov> The sweet spot is just about where Haskell is. :-)
18:36:47 <Spark> can a language be sufficiently expressive (assume some threshold here, and that the turing machine is too expressive) without being infinite
18:36:50 <SamB> anyway, it is hard to optimize an infinite table
18:37:31 <Spark> samb: an infinite table would not be a computer, since all it does is give the correct result :)
18:37:35 <SamB> Spark: turing machine is not very expressive if you mean it in terms of being able to say what you mean.
18:37:45 <Spark> thats not what i mean by expressive
18:37:55 <SamB> what do you mean?
18:37:57 <Spark> im talking about the set of programs that can be represented with the language
18:38:05 <SamB> oh. tat.
18:38:11 <SamB> s/tat/that/
18:38:26 <SamB> well, you see, infinite languages are a lot easier to deal with.
18:38:34 <Spark> obviously some programs are meaningless, like fun f x = f x;
18:39:36 <Cale> "To be is to be the value of a bound variable"
18:40:03 <SamB> Spark: no, that means "I feel like doing an infinite no-op" or "I feel like crashing"
18:40:29 <SamB> or is fun a function name?
18:40:39 <Cale> wouldn't that be more like a 2 parameter id?
18:40:43 <Pseudonym> I can create a language which is maximally expressive for any problem you care to name.
18:40:49 <Spark> its not in haskell i'm afraid :p
18:40:54 <SamB> Cale: that would be $
18:40:58 <Spark> i'll learn haskell when i finish my exams
18:40:58 <Pseudonym> You name a problem, I'll make a language which can express the problem in one bit.
18:41:07 <Pseudonym> All other programs will be expanded by a bit, though.
18:41:15 <Pseudonym> Hope that's not a problem.
18:41:17 <Spark> note that this conversation is born from my procastination desires
18:41:30 <Smerdyakov> Pseudonym, no problem at all. Make it the halting problem for x86 machine code, please.
18:41:40 <Pseudonym> Sure.
18:41:44 <SamB> Pseudonym: but what if the 1-bit program is not the best solution to the problem?
18:41:56 <Pseudonym> OK, the halting problem for x86 machine code will be represented by the bit zero.
18:42:08 <Spark> ok what about "1"
18:42:14 <Pseudonym> All other problems will be represented by Haskell code with a 1 bit prepended.
18:42:15 <SamB> Smerdyakov: he said nothing about the interpreter
18:42:23 <Pseudonym> Your exercise is to write the implementation.
18:42:33 <Pseudonym> But I have expressed the problem adequately, I think.
18:43:04 <Spark> < Pseudonym> You name a problem, I'll express it for you, adequately.
18:43:08 <Smerdyakov> Great. I have convinced my boss to switch to your language.
18:43:17 <SamB> <Pseudonym> You name a problem, I'll make a language which can express the problem in one bit.
18:43:20 <Smerdyakov> We are now putting out job advertisements that require 5 years of experience with it.
18:43:52 <Pseudonym> SamB: I said I'd create the language, not implement it.
18:44:03 <Spark> the language is meaningless without semantics
18:44:13 <SamB> Smerdyakov: much of it is many years old!
18:44:18 <Spark> loosely translates to "without meaning, there is no meaning"
18:44:25 <Spark> which you'd hope would be quite obvious :)
18:44:35 <Cale> Heh, talk about domain-specific languages :)
18:44:48 <Pseudonym> Spark: Understood.  I need a slightly better spec from Smerdyakov about the problem he needs to express.
18:44:53 <Pseudonym> But that's details.
18:45:00 <Spark> heh ok
18:45:08 <SamB> Cale: no. it is only a single bit less general than haskell!
18:45:36 <SamB> Smerdyakov: define halt!
18:46:16 <Smerdyakov> SamB, halt = halt
18:47:04 <Pseudonym> halt = error "this program halts"
18:47:21 <Spark> what if he wanted a compositional definition
18:47:38 <SamB> hmm. a good way to implement this language might be to somehow cause the kernel to kill the program (by eating ram, etc.), and then answer yes.
18:47:59 <Spark> s/good //
18:48:12 <Pseudonym> Oh, good idea.
18:48:14 <Spark> ive solved the halting problem: no problems halt
18:48:35 <Pseudonym> No problems halt when implemented on MY virtual machine, anyway.
18:48:37 <Spark> s/problems/programs/
18:48:43 <Spark> yeah :)
18:49:02 <Spark> multiple people provide linguistic redundancy
18:49:48 <Pseudonym> We also made the same mistake, I notice.  I suspect one of us cheated.
18:50:25 <Spark> ah
18:50:46 <Spark> humans are quite prone to doing that
18:51:01 <Spark> as well as interpreting stuff at a high level
18:51:09 <Spark> they tend to keep track of the low level stuff
18:51:21 <Spark> a bit like how compilers keep track of code, to give helpful error messages
18:52:03 <Spark> so when you repeated the same high level meaning, you did so using the same low level syntax :)
18:52:13 <Spark> or "token", should i say
19:02:30 <stepcut> mmmm tokens
19:16:59 <Cale> Tokens are delicious, but you have to pace yourself. I ate so many tokens once that I threw up an abstract syntax tree.
19:24:43 <SamB> Cale: umm... I would say, the more tokens you eat, the less likely that is to happen.
19:29:38 <Cale> Well, you just can't digest them if you eat too fast, getting them all out of order and such.
19:32:52 <Cale> So you end up with a bunch of partially digested stuff.
19:44:49 <stepcut> hah, there is an 'unclose tab' extension for firefox
19:45:09 <Jerub> stepcut: I saw that one.
19:45:13 <Jerub> I think I installed it.
19:45:50 * stepcut goes to watch so anime
21:11:35 * desrt shakes head
21:24:59 <heatsink> The way Haskell is structured makes me expect it to be tail recursive, but recursive calls can cause stack overflows
21:25:25 <heatsink> Does haskell do some kind of tail call elimination?
21:26:26 <d3z> GHC certainly appears to, at least when the calls are strict.
21:26:54 <Pseudonym> Yes, it does.
21:27:01 <Pseudonym> However, beware lazy evaluation.
21:27:08 <d3z> Actually, I don't think it is going to matter if it is strict or not, since a tail call will have to continue reduction.
21:27:36 <Pseudonym> Your carefully tail-recursive calls can build up huge thunks which require much stack to evaluate.
21:27:42 <SamB> Pseudonym: whats the problem with lazy evaluation?
21:27:57 <Pseudonym> Same problem as foldr (+) that we mentioned earlier.
21:29:24 <Pseudonym> Sorry, foldl.
21:29:41 <Pseudonym> foldl is tail-recursive, but the accumulator isn't strict.
21:31:35 <heatsink> Hmm... foldl f 0 [a,b,c] = 0:f 0 a:f (f 0 a) b:f (f (f 0 a) b) c
21:32:06 <Pseudonym> Consider, for example, foldl (+) 0 [1,2,3]
21:32:12 <Pseudonym> foldl (+) 0 [1,2,3]
21:32:22 <Pseudonym> = foldo (+) (0+1) [2,3]
21:32:33 <Pseudonym> = foldl (+) ((0+1)+2) [3]
21:32:42 <Pseudonym> = foldl (+) (((0+1)+2)+3) []
21:32:54 <Pseudonym> And finally, the accumulator is evaluated, using O(n) stack space.
21:33:18 <SamB> thunks on the stack???
21:33:32 <heatsink> It gets evaluated?
21:33:35 <Pseudonym> Yes.
21:33:56 <d3z> So, when would you ever want a tail-recursive function to be lazy?
21:34:18 <Pseudonym> reverse xs = foldl (flip (:)) [] xs
21:34:35 <Pseudonym> Maybe.
21:34:42 <Pseudonym> Assuming flip was inlined, anyway.
21:35:37 <Pseudonym> foldl (\x y -> case y of { Nothing -> x; Just a -> a }) Nothing [Nothing,Nothing,Just 3]
21:35:45 <Pseudonym> That's faster if foldl is lazy.
21:36:30 <Pseudonym> Oh, better example!
21:36:58 <Pseudonym> foldl (\x y -> error y) "" ["hello","world"]
21:37:15 <Pseudonym> That actually raises different errors depending on whether the accumulator is lazy or strict.
21:37:44 <SamB> oh.
21:38:04 <Pseudonym> Prelude Data.List> foldl (\x y -> error y) "" ["hello","world"]
21:38:04 <Pseudonym> "*** Exception: world
21:38:06 <Pseudonym> Prelude Data.List> foldl' (\x y -> error y) "" ["hello","world"]
21:38:06 <Pseudonym> "*** Exception: hello
21:38:35 <SamB> Pseudonym: is that actually lazy?
21:38:47 <heatsink> so foldl' is strict? 
21:38:51 <Pseudonym> Boss key.
21:40:58 <heatsink> I don't know what to conclude from what was said earlier. GHC converts the strict part of a recursion to iteration?
21:44:38 * heatsink looks at the definition of foldl'
21:45:35 <heatsink> So the `seq` operator forces its first operand to evaluate before the second operand. Is it a builtin?
21:46:22 <heatsink> That's probably what the 'primitive' operator means...
21:57:54 <Pseudonym> Sorry, back.
22:02:13 <heatsink> when you define a function with pattern matching in its arguments, does the compiler convert the pattern matching to a case statement?
22:02:59 <Pseudonym> Yes.
22:03:09 <Pseudonym> There's a piece of code in the compiler called the "pattern matching compiler".
22:03:18 <Pseudonym> Which desperately needs a rewrite if you're game. :-)
22:03:31 <heatsink> ummmm...
22:03:49 <Pseudonym> It basically compiles pattern matching into case statements.
22:03:51 * heatsink points to someone else
22:04:04 <Pseudonym> It also rewrites, for example: case x of { (1,y) -> foo ; z -> bar }
22:04:42 <Pseudonym> into: case x of { (x',y) -> case x' of { 1 -> foo ; z -> bar } }
22:04:53 <Pseudonym> So it breaks case statements into primitive case statements.
22:05:48 <heatsink> I thought that the original case statement was illegal because it is badly typed...
22:06:04 <heatsink> can I do case x of { (Char c) -> foo c; (Int i) -> bar i } ?
22:08:59 <heatsink> Hmm, that would actually be
22:09:13 <heatsink>   case x of { (c :: Char) -> foo c; (i :: Int) -> bar i }
22:09:26 <Pseudonym> No.
22:09:33 <heatsink> Okay
22:09:52 <heatsink> I thought that (1,y) is a different type than z so it would be the same kind of error
22:10:36 <heatsink> Oh wait, I'm thinking of z as a 1-tuple rather than any-value
22:11:05 <heatsink> so z would have to be a 2-tuple with its first value /= 1
22:11:15 <heatsink> now it makes sense
22:13:02 <heatsink> Okay, so case forces evaluation of its argument, so, like a subroutine call, it puts a value on the return stack...
22:14:49 <heatsink> And all recursive functions on lists have a case statement that tests whether the argument is Nil or Cons
22:15:06 <heatsink> so all recursive functions on lists take O(n) space on the return stack
22:15:08 <heatsink> is that right?
22:15:27 <Pseudonym> Uhm... no.
22:15:53 <heatsink> good!
22:16:18 <Pseudonym> If it's tail recursive, even if there's a case statement, _that_ part takes O(1) stack.
22:21:28 <heatsink> foldl f z l = case l of { Nil -> z; Cons x xs -> foldl f (f z x) xs } = foldl f (f z x) xs
22:22:24 <heatsink> Because it's lazy, it returns from the case statement before evaluating the recursive application?
22:23:27 <heatsink> Oh, it's l that's evaluated in the new return context, not foldl f (f z x) xs
22:28:54 <heatsink> Okay, I've satisfied myself that the argument stack and return stack do not grow with recursion depth... now I have to think about the update stack
22:30:40 <Pseudonym> Yes, the problem here is that (f z x) is unevaluated, not foldl.
22:40:04 <heatsink> (f z x) = push f; eval; push z; apply; eval; push x; apply; eval
22:41:51 <heatsink> maybe
22:44:55 <heatsink> It pushes arguments onto the stack and recurses, eventually overflowing the argument stack
22:46:28 <heatsink> The strict version, on the other hand, makes lots of closures
22:48:16 <heatsink> Hey, Prelude and Data.List define foldl' in different ways
22:48:21 <heatsink> how come?
22:50:57 <SamB> heatsink: do they do the same thing?
22:54:25 <heatsink> SamB: when optimization is turned on, they do
22:54:36 <eivuokko> Also, Perlude foldl' is not part of the interface?
22:55:45 <heatsink> oh, that foldl' is not exported from the module
23:02:01 <SamB> heatsink: well, that would probably be why it is implemented both places ;-)
23:03:06 <eivuokko> And diffrent definitions make more sense.
23:03:40 <heatsink> different definitions make more sense?
23:04:09 <heatsink> yay for polymorphism!
23:04:46 <eivuokko> It's strict in List, but in prelude it's helper for generic foldl or other functions?
23:06:07 <heatsink> it's a helper function for some list reducing functions
23:07:20 <SamB> heatsink: "other functions"
23:08:12 <Pseudonym> foldl' should be standard, I think.
23:08:16 <Pseudonym> But I don't think seq is standard.
23:08:43 <heatsink> I thought that everything in prelude was standard.
23:08:59 <Pseudonym> <aybe.
23:09:29 <SamB> heatsink: things exported from Prelude need to be standard.
23:09:49 <Pseudonym> http://www.haskell.org/onlinereport/basic.html#sect6.2
23:09:53 <Pseudonym> Yes, seq is standard.
23:10:09 <eivuokko> Pseudonym, seq is in the report...  and iirc strictness in records is defined with it.
23:10:16 <eivuokko> Oh.  Late, sorry.
23:10:17 <heatsink> ok
23:10:45 <Pseudonym> I think that ($) and ($!) definitely have the wrong associativity.
23:10:48 <Pseudonym> That's just a bug.
23:11:20 <eivuokko> Care to elaborate?
23:11:26 <Pseudonym> f $ g $ h x should mean f g (h x)
23:11:48 <Pseudonym> so you can do: f $ g x $ h y meaning f (g x) (h y)
23:12:10 <Pseudonym> If you need f (g x (h y)), then you can do: f . g x $ h y
23:13:15 <eivuokko> I guess.
23:13:37 * eivuokko 's not too good with precedence
23:13:53 <Pseudonym> I think the more likely scenario is that you have a function where you want one argument to be strict.
23:14:04 <Pseudonym> Say: f x y z, and you want y to be strict.
23:14:24 <Pseudonym> If $ and $! were left-associative, you could say:
23:14:28 <Pseudonym> f x $! y $ z
23:15:59 <eivuokko> Eh.  But shouldn't y and z be a functions in that case?
23:16:38 <heatsink> Well, I still don't understand haskell, but at least I can write haskell programs
23:16:44 <heatsink> goodnight :)
23:16:48 <Pseudonym> Night.
23:16:54 <Pseudonym> eivuokko: I don't understand the question.
23:17:12 <Pseudonym> If $! and $ are left-associative, then it's parsed as:
23:17:21 <Pseudonym> ((f x) $! y) z)
23:17:37 <Pseudonym> As it is they're right-associative, and expressing this intent is hard.
23:17:57 <Pseudonym> You need to put in more parentheses or insert a seq.
23:18:37 <SamB> Pseudonym: if you really want that, you can make your own name. 
23:18:43 <eivuokko> Yes, I understood your intent. :) And I was just confused, nvm.
23:24:10 <Pseudonym> SamB: My problem with it is two-part.
23:24:46 <Pseudonym> First off, I think the current behaviour is counter-intuitive.
23:24:54 <Pseudonym> Second, I think the alternative behaviour is more useful in practice.
23:25:14 <Pseudonym> Especially because if you need it to be right-associative, there's already a way to do that using (.).
23:25:54 <Pseudonym> If you want f (g (h x)), you can write f . g . h $ x
23:26:02 <Pseudonym> Writing it as f $ g $ h $ x is no simpler.
23:26:44 <SamB> Pseudonym: but f $ g $ h $ s requires less thinking
23:26:47 <Pseudonym> Whereas if you want y `seq` f x y z, there's no simple way to do that using $ and $!.
23:26:54 <Pseudonym> SamB: I actually disagree with that.
23:27:04 <Pseudonym> The pipelined function idiom is very well-established.
23:27:28 <Pseudonym> And it was established before ($) came along.
23:27:31 <SamB> I mean, you'd have to remember the precence and all
23:27:45 <Pseudonym> Well the precedence is low.
23:27:49 <Pseudonym> SO that's not an issue.
23:28:08 <SamB> hmm. I guess I ought to remember that...
23:28:29 <Pseudonym> The point of $ is that its precedence is as low as possible.
23:28:53 <SamB> no, the precence of .!
23:28:58 <Pseudonym> Oh, right.
23:29:20 <Pseudonym> Let me find out what that is.
23:29:47 <Pseudonym> OK, the precedence of . is as high as possible.
23:29:52 <SamB> yep
23:29:55 <Pseudonym> And the precedence of $ is as low as possible.
23:30:14 <SamB> hmm.
23:30:15 <Pseudonym> So they're not entirely equivalent if there are operators.
23:30:41 <SamB> where?
23:31:12 <SamB> nice to know . binds tight, though.
23:31:30 <Pseudonym> You need an operator with three arguments.
23:31:36 <Pseudonym> Trying to think of an example.
23:32:09 <SamB> there aren't many of those, since they aren't very intuitive ;-)
23:32:13 <Pseudonym> Right.
23:32:23 <Pseudonym> They turn up when you're using context-passing style.
23:32:39 <Pseudonym> (a `operator` b) ctx = ...
23:32:54 <Pseudonym> So I'd say it's not a case which turns up in practice.
23:33:38 <SamB> well, I doub't you'd want to mix $s or .s with that anyway...
23:33:48 <Pseudonym> Oh, you never know.
23:34:02 <Pseudonym> If you want to chain contexts together.
23:34:04 <SamB> what is a typical type of that kind of thing?
23:34:14 <Pseudonym> Let's see...
23:34:17 <SamB> in abstract terms
23:34:22 <Pseudonym> OK, here's an example.
23:34:34 <Pseudonym> Evaluating expressions with an environment.
23:35:02 <Pseudonym> data Exp = Var Var | Exp `Plus` Exp
23:35:11 <Pseudonym> Hmmm... not a good example.
23:35:12 <Pseudonym> Hang on.
23:35:25 <Pseudonym> OK.
23:35:58 <Pseudonym> type Env = FiniteMap String Int
23:36:10 <Pseudonym> var :: String -> Env -> Int
23:36:32 <Pseudonym> (var v) e = just (lookupFM v e)
23:36:48 <Pseudonym> Uhm.
23:36:58 <Pseudonym> No, sorry.  Lost it.
23:37:15 <Pseudonym> The idea, though, is if you need a binary operator which needs to be passed a context.
23:37:34 <Pseudonym> (a `op` b) ctx = (a ctx) `realop` (b ctx)
23:37:42 <Pseudonym> Ah!
23:37:43 <Pseudonym> Right.
23:37:47 <Pseudonym> Got it.  You with me?
23:37:59 <SamB> hmm. 
23:38:07 <Pseudonym> I'll go through the example.
23:38:20 <Pseudonym> type Env = FiniteMap String Int
23:38:24 <SamB> like if that was a map of bindings?
23:38:29 <Pseudonym> That's an environment which maps names to values.
23:38:47 <Pseudonym> type Expression = Env -> Int
23:38:49 <SamB> so ctx :: Env?
23:38:52 <Pseudonym> Yes.
23:39:04 <Pseudonym> OK?  An expression takes some name bindings and coughs up some values.
23:39:12 <Pseudonym> A value.
23:39:14 <Pseudonym> Not some values.
23:39:15 <Pseudonym> OK.
23:39:27 <Pseudonym> var :: String -> Expression
23:39:41 <Pseudonym> (var v) env = just (lookupFM env v)
23:40:12 <Pseudonym> Now, here's the cool part.
23:40:28 <Pseudonym> (e1 `plus` e2) env = (e1 env) + (e2 env)
23:40:48 <SamB> yes, cool ;-)
23:40:58 <Pseudonym> That's an operator which takes three arguments.
23:41:04 <SamB> I can see that.
23:41:06 <Pseudonym> Right.
23:41:19 <Pseudonym> So in that case, you might concievably mix `plus` and (.).
23:41:23 <Pseudonym> and ($)
23:41:25 <SamB> although, intuitively, it takes two and returns a function ;-)
23:41:29 <Pseudonym> Right.
23:41:45 <Pseudonym> I've written code like that before.
23:41:58 <Pseudonym> But nowadays I tend to use newtype.
23:42:09 <Pseudonym> Instead of: type Expression = Env -> Int
23:42:20 <Pseudonym> I write: newtype Expression = Expression (Env -> Int)
23:42:28 <Pseudonym> That way, I can make a Num instance for Expression.
23:42:43 <SamB> and also, it helps with QuickCheck, or so I hear.
23:42:45 <Pseudonym> So I don't need my own operators like `plus`.
23:44:19 <SamB> so does using newtype to wrap up a single value actually cost anything in terms of space?
23:44:25 <Pseudonym> No.
23:44:33 <Pseudonym> data does, newtype doesn't.
23:44:38 <SamB> ok.
23:44:43 <SamB> that is nice ;-)
23:44:48 <Pseudonym> What it does do is help out the type checker.
23:45:23 <Pseudonym> For example, you can declare class instances on a newtype but not on a type synonym.
23:45:47 <Pseudonym> It's a feature that I'd love to see in other languages.
23:45:52 <Pseudonym> Compiler-checked typedefs.
23:45:52 <eivuokko> Indeed.
23:46:32 <SamB> I think I heard about something like that somewhere, but I have no recollection where or for what language.
