00:00:53 <Lemmih> That isn't well typed.
00:33:19 <osqulda> I agree ;-)
00:39:03 <goltrpoat> just had a thought..  correctness proofs in pure declarative languages are trivial, comparatively speaking, right?
00:39:33 <shapr> Comparatively, yes.
00:39:47 <shapr> They're pretty much impossible to do for Java.
00:39:48 <goltrpoat> so if there is a correctness metric, and a set of transformation rules, doesn't that turn optimization into an IP problem, with the usual approximations?
00:39:59 <shapr> IP?
00:40:03 <goltrpoat> integer programming
00:40:11 <goltrpoat> sorry, 1,0-IP i guess
00:40:34 <goltrpoat> it's NP, but there's a bunch of 1.5-approximation strategies, iirc
00:41:06 <shapr> The only correctness proofs I've seen for Haskell code required transformations to a proof assistant form. Those transformations were done by hand or by a tool.
00:41:19 <shapr> Of course, that doesn't mean you have a proof for the runtime..
00:42:26 <shapr> goltrpoat: Wait, optimization or correctness?
00:42:31 * shapr is still sleeping.
00:42:46 <goltrpoat> i guess what i'm saying is..  suppose A is provably correct, and A = f o g o h, and we have rules f->x, g o h->z, h^-1 f->y, etc.  we can pick a program B such that A=B in terms of correctness, and B ~= f o g o h
00:43:11 <goltrpoat> optimization through equivalency, i guess.
00:43:33 <goltrpoat> sort of a generalization of peephole optimization
00:44:52 <shapr> Oh I think I understand.
00:45:39 <goltrpoat> instead of saying "these are the patterns that i look for, and replace with these patterns, greedily", you say "these are the function compositions that i look for, and i replace these by using more heavyweight algorithms than the naive greedy approach"
00:46:09 <shapr> related? http://lambda-the-ultimate.org/node/1138
00:47:05 <goltrpoat> cool, thanks.. yah, possibly related
00:48:31 <shapr> I'm still not sure I understand though...
00:52:07 <goltrpoat> hmm trying to come up with an example
00:52:24 <goltrpoat> you know how peephole optimization works in compilers, in general, right
00:53:49 <goltrpoat> you basically end up code like push eax / pop eax / add eax, ebx / push eax and you look for that pattern (suitably described), and you replace it with add eax, ebx / push eax
00:53:57 <goltrpoat> end up = end up with
00:54:30 <goltrpoat> this is generally done greedily, with some heuristic hacks (or that's my experience anyway)
00:56:04 <Smokey`> goltrpoat: Not exactly contributing to the conversation, but damn...  never realised you knew soo much about languages/compilers :)
00:56:22 <goltrpoat> im proposing a) movinng peephole optimization from the backend to a separate tool that operates on source, b) rewriting the problem in algorithmics/complexity parlance and using existing tools
00:56:53 <goltrpoat> smokey:  oh hey dude, didn't realize you hung out here.
01:00:10 <goltrpoat> hell, maybe even lifting SSA and whatnot to source level might make sense.
01:02:03 <Korollary> so you're saying that peephole optimization is done at the target code level?
01:02:30 <goltrpoat> ?
01:02:37 <Korollary> push, pop, etc.
01:02:43 <goltrpoat> oh
01:02:48 <goltrpoat> IL level, usually?
01:03:36 <goltrpoat> i think this bit is still a black art, you hand off your SSA optimized IL over to the backend and then it performs some set of peephole optimizations before and after
01:04:04 <goltrpoat> or that's my impression anyway
01:04:17 <Smokey`> goltrpoat: aye, started comming here last week. (Doing Haskell at uni.)
01:04:25 <goltrpoat> ah cool
01:04:29 <goltrpoat> how you liking it so far
01:05:12 <Smokey`> it's great, just need to familiarise myself with the syntax, and differences from C++.  loving it though :)
01:05:38 <Korollary> well, such processing may still be useful because at that level you could prefer a set of instruction over another (equivalent) set because you see lower cpu cycles, etc.
01:06:03 <goltrpoat> oh yah, absolutely.  i'm not saying get rid of low level optimization, i'm talking about a set of tools to optimize high-level code
01:06:24 <goltrpoat> sort of like ultra, but perhaps more geared towards optimization, not refactoring
01:06:31 <goltrpoat> and perhaps more automatic
01:06:57 <goltrpoat> i guess the 'automatic' part is where the NP bit comes in.
01:07:20 <goltrpoat> i just wanted to verify that i wasn't on crack and that this could theoretically be done.
01:07:41 <Korollary> I thought the np-bit always came in regardless of the language.
01:07:49 <goltrpoat> yeah absolutely
01:08:05 <goltrpoat> you mean with peephole optimization, right
01:08:11 <goltrpoat> it's equivalent to BP, i think
01:08:15 <Korollary> no, optimization in general
01:08:17 <goltrpoat> which -- has 1.5 optimizations
01:08:19 <goltrpoat> er
01:08:23 <goltrpoat> pardon.  1.5 approximations
01:08:44 <ibid> the full employment of compiler writers theorem? :)
01:09:14 <Korollary> you may want to look at what jhc does, or poke JohnMeacham's head when he's around.
01:09:15 <goltrpoat> which one is that?
01:09:15 <goltrpoat> hehe
01:09:54 <goltrpoat> korollary:  oh thanks, ive been trying to remember the name
01:09:59 <goltrpoat> (jhc that is)
01:10:05 <Korollary> @where jhc
01:10:06 <lambdabot> http://repetae.net/john/computer/jhc/
01:10:10 <goltrpoat> i drunkenly came across it recently
01:10:17 <ibid> goltrpoat: istr, given an optimizing compiler, it is always possible to write a better optimizing compiler
01:10:18 <goltrpoat> yah im there right now, bookmarked it
01:10:23 <goltrpoat> ibid:  haha
01:10:43 <goltrpoat> ibid:  doesn't that mean that every program can execute in one cycle, by induction
01:11:09 <ibid> goltrpoat: no, because the compiler doesn't need to improve on every program :)
01:11:13 <goltrpoat> ah true :)
01:11:15 <ibid> the *new* compiler
01:11:28 <Korollary> you mean on the same program?
01:11:40 * shapr poinks
01:11:42 <Korollary> i.e. the better compiler scores better on the aggregate of all programs
01:12:37 * Korollary discharges and recharges shapr for maximum lambda
01:13:13 <shapr> w0000
01:13:57 <shapr> jhc is way nifty.
01:14:35 <shapr> John Meacham told me that he got it to cross compile to the Nokia 770
01:14:49 <Korollary> I thought it was barfing unless you had gigs of ram?
01:14:57 <Korollary> err
01:15:01 <Korollary> I read that wrong
01:15:21 <shapr> jhc does require a lot of ram when building.
01:15:37 <shapr> I started building the compiled form of the libs last night.
01:16:12 <shapr> This morning I woke up to see 1.5gb of swap space used and the drive arms sounded like a silenced machine pistol.
01:16:22 <Korollary> how much ram do you have?
01:16:43 <shapr> 512mb
01:16:59 <goltrpoat> ibid:  so wait, if i assign a program to every natural number that prints out that number, and i assign a compiler to every real.. ?
01:17:00 <Korollary> I have a 2G machine. Maybe I will give it a try.
01:17:00 <goltrpoat> :)
01:17:03 <shapr> registered ecc ram is expensive :-(
01:17:29 <shapr> I want to buy a dual-core amd64 and start out with 4gb of ram.
01:17:55 <Korollary> I was going to a buy a dual-core, but they're ridiculously expensive and useless at the moment.
01:18:08 <Korollary> $700 or something
01:18:19 <Korollary> so I got an amd64-4000+
01:19:06 <shapr> When did you buy it?
01:19:16 <Korollary> 2-3 months ago
01:19:42 <shapr> Yeah, dual-core prices haven't gone down much since then.
01:20:05 <Muad_Dibber> at home i got an amd64-3200+ and its about 1.5 years old..they don't seem to improve much in numbers but with multiple-cores anyway
01:20:22 <Korollary> I can't believe the industry nowadays. Today you can plunk down $13k for *the true* high end machine. I've been building machines for years, I don't remember a time where you could spend $13k.
01:23:48 <shapr> Hm, yeah... $5k was the ceiling in the 486 era.
01:24:07 <goltrpoat> korollary:  building machines stopped being cost-effective a good five years ago though
01:24:26 <shapr> How so?
01:24:33 <shapr> It's still cheaper to build a machine than to buy it pre-built.
01:24:35 <Korollary> goltrpoat: why? I don't think I've done badly. I checked out alienware and didn't like their stuff.
01:24:59 <goltrpoat> well, you're looking at what, $500 markup or so on the parts, unless you go to fry's or something, in which case your machine will last about three months because they put returned parts back on the shelves.
01:25:12 <shapr> Total time to build a machine is a few hours of web research and one hour putting the pieces together when they show up on your doorstep.
01:25:14 <goltrpoat> say it takes you two days to put it together, is $500 really worth it
01:25:24 <shapr> two days??
01:25:32 <goltrpoat> including ordering?
01:25:32 <Korollary> goltrpoat: I bought it all online. Fry's sucks imho.
01:25:53 <goltrpoat> dunno, i mean, i don't usually have more than an hour or two of free time a day
01:25:58 <shapr> Shipping time is usually three or four days here.
01:26:20 <goltrpoat> no i mean the process of going online and clicking stuff and whatnot.  that takes time, right
01:26:21 <shapr> I'm self-employed, my free time is weird. I have either too much work or not enough money.
01:26:35 <goltrpoat> yeah im self-employed as well
01:26:36 <Korollary> well, you dont have to make up your mind on what you will buy in one day. My research lasted about a week.
01:26:59 <goltrpoat> but im usually swamped, and $500 of company money is peanuts compared to a day or two of my time
01:27:08 <Korollary> oh, company money
01:27:11 <shapr> There are about three decent online shops in Sweden, ordering from them is very easy.
01:27:13 <Korollary> that changes things :)
01:28:12 <shapr> goltrpoat: buy a nokia 770 and shop online while you travel :-)
01:28:18 <goltrpoat> i don't travel :)
01:28:19 <goltrpoat> hehe
01:28:28 <goltrpoat> i'm also resolved to never own a cell phone
01:28:35 <shapr> I own one.
01:28:40 <Korollary> what's wrong with you
01:28:41 <shapr> Only my gf calls me though.
01:28:46 <Saulzar> Cellphones are evil :)
01:29:06 <Korollary> buy one. don't give out the number if you want. Use it like a portable pay phone.
01:29:23 <goltrpoat> no but you can't not give out the number
01:29:32 <Korollary> peer pressure?
01:29:41 <Korollary> turn it off then
01:29:44 <Saulzar> Don't want the distraction, it's annoying and other people find it annoying
01:29:53 <goltrpoat> saulzar:  yep.
01:30:03 <Korollary> you can keep it off until you need to use it.
01:30:10 <goltrpoat> but i don't want to use it.
01:30:11 <goltrpoat> hehe
01:30:16 <shapr> I'm much more comfortable with email and text chat, maybe because I've been using IRC since 1989.
01:30:16 <shapr> oj, that's seventeen years!
01:30:21 <Korollary> I said "need" for a reason eh
01:30:25 <goltrpoat> i don't want to be on a fucking leash to the humanity.
01:30:49 <goltrpoat> shapr:  damn.  what the hell kind of version of ircd is that.
01:30:51 <goltrpoat> 93 here.
01:30:59 <Korollary> 94
01:31:17 <sieni> 93 as well (although it was pretty close to the end of the year :-)
01:31:45 <Korollary> #amiga on efnet
01:31:57 <goltrpoat> efnet or ircnet
01:32:13 <goltrpoat> you guys remember the big old riff right
01:32:15 <Korollary> I think it was efnet
01:32:28 <shapr> Well, my first irc session was in 1988 but I wasn't a real user till 89.
01:32:55 <shapr> goltrpoat: I was in the next room away from the original irc server in Oulu :-)
01:33:07 <goltrpoat> korollary:  i may have joined #amiga once or twice, but i was mostly on #coders, and sometimes #pixel and #trax
01:33:14 <goltrpoat> shapr:  oh nuts.
01:33:17 <shapr> Sadly the prof who has turned that first sun sparc into a flowerpot was out for lunch... so I didn't get to see it.
01:33:24 <goltrpoat> hahah
01:33:49 <sieni> shapr: cool as fuck
01:33:52 <goltrpoat> korollary:  but if you were on #amiga, you were probably in the demo scene, right.. or at least tangentially involved
01:34:05 <shapr> sieni: Would have been cooler if I had gotten to touch it =)
01:34:12 <Korollary> goltrpoat: I used to be before 94. by then I had stopped writing assembly.
01:34:29 <goltrpoat> ah ok.  i don't think i started until 94 or so.
01:34:47 <goltrpoat> with demo stuff anyway.  had been programming for a good 7-8 years before that.
01:35:32 <goltrpoat> wait no, i can't count.  5 or 6 years, not 7-8.
01:37:02 <goltrpoat> looks like shapr takes the "i'm oldschool" prize, plus the honorary irc prize for being at oulu in 89.
01:37:32 <Korollary> shapr takes a lot of prizes.
01:38:17 <Korollary> unicycle prize foremost
01:53:32 <beelsebob_> @where happy
01:53:32 <lambdabot> http://www.haskell.org/happy/
02:03:01 <shapr> goltrpoat: Ah, I just checked the logs. I wasn't in Oulu in 1989, I was there in 2002 or so.
02:03:17 <dcnstrct> takuan, did you get your name from the priest in the Eiji Yoshikawa book ?
02:03:20 <shapr> I did get onto IRC in 1989, but I was in Tennessee at that point.
02:05:21 <dcnstrct> boy ghc must be one of the slowest compilers on earth
02:05:37 <dcnstrct> my computer is taking longer to compile darcs than it took to compile mozilla
02:09:01 <shapr> dcnstrct: GHC does a lot of cool stuff. You're investing effort now to get a faster binary, yeah?
02:09:39 <dcnstrct> ok good point.  
02:09:57 <dcnstrct> I had distcc to help me with mozilla also
02:10:02 <dcnstrct> so its not really a fair comparison
02:26:02 <goltrpoat> shapr:  good move.  (tennessee to oulu)
02:31:29 <osquar> how are Data.Array implemented? Is it really O(1) access as Hudak's book says?
02:32:00 <Lemmih> osquar: Yep.
02:32:12 <osquar> how can it be?
02:32:18 <osquar> built-in?
02:32:25 <osquar> hash?
02:32:32 <osquar> but even hash makes this hard
02:32:57 <osquar> do I write import Array or import Data.Array btw?
02:33:09 <Lemmih> Data.Array
02:34:21 <Lemmih> osquar: O(1) access time doesn't conflict with purely functional programming as long as the array is immutable.
02:35:00 <osquar> and fixed size?
02:35:41 <Lemmih> Yes.
02:35:57 <Lemmih> And when we want a mutable array, we can just use a monad to sequence our actions.
02:36:23 <osquar> but how are they represented, precisely? URL?
02:36:33 <sieni> how about ghc sources?
02:36:35 <Lemmih> That depends on the implementation.
02:36:40 <osquar> GHC?
02:38:05 <sieni> The Glorious Glasgow Haskell Compilation System
02:38:56 <Lemmih> osquar: Arrays in GHC are pretty much like arrays in C or most other langauge.
02:39:20 <osquar> ... so what's the algebraic datatypes for arrays? ;-)
02:39:25 <osquar> a tuple?
02:39:55 <Lemmih> data Ix i => Array i e = Array !i !i (GHC.Prim.Array# e)
02:40:24 <goltrpoat> i was reading these talk slides by tim sweeney earlier (epic)..  and there were about five slides titled "why haskell isn't my favorite language".  it was one of the most unintentionally funny things ive read in weeks.
02:41:15 <Lemmih> goltrpoat: Are the slides online?
02:42:51 <goltrpoat> he was complaining about lazy evaluation, im assuming he didn't know about memoization at the time, about scalability (which is weird, because i find haskell a lot more scalable than c++ is), and there was a third argument, that i can only assume revolved around him not liking the lack of explicit type coercion between Int and [Char] (i spent a good 5 minutes trying to figure out what he meant in that slide)
02:43:06 <goltrpoat> lemmih:  yah trying to find him.. saw it on a coworkers machine originally
02:43:41 <pejo> Lemmih, http://www.cs.princeton.edu/~dpw/popl/06/
02:45:12 <pejo> goltrpoat, regarding the scaling: wasn't it "type inference doesn't scale"?
02:45:29 <goltrpoat> yes, yes it was
02:45:48 <goltrpoat> http://www.st.cs.uni-sb.de/edu/seminare/2005/advanced-fp/docs/sweeny.pdf
02:46:07 <pejo> goltrpoat, I think you underestimate his knowledge though. 
02:47:27 <goltrpoat> pejo:  no, i respect him for certain things
02:48:17 <goltrpoat> i don't underestimate his knowledge, i think his knowledge doesn't lie anywhere near compilers though.  or renderers.  he's got good business sense, if that helps.
02:48:17 <goltrpoat> heh
02:48:32 <pejo> goltrpoat, one could question what he means with hte type inference doesn't scale comment, my interpretation is that he finds the error messages given by compilers pretty bad - and he's right. Not enough research is done on giving clear and concise type error messages.
02:48:43 <pejo> goltrpoat, he said he spent ~50% of his day writing code iirc. 
02:48:56 <goltrpoat> is that good or bad
02:49:25 <goltrpoat> pejo:  that whole talk can be summed up as "programming is hard".  most of his other talks can be summed up similarly.
02:51:07 <pejo> goltrpoat, I don't think that is a fair summary. He has identified a set of problems that show up in real code, and discussed possible solutions to them. 
02:52:28 <goltrpoat> direct quote:  "shared state concurrency".  * this is hard!  * how we cope:  ... * huge productivity burden  * scales poorly.  
02:52:30 <pejo> Or atleast possible solutions according to him. 
02:54:40 <goltrpoat> he then proceeds to show the usual (idiotic, in my opinion) quicksort in haskell vs quicksort in C example, and concludes with "haskell syntax is scary, lazy evaluation is costly, but i like Maybe and ST".
02:55:30 <pejo> goltrpoat, are you talking about page 49 of those pdf-slides for the shared state concurrency statement? 
02:56:33 <goltrpoat> it'd be the one where he says that shared state concurrency is hard, and that UE3 uses a single main thread with a bunch of worker threads -- which, in turn, he correctly identifies as unscalable and a productivity burden
02:57:53 <goltrpoat> i'm actually not entirely sure what the point of that slide is, other than demonstrating that they came up with a subpar solution to a common problem
02:58:42 <goltrpoat> "look, we tried it, but it was hard, therefore:  one main thread, and a pool of worker threads, which sucks, but oh well."
03:00:54 <goltrpoat> sorry, didn't mean to turn this into a sweeney discussion.
03:03:47 <pejo> Shrug, you're free to ignore him. I honestly believe he is quite a lot better than the average programmer and if Sweeney says he finds it hard we should listen. 
03:05:52 <goltrpoat> my point is that sweeney finds everything hard.  he finds it hard to go from software rasterizers to hardware, he finds it hard to release updates in such a manner as to not fuck his clients who are licensing his engine, he finds it hard to write multithreaded applications that everyone else in the industry has been doing for a good five years, etc.
03:07:04 <goltrpoat> he was great before 3d accelerators came along, then he quickly got mediocre, then he got a bit better, then, and this is where i give him credit, he caught up.
03:08:02 <goltrpoat> he's on the way out though, and i'm not sure why i should "listen" to sentiments along the lines of "shared state concurrency is hard".
03:08:10 <goltrpoat> and "haskell syntax is scary".
03:09:49 <Itkovian> goltrpoat: which sweeney are you talking about?
03:12:06 <goltrpoat> tim
03:12:36 <goltrpoat> pejo:  don't get me wrong, i have a lot of respect for the guy -- a lot more than most people i know who have actually worked with him or talked to him do
03:13:34 <goltrpoat> according to one guy who worked at epic for a while, he basically wrote unrealscript based on some vague idea of how recursive descent parsers work, he just hacked through it until it worked
03:13:58 <goltrpoat> i just think that he tends to take this professorial tone on subjects that he doesn't have the mildest clue about, and that irks me.
03:16:23 <shapr> goltrpoat: I think Tim Sweeney is a heavy duty programming language theory guy.
03:16:31 <goltrpoat> ??
03:16:32 <lambdabot> Not enough arguments to @.
03:16:33 <goltrpoat> he's not
03:16:44 <goltrpoat> he's a hacker with an interest in languages
03:17:04 <shapr> He's certainly seemed to be one in the discussions he's had on lambda-the-ultimate.
03:17:20 <shapr> From one viewpoint, I'm just an art student with an interest in languages :-)
03:17:25 <goltrpoat> hehe
03:17:58 <shapr> I don't think Sweeney is necessarily correct about everything he says, but I do think he's worth taking seriously.
03:18:10 <goltrpoat> hmm.
03:18:26 <shapr> Now if you want to cast aspersions on TopMind or the SubText project, I'm all with you...
03:18:40 <goltrpoat> i dunno, maybe i haven't seen him say enough things that weren't outright wrong.
03:18:56 <shapr> Well, write up a careful reasoned response.. maybe he'll hire you =)
03:19:06 <goltrpoat> i don't want to be hired :)
03:19:09 * shapr grins
03:20:23 <shapr> goltrpoat: You may be right, maybe he is clueless and deceptive. But he has seemed to have some clue on LtU.
03:20:28 <goltrpoat> im basically in a position where if my company doesn't work out, i'll probably just go work at taco bell or something.  i hear you can get to cashier in about three months.  i could code in the meantime.
03:20:45 <shapr> What does your company do?
03:20:47 <goltrpoat> don't ever wanna work for anyone else again.
03:21:04 <goltrpoat> shapr:  using games/graphics consulting to pay for our R&D
03:21:21 <shapr> I don't ever want to be an employee either.
03:21:49 <goltrpoat> yeah -- it's so much worse in this industry than in most other IT branches, too
03:22:00 <shapr> I really do think ghc-smp and parallel arrays could make the ps3 a kick-ass platform.
03:22:13 <shapr> I knew a guy at raven software for awhile...
03:22:33 <goltrpoat> oh i think i know someone at raven, not by name though
03:22:59 <goltrpoat> of course now i can't remember his irc nick either
03:23:23 <goltrpoat> yah i spent some time thinking about what you were talking about yesterday
03:23:45 <shapr> and?
03:23:52 <goltrpoat> i sort of have trouble wrapping my head around it, knowing the architecture -- i think there's a fundamental limitation to it that makes things weird
03:24:01 <shapr> What limitation?
03:24:06 <goltrpoat> as in, there's only so much you can do with bite-sized chunks of code, and bite-sized chunks of data
03:24:44 <shapr> and that leads you to what conclusion?
03:25:03 <goltrpoat> well, it suggests that it may be difficult to perform a large number of tasks on the SPEs.
03:25:32 <goltrpoat> not in terms of performance, but rather in terms of feasibility.
03:25:49 <shapr> how so?
03:26:32 <goltrpoat> a large number of algorithms in graphics or game development require knowledge of the entire dataset
03:26:40 <shapr> Like what?
03:27:05 <goltrpoat> like (first thing off the top of my head) accelerating a raycast through the scene using spatial partitioning.
03:27:45 <goltrpoat> now, in this particular case, there's a fix
03:27:49 <shapr> Why would that be hard to do on SPEs?
03:28:23 <goltrpoat> we can write the spatial partitioning as a generalization of a skiplist, say, and search through the topmost list, until a better one arrives, say
03:29:05 <shapr> Sounds like the limitation would be getting data to and from memory.
03:29:18 <goltrpoat> shapr:  because you put the entire.. uh, BSP, say, on the SPE, you're talking about a huge dataset.  so you'll be uploading it in chunks -- pack the top n nodes in a dma packet, let the SPE process it, figure out which subtree it wants, pack that up, etc.
03:30:04 <goltrpoat> now, when you send a dma packet, you're tying up the bus.  none of the other six units can get or send any data during this time.
03:30:21 <shapr> It sounds to me like it'd lend itself to incremental calculation.
03:31:08 <goltrpoat> yes, but my "use a skiplist generalization to octrees instead of a BSP" is a very non-trivial optimization.  the raycast i was talking about does not lend itself to incremental calculation at all.
03:31:09 * shapr looks up the details of BSPs.
03:32:02 <goltrpoat> oh, you just pick a plane, partition the space in two, everything in front is the front leaf, everything behind is the back leaf, iterate.  leaves are convex by construction.
03:32:53 <shapr> hmm
03:33:15 <shapr> Why not cache adjacency info?
03:33:29 <goltrpoat> adjacency between leaves?
03:33:40 <goltrpoat> that's called a roped tree, very useful, but a different problem entirely
03:34:14 <shapr> Not necessarily, you'd know you only need to fetch adjacent leaves for collision detection.
03:34:34 <shapr> Or am I misunderstanding the problem?
03:34:41 <goltrpoat> oh you mean while traversing the tree for my raycast?  well, first you say, find which leaf the ray origin is in
03:34:46 <goltrpoat> this is a full tree traversal, right.
03:34:52 <shapr> whatever for?
03:35:04 <goltrpoat> you mean what is the raycast for?
03:35:05 <shapr> Why not build a zipper with adjency info?
03:35:11 <shapr> No, I mean, why the full tree traversal?
03:35:22 <shapr> adjacency*
03:35:35 <goltrpoat> because the point is arbitrary, and i want log n retrieval?
03:36:14 <shapr> Yeah, but if you frob the tree such that the origin is the root, it's easy to traverse after that, yeah?
03:36:14 <goltrpoat> i mean, i can use temporal coherency to cache the last leaf i was in, do a linear search from there, etc
03:36:51 <goltrpoat> not following.  the origin is the root?
03:37:00 <shapr> @oldwiki TheZipper
03:37:01 <lambdabot> http://www.haskell.org/hawiki/TheZipper
03:37:06 <shapr> Do you know Huet's zipper?
03:37:12 <goltrpoat> nope
03:37:34 <goltrpoat> unless you mean zip?
03:37:49 <shapr> Nah, this is more like a data structure that works like a rubik's cube.
03:37:59 <shapr> You can twist it around to make any node the root, sort of.
03:38:06 <sieni> There's zipper: http://www.math.washington.edu/~marshall/zipper.html
03:39:05 <goltrpoat> ok, seeing conformal mappings in a data structure context is .. a weird thing.
03:39:07 <goltrpoat> sec, reading.
03:39:22 <shapr> sieni: Is that the same as Huet's zipper?
03:39:24 <Saulzar> All sounds rather complex, ray traversal of a BSP should be simple
03:39:34 <goltrpoat> oh.  not much to read there.
03:39:43 <sieni> shapr: oops
03:40:18 <goltrpoat> saulzar:  exactly, it's a trivial operation, which is made very complicated (by virtue of us even having this discussion) by the fact that the PS3 is fragmented into tiny crippled chunks with courier pigeons for a communications channel
03:41:14 <shapr> I still don't see why it's necessary to load so much info to traverse the tree...
03:41:31 <Saulzar> Each processor is limited in it's memory access? (Has to be serial, like a shader or something?)
03:41:44 <goltrpoat> no, you don't have to load the WHOLE tree, but if you're doing a traversal, you'll have to serialize chunks of it in and out
03:41:45 <shapr> Each SPE has 512k of cache/ram.
03:41:49 <goltrpoat> and that's where the bus becomes an issue
03:42:09 <goltrpoat> 512k?  i'm currently porting a game from 360 to PC that has 6.5 GIGS of data.
03:42:25 <goltrpoat> i mean.. you see the discrepancy, right
03:42:54 * shapr thinks about efficient ways to check collision in BSPs
03:43:23 <goltrpoat> i think that problem is like.. a good 30 years old, i doubt there's anything new there
03:43:57 <goltrpoat> i mean.. ok i should rephrase
03:43:57 <Saulzar> Hmm
03:44:38 <goltrpoat> there's a far bit of research concentrated on constructing optimal kd trees (a variant of bsp trees, splitters are axis aligned) for raytracing, in particular
03:45:07 <Saulzar> Maybe still requires special purpose constructs which the programmer can make use of
03:45:09 <goltrpoat> but tracing rays through the scene is generally not the primary task of a realtime graphics engine, so that bit is mostly irrelevant
03:45:19 <shapr> Ok, what is the primary task?
03:45:39 <shapr> I know you have a worthwhile point, I just don't understand it yet :-)
03:45:45 <goltrpoat> the primary task is to a) figure out what needs to be displayed, b) figure out how to display it.
03:46:00 <goltrpoat> a can be classified under visibility, b can be classified under lighting (rough terms)
03:46:15 <goltrpoat> a deals with large datasets, in general
03:47:34 <goltrpoat> now, it just seems to me that when you commonly have tasks that require a large amount of code to operate on a large amount of data, architectures with a small amount of code cache and ram, and no branch prediction, are, perhaps, not the best way to address the issue.
03:47:58 <shapr> Still, it sounds like the issue you have the PS3 is that DMA'ing data from memory is slower than direct access?
03:48:15 <goltrpoat> well -- that's part of it
03:48:26 <goltrpoat> it's slow to go over the bus, *and* that ties up the rest of the SPEs
03:48:36 <goltrpoat> as in, everything is sitting on the same bus, right
03:48:47 <goltrpoat> if i'm using it, you can't use it, if you're using it, i can't use it
03:48:48 <shapr> So the real problem is the bandwidth between ram and cpu?
03:49:30 <goltrpoat> hmm..  primarily that, yes
03:49:38 <goltrpoat> i guess that absorbs a lot of the other concerns i have.
03:50:11 <shapr> Outside of that, what other concerns do you have?
03:50:25 <goltrpoat> i think the reason i was hesitant to say that that's the primary issue is that the way it absorbs the other concerns is like.. assuming that issue is solved, you end up having to come up with very non-trivial solutions to very trivial problems
03:50:36 <shapr> I don't see a near-term solution for ram <-> cpu bandwidth problems. The industry is focussing on size of ram and speed of cpu.
03:51:12 <goltrpoat> sure.  but, hey, what prevented them from dumping the SPEs, adding another altivec unit, and stamping out a second core per unit while they were at it
03:51:23 <goltrpoat> oh and adding shared L2 cache so the graphics card can access it
03:51:24 <goltrpoat> hehe
03:51:27 <shapr> Actually, there is a pretty simple near-term solution, but I doubt anyone will take it :-)
03:51:29 <goltrpoat> oh wait, then it'd be 360.
03:52:04 <goltrpoat> a beowulf cluster of PS3s?
03:52:05 <goltrpoat> hehe
03:52:24 <shapr> Nah, a quilt setup of hexagons and squares where the squares are cpus and the hexes are ram.
03:52:44 <goltrpoat> oh hey that makes sense.
03:53:24 <goltrpoat> (im about six guinness and four murphy's into the day, so that took a little while to visualize)
03:53:56 <goltrpoat> cept..
03:54:03 <goltrpoat> hexagons tile space, hexagons + squares don't
03:54:05 <goltrpoat> unless i'm on crack.
03:54:32 <goltrpoat> no i'm on crack, i can imagine a tiling.  nevermind.
03:54:44 <shapr> Seen the floor tiles with the little squares on the corners of the big hexes?
03:54:50 <goltrpoat> yah
03:55:39 <shapr> Actually, that design is a generalization of the Cell idea.
03:55:45 <goltrpoat> and you mean to represent like.. dataflow with adjacency, not physical proximity right
03:56:05 <shapr> I came up with after staring at a picture of the the Cell die and noticing how many of the cores hang off the edges.
03:56:10 <goltrpoat> hah
03:56:45 <shapr> I mean real physical proximity where each edge between tiles is a link to the next tile.
03:56:56 <goltrpoat> at this point, the embedded IBM spy calls back the headquarters and says "guys, we need to switch to a penrose tiling"
03:57:10 <shapr> Hm, interesting idea...
03:57:19 <goltrpoat> brb smoke
03:57:29 <shapr> A penrose tiling might give some routing calculation advantages...
03:58:16 <shapr> Anyway, it's just another crazy idea of mine. I have no idea how you'd package the thing for example.
03:58:31 * shapr thinks...
04:00:36 <goltrpoat> why not just slice a quasicrystal along a plane that doesn't produce a penrose tiling -- then the slogan could be "QuasiCPU:  each one is maddeningly just a little bit different."
04:01:54 <goltrpoat> this, in turn, would result in a proliferation of stochastic languages, along the lines of Java2k.
04:02:09 <goltrpoat> ...
04:02:11 <goltrpoat> profit!
04:03:21 <Lemmih> Yes! I finally figured out how to call a static function from bytecode.
04:03:56 <Lemmih> dons: This is a pretty big step, I think.
04:04:42 <goltrpoat> sorry, was trying to be amusing.  one of my partners has done a ton of FPGA design at celoxica, he would probably have responded with something semi-productive.
04:04:45 <shapr> goltrpoat: When you say 'requires non-trivial solutions to trivial problems' do you mean that SPEs require a different approach to use effectively?
04:05:01 <goltrpoat> shapr:  yes, very much so.  i ran into a lot of that while working with the PS2
04:06:11 <shapr> Well, why not ghc-smp and parallel arrays? :-)
04:06:31 <goltrpoat> i (apparently) impressed the living shit out of a ps2 magazine editor with a stencil shadow demo on the PS2 -- it's a trivial algorithm on the PC, which required a lot of thinking and refactoring and rewriting things on paper to make it happen on the PS2.  i guess the guy said he'd never seen anything like it on the PS2, and that's why -- it takes a stupid amount of effort to do anything with these architectures
04:06:34 <shapr> I rather like the idea of a totally different approach being required. The games industry innovates less than the music these days.
04:06:50 <goltrpoat> because we're talking about qualitatively different algorithms.
04:06:58 <shapr> Maybe the Cell will get 'em out of their rut.
04:07:08 <shapr> I'd be happy with a job writing Haskell for the Cell. :-)
04:07:11 <goltrpoat> but yeah, the games industry is very much in a rut right now.
04:07:18 <Lemmih> dons: I'll poke you tomorrow. 'night.
04:08:49 <goltrpoat> i haven't really played any games in about five years, i rather dislike game development, but i love being able to solve the problems that come up when you're working in a sort of an abstract R&D type setting
04:09:05 <shapr> Yeah, me too.
04:09:13 <goltrpoat> it tends to be more paper than code, which is like.. excellent.
04:11:26 <goltrpoat> ideally, i'd want to get paid to do that specifically, instead of getting paid to do random crap, and using that to do that -- but the only way i can think of that would result in that, would involve four years of undergrad and a lot of TAing
04:11:37 <goltrpoat> and that just doesn't sound like fun.
04:11:49 <shapr> It's hard to do research without a degree.
04:11:54 <shapr> But you could test out of most of a degree.
04:12:09 <goltrpoat> hmm
04:12:11 <goltrpoat> not from what i've seen
04:12:33 <goltrpoat> ideally, i would want to major in math
04:13:01 <goltrpoat> i talked to a UT advisor, and asked her if it'd help me any if i took the math GRE and got a reasonable score.
04:13:19 <goltrpoat> she sort of looked at me as if i was on crack, and said, nah, the SATs will help you though.
04:13:34 <xX[ReP]Xx> GRE and SAT are basically the same thing
04:13:44 <goltrpoat> uh.. subject GREs
04:14:02 <xX[ReP]Xx> yes, math GRE and math SAT/SAT2 are basically the same thing.
04:14:04 <goltrpoat> it's what you take before grad school.  fairly hard.
04:14:15 <xX[ReP]Xx> no, i've taken them, and they are a fucking joke
04:14:20 <goltrpoat> i've taken practice tests of both
04:14:24 <goltrpoat> SAT/SAT2 were a joke
04:14:35 <xX[ReP]Xx> GRE is a joke too, believe me
04:14:35 <goltrpoat> the math GRE (not the math portion of the general GRE)
04:14:43 <goltrpoat> is at about 3rd-4th year of undergrad, level
04:15:02 <goltrpoat> i think we're talking about different tests here
04:15:19 <xX[ReP]Xx> maybe
04:16:32 <shapr> Anyway, I know you can test out of a bunch of classes.
04:16:52 <goltrpoat> i think i'd have better luck going the CS route if i were to do that
04:16:54 <shapr> I asked at uab, and they said I could take a test, and if I pass, I get credit. I still have to pay for the class though.
04:17:00 <shapr> In sweden, you just take a test, since education is free.
04:17:12 <goltrpoat> at least i can say look, ive been doing this professionally longer than half your TAs have been alive.
04:17:33 <goltrpoat> or.. well, something more historically accurate
04:17:36 <goltrpoat> but you get the drift.
04:18:12 <goltrpoat> shapr:  odd.  i basically asked the lady up front if i could do that.
04:18:26 <goltrpoat> (the advisor at UT that is)
04:18:43 <goltrpoat> she was like.. no, but here's a bunch of ways you can earn credit
04:18:58 <shapr> In the late 90s in Alabama, I got a positive answer.
04:19:02 <shapr> Maybe it's per university?
04:19:09 <goltrpoat> i think it might be
04:21:53 <goltrpoat> xx[rep]xx:  i'm looking at a sample test now, http://www.ets.org/Media/Tests/GRE/pdf/Math.pdf , and i think you might be right
04:22:31 <xX[ReP]Xx> i told u!
04:22:32 <goltrpoat> it could be that i just got really impressed by it five or six years ago or something, and never got over that.  it seems really easy so far.
04:22:58 <goltrpoat> ok, 27 is tricky.
04:23:40 <goltrpoat> wait
04:23:43 <goltrpoat> fuck, no it's not.
04:24:18 <goltrpoat> wait.. no it is.
04:25:23 <goltrpoat> no it's not.  maybe i should go to bed at some point.
04:26:00 * shapr grins
04:27:30 <pejo> goltr, what kind of education are people expected to have for that test? 
04:27:57 <goltrpoat> i thought it was one of those "f(x^2) = f(x), f(0) = 5, i have no clue where i left my bong, prove that f is constant" type deals
04:28:07 <xX[ReP]Xx> how old are you, goltrpoat 
04:28:09 <goltrpoat> pejo:  i think it's part of graduate school admissions
04:28:23 <goltrpoat> xx:  26
04:28:29 <xX[ReP]Xx> i see
04:28:34 <goltrpoat> brb.
04:28:48 <pejo> goltr, do the chem grads take the math test?
04:31:43 <goltrpoat> pejo:  i don't think so.  im pretty sure there's a separate chem gre.
04:32:10 <goltrpoat> i guess what i would ideally like to do is -- there's a lot of stuff i don't know, and i'd like to learn it, and i want the government to pay me to learn it.
04:32:18 <goltrpoat> i don't think there's an easy way to swing that though.
04:33:33 <goltrpoat> i should also amend my "f(x^2) = f(x)" bit with "f is diffable", since otherwise it doesn't make any sense.
04:34:47 <goltrpoat> or does it have to be continuous for that to hold
04:34:52 <goltrpoat> see?  that's what i mean.  hehe
04:38:14 <goltrpoat> hmm i'd have to guess on 51.
04:39:13 <goltrpoat> oh wait.. maybe i wouldn't
04:42:24 <goltrpoat> xx:  ok, i guess it's not terribly hard.  you have to know MVT and IVT, you have to know how to integrate, you have to know a tiny bit of algebra, and you have to be not too hung over.
05:45:12 <dons> ?/yow
05:45:13 <lambdabot> Should I get locked in the PRINCICAL'S OFFICE today -- or have a
05:45:13 <lambdabot> VASECTOMY??
06:02:58 <dons> ?hylo insert (x,[]) = [x] ; insert (x,y:ys) | x<=y = x:y:ys | otherwise = y:(insert (x,ys))
06:02:59 <lambdabot> insert
06:02:59 <lambdabot>  = hylo (_L :: Mu (Const v0 :+: (Const v1 :+: (Const v3 :*: Id)))) g
06:02:59 <lambdabot>   h
06:02:59 <lambdabot>  where g (Left ((x))) = [x]
06:02:59 <lambdabot>    g (Right (Left ((x, y, ys)))) = x : y : ys
06:03:01 <lambdabot>    g (Right (Right (((y), v2)))) = y : (v2)
06:03:03 <lambdabot>    h (x, []) = Left ((x))
06:03:05 <lambdabot>    h (x, y : ys) | x <= y = Right (Left ((x, y, ys)))
06:03:07 <lambdabot>    h (x, y : ys) | otherwise = Right (Right (((y), (x, ys))))
06:03:20 <dons> wacky
06:08:00 <twobitsprite> why is it then when I "import Data.List (nub)" it seems to import (Data.List.!!) as well?
06:09:07 <int-e> Because it's reexported by the Prelude
06:09:17 <twobitsprite> bleh
06:09:42 <dcoutts> import Prelude hiding (!!)
06:09:44 <dcoutts> or
06:09:51 <dcoutts> import qualified Prelude
06:10:04 <dcoutts> to get nothing from the prelude unless you explicitly want it
06:10:19 <int-e> > 6 Prelude.* 7
06:10:20 <lambdabot> 42
06:10:46 <twobitsprite> I see
06:11:11 <twobitsprite> nah... I'll just rename my operator
06:12:23 <twobitsprite> ?archives
06:12:24 <lambdabot> Unknown command, try @listcommands.
06:12:31 <twobitsprite> ?where logs
06:12:31 <lambdabot> http://tunes.org/~nef/logs/haskell/
06:24:45 <twobitsprite> so... I never got an answer to my question on how do make my data-type act like a list...
06:26:28 <twobitsprite> any ideas?
06:26:35 <twobitsprite> can I do it as a list monad?
06:27:02 * twobitsprite is stabbing in the dark
06:27:50 <mauke> what do you mean by "act like a list"?
06:28:37 <twobitsprite> I.e.: let l = MyList "extra data" [1,2,3] in map (+3) l
06:28:53 <twobitsprite>  --> [4,5,6]
06:28:55 <shapr> lennart: How was the eclipse?
06:29:18 <mauke> twobitsprite: make that fmap and MyList an instance of Functor
06:29:28 <mauke> or define a toList function
06:29:43 * twobitsprite looks into Functor
06:29:52 <shapr> JohnMeacham's JHC produces a stripped hello world binary of a total size of 6596 bytes on the Nokia 770, whee!
06:31:21 <lennart> The eclipse was incredible (as usual)!
06:32:10 <lennart> Now it's just a long wait until August 1, 2008 :)
06:32:15 <shapr> :-)
06:32:26 <shapr> I saw one once in the southeast USA.
06:32:47 <lennart> Total?
06:33:14 <shapr> I think it was a total eclipse, but it was quite a few years ago... fifteen to twenty maybe?
06:33:58 <lennart> well, usually you're not in any doubt if you've seen a total one.  it's a special sight
06:34:37 <shapr> Since I've only seen one, I don't know the difference between a partial and total eclipse.
06:34:51 <shapr> I know I used a welding helmet to look at the sun, and it was completely covered.
06:35:14 <shapr> We also used pinholes in a sheet of paper to watch the progress.
06:36:04 <shapr> It was really spooky.
06:37:38 <shapr> lennart: Are you traveling back to .se now?
06:38:30 <dcoutts> liyang, ping
06:39:20 <lennart> in a few days i'll be back in sweden
06:39:51 <shapr> lennart: When will you see an eclipse from the other side?
06:40:40 <shapr> With space tourism becoming popular, and the US planning a permanent moonbase...
06:46:18 <swiert> dcoutss: liyang just went out for coffee.
06:53:37 <edwinb> the sun is regularly covered up here. it's nothing special ;)
06:56:57 <sebell> Hmm. res <- unsafeEval "product $ enumFromTo 1 5" [], res is Just (). Providing type information works as expected however. Any insight?
06:58:13 <liyang> dcoutts: pong
06:59:08 <dcoutts> liyang, looks like the paths in the ghc package files for gtk2hs are not quite right in the debian package
06:59:54 <liyang> oh, okay. Which ones?
07:00:00 <dcoutts> liyang, http://sourceforge.net/mailarchive/forum.php?thread_id=10066240&forum_id=8415
07:00:08 <dcoutts> lispy, specifically the last email from me
07:00:15 <dcoutts> oops not lispy, soory
07:00:57 <dcoutts> liyang, basically, you're overriding "library-dirs: ..." in the gtk.package.conf file
07:01:14 <dcoutts> liyang, that's fine, you're changing the location of the libHSgtk.a file
07:01:48 <dcoutts> liyang, but that entry can contain more than one path sometimes and you're replacing all of them with your one modified entry
07:01:53 <sebell> Hmm.. this is possibly a GHCI issue
07:02:10 <dcoutts> so we're loosing the linker paths
07:02:45 <dcoutts> liyang, specifically /usr/X11R6/lib
07:03:45 <dcoutts> liyang, the gtk2hs configure finds the necessary paths and substitutes them into the gtk.package.conf, but now we're missing one
07:04:40 <Igloo> Why does anything need to be overridden?
07:07:17 <dcoutts> Igloo, because you want to change the install location of the component gtk2hs bits
07:07:47 <liyang> dcoutts: I see.
07:07:50 <dcoutts> the gtk2hs build system should be more flexible to make it possible for you to do what you are trying to do, but in the last release it wasn't
07:08:00 <Igloo> but that should just be PREFIX=`pwd`/debian/... in the install target
07:08:07 <Igloo> configure shuld be given the final path
07:08:28 <dcoutts> Igloo, gtk2hs has several packages, but default we install them under one prefix, you want them each under their own prefix
07:08:53 <Igloo> Ah, I see
07:08:57 <dcoutts> ie be default we do: {libdir}/gtk2hs/libHSgtk.a, {libdir}/gtk2hs/libHScairo.a
07:09:10 <dcoutts> you want {libdir}/ghc6-gtk-dev/libHSgtk.a
07:09:24 <dcoutts> {libdir}/ghc6-cairo-dev/libHScairo.a
07:09:25 <dcoutts> etc
07:09:28 <liyang> configure is given the final path. But I still need to mangle the paths to figure out the dependencies at build time.
07:10:50 <liyang> library-dirs: "/usr/X11R6/lib","/usr/lib/haskell-packages/ghc6/lib/gtk-0.9.10"
07:11:02 <dcoutts> liyang, right
07:11:43 <liyang> That's what it should be, right?
07:11:50 <dcoutts> I think so
07:12:01 <dcoutts> so on it's own gtk2hs would have generated (assuming no configure args):
07:12:11 <liyang> That's what they are in my packages... I'm almost certain.
07:12:25 <dcoutts> liyang, really? it didn't seem to be
07:12:42 <dcoutts> the one the user showed me was missing the "/usr/X11R6/lib"
07:12:54 <dcoutts> import-dirs: /usr/lib/libghc6-gtk-dev/imports
07:12:54 <dcoutts> library-dirs: /usr/lib/libghc6-gtk-dev
07:12:59 <dcoutts> is what it says
07:13:16 <liyang> Was he actually using my packages?
07:13:31 <liyang> They're not in Debian official yet.
07:13:34 <dcoutts> he followed the link from the gtk2hs page
07:13:37 <dcoutts> let's see...
07:13:55 <dcoutts> liyang, maybe they're different :-)
07:14:05 * dcoutts might find himself embarassed
07:14:10 <liyang> Those are Marco's packages. :)
07:14:24 <dcoutts> ah!
07:14:33 <dcoutts> I should take the link down :-)
07:15:20 <liyang> http://carwash.cs.nott.ac.uk:992/~lyh/gtk2hs/ -- the directory is a bit messy but the necessary files are there.
07:16:01 <dcoutts> liyang, ok should I link there?
07:16:14 <dcoutts> I mean change the link on the gtk2hs download page do you think?
07:16:34 <dcoutts> liyang, sorry for the mistake :-)
07:16:34 <liyang> Until it goes in to Debian, yeah, sure. :)
07:16:39 <dcoutts> ok, will do
07:20:28 <Igloo> liyang: You might want to make it readable  :-)
07:20:52 <liyang> Playing around with .htaccess stuff. Sorry. :o)
07:21:54 <liyang> touch -m 644 didn't do what I wanted it to do. :)
07:24:58 <ulfdoz> => do something different. :)
07:27:35 <Igloo> I can't help but think there should be a better way to do the foreach LETTER in Makefile.am, but I don't care enough to get the source and see what it's doing  :-)
07:29:49 <liyang> Something involving find?
07:30:56 <Igloo> I'd have thought shell globbing would do; you just need to arrange for the glob to happen at a time the files exist
07:31:43 <ihope> data Self a = Self {runSelf :: Self a -> a}
07:31:46 <ihope> fix f = (\(Self g) -> f (g (Self g))) (Self (\(Self g) -> f (g (Self g))))
07:32:37 <ihope> GHCi doesn't do anything with that.
07:32:58 <ihope> It just takes the source stuff and tries to find the type of it forever, or something.
07:35:42 <ihope> Yay, I broke it! :-P
07:36:53 <Igloo> There is a known bug along those lines; I think it'll be in the BS if you want to se if that's it
07:36:58 <Igloo> s/BS/BTS/
07:37:52 <ihope> What's that?
07:43:45 <TheJohn> hello.
07:44:36 <TheJohn> what are some good books on haskell (like a language reference with a Backus Nur form) (and a tutorial)?
07:45:22 <liyang> The Haskell 98 Report for the former, I guess.
07:46:45 <neologism> TheJohn: haskell' syntax is dead simple
07:46:52 <neologism> it has like 5 keywords
07:47:14 <TheJohn> neologism, i am contemplating making an interpreter for the c64 (in asm).
07:47:17 <ihope> Now, the Haskell tutorial... there's lots of them. Choose one, and if it's too confusing, choose a different one.
07:49:20 <TheJohn> are there dead tree versions?
07:50:51 <neologism> huh... haskell interpreter for c64, that sounds wild ;)
07:51:55 <TheJohn> neologism, yah, i figure if i run out of ram i'll just swap to disk. 
08:09:15 <TheJohn> there aren't many haskell books.
08:21:05 <Ulfalizer> how would you make a tuple with e.g. the last two elements of some tuple without using a pattern match like  fun (x, y, z) = (y, z)  ?
08:22:23 <sjanssen> you can't
08:23:02 <sjanssen> there is fst and snd to get elements out of a two-tuple, but with three you either pattern match or define a little function to do it
08:23:12 <Ulfalizer> ok
08:23:45 <sjanssen> @pl \(x, y, z) -> (y, z)
08:23:46 <lambdabot> (line 1, column 7):
08:23:46 <lambdabot> [3 @more lines]
08:23:52 <sjanssen> @more
08:23:52 <lambdabot> unexpected ","
08:23:52 <lambdabot> expecting letter or digit, operator or ")"
08:23:52 <lambdabot> ambiguous use of a non associative operator
08:24:11 <Ulfalizer> yeah, i get the point :)
08:27:10 <Ulfalizer> heh, is \ supposed to remind you of a stylized lambda?
08:27:36 <mauke> yes
08:28:04 <Ulfalizer> ok, i always found that syntax kinda wierd up until now :)
09:01:24 <Dazhbog> hello
09:03:55 <Dazhbog> I just started learning haskell
09:04:05 <metaperl-> Dazhbog: hi
09:04:19 <metaperl-> Dazhbog: what got you into it?
09:04:39 <Dazhbog> well.. I've known it for a while
09:04:51 <Dazhbog> I have been on a crusade for the holy grail of programming languages ;)
09:05:06 <IkBenGeenGuest> There is no spoon.
09:05:06 <IkBenGeenGuest> err
09:05:10 <IkBenGeenGuest> There is no holy grail
09:05:19 <sjanssen> Dazhbog: well, you've found it, you can stop looking now.
09:05:25 <Dazhbog> sjanssen: thanks ;)
09:05:26 <metaperl-> Dazhbog: I think it depends on whether you want the best _language_ or the most practical tool to get the most done
09:05:35 <Dazhbog> metaperl-: exactly
09:05:39 <metaperl-> I think Haskell is the best _language_
09:06:07 <metaperl-> I dont think it competes with PHP, C++, C# in terms of ready-to-go functionality at all
09:06:07 <Dazhbog> heh.. so far I think common lisp is the best language ;)
09:06:07 <Muad_Dibber> metaperl: you forget at least one word :P
09:06:13 <Philippa> it's also frequently a more practical tool than you'd expect, but CPAN has haskell beat in that regard atm I gues
09:06:13 <Philippa> +s
09:06:53 <Dazhbog> but .. the thing that caught my attention is that people really seem to do (and get done) stuff with haskell ;)
09:07:01 <Philippa> yeah, haskell's not so hot at describing superturing concepts :-)
09:07:03 <metaperl-> Haskell requires too much intelligence. It filters out all the weenies... but weenies can k0de some great software
09:07:20 <Philippa> metaperl: it only requires that now because we don't have the libs
09:07:27 <Philippa> weenies can do a lot of the basic haskell stuff still...
09:07:47 <Dazhbog> well.. I do perl for living.. CPAN helps but it can be a pain too
09:07:55 <metaperl-> Haskell is fundamentally different. It all starts with "=" ... "=" has a totally different meaning in Haskell
09:08:04 <metaperl-> Dazhbog: I do perl for a living too: http://www.metaperl.com
09:08:34 <Dazhbog> metaperl-: looks.. interesting =)
09:08:42 <Philippa> metaperl: yes, but it's one most weenies've met before still
09:09:15 <Dazhbog> metaperl-: I used to do prolog for living too so not having destructive assignment does not bother me .. I actually feel at home ;)
09:09:25 <davidhouse> @where yi
09:09:26 <lambdabot> http://www.cse.unsw.edu.au/~dons/yi.html
09:09:44 <davidhouse> are there any nice front-ends to yi?
09:09:44 <Philippa> (in fact, IME weenies find it harder to get the imperative =... "you mean before it was this and now it's that?")
09:10:05 <Dazhbog> Philippa: yea I've heard that too =)
09:10:46 <davidhouse> as in, GUIs
09:11:12 <Dazhbog> what do you people use as an editor (on linux)?
09:11:28 <Dazhbog> um for writing haskell code
09:11:46 <Philippa> most people just use their text editor of choice on any platform
09:11:53 <Philippa> (I'm on windows and use textpad)
09:12:33 <davidhouse> a lot of people go for emacs/vim etc.
09:12:35 <Philippa> a multifile nano with syntax highlighting'd stand up surprisingly well, anything beyond that's a bonus anyway
09:12:42 <davidhouse> i'm a fan of kate but am trying out the options
09:12:58 <Dazhbog> is there anything that can talk to hugs or use ghc ..?
09:13:30 <Philippa> dunno. I don't miss that much though because in practice you tend to want to reload the modules you're working with regularly anyway
09:13:39 <Philippa> you can't easily extract definitions from hugs or GHCi IIRC
09:14:03 <davidhouse> Dazhbog: anything with a terminal emulator?
09:15:26 <pejo> Dazhbog, eclipsefp.sf.net might be the thing if you like eclipse. 
09:17:26 <Dazhbog> pejo: well.. I'm not too fond of eclipse but I'll check that out, thanks
09:17:56 <davidhouse> @localtime dons
09:17:57 <lambdabot> Local time for dons is Thu Mar 30 03:18:24 2006
09:18:07 <davidhouse> anyone familiar with yi?
09:18:27 <davidhouse> i'm trying to build it, configure is telling me "cannot satisfy dependency fps-any"
09:18:36 <davidhouse> what's fps?
09:18:37 <davidhouse> @where fps
09:18:38 <lambdabot> http://www.cse.unsw.edu.au/~dons/fps.html
09:18:42 <davidhouse> oh. :)
09:33:21 <davidhouse> okay, yi's build is still erroring.
09:33:30 <davidhouse> ld is now reporting "cannot find -lcurses"
09:36:14 <Igloo> You need something like libreadline5-dev from your OS
09:37:32 <davidhouse> Igloo: i've got readline5 installed
09:55:21 <petekaz> Could someone help me understand the $ operator?  I'm a newbie, going through the new haskell tutorial on how to write a scheme in 48 hours (http://halogen.note.amherst.edu/~jdtang/scheme_in_48/tutorial/overview.html).  Specifically, how does this line get evaluated?  apply func $ map eval args  ?
09:55:48 <wilx> @type $
09:55:49 <lambdabot> parse error on input `$'
09:55:52 <wilx> @type ($)
09:55:53 <lambdabot> forall b a. (a -> b) -> a -> b
09:55:54 <petekaz> Does the map happen first?
09:56:17 <petekaz> As a newbie, that means absolutely nothing to me :-)
09:56:35 <petekaz> I was under the impression that $ was an infix function application operator.
09:56:57 <petekaz> somefun $ args  would be equiv to  somefun(args) 
09:57:22 <wilx> Yeah. 
09:57:30 <Descolada|Work> if i have type A and type B, both are part of a class, how would i do 'type A (operator) type B' functions using the same function names? make a new class with those functions?
09:57:34 <petekaz> so does that mean in that example I posted, it turns into:  apply func(map eval args)?
09:57:55 <petekaz> but the apply function is defined to take 2 args, thus the confusion.
09:58:18 <wilx> I don't think that there is apply function...at least I haven't noticed it yet :)
09:58:22 <wilx> @index apply
09:58:22 <lambdabot> Data.Graph.Inductive.Query.Monad, Data.Graph.Inductive.Query, Data.Graph.Inductive
09:58:35 <davidhouse> petekaz: it's just a way of saving brackets.
09:58:44 <wilx> Function application binds the strongest.
09:58:46 <davidhouse> > sin $ 60 + 30
09:58:46 <petekaz> wilx: that's from the tutorial, its a function that takes 2 args, a function and args.
09:58:47 <lambdabot> 0.8939966636005579
09:58:52 <davidhouse> > sin (60 + 30)
09:58:53 <lambdabot> 0.8939966636005579
09:59:17 <wilx> So it is (apply func) $ (map eval args)
09:59:35 <petekaz> ahhhh ....
09:59:45 <wilx> If apply takes two arguments then (apply func) returns a function that accepts 1 argument.
09:59:47 <petekaz> apply func is stronger than $
09:59:57 <wilx> Yes.
10:00:00 <petekaz> Thats why I was confused ...
10:00:20 <petekaz> and (apply func) is just a curried function waiting for one more arg.
10:00:27 <petekaz> the result of the map
10:00:35 <petekaz> right?
10:00:39 <wilx> Yup.
10:00:46 <petekaz> great, thanks for the help.
10:01:03 <wilx> Basically, you can think of $ as the pipe operator in shell :)
10:02:00 <flux__> actually, isn't it the reverse of it?-)
10:02:31 <wilx> Is it?
10:02:56 <flux__> on the left is the operation that is done the last?
10:02:58 <int-e> yes.
10:03:13 <int-e> do_this $ do_that $ get_data --> get_data | do_that | do_this
10:03:21 <petekaz> I see.  I was trying to understand what happened because that 'apply' func above is defined as: apply func args = maybe (Bool False) ($ args) $ lookup func primitives   and the ($ args) bit was confusing to me.  But now I understand that ($ args) is just a curried function again waiting for the left operand.
10:03:27 <int-e> if you want to think of unix pipes.
10:03:41 <wilx> Ah, I see  what you mean, yeah, the order is reversed.
10:03:52 * int-e prefers to think of it as a ( with the matching ) as far right as possible
10:04:20 <petekaz> Is it possible to do (args $)?  That would be pointless I suppose, but can it be done?
10:04:55 <int-e> it can be done
10:04:58 <flux__> indeed, it does look like something that @pointless would produce
10:05:16 <int-e> using ($ arg) might even be useful occasionally.
10:05:17 <petekaz> ok, at least I'm "getting" it.
10:06:23 <int-e> And writing ($) instead of id can emphasize the fact that you use function application underneath.
10:09:50 <int-e> @query lambdabot 
10:09:50 <lambdabot> Unknown command, try @listcommands.
10:18:10 <SyntaxNinja> 'morning
10:26:33 <dcoutts> SyntaxNinja, 'evenin
10:39:51 <vincenz> Yay
10:40:06 * vincenz got himself a webhost
10:41:04 <vincenz> with the very semantically correct name:  http://www.notvincenz.com
10:41:14 <vincenz> expect to see content on it soon (it's not fully initialised yet)
10:43:35 * SyntaxNinja hax0rs it
10:45:08 <vincenz> I came to a realization today
10:45:19 <vincenz> most people say irc-spam is not really harmful...well I just fouond out today why it is
10:45:25 <vincenz> I remember 5 years ago you could easily get free ssh accounts
10:45:32 <vincenz> not the best of connections, but oh well
10:45:36 <vincenz> nowadays they all block irc
10:46:12 <SyntaxNinja> vincenz: are you using a virtual colo?
10:47:16 <vincenz> no
10:47:18 <vincenz> just we
10:47:20 <vincenz> web/ft
10:47:23 <vincenz> ...p
10:47:31 <vincenz> at an incredible rate of 20 euros/year
10:47:37 <SyntaxNinja> nice
10:49:35 <vincenz> yep
10:49:37 <vincenz> unlimited storage
10:49:40 <vincenz> high speed
10:49:44 <vincenz> no mention of monthly bandwidth
10:49:55 <vincenz> www.aruba.it
10:53:51 * vincenz smirks
10:53:54 <vincenz> the follies of internet
10:54:01 <vincenz> one letter difference and you enter a completely different world
10:54:08 <vincenz> "latex babe" iso "latex babel" 
10:56:17 <SyntaxNinja> is there a word for a Freudian slip that you type into google? a googlian slip?
10:56:41 <ncalexan> Yeah... 'I'm feeling lucky.'
10:56:45 * vincenz hehe
10:57:49 <Descolada|Work> i think someone came into this small room, farted, then left
10:58:17 <SyntaxNinja> let's talk about haskell.
10:58:19 <SyntaxNinja> who likes concurrnecy?
10:58:40 <Descolada|Work> explain it and i might like it
10:59:22 <ncalexan> Hehe...
11:00:33 <SyntaxNinja> Descoloda: it makes your computer look like it's doing more than one thing at a time.
11:00:45 <Descolada|Work> ahh
11:00:57 <Descolada|Work> and how is that about haskell?
11:01:09 <vincenz> SyntaxNinja: I do but my concurrent self doens't
11:01:59 <vincenz> Descolada|Work: it's like people who work for governments
11:02:13 <vincenz> they also attempt to portray themselves as if they're doing more than one thing (or one thing)
11:02:20 <Descolada|Work> ahh
11:02:38 <vincenz> SyntaxNinja: I like concurrency on multicore systems
11:02:46 <SyntaxNinja> Descoloda: haskell is a programming language in which one might like to write programs which look like they're doing more than one thing
11:02:57 <SyntaxNinja> vincenz: do you like web servers? ;)
11:03:05 <Descolada|Work> when i was in the Air Force and in casual status (they have no use for you, so you just do odd jobs), i was the lazyest guy there, but i knew how to look busy
11:03:06 <vincenz> SyntaxNinja: I was thinking of more advanced things
11:03:58 <vincenz> SyntaxNinja: what would be even cooler is mobile code
11:04:04 <vincenz> anyways
11:04:08 <vincenz> I have to go to a goodbye drink
11:04:10 * vincenz waves
11:04:11 <ncalexan> Descolada|Work: Haskell (specifically GHC) has rather good concurrency primitivies.
11:04:11 <vincenz> later
11:04:21 <Descolada|Work> cool
11:04:36 <Descolada|Work> is GHC like, the best haskell compilier?
11:04:52 <vincenz> SyntaxNinja: Imagine mobile code... just distribute shell clients that do nothing...and you can do stuff like SETI...and the next week you can tell them to upload another behaviour that does something else
11:04:57 <ncalexan> 'best' is a loaded term, but I think any reasonable observer would say yes.
11:05:16 <ncalexan> vincenz: that's called Windows.
11:05:24 <astrolabe> GGC produces the fastest executables, but takes longer to compile I think.
11:05:26 <vincenz> ncalexan: I meant without people doing something
11:05:29 <vincenz> ncalexan: mobile code
11:05:30 <astrolabe> *GHC
11:05:34 <ncalexan> Also like Windows :)
11:05:37 <vincenz> hehe
11:05:43 <vincenz> well -secure-
11:06:04 <ncalexan> Right :)
11:06:09 <vincenz> besides I'm on linux, I dont get that perk :(
11:06:10 <Descolada|Work> ahh
11:06:31 <ncalexan> Descolada|Work: GHC also supports the most extensions, and is the most tested compiler.
11:06:33 <vincenz> no 1GB files in my root directory of indexing of files that I don't want and can't remove :/
11:06:49 <vincenz> anyways, I reallly gotta go
11:06:52 <ncalexan> Ciao!
11:06:52 <vincenz> be back later
11:07:24 <Descolada|Work> so then its almost a defacto(sp?) standard
11:08:30 <ncalexan> I think it is a de facto standard, but the yhc/nhc people might not appreciate that.  Also, hugs has a lot of users.
11:08:40 <Descolada|Work> heh
11:09:17 <ncalexan> And there is John Meacham's jhc, and the helium project.
11:09:29 <ncalexan> There are lots of interesting compiler projects.
11:10:15 <Descolada|Work> it makes more sense to me to put the 'neet' things of those projects into ghc extentions
11:11:37 <Philippa> no it doesn't
11:11:51 <Philippa> not when the neat things're only readily implementable by using a completely different underlying model
11:12:03 <ncalexan> Yeah, jhc is wildly divergent from ghc.
11:12:05 <Philippa> (see JHC, most of the fun stuff you can potentially do with YHC's bytecode)
11:12:18 <Descolada|Work> ahh
11:13:55 <Philippa> many of JHC's optimisations just aren't possible with GHC's model, and it deliberately uses a totally different implementation of typeclasses that plays into their optimisations
11:20:24 <robokop> @hoogle [a] -> Int -> [a]
11:20:26 <lambdabot> Prelude.drop :: Int -> [a] -> [a]
11:20:26 <lambdabot> Prelude.take :: Int -> [a] -> [a]
11:20:26 <lambdabot> Control.Parallel.Strategies.parBuffer :: Int -> Strategy a -> [a] -> [a]
11:29:15 <pierre__> hello.
11:32:42 <ncalexan> hi.
12:03:36 <ChilliX> SyntaxNinja: Do we still have the H' channel?
12:03:53 <Philippa> we had one? I missed that
12:05:07 <Descolada|Work> okay, so what is a monad?
12:05:23 * ChilliX laughs out loud.
12:05:47 <ChilliX> Ph: At least some committee members used to hang out in one for a while...
12:05:58 <Descolada|Work> okay, okay, enough making fun of me :)
12:06:34 <Philippa> Descolada|Work: a monad is an algebraic structure (that's mathmo for 'formalised design pattern' ;-)
12:08:02 <Descolada|Work> and they can be used to update data?
12:08:08 <Philippa> beyond that, do you first want to know "what does it represent?" or "what does the structure look like in Haskell?"
12:08:10 <ChilliX> Descoloda, sorry
12:08:14 <greenrd> But you don't you the word "monad" like you use "list" or "stack"
12:08:22 <Philippa> sort of. They can be used to describe programs that do so
12:08:24 <Descolada|Work> ahh
12:08:35 <greenrd> You can say there's a list, whose type is list of foo
12:08:42 <Philippa> right. More "class of programs with lists". The programs being called "computations" or (with the IO monad) "actions"
12:09:29 <florenz> hi
12:09:39 <Descolada|Work> hello
12:09:54 <Philippa> 'lo florenz
12:10:00 <Philippa> we're just explaining monads to Descolada|Work, 'sup?
12:10:22 <ChilliX> Concerning that topic, I am always reminded of the following poem: http://www.cse.unsw.edu.au/~chak/haskell-poem.html
12:10:42 <florenz> well, i have a (possibly easy) question about haskell's types
12:10:56 <Philippa> go for it
12:11:05 <florenz> is there a possibilty to test if a given value was constructed by a certain constructor except for patterns?
12:11:09 <Descolada|Work> so, a list of actions? (functions) or a list of objects that you apply a action to...
12:11:19 <Philippa> florenz: ultimately no
12:11:19 <florenz> i mean, i have data t = A | B
12:11:33 <Philippa> Descolada|Work: in the general case? More general than that
12:12:02 <florenz> Philippa: ok, thank you.
12:12:05 <Philippa> You have a type constructor (which for now I'll call M) which takes a parm that's the type computations in that monad return, so M a or M Int...
12:12:14 <florenz> that saved me lots of searching the web
12:12:25 <Philippa> florenz: there're many ways to invoke pattern matching, hence "ultimately"
12:12:44 <Philippa> you also can't distinguish between type constructors on the term (that is, "ordinary code") level in haskell 98
12:12:56 <Philippa> language extensions might let you, you'll know 'em when you see 'em
12:13:13 <Philippa> Descolada|Work: OK, so we've got a type constructor. Not a lot of structure yet, right?
12:13:23 <Descolada|Work> right
12:13:29 <florenz> what i wanted was something like an A? predicate if I have  data T = A | B
12:13:38 <Philippa> Descolada|Work: Every instance of the Monad typeclass has two functions associated with it...
12:13:44 <florenz> to pass to another function which takes T or [T]
12:13:53 <Philippa> florenz: isA foo = case foo of {A -> True; B -> False}
12:14:07 <Philippa> you're ultimately using pattern-matching, but you can wrap it in a function
12:14:19 <florenz> ok, i suspected that.
12:14:46 <Philippa> Descolada|Work: the first one is return. Talking about M for a specific instance of Monad still...
12:14:50 <florenz> i was just wondering if the compiler generated something a discriminator for me or if i had to write it myself
12:14:53 <Philippa> return :: a -> M a
12:15:03 <Philippa> (more generally, return :: (Monad a) => a -> m a)
12:15:24 <Philippa> florenz: yourself. Though it's comparatively rare to need the function anyway
12:15:42 <Philippa> Descolada|Work: return takes a value, gives you a computation that returns that value and otherwise does nothing
12:15:50 <Philippa> Descolada|Work: following okay so far? The tricky bit comes next...
12:15:52 <florenz> Philippa: yes, that's true. because of the very good support for pattern matching.
12:16:17 <Descolada|Work> ok
12:16:22 <Philippa> florenz: compared to some languages it's actually mediocre. But yes, it supports basic pattern-matching about as well as possible :-)
12:17:24 <florenz> well, it's not prolog, is it?
12:17:38 <florenz> and it can be used in more than one place.
12:17:40 <Philippa> Descolada|Work: the other function's an operator. (>>=)::M a -> (a -> M b) -> M b
12:17:43 <Philippa> florenz: exactly
12:18:20 <florenz> ok, i'm off to write some A?, B? functions by hand then...
12:18:26 <Philippa> Descolada|Work: (>>=) is effectively function composition for computations. It's enough to let you express the same kinds of things with them you can with functions, because you've got currying and all that stuff
12:18:43 <Philippa> florenz: remember if it starts with a capital it's a constructor. isA might be a better name
12:19:35 <florenz> Philippa: oh yes, thank you. I'm sure the compiler would have reminded me...
12:19:47 <Philippa> Descolada|Work: the good news is that there's syntactic sugar in the form of the do notation so you don't have to use (>>=) directly ever. So all you need to remember is what the algebraic properties of it tell you you can do
12:20:15 <Descolada|Work> yay for sugar
12:21:18 <Philippa> Descolada|Work: the do notation's specified in the Haskell 98 report, but for now really all you need to remember is that a <- foo; ... translates to foo >>= (\a -> ...), and foo; ... to foo >> ..., where there's a law for all monads that a >> b = a >>= (\_ -> b)
12:21:33 <Philippa> and even that's irrelevant if you just pretend you're mostly programming in an imperative language :-)
12:22:14 <Philippa> Monads tend to come bundled with some functions that return some variant of M a, too. Those're often called "monadic operations" - a state monad would have operations to get and update the state for example, and the IO monad has operations to do disk access, networking, you name it
12:23:01 <Philippa> because >>= is fairly loosely specified, there's a really wide range of monads. If you squint at it the right way they effectively define languages based around Haskell - sort of a bastard mix of macros and CPS only well-typed
12:23:35 <Philippa> and because the type M a's visible to the type system, you can tell when code's using a monad and which one - which effectively gives you sandboxing via the type system :-)
12:23:43 <Philippa> that make any sense?
12:23:52 <Descolada|Work> yes
12:24:00 <goron> Is "Visa versa" in the English language? Since it's Latin...
12:24:04 <Philippa> okay. Any questions?
12:24:15 <pierre__> Philippa: thx for your lecture... 
12:24:20 <Descolada|Work> can you say all that again? j/k :p
12:24:21 <Philippa> goron: Vice versa is, I don't here visa versa
12:24:30 <Philippa> Descolada|Work: Logs :-)
12:24:35 <Descolada|Work> hehe
12:24:37 <goron> Philippa: Thanks
12:24:39 <Philippa> goron: I mean hear. D'oh :-)
12:24:58 <pierre__> Philippa: could you explain me why Maybe is a monad?
12:25:06 <goron> Philippa: Oh, then it must be something only in my native language, which we stole from the Romans.
12:25:35 <Descolada|Work> what could help me with my problem of having two types that are sorta the same but diffrent, using the same function names for both like A `dot` A, B `dot` B, A `dot` B and B `dot` A...? i dont think monads can
12:25:41 <Philippa> pierre__: Sure. Return = Just, a >>= b is "return Nothing if a is Nothing or b otherwise". We do this because it gives you basic exceptions
12:25:51 <Philippa> (Either gives you exceptions plus enough info to find out what went wrong)
12:26:09 <Philippa> Descolada|Work: typeclasses for overloading
12:26:45 <Philippa> if you can't encode it with the typeclass system one way or another you're stuck - unfortunately Haskell98 only supports "single parameter typeclasses", which isn't enough if you want to do vector * matrix for example
12:26:49 <Descolada|Work> i made one and it complains that it cant decide whats what
12:27:00 <Philippa> "undecidable instances"?
12:27:20 <Philippa> yeah, that's a bitch. My typeclass fu isn't good enough to debug it for you, you might find functional dependencies useful if you're working with GHC or Hugs
12:27:25 <Descolada|Work> Functional Dependencies conflict
12:27:58 <Philippa> rewrite them so they don't. If they can't, you're stuck anyway
12:28:07 <Descolada|Work> hmm
12:28:12 <pierre__> Philippa: so, when we're computing Just 2 + Just 3, which monadic operations are being performed? 
12:28:39 <Philippa> pierre__: Most likely none. Does Maybe have a Num instance?
12:29:13 <Philippa> pierre__: most of the "fancy" monads hide their data constructors, the ones where that's not done often it's not helpful to think of everything you do with them as monadic anyway
12:29:18 <Descolada|Work> how much faster are gates and pattern matching? maybe i wont need todo this if there is not much speed gain
12:29:47 <Philippa> when in doubt, benchmark. Or just don't overload so heavily
12:30:02 <Philippa> (yeah, I know, it's a pain when that happens)
12:30:19 <Philippa> you *should* be able to do enough by having the return type determined by the two parms though?
12:30:28 <Philippa> (which is the kind of thing fundeps are supposed to express)
12:30:42 <Descolada|Work> i think so
12:30:53 <pierre__> > Just 2 `mplus` Just 3
12:30:54 <lambdabot> Just 2
12:31:15 <pierre__> > Just 2  Just 3
12:31:16 <lambdabot> Couldn't match `Maybe a' against `t -> t1 -> t2'
12:31:19 <Philippa> pierre__: the plus there isn't arithmetic addition :-)
12:31:23 <pierre__> > Just 2 + Just 3
12:31:24 <lambdabot>  add an instance declaration for (Num (Maybe a))
12:31:24 <lambdabot>   In the definition of `ptc': ptc = (Just 2) + (Just 3)
12:31:24 <lambdabot>   In the definition of `v':
12:31:36 <Descolada|Work> i call it typemaster and typeslave, all fucntions return typemaster and all functions use typeslave as the second param
12:31:36 <Philippa> hah, thought there wasn't an instance :-)
12:31:37 <pierre__> hmm, then how to do it?
12:31:45 <Philippa> pierre__: pattern-match
12:32:07 <Philippa> take the values out of the Just constructors, add them and return the result (or Just the result)
12:32:28 <goron> Philippa: does modeling mean the same as modelling?
12:32:29 <Philippa> or possibly: do a <- Just 2; b <- Just 3; return a+b
12:32:52 <Philippa> goron: good question. RTFDictionary :-)
12:32:54 <wchogg> Can I ask some stupid monad/category questions?
12:32:56 <pierre__> > do a <- Just 2; b <- Just 3; return a+b
12:32:57 <lambdabot>  add an instance declaration for (Num (Maybe a))
12:32:57 <lambdabot>   In the result of a 'do' expression: (return a) + b
12:32:57 <lambdabot>   In the definition of `wrl':
12:33:05 <Philippa> (it might be that one's clothes modeling and the other's modelling as in "building a model of..."
12:33:08 <Philippa> )
12:33:18 <Philippa> wchogg: we can't stop you :-)
12:33:21 <pierre__> do a <- Just 2; b <- Just 3; return a `mplus` b
12:33:32 <pierre__> > do a <- Just 2; b <- Just 3; return a `mplus` b
12:33:33 <lambdabot>  add an instance declaration for (Num (Maybe a))
12:33:48 <Philippa> > do a <- Just 2; b <- Just 3; return (a+b)
12:33:49 <lambdabot> Just 5
12:33:56 <Philippa> (I missed the parens out, my bad)
12:34:02 <Philippa> (return's a one parm function)
12:34:10 <Descolada|Work> ahh
12:34:11 <Descolada|Work> neet
12:34:13 <pierre__> Philippa: thx
12:34:21 <wchogg> True.  So I just reread Monads As Containers, and I was wondering if it is possible to represent finite graphs in some way by monads.  It seems to make sense in terms of join and return, but I'm not sure.
12:35:02 <ChilliX> > Monad.liftM2 (+) (Just 1) (Just 2)
12:35:03 <lambdabot>  Not in scope: `Monad.liftM2'
12:35:13 <ChilliX> tsss
12:35:16 <goron> Philippa: Hmm, it's a bit inconsistent. I think modelling is better though in this context. 
12:35:23 <ChilliX> pierre__, anyway, that works
12:36:06 <pierre__> > Control.Monad.liftM2 (+) (Just 1) (Just 2)
12:36:06 <Philippa> wchogg: I imagine it's possible. Got a use for it?
12:36:07 <lambdabot> Just 3
12:36:31 <Philippa> ChilliX: point
12:36:37 <ChilliX> pierre__, yeah, a pity lambdabot doesn't know H98
12:36:51 <wchogg> Philippa-Eh, just wondering if it would be possible to make automating calculations of Feynman diagrams easier by use a monad structure to organize it.  Idle speculation, really.
12:36:53 <pierre__> ChilliX: :-)
12:37:15 <Philippa> wchogg: play around with it and see what you get :-)
12:37:25 <pierre__> how does liftM works?
12:37:53 <wchogg> Phillippa:  But that's, y'know, work!  Gosh.
12:38:12 <Philippa> pierre__: liftM f = return . f
12:38:20 <Philippa> "perform the function then return the result into the monad"
12:38:26 <ChilliX> pier: Do you know what a functor is?
12:38:36 <pierre__> ChilliX: no 
12:38:38 <Philippa> :-)
12:38:39 <Philippa> liftM = fmap
12:38:50 <Philippa> fmap = generalised map
12:38:58 <ChilliX> pier: ok, then take Phillippa's explanation
12:41:52 <pierre__> do a <- Just 2; b <- Just 3; return (a+b) -- seems strange for me... What type does '+' have here? 
12:42:00 <Philippa> the usual type
12:42:08 <Philippa> desugar the do notation...
12:42:25 <Philippa> Just 2 >>= (\a -> (Just 3 >>= (\b -> return (a+b))))
12:42:53 <Descolada|Work> > do a <- Just "hello, "; b <- "#Haskell"; return (a++b)
12:42:53 <lambdabot> Couldn't match `Maybe' against `[]'
12:43:04 <Descolada|Work> > do a <- Just "hello, "; b <- Just "#Haskell"; return (a++b)
12:43:05 <lambdabot> Just "hello, #Haskell"
12:43:12 <Descolada|Work> ok
12:44:48 <pierre__> if (+) is here Num -> Num -> Num, why type check don't fails?
12:45:08 <pierre__> or a and b aren't of Maybe type? 
12:45:16 <Descolada|Work> i was wondering around the same thing
12:46:05 <Philippa> pierre__: bingo
12:46:09 <Philippa> read the desugared code...
12:46:13 <Philippa> typecheck it by hand
12:46:34 <Philippa> :t (>>=)
12:47:12 <pierre__> @type (>>=)
12:47:13 <lambdabot> forall (m :: * -> *) b a.
12:47:13 <lambdabot>    (Monad m) =>
12:47:13 <lambdabot>    m a -> (a -> m b) -> m b
12:47:30 <Philippa> bah, my brain's clearly not working today :-)
12:47:41 <Philippa> so... Just 2 >>= (\a -> return (a+1), say
12:47:47 <Philippa> what's the type of a there?
12:48:42 <pierre__> yeah... Int
12:50:27 <Philippa> I can't be arsed to work out if you're right (modulo defaulting statements) or it's (Num a) => a :-) Bloody monomorphisation restriction...
12:50:31 <goron> Philippa: In your experience: is code using instances of Functor etc. more than an academic exercise?
12:51:05 <Philippa> goron: it seems to me map-like ideas arise all the time. Whether you bother to write the Functor instance is another matter, I guess
12:51:22 <ChilliX> pierre__: defaulting
12:53:07 <pierre__> great, i think i got it... thx
12:58:21 <goron> Philippa: you don't need the type system for map to work. 
12:58:46 <Philippa> Functors're just things that support some map-like operation though
12:59:00 <Philippa> so either you call it treeMap or you call it fmap, what's the difference?
12:59:14 <Philippa> it's not what haskell calls map either way, and it's recognisable as "a map" either way
13:00:13 <goron> Philippa: oh, that's not the problem. I just wonder about the practical usefulness of the type system. 
13:00:26 <Philippa> ah, heh
13:00:43 <Philippa> weeell, getting instant access to a bunch of operations that use fmap has its advantages I guess
13:00:54 <Philippa> or is the point for comparison something like lisp?
13:02:24 <goron> Philippa: no, we had lisp yesterday ;)
13:03:21 <goron> Philippa: or well, maybe it is. E.g. Stratego which is like Lisp just crashes when you give the wrong number of arguments to a higher order function.
13:04:20 <goron> Philippa: so basically the advantage is that you have a standard for documentation for which you can use e.g. Hoogle to simply answer the question:"what can I do with values of type Foo?"
13:05:38 <goron> Philippa: I am assuming the prob. of an untyped program not crashing on non-trivial test data is small. 
13:06:49 <wchogg> goron-It seems like what you're saying is that a static type system just gives you and the compiler more information about the program?  If that's what you mean, I guess the answer is yes.
13:07:13 <Philippa> I find being told that monads're all functors somewhat useful, although the lack of an actual instance less so
13:07:23 <Lemmih> dons: ping.
13:08:17 <goron> wchogg: well, in the case of Lisp, you can help the compiler too by adding type annotations. (I only guess these are implementation dependent)
13:08:21 <ChilliX> Lemmih: It's 8am in Sydney.
13:09:05 <Philippa> goron: true. Personally I find the checking useful. I'm also thinking about classes of system where it can effectively tell you "yep, it's safe to run this code" once you've typechecked it
13:09:35 <goron> Philippa: "classes of system"?
13:10:38 <goron> Philippa: I think Haskell code is of higher quality on average than Lisp code. I only wonder what  the cause is.  
13:10:44 <Philippa> yup. For example, systems that can be updated live with new code transmitted to them through a potentially hostile environment
13:10:49 <wchogg> goron-Yeah, you can add type annotations and in at least SB/CMUCL you can define your own types.  I think Haskell is interesting in terms of the framework it gives you for types and type safety.  You give up some freedom and buy some conveniences.
13:10:54 <goron> Philippa: most of the people writing Haskell had proper education. 
13:11:20 <Philippa> yup. And the type system actively encourages us to think things like "is this pattern exhaustive?" rather than "and if someone passes me something stupid then fuck 'em"
13:11:29 <goron> Philippa: Haskell (at least 98) has no support for that, right? Other type systems do. 
13:11:38 <ihope> Unboxed types are fun. It's easy to get a kind error.
13:11:41 <SyntaxNinja> y0
13:11:49 <Philippa> Haskell won't check exhaustiveness, no
13:12:01 <SyntaxNinja> ChilliX: yeah, /join #haskell'
13:12:12 <SyntaxNinja> ChilliX: no one ever speaks there, though
13:12:14 <ihope> Philippa: just pass your function every value. :-)
13:12:20 <Philippa> OTOH, up until you use guards the exhaustiveness check is algorithmic and easy to perform once you know the type of the scrutinee
13:12:23 <Lemmih> ChilliX: You're saying dons isn't addicted enough to need his morning IRC fix? (:
13:12:34 <goron> Philippa: Dependent type systems *can* give you that certainty.
13:12:39 <Philippa> yes, I know
13:12:57 <goron> Philippa: I am not surprised.
13:12:58 <ChilliX> Lemmih: I am saying, he is probably still sleeping :)
13:13:09 <Philippa> you seem to be ignoring my point though
13:13:21 <wchogg> Are there dependent type systems that are in readily usable languages?
13:13:45 <goron> Philippa: Hmm, I miss your point, I guess.
13:15:18 <goron> Philippa: you mean run-time checks?
13:15:23 <Philippa> it's often easy to think about the total set of values you can receive in Haskell in a way it isn't in a language where you could get anything or where you have to encode a disjoint union via a C-style one and an enumeration or worse yet a more generic int-based encoding
13:15:44 <Philippa> I don't mean "language implementations come with hard checks", I mean "language conducive to certain kinds of thinking"
13:16:04 <Philippa> (cf thinking about logic programming in C - if you're sane you don't, you think about logic programming and then you tediously encode it afterwards...)
13:19:59 <Philippa> and the "well-checked" aspects can be significant parts of a program's control flow, too - often you can embed much of how that behaves in a parametric ADT and some primitives
13:20:04 <vincenz> aloha
13:20:28 <Philippa> I *just can't do that* in C++, some arsehole'll find a way to screw with the abstraction
13:21:15 <goron> Philippa: hmm, yeah, most languages provide an "escape" into hackery.
13:21:33 <goron> Philippa: Well, an easy escape.
13:21:47 <ihope> Oh, why does unsafeCoerce# end with a #? It doesn't work on unboxed types.
13:21:56 <Philippa> ihope: it's a primitive action
13:22:08 <Philippa> much as the unboxed types are primitive values
13:22:23 <Philippa> the integer addition function's name ends in a # too IIRC
13:22:29 <ihope> Ah.
13:22:30 <Igloo> It does work on unboxed types
13:22:36 <ihope> Igloo: how?
13:22:42 <Igloo> What do you mean?
13:22:52 <Igloo> unsafeCoerce# 5#
13:22:53 <vincenz> hey people
13:22:55 <vincenz> :)
13:23:01 <ihope> Igloo: kind error?
13:23:14 <Igloo> What version of GHC?
13:23:15 <wchogg> It's funny how often I've seen Haskell and Lisp compared.  I take it to mean that I'm not the only one who finds them to be the two most interesting languages.
13:23:21 <ihope> 6.4.1
13:23:22 <Philippa> @type unsafeCoerce# 5#
13:23:22 <lambdabot> Not in scope: `unsafeCoerce#'
13:23:39 <ihope> @type GHC.Base.unsafeCoerce# 5#
13:23:39 <lambdabot> forall b. b
13:23:44 <ihope> Hmm.
13:23:50 <ihope> @type GHC.Base.unsafeCoerce# 5# :: GHC.Base.Int#
13:23:51 <lambdabot> GHC.Base.Int# :: GHC.Prim.Int#
13:23:54 <Philippa> wchogg: they certainly describe the reasonably-visible representatives of two apparent extremes in programming language design
13:23:59 <ihope> Oh.
13:24:11 <ihope> Hmm...
13:24:38 <Philippa> (Epigram's further into Haskell's design-space corner, but isn't anywhere near ready for real world use)
13:25:53 <ihope> Yeah, I guess it works.
13:26:51 <wchogg> I'm hoping to play with Epigram 2 when it is available.
13:27:03 <mwc> How exactly does epigram works? Do the values the types are dependent on become runtime values that are checked?
13:27:07 <Philippa> me too. I'll still be writing web apps in haskell though
13:27:18 <mwc> or does it manage to entirely take care of all of that at compile time?\
13:27:35 <Philippa> mwc: that's not quite necessary at the moment because Epigram's a total language and every function terminates
13:27:35 <goron> Philippa: What webserver/frameworks do you use?
13:27:56 <wchogg> How do you prove that every function terminates?
13:28:14 <mwc> wchogg, very carefully :P
13:28:17 <ihope> Weird... I# (unsafeCoerce# 3#) works, but I# undefined doesn't. Why isn't there something in the type that says that?
13:28:18 <Philippa> wchogg: RTFPapers :-) It's a property of the underlying calculus
13:28:38 <Philippa> goron: bearing in mind these're more personal toys than anything else, atm I'm just using lighttpd (any old thing that works'll do) and variants of Network.CGI. The apps aren't anything complicated, we're talking wiki type stuff
13:28:58 <Philippa> no AJAX-type flashy UI or anything atm
13:29:03 <goron> Philippa: no AJAX ?;)
13:29:14 <goron> Philippa: I didn't read your comment, before I posted mine.
13:29:16 <goron> :)
13:29:26 <Philippa> (though given some good libs to build javascript as an EDSL and that kind of thing I imagine Haskell'd be damn good at it)
13:29:55 <goron> Philippa: personally I rather have a real language then an ESDL. 
13:30:41 <goron> Philippa: E.g. with type safe regular expressions, they are a pain. (unless you first parse them at compile time and them compile them). 
13:37:15 <Philippa> goron: I want to build them with functions not from strings, certainly
13:37:48 <goron> Philippa: Stratego parses the regular expression and when it can't parse it, it gives a compile time error.
13:37:53 * Philippa nods
13:38:06 <Philippa> mostly I cba to go that far. Regexes annoy me anyway :-)
13:38:10 <goron> Philippa: that's what I want to do in my language too.
13:38:16 <goron> cba?
13:38:21 <Philippa> can't be arsed
13:38:25 <goron> heh
13:38:27 <Philippa> (possibly a britishism?)
13:38:38 <goron> yeah, definitely
13:40:18 <goron> Philippa: yeah, I pick Parsec over regular expressions.
13:40:38 <Philippa> I wish I could get my hands on enough kit to simulate both the environment and the stresses involved in an enterprise-level setup. Largely because I really want to have a go at attacking the scalability stuff, because if you get that then Haskell suddenly becomes sellable
13:41:09 <Philippa> I mean yeah, only a handful of graduates from each uni that teaches it're any good with it, but they're the best graduates generally
13:41:28 <Philippa> and they're likely the same people who'll get how not to fuck up with a big database setup
13:43:18 <Philippa> for bonus points, the brighter folks from the 70s and 80s environments'll get what's going on pretty fast because usually it's exactly the same kind of code, just in a nicer language :-)
13:43:52 <Philippa> (my dad went *blink* and then got it immediately when I told him you don't take things out of the IO monad - apps used to be written exactly that way anyway...)
13:44:11 <goron> Philippa: oh I didn't know that.
13:44:14 <vincenz> bbye people
13:44:20 <Philippa> seeya
13:44:25 <goron> bye
13:44:36 <goron> Philippa: sounds like good engineering practice, yes.
13:44:51 <Philippa> and pretty much the only way not to really fuck your head up working in COBOL :-)
13:46:09 * Philippa wonders now how hard that javascript binding'd be to do
13:46:24 <Philippa> I'd need to learn more about the kind of things AJAX apps actually do with it, I guess
13:46:51 <goron> Philippa: Still it would be damn nice to write foo = regEx <some expression> and be safe. In Haskell the only way to do it would be to parse it, then compile Haskell code. Then you could use that module in your other code. But you would need e.g. a MakeFile to do it.
13:47:50 <integral> that's one problem which perl6 does actually tackle...
13:48:04 <Philippa> or Template Haskell
13:48:21 <integral> Philippa: that doesn't allow a custom grammar for a custom quote operator...
13:48:34 <integral> you still have to do awful stuff like "\\d"
13:48:47 <Philippa> integral: no, but $(regex "foo") is a good start, and you /can/ read text from other files
13:48:50 <goron> But then you get Greenspun's Tenth Rule of Programming
13:49:00 <goron> Any sufficiently complicated C or Fortran program contains an ad hoc, informally-specified, bug-ridden, slow implementation of half of Common Lisp.
13:49:34 <integral> Philippa: it's not a good start, regexps are an easy notation, but it's *very* painful to have to fiddle with the escaping.
13:49:46 <goron> integral: indeed
13:50:00 <mauke> integral: there's an easy solution to that
13:50:01 <Philippa> integral: it is, however, an excellent start for almost any other notation
13:50:16 <goron> A regular expression is supposed to be easy to use. 
13:50:17 <integral> mauke: ?  change the notation?
13:50:27 <integral> (ie, bin using \ as a metachar)
13:50:38 <Philippa> integral: 'twould be one way to do it. You'd only need a transliteration, nothing deeper
13:50:54 <mauke> integral: either use a different syntax for regexes than strings (see perl) or change the regex syntax (ploki)
13:51:28 <integral> yeah, but perl5 special cases regexps, which IMHO isn't quite in the spirit of haskell :)
13:51:33 <Philippa> personally I'm happy using a full-blown EDSL rather than parsing a DSL into haskell values anyway
13:51:58 <mauke> yeah, so use a different regex syntax (not ploki's though :-)
13:52:03 <integral> Philippa: * in regexps is a postfix operator,  but haskell's grammar doesn't do postfix operator
13:52:12 <goron> mauke: or give a possibility for the user to extend the parser (Stratego not that extreme/Lisp very extreme (but bad libraries))
13:52:29 <Philippa> integral: I can live with that. I have no difficulty reading many foo in Parsec code, for example
13:52:30 <mauke> goron: sounds like a bad idea for regexes
13:52:45 <goron> mauke: why? They give you compile time safety?
13:52:49 <Philippa> mauke: not really so long as the delimiter's clear
13:53:21 <mauke> huh?
13:53:31 <mauke> I think I'm missing something
13:53:49 <Philippa> mauke: so long as there's a clear boundary in the syntax between regexes and everything else, the extensibility's not a bad idea for them in particular
13:54:00 <Philippa> (I agree if there was a risk of eg arithmetic and regexes being confused that'd suck)
13:54:19 <goron> mauke: In Stratego you can put e.g. in  a Java source code: RegExp myRegExp = <escape to language of regexes a*>
13:54:20 <mauke> ah, I misunderstood
13:54:22 <integral> (see emacs' perl-mode for a demo of that suck)
13:56:09 <goron> integral: I have it before me. What do you mean?
13:57:01 <goron> ("\\(^\\|[?:.,;=!~({[ \t]\\)\\([msy]\\|q[qxrw]?\\|tr\\)\\>\\s-*\\([^])}> \n\t]\\)
13:57:16 <goron> heh, regular expressions, got to love them, I guess.
13:57:18 <mauke> %@#$%!!
13:57:35 <goron> perl-mode.el
13:57:35 <integral> goron: hmm, can't think of an example.  haven't used it in years now
13:57:42 <Philippa> goron: perhaps. "To love" used to mean much the same as "to fuck" - that's a sentiment I could agree with ;-)
13:57:46 <mauke> another point: don't use escaped punctuation for regex metachars
13:58:07 <mauke> \| is stupid; doubly so when embedded in strings
13:59:46 <goron> Would anyone object to adding a hook into the parser?
13:59:56 <goron> (of e.g. GHC)
14:00:38 <goron> Or maybe GHC API does support these things?
14:00:42 <ncalexan> What other uses would such a hook have?  And what would other parsers do?
14:00:53 <ncalexan> There are a lot of haskell mungers floating around.
14:01:10 <goron> ncalexan: we just discussed that.
14:01:35 <ncalexan> I see only discussion of regexps.
14:02:02 <ihope> I can munge Haskell...
14:02:29 <Philippa> ncalexan: coupled with Template Haskell, such hooks'd allow arbitrary extensions ala Lisp macros
14:02:42 <goron> ncalexan: You can use real DSL's in Haskell code. Not just library level ones.
14:02:48 <Philippa> a good example might be building the next notation ala do or arrows that way
14:02:59 <ihope> @pl let compose 0 f p = p; compose n f p = compose (n-1) f (f p); big f 0 y = f y; big f x y = compose y (big (x-1)) y in big
14:02:59 <lambdabot> flip flip id . ((ap . const id) .) . (. subtract 1)
14:02:59 <goron> Philippa: good example indeed.
14:03:04 <ihope> Whoa.
14:03:51 <ncalexan> I think ocaml went that route with redefinable syntax.
14:04:47 <Philippa> ncalexan: yeah, though there's nominally a preprocessor that does it
14:04:57 <Philippa> it's just well-integrated with the interpreter as well
14:04:59 <ncalexan> I played with it a little bit, and it appeared to be just one dark corner after another.  Very little actually uses it... much like TH.
14:05:49 <Philippa> a camlp4 clone coupled with TH would be something I have some direct uses in mind
14:06:07 <Philippa> they wouldn't be seen as "common" unless what I'm doing takes off, because it's the sort of thing you do sparingly and to good effect
14:06:12 <ncalexan> Of course, it solves a few problems beautifully.  But at an extreme cost.
14:06:19 <ihope> > (flip flip id . ((ap . const id) .) . (. subtract 1)) 2 2
14:06:19 <lambdabot>   Occurs check: cannot construct the infinite type: a = a -> b
14:06:19 <lambdabot>   Expected type: (a1 -> c1) -> a1 -> ((a -> b) -> a -> b) -> c
14:06:19 <lambdabot>   Inferred type: (a1 -> c1) -> a1 -> ((a -> b) -> a) -> (a -> b) -> b
14:06:28 <ihope> > flip flip id . ((ap . const id) .) . (. subtract 1)
14:06:28 <lambdabot>   Occurs check: cannot construct the infinite type: a = a -> b
14:06:28 <lambdabot>   Expected type: (a1 -> c1) -> a1 -> ((a -> b) -> a -> b) -> c
14:06:28 <lambdabot>   Inferred type: (a1 -> c1) -> a1 -> ((a -> b) -> a) -> (a -> b) -> b
14:07:03 <Philippa> the cost's not really that extreme unless you're actually using any of it
14:07:05 <ncalexan> One thing I like about haskell is that it's (relatively) easy to read.  Compare to heavily macro'd lisp... that kind of hackery worries me.
14:07:22 <ncalexan> But I really can't support that with anything other than anecdote.
14:07:39 <Philippa> even then it's potentially rather mild - there're some good potential conventions that could be used to keep things sane
14:08:04 <Philippa> eg "where possible, make your new syntax blocks start with a keyboard followed by a brace-block everything happens in"
14:08:22 <ihope> > flip flip id . ((ap . const id) .) . (. subtract 1) . join id 2 2
14:08:23 <lambdabot>   Occurs check: cannot construct the infinite type: a = a -> t -> a2 -> a1 -> c
14:08:23 <lambdabot>   Expected type: a -> a -> t -> a2 -> a1 -> c
14:08:23 <lambdabot>   Inferred type: a -> a
14:08:26 <ncalexan> keyboard?  keyword?
14:08:31 <Philippa> then it's just the keyword that needs looking out for, and it should be possible to eg write tools that spot such keywords in the preprocessor definition and tell your editor to look out for them
14:08:42 <ncalexan> mmm.
14:08:56 <Philippa> ncalexan: sure, if you can't then you can't do the equivalent of do or arrow notation, no?
14:10:02 <ncalexan> If you can't what?  Match the keyword?  I agree that keyword { } is a good convention.
14:11:01 <Philippa> if you can't add new keywords :-)
14:11:05 <goron> In Statego it's solved by a language file. A .foo file (I forgot the exact name)
14:11:11 * Philippa nods
14:11:23 <Philippa> you'd need the equivalent in haskell, or at the least something to go alongside modules
14:11:31 <goron> I agree that deviations should be noticable.
14:11:32 <Philippa> (which'd inevitably get mapped to a new file anyway)
14:12:01 <ihope> So why the heck does @pl output something that includes join id?
14:12:15 <Philippa> ncalexan: one reason I suggest braces specifically is they interact with the layout rule. I guess that might suck in the case of regexes though?
14:12:19 <ihope> And no, I didn't @pl \x -> x x
14:12:21 <goron> In Stratego they also used it for describing GUIs. 
14:12:48 <goron> I don't think that was the best example ever, though.
14:13:46 <thedward> I have what may be a dumb parsec question; Is there any way to combine two (or more) character parsers in such a way that everything matched is returned?
14:14:50 <goron> do {x<-string "foo";y<-string "bar"; return (x++y)}?
14:15:07 <ncalexan> Philippa: I think the regex case is really tricky because there's some interaction with pattern binding, no?
14:15:40 <Philippa> ncalexan: probably. I don't do regexes. Using a different delimiter there might make sense
14:16:10 <ihope> @index string
14:16:10 <lambdabot> Text.ParserCombinators.Parsec.Char, Text.ParserCombinators.Parsec, Text.ParserCombinators.ReadP, Distribution.Compat.ReadP
14:16:25 <ihope> @type Text.ParserCombinators.Parsec
14:16:26 <lambdabot> Couldn't find qualified module.
14:16:26 <lambdabot> Maybe you're using the wrong syntax: Data.List.(\\) instead of (Data.List.\\)?
14:16:32 <ihope> @type Text.ParserCombinators.Parsec.string
14:16:33 <lambdabot> forall st.
14:16:33 <lambdabot>               -> Text.ParserCombinators.Parsec.Char.CharParser st String
14:16:39 <Philippa> thedward: in parallel or sequence?
14:16:51 <ihope> What the...
14:18:23 <thedward> Philippa: in sequence. I feel like I am missing something basic here. I want to scan a string, allowing certain characters, but also treat certain multi character sequences specially...
14:19:23 <Philippa> thedward: many (try (multicharsequence) <|> characters)?
14:19:29 <Philippa> (give or take some munging to get the type right)
14:20:17 <thedward> Philippa: the multicharsequence part is where I am stuck. If I use string the types don't work, and if I sequence with >> I loose characters
14:20:25 <Philippa> Either is your friend
14:23:59 <goron> Philippa: bye
14:24:29 <Philippa> goron: seeya
14:29:29 * ihope shakes his head
14:30:00 <ihope> @type Control.Monad.Reader.ap . const id
14:30:01 <lambdabot> forall a b a1.
14:30:01 <lambdabot>              a1 -> ((a -> b) -> a) -> (a -> b) -> b
14:30:14 <ihope> @djinn a1 -> ((a -> b) -> a) -> (a -> b) -> b
14:30:14 <lambdabot> f _ a b = b (a b)
14:30:27 <ihope> @pl f _ a b = b (a b)
14:30:28 <lambdabot> f = const (ap id)
14:30:36 <ihope> Hmm.
14:31:28 <ihope> Why the heck would @pl output a composition including a constant?
14:31:48 <ihope> @pl f x y z = x . const y . z
14:31:49 <lambdabot> f = (. ((.) . const)) . (.) . (.)
14:32:41 <ihope> @type \x y z -> x . const y . z
14:32:42 <lambdabot> forall c b a a1.
14:32:42 <lambdabot>           (a -> c) -> a -> (a1 -> b) -> a1 -> c
14:33:38 <ihope> Its third and fourth arguments are ignored completely...
14:52:04 * Philippa reads the ECMAscript standard and curses informal language definitions
14:52:10 <Philippa> dear /god/ is this tedious to read...
14:54:45 <Descolada|Work> then write a program to parse it, throw the non-jucy bits out, and save into your fav format
14:55:07 <Philippa> see the word "informal"?
14:55:14 <Philippa> there's a reason for it :-)
14:55:17 <Descolada|Work> heh
14:55:25 <Descolada|Work> just trying to help
14:55:36 <Philippa> if it were a matter of translating a formal spec in one language to one in another I'd've done it already
14:55:44 * Descolada|Work dislikes ASP.. verry much
14:56:29 <Descolada|Work> sometimes it forgets its ASP, and complains that im compiling it wrong
14:57:33 <ihope> "Aww, Shirl, Fonzie es jumpo el sharko!" <- ouch
15:02:09 <TheJohn> what are some good dead tree haskell tutorials?
15:04:10 <ihope> Fonzie est saltando encima del tiburn...
15:05:48 <Lemmih> TheJohn: The Craft of Functional Programming is the best Haskell book I've read (and the only one, incidently).
15:06:43 <TheJohn> lemmih, ok.
15:06:45 <TheJohn> thx.
15:06:58 <TheJohn> lemmih, is it very advanced?
15:07:01 <TheJohn> i'm sorta a beginner.
15:07:06 * ihope rebootes
15:07:11 * ihope reboots, too
15:07:20 <Descolada|Work> reboote.. i wonder
15:07:23 <TheJohn> interested in type systems and development of the language. 
15:27:25 <metaperl-> Lemmih: I liked "The Haskell Road to Logic, Maths, and COmputer Programming" --- Craft cuts corners on theory sometimes
15:27:57 <metaperl-> Cale: ping?
15:56:01 <Cale> metaperl-: hello
15:57:09 * beelsebob_ prods Philippa 
15:57:16 <Philippa> 'lo
16:07:33 <palomer> grrr
16:07:44 * palomer wishes the people who wrote midi files would keep everything on 1 channel
16:08:40 <Philippa> uh, that doesn't make any practical sense whatsoever unless I'm getting something mixed up here
16:09:35 <Philippa> I mean, if it's a midi file meant for one instrument as would otherwise be played by one person that's different
16:12:18 <palomer> Philippa: yeah, that's what I meant
16:13:05 <dons> Moin
16:13:08 <palomer> btw, we need a haskell version of scons
16:13:20 <dons> isn't that Cabal?
16:13:25 <palomer> is it?
16:13:27 <dons> or is scons not a build system?
16:14:07 <palomer> scons == make
16:14:10 <dons> what would you do with a haskell scons?
16:14:13 <dons> ah, so it is Cabal
16:14:17 <dons> we don't use make anymore
16:14:43 <dons> you write a cabal file and get everything else for free
16:14:50 <dons> ala http://www.cse.unsw.edu.au/~dons/code/hmp3/hmp3.cabal
16:15:02 <dons> then Cabal just bulids the thing
16:15:47 <palomer> is cabal haskell specific?
16:16:18 <dons> i would say "yes", but I'm not 100%. It does let you build cbits with your haskell
16:16:27 <dons> i've not seen it usedd to build anything else
16:16:33 <dons> and that's not its purpose
16:24:59 <Lemmih> dons: It's pretty easy to bind closures to names with the GHC-api. I think it might be a neat way of passing information from and to plugins in a (relatively) type-safe way.
16:25:13 <dons> ah!
16:25:43 <dons> so you just bind to a name, and on the plugin side suddenly that name has aaaa value?
16:25:49 <palomer> Philippa: you wouldn't happen to know if you can have errand note offs (note offs without any note ons), would you?
16:25:57 <Lemmih> dons: Yep.
16:26:09 <dons> scary. that's very powerful. dynamic scoping just about
16:26:34 <Philippa> palomer: sure, a note off is just a note on with velocity 0
16:27:10 <Philippa> when in doubt, assume that you're receiving data from someone who suffers bursts of incredibly inventive malice :-)
16:27:26 <Lemmih> I'm so in love with the ghc-api. It makes all my wildest Haskell dreams come true. (:
16:27:43 <palomer> Philippa: positive?
16:28:10 <Philippa> palomer: pretty positive about the "note off = note on w/ velocity 0". Everything else? Assume the other guy doesn't quite understand the protocol :-)
16:28:26 <Philippa> I would be exceedingly pissed off with a synth that crashed upon receiving a note off when not playing
16:28:28 <palomer> well, I can agree with "a note on with velocity 0 is a note off"
16:28:36 <palomer> however, the other way around isn't true
16:28:48 <palomer> since every note on needs to have a note off
16:29:13 <dons> Lemmih, could it be usedd to do a dynamic 'import' statement, effectively, for plugins?
16:29:28 <Philippa> OK, *lexically* a note off is a note on w/velocity 0, happy?
16:29:32 <dons> that are brought/imported when required, somehow
16:30:06 <Philippa> also, is it actually in the MIDI specs that all notes must ultimately get note offs?
16:30:18 <Philippa> or is that just an assumption and an exceedingly common sequencer convention?
16:30:26 <palomer> I believe I read it somewhere
16:30:45 * Philippa nods
16:30:54 <palomer> it doesn't make sense to me to send 2 note ons without a note off
16:31:09 <Philippa> it makes fucking excellent sense when talking to a monosynth
16:31:38 <Philippa> though I think if you send a note off for the second note played before the first it'll slide back to it
16:31:45 <Philippa> (usually)
16:31:48 <Lemmih> dons: Dynamic import?
16:31:55 <Philippa> in fact, pretty sure of that behaviour, am pretty sure I've abused it while playing before :-0
16:32:00 <Philippa> :-), even
16:32:07 <palomer> Philippa: that doesn't make any sense! if you want that behaviour you have to use channels
16:32:20 <Philippa> er, yerwhat?
16:32:23 <Philippa> *that* doesn't make sense
16:32:24 <Philippa> really
16:32:45 <Philippa> note sliding by playing on two channels just really does not figure. How's the poor bugger on the controller keyboard meant to do it?
16:32:49 <dons> Lemmih, a module could `import' a plugin. but the actual code for that plugin isnn't linked until runtime -- when suddenly all its values appear in scope
16:32:59 <palomer> note on at pitch 50 with velocity 20, note on at pitch 50 with velocity 30; note on at pitch 50 with velocity 0 <--after this we have silence
16:33:03 <dons> it was an idea stefanw and I were playing with at one time
16:33:16 <dons> (we were going to use ld scripts to rewrite binaries..)
16:33:17 <palomer> well, if it's even legal
16:33:18 <Philippa> palomer: ah, that's different. I'm talking different pitches
16:33:22 <palomer> (which I doubt)
16:33:36 <palomer> Philippa: what are you saying exactly?
16:33:37 <Philippa> in practice, if you're on the receiving end it's no good saying "that's illegal!" if you just got run over fatally
16:33:44 <dons> anyway, this sounds like an interesting feature. it'll be fun thinking up a safe interface..
16:33:54 <dons> (to the closure-dynamic-scope thing)
16:34:08 <Philippa> palomer: ever played with a hardware monosynth, or a keyboard hooked up to an emulation thereof?
16:34:40 <palomer> nope, only played with midi in software
16:34:56 <Philippa> some fire a new note outright when you play a second key while the first's down, but most slide pitch because it adds to the range of things you can play with the keyboard
16:35:13 <Lemmih> dons: Well, the scope can't be modified after the code has been compiled.
16:35:19 <palomer> I would expect it to fire a new note
16:35:32 <palomer> (in fact, my keyboard does that)
16:35:35 <Philippa> that's because you're used to polyphonic instruments, no?
16:35:53 <dons> right, but a name in some module effectively binds to whatever the ghc-api associates with that name?
16:35:57 <Philippa> note that I'm talking about "what the synth does with the data", not "what signals the keyboard produces"
16:36:15 <Philippa> (some implementations fuse the two, but what the hell ;-)
16:36:15 <dons> or are there further constraints, Lemmih?
16:36:52 <palomer> my keyboard is definitely not polyphonic
16:36:53 <Lemmih> dons: The type-checker+renamer needs to be aware of the bindings (and their type).
16:37:05 <dons> huh "At Galois, we often fix this with a deepSeq (actually using NFData)" -- Andy Gill
16:37:09 <dons> Cale, someone uses NFDAta ^^
16:37:29 <Cale> dons: hehe
16:37:32 <palomer> by polyphonic I mean you can't tweak the pitch of a key
16:37:38 <dons> Lemmih, ah ok.
16:37:42 <Philippa> that's not what polyphonic means
16:37:54 <palomer> oh, then it's polyphonic
16:38:00 <dons> ?karma+ NFData
16:38:01 <lambdabot> NFData's karma raised to 1.
16:38:02 <palomer> Cale: do you remember that problem I posed the other day
16:38:10 <Cale> palomer: which?
16:38:11 <palomer> given a goal, how many solutions are there
16:38:15 <Philippa> polysynths don't do what I'm describing *because that'd stop you doing the polyphonic thing*
16:38:15 <palomer> and what are those solutions
16:38:17 <Philippa> which'd be nuts, no?
16:38:20 <Cale> palomer: oh, right
16:38:34 <Philippa> I was talking about monosynths - that is, monophonic synths
16:38:35 <palomer> Philippa: polyphonic == two pitches at once?
16:38:40 <palomer> or more
16:38:45 <Cale> palomer: I recall you changed your problem a few times -- did you sort out what the real problem was?
16:38:48 <palomer> Cale: what if the functions couldn't be recursive
16:38:50 <Philippa> palomer: yes
16:39:08 <Philippa> (usually four or more, 'cos you can't play chords worth a crap otherwise - 8 or 10 is preferable)
16:39:09 <palomer> Cale: yeah, no recursion
16:39:11 <dons> oh my: We have used a version of deepSeq that that looked up a table
16:39:12 <dons> at runtime, to find what to make strict and what not to make strict.
16:39:19 <dons> -- Andy Gill
16:39:20 <dons> this is amazing.
16:39:22 <palomer> who the hell has monosynths!
16:39:45 <Philippa> a lot of people
16:39:51 <Philippa> they're fun instruments and do cool things
16:40:05 <Philippa> a lot of softsynths very deliberately include the option for monosynth patches that behave as I've described
16:40:15 <Philippa> and hey, you know that TB-303 craze?
16:40:21 <palomer> nope
16:40:21 <Philippa> I don't recall the 303 being polyphonic :-)
16:41:17 <Philippa> hrmm. If you don't know about the 303 at all I'm guessing you're not that familiar with synths beyond perhaps workstation-type keyboards?
16:42:55 <palomer> yeah
16:43:25 <dons> mmm. coffee
16:46:25 <Philippa> okay. Basically, don't disparage old stuff when you've not given any chance for people to explain the advantages :-)
16:46:39 <Philippa> the monosynths were often much more expressive devices, because it's possible to have complete control over the one note in a way you can't with a polysynth
16:47:21 <palomer> does anyone use them anymore?
16:48:09 * palomer is sending thousands of notes per second to his synth
16:50:48 <Philippa> palomer: yes
16:51:13 <Philippa> and a lot more people use polysynths doing a monosynth emulation on one or more channels, fed from whatever source
16:53:14 <palomer> maybe I should h ave mentioned that I was talking about midi files
16:54:11 <Descolada|Work> maybe
16:57:24 <Philippa> I figured you would be
16:58:11 <Philippa> OTOH, I never saw a specification of what's going on in MIDI files that amounted to much more than "this is a network log" but for the one variant I've forgotten the name of where everything gets split up into chunks
17:02:57 <Philippa> (warning: my intro to midi files was a long time ago and came from a rather informal game coder's description from the PCGPE)
17:03:30 <Philippa> (OTOH, if you're taking input then see earlier comments about handling weird stuff and if you're generating it then WTF do you mean by that series of events anyway?)
17:08:46 <palomer> eh?
17:09:02 <palomer> I suggest we continue in #haskell-blah
17:15:12 <Philippa> sure
17:21:52 <freegoldbar> Does anyone know about haskell versions where unicode works in identifiers and operators as stated in the language definition? I would like to try haskell as an alternative to Mathematica and would prefer an interpreter.
17:23:41 <dons> works in ghc (but you need to use a cvs snapshot)
17:23:59 <dons> why an interpreter? have you used ghci?
17:24:07 <dons> you mean, you want an interactive interface?
17:25:28 <freegoldbar> I use Mathematca for experiments so an interactive environment is nice, but not required. I tried VisualHaskell but unicode doesn't work.
17:26:21 <freegoldbar> I am using windows. I looked at the snapshots of ghc but I'm not sure which one to get.
17:31:34 <dons> ok. I think to get the best answer you should email glasgow-haskell-users@haskell.org, and ask about unicode support
17:31:47 <dons> they should be able to point to the exact version of ghc you'll need
17:32:11 <dons> have a look at the mailing liss on http://haskell.org for the full details on these mailing lists
17:32:37 <freegoldbar> Ok, thank you very much!
17:32:40 <dons> I'm pretty sure you'll need to build your own windows version, or wait for ghc 6.6 to be released. but i might be wrong
17:33:09 <Igloo> Aren't there Windows HEAD snapshots?
17:33:27 <dons> once you have a haskell compiler/interpreter that supports unicode, as ghc now does, you'll then need to get an editor that also supports it. emacs is one such editor
17:33:31 <dons> Igloo, not sure.
17:33:33 <dons> let's see..
17:34:14 <dons> the snapshots are here: http://www.haskell.org/ghc/dist/current/dist/
17:34:19 <dons> but I see only linux {x86, amd64}
17:35:21 <Igloo> Hmm, the last mingw is 20060117
17:35:40 <Igloo> Oh, no, 20060228
17:36:00 <freegoldbar> The editor is no probelm. So I should get ghc-6.5.20060328-i386-unknown-mingw32.tar.gz?
17:36:01 <Igloo> But rather oddly named: http://www.haskell.org/ghc/dist/current/dist/ghc-6.5.20060117.20060228-i386-unknown-mingw32.tar.gz
17:36:22 <dons> freegoldbar, Haskell on its own is pretty good for maths (http://sigfpe.blogspot.com/2006/01/eleven-reasons-to-use-haskell-as.html), but you might also be interested in the libraries http://www.haskell.org/haskellwiki/Libraries_and_tools/Mathematics
17:36:35 <dons> once you have a ghc you can type unicode maths into
17:36:44 <Igloo> Oh, yes, there's also a ghc-6.5.20060328-i386-unknown-mingw32.tar.gz but nothing in the middle. How very odd.
17:37:10 <Igloo> That's probably what you want, anyway
17:37:25 <Cale> Of course, Haskell isn't really a CAS. I suppose you could turn it into one with enough effort though :)
17:39:07 <freegoldbar> I'll try it right away. I know it isn't a CAS, Mathematica is very easy for many experiments, but the price is steep except for the student version.
17:39:12 <dons> freegoldbar, feel free to ask more maths ++ haskell questions here:) it's a pet topic of many of us
17:39:26 <dons> there's also a good textbook
17:39:32 <dons> ?google the haskell road to maths and logic
17:39:33 <lambdabot> http://www.cwi.nl/~jve/HR/
17:40:07 <dons> which is used in at least some proofs++CS courses
17:40:08 <wchogg> freegoldbar:What kindof experimenting are you talking about?
17:40:55 <dons> Cale, what's the gnu version of mathematica called? how complete is it?
17:41:07 <freegoldbar> I use it for developing algorithm and numerical math. So I don't really need what Mathematica provides.
17:41:42 <Cale> dons: Well, there's maxima, which is all right. There's nothing that *really* compares too well.
17:42:09 <freegoldbar> I love the document style interface and have often thought of writing something similar for haskell.
17:42:36 <dons> some of the libraries on the  above "Numerical algorithms" page on haskell.org might be of use, then
17:42:41 <Cale> Yeah, something like mathematica's UI would work well with Haskell..
17:43:16 <audreyt> wow, they just published a "Learning Haskell" book in Japanese today, and the 1st chapter starts with that quote from me :)
17:43:26 <audreyt> (it looks like a quite good intro book)
17:43:42 <freegoldbar> Well, first I have to compile ghc... Thanks for the help!
17:43:53 <audreyt> <- about to deliver the "Learning Haskell" talk in tokyo.yapcasia.org
17:44:22 <dons> audreyt, international haskell ambassador
17:45:25 <wchogg> Are there things people don't think the existing books do well?
17:46:01 <Philippa> "teach haskell as people around here actually use it"?
17:46:12 <Philippa> audreyt: cool :-)
17:47:01 <audreyt> wchogg: er, most of them are in English
17:47:12 <Philippa> point, that too
17:47:48 <wchogg> audreyt:Not everyone is an American? :shock:
17:47:53 <audreyt> ...
17:47:54 <audreyt> ;)
17:48:57 <dons> the'res no systems programming in Haskell book
17:49:05 <dons> or one that teaches high performance code
17:49:10 <wchogg> I meant more if there are things that pedagogically could be improved in the current books.  Honestly, the books I've read shy away far too much from monads and the more advanced concepts.
17:49:11 <Philippa> wchogg: not all native english speakers are either?
17:49:13 <astrolabe> I gather they don't have much about functional data structures (except specialist functional data structures books).
17:49:19 <dons> or even monad transformers, probably
17:49:24 <wchogg> Philippa:Now you're just talking crazy!
17:49:32 <dons> the're okasaki's book on functional data structures
17:49:42 <dons> maybe that's specialist, maybe
17:49:45 <wchogg> I like okasaki's book.  It's a pretty good read.
17:49:46 <Philippa> astrolabe: that's weird voodoo shit to a lot of otherwise serious coders still. I suspect it shouldn't be, but it's not often you can't use an off-the-shelf implementation
17:50:03 <Philippa> dons: it's only about data structures, that's specialist compared to a generic coding book
17:50:15 <dons> yup
17:50:16 <dons> we have no haskell cookbooks
17:50:44 <dons> there's one wiki page of haskell for each perl book published, I think
17:50:49 <wchogg> dons:I think you're right about needing a book about writing high performance haskell.  I wish I had one right now.
17:51:06 <dons> ?wiki Performance
17:51:06 <lambdabot> http://www.haskell.org/haskellwiki/Performance
17:51:35 <astrolabe> Oooh good page.
17:51:40 <dons> someone will eventually publish the haskell.org wiki, right?
17:51:45 <dons> like they publish the vi man page
17:51:53 <Philippa> heh
17:51:59 <Philippa> in fairness I suspect our wiki's got a high average quality
17:52:11 <dons> there''s a lot of published authors in the community
17:52:15 <wchogg> Wow...I've never found that page before, thanks dons.
17:53:20 <Philippa> that, and a lot of good thinkers to supply an ideas framework even if the writing could use tweaking a bit
17:53:23 <dons> yeah, the wiki pages are written by fairly serious people
17:53:51 <dons> now, wchogg, did you loook on the wiki for performance resources?
17:54:00 <dons> if so, then maybe we need to link more prominatntly
17:54:04 <Philippa> I should probably find stuff to contribute now TMR's down. I'm not sure I rate my writing, but MonadsAsComputations is long overdue...
17:54:13 <astrolabe> Maybe the wiki needs a better contents page than I've managed to find.
17:54:20 <dons> ah, and the TMR back catalog should go up
17:54:36 * Igloo hopes if a vi manpage is published that it's rather more complete than mine is
17:54:37 <dons> it's linked in 'Idioms' but that's a bit obscure it seems
17:54:59 <wchogg> No, honestly I didn't look any further than the general categories.
17:55:03 <Philippa> if anyone wants to take up my WIP TMR article on monads and add more I'm happy for that to happen btw
17:55:09 <dons> Igloo, i once found 'the vi editor' -- a man page book from the bsd systems -- in a st.vinnies op shop here in sydney
17:55:12 <dons> bizarre
17:55:30 <Philippa> am happy to explain where I was trying to go with it
17:55:38 <dons> just about every TMR article could go under Idioms/ on the wiki
17:56:16 <Philippa> my second Impure Thoughts article certainly belongs there :-)
17:56:44 <dons> is that the discipline and bondage one?
17:56:48 <Philippa> (though what I'm really waiting for is hearing of a dev team where some people're given the title of "top" and write all the types)
17:56:50 <Philippa> yeah
17:57:20 <dons> hehe
17:57:30 <astrolabe> The idioms page doesn't have any discussion.  You need to follow each of the links to find out what they are about.
17:57:32 <Philippa> although possibly more importantly it's the type hacking for fun and profit one :-)
17:58:07 <dons> astrolabe, yeah. it's a flaw
17:58:19 <dons> unless you know what 'Smart constructors' are fbefore hand :(
17:58:30 <palomer> Philippa: "A corresponding note-off message must be sent for each and every note-on"
17:59:37 <palomer> and I believe that every note off must have a corresponding note on
17:59:40 <palomer> though I'm not sure
18:00:42 <palomer> I would actually like to know:O!
18:01:55 <Philippa> I would hope not the latter, at least not in any strict sense
18:04:17 <twobitsprite> google for "haskell genetic programming" and check out the second link (not work safe)
18:05:32 <palomer> Recursion, Lambda Abstractions and Genetic Programming
18:05:40 <palomer> or Structure Abstraction and Genetic Programming
18:05:48 <twobitsprite> recursion...
18:05:58 <palomer> haha
18:06:06 <twobitsprite> wtf is up with that?? :P
18:06:11 <palomer> go google!
18:06:19 <twobitsprite> heheh
18:06:38 <sjanssen> holy crap
18:07:23 * palomer wishes that it was possible to "edit as new" sent mail in gmail
18:07:24 <twobitsprite> if you click on the "group of 3" link, and go to one of the other links it will take you to the real paper... but for some reason the nudist site got stuck in there somehow...
18:08:46 <palomer> if we click on it enough we can get it to #1!
18:09:06 <twobitsprite> lol
18:09:18 <twobitsprite> I thought it was if we link to it enough...
18:09:58 <palomer> some of these pictures are quite awful
18:10:03 <twobitsprite> indeed :P
18:11:04 <palomer> learning about haskell can be fun!
19:09:24 * ptolomy is happy to find that the haskell version of his ray tracer is considerably faster than the ruby version.
19:16:16 <dons> ah, ray tracers.
19:16:24 <dons> now there are lots that have been written in Haskell
19:16:33 <dons> but I think none are on the wiki graphics page
19:16:38 <dons> anyone have some links??
19:46:14 <Smokey`> dons: you want ray tracers written in haskell, or in general?
19:46:58 <Smokey`> I've got my old ray tracer, + current (which has both standard ray tracing (purposely not recursive though, so no reflections/refractions), and path tracing),  but that's C++ and uses about 4 of my libs, so not exactly what you're looking for </end of brag :P  lol>
19:47:16 <Smokey`> i recall seeing a page with ray tracers written in different languages (all rendering the same scene, using same alg),  with performance benchmarks not long ago
19:47:22 <Smokey`> i'll try and dig it up for ya
19:48:28 <dons> I'm looking to document all the ray tracers written in Haskell, that are available 
19:49:25 <Smokey`> why?  for source code reasons, or?
19:49:45 <dons> for the wiki. 
19:49:52 <dons> keeping track of all the haskell projects out there
19:50:17 <dons> currently, I don't think we've mentioned any of the ray tracers on the haskell.org wiki, despite there being heaps and heaps of them
19:50:36 <Smokey`> mmm
19:50:37 <dons> ?wiki Libraries_and_tools/Graphics
19:50:38 <lambdabot> http://www.haskell.org/haskellwiki/Libraries_and_tools/Graphics
19:50:54 <dons> its got the 4 or 5 implementations of Pan.
19:50:56 <Smokey`> http://www.cse.unsw.edu.au/~cgray/banky/  just something that came up on google
19:51:08 <dons> ah yes. I remember chuck writing that one
19:51:22 <dons> we were in the same AFP class. that was his project
19:51:31 <Smokey`> http://www.haskell.org/tmrwiki/HRay (I assume you know of this however)
19:51:33 <dons> thanks. I'd forgotten all about it.
19:51:36 <dons> ah another one.
19:51:51 <dons> boegel's one
19:51:54 <Smokey`> http://staff.science.uva.nl/~kort/hart/index.html
19:52:10 <dons> Pseudonym, didn't you write a ray tracer at some point?
19:52:17 <dons> oh, didn't know abotu Hart?
19:52:57 <Smokey`> http://www.google.com/search?hl=en&q=ray+tracing+haskell
19:53:11 <dons> hehe
19:53:13 <Smokey`> google has all the answers :)
19:53:29 <dons> yeah. I was hoping the authors would still be in #haskell and would speak up too :)
19:53:41 <dons> but that's a good start. cheers!
19:54:01 <dons>  Stefan Holdermans wrote one?
19:54:15 <dons> ah no. he documents boegel's
19:54:31 <dons> like I said, I knew there would be heaps of them
19:55:07 <Smokey`> dons: Chances are i'm going to write one soon,  just for shits-n-giggles :)   but we'll see...
19:55:19 <Smokey`> I still dont know how the hell to make data structures/classes yet, so i'm kinda stuck there :)
19:55:43 <dons> data X = X Int Int Bool :)
19:55:44 <dons> or something
19:55:58 <dons> have a look at boegel's . he wrote it in a few days.
19:56:46 <Smokey`> data Object = Sphere Double Point3D
19:56:46 <Smokey`>             | Plane (Double,Double,Double,Double) 
19:57:22 <dons> there you go. though I'd skip the tuple in the last line
19:57:28 <Smokey`> i take it you cant have functions like intersect :: Ray -> Object -> Something?
19:57:33 <dons>  | Plane !Double !Double !Double !Double
19:57:37 <Smokey`> you'd have to have intersect :: Ray -> Sphere -> Something ?
19:57:42 <dons> -- add some strictness on the numbers, you'll get better code
19:57:53 <dons> why not?
19:58:10 <Smokey`> okay, say when writing intersect :: Ray -> Object -> Something,  how would I go about figuring out if it's a sphere or a plane?
19:58:11 <dons> it has to be: intersect :: Ray -> Object ->  ...
19:58:19 <Smokey`> okay, that's what I thought
19:58:21 <Smokey`> wait,
19:58:22 <Smokey`> bleh
19:58:23 <Smokey`> no.
19:58:27 <Smokey`> explain? :)
19:59:03 <dons> hang on. phone call
19:59:06 <Smokey`> n/p
19:59:31 <palomer> > let {foo x = case x of {0 -> 0;y->y}; y f = f (y f)} in y foo
19:59:32 <lambdabot> Exception: stack overflow
19:59:48 <palomer> ok, see, the fixed point of the identity should be any elemen
19:59:51 <palomer> element
19:59:51 <Smokey`> dons: never mind, found a good example in the ray tracer. :)
20:00:03 <palomer> someone give me a fixed point operator that works, this one is broken
20:01:19 <palomer> > let y f = f (y f) in y id
20:01:20 <lambdabot> Add a type signature
20:01:29 <palomer> > let y f = f (y f) in (y id)::Int
20:01:32 <lambdabot> Terminated
20:03:22 <palomer> doesn't anyone have a fixed point operator that works?
20:05:59 <Lemmih> How is it broken?
20:06:02 <dons> Control.Monad.Fix
20:06:05 <dons> ?
20:06:15 <Lemmih> > fix id :: Int
20:06:16 <lambdabot> Terminated
20:06:53 <sjanssen> palomer: what you expect "fix id :: Int" to return?
20:07:04 <sjanssen> *insert a do somewhere in that statement
20:08:55 <dons> Smokey`, Object is the type, Sphere and Plane are elements of the Object type
20:09:05 <dons> so any function of them has to be of type :: Object -> ..
20:09:16 <dons> you distinguish between which element with pattern matching
20:09:24 <dons> f (Sphere x y ) = ...
20:09:26 <palomer> sjanssen: 0
20:09:31 <dons> f (Planet x y z a) = ...
20:09:48 <Lemmih> palomer: Where would it get the '0' from?
20:10:18 <palomer> does there exist a fixed point operator that works some of the time?
20:10:42 <palomer> > let y f = f (y f) in y const 0 1 2 3 4
20:10:43 <lambdabot>   Occurs check: cannot construct the infinite type: t = b -> t
20:10:43 <lambdabot>   Expected type: (b -> b -> b -> b -> b -> b -> t)
20:10:43 <lambdabot>        -> b
20:10:59 <sjanssen> palomer: to do that in the general case, you'd need a function of type "arbitrary :: a"
20:11:13 <sjanssen> palomer: I know one function with that type: undefined
20:11:32 <palomer> I want a function that actually looks for fixed points
20:11:47 <sjanssen> Haskell can't do this.
20:12:00 <Lemmih> Fixed points?
20:12:16 <palomer> well, erm, actually it can (quite trivially) with Eq types
20:12:34 <palomer> oh, and they have to be enumerable
20:13:09 <Smokey`> dons: aye :)
20:13:19 * Lemmih gives up trying to understand what's going on around him and returns to hacking.
20:21:31 <sjanssen> how can fix inspect it's argument and figure out that it is the identify function?
20:24:40 <palomer> I can right a function of type (Eq a, Enumerable a) => (a -> a) -> a which will always find a fixed point if it exists
20:25:44 <sjanssen> yeah, I'll agree to that
20:26:16 <dons> cool, I now have 3 alphas on my desk :D
20:26:28 <dons> maybe we can get ghc up on one of them or more.
20:27:56 <sjanssen> palomer: can we agree that you can't write a fix in Haskell and expect it to work on all types?
20:28:12 <palomer> yes
20:28:23 <dons> so that makes 5 x86s, 2 powerpcs and 3 alphas. this desk is getting a bit crowded. and the mips64 and sparc64 in the next cubicle
20:28:34 <palomer> though I can't see how you could possibly prove that to me
20:29:28 <sjanssen> I don't think I have the math background to prove it, but the intuition is fairly straightforward
20:29:43 * palomer doesn't like this intuition
21:20:11 <ptolomy> Huh. Apparently, if you want items sorted by index, it can be faster to make an array of them and apply "elem" than to just build the list with indexes and sort.
21:21:56 <Lemmih> By index?
21:28:34 <sjanssen> ptolomy: if I'm catching your drift, this is the basic idea behind sorting in linear time
21:30:32 <pierre-> hello.
21:31:29 <sjanssen> welcome
21:35:05 <adu> haskell rocks!
21:35:07 <sjanssen> > let sort low high = concat . elems . accumArray (flip (:)) [] (low, high) . map (\x -> (x, x)) in sort 1 50 [7, 49, 50, 3, 4, 2, 7, 7, 6, 5, 8]
21:35:07 <lambdabot>  Not in scope: `accumArray'
21:35:11 <sjanssen> blast
21:35:32 <sjanssen> adu: yes it does.  I just wrote a linear time sort in under 3 minutes!
21:35:46 <sjanssen> no way I could ever do that in some crappy language
21:37:42 <Lemmih> dons: We don't have Data.Array in scope?
21:42:51 <Descoloda> is it me or does that look a bit like qsort would work?
21:43:34 <sjanssen> Descoloda: it isn't quicksort
21:43:51 <Descoloda> i know that
21:44:15 <Descoloda> i mearly asked if qsort would be able to sort it out too
21:45:19 <dons> Lemmih, hmm. we could have Data.Array, i guess
21:45:19 <sjanssen> well, quick sort would give us O(n log n), where this sort gives O( n + high - low)
21:45:30 <dons> though I seem to remember a trick that caused a segfault in ghc...
21:45:42 <dons> let's try it.
21:46:02 <palomer> bucket sort!
21:46:08 <sjanssen> however, we have to constrain the elements we're sorting, and they have to be valid indexes of an array
21:46:54 <dons> > let sort low high = concat . elems . accumArray (flip (:)) [] (low, high) . map (\x -> (x, x)) in sort 1 50 [7, 49, 50, 3, 4, 2, 7, 7, 6, 5, 8]
21:46:56 <lambdabot> [2,3,4,5,6,7,7,7,8,49,50]
21:46:59 <dons> there you go :)
21:47:52 <Lemmih> > let sort = (((join . elems) .) .) . flip flip (map (join (,))) . (((.) . accumArray (flip (:)) []) .) . (,) in sort 1 50 [7, 49, 50, 3, 4, 2, 7, 7, 6, 5, 8]
21:47:53 <lambdabot> [2,3,4,5,6,7,7,7,8,49,50]
21:48:26 <Lemmih> lambdabot is great.
21:48:40 <Descoloda> @botsnack
21:48:40 <lambdabot> :)
21:49:21 <dons> how many haskell ray tracers are there??
21:49:29 <dons> I've found 6 in 5 minutes of searching..
21:49:33 <Descoloda> lol
21:51:39 <dons> 7 now.
21:52:49 <dons> finally I get to page 2 of the google search
21:54:32 <dons> up to 8.
21:59:07 <dons> anyone got at Galois got a copy of the icfp2000 ray tracer entry?
22:08:00 <dons> ok, found 10 
22:42:16 <astrolabe> @type concat
22:42:29 <Lemmih> @bot
22:43:00 <dons> oops. crashed lambdabot's box
22:43:07 <dons> doesn't like usb keys or something
22:45:29 <dons> ok. i'll try not to crash that box again
22:45:36 <dons> ?bot is that ok with you?
22:45:50 <lambdabot> :)
22:47:30 <dons> ok. hopefully this won't crash it....
22:47:37 <dons> all good :}
22:47:38 <dons> phew
22:47:53 <astrolabe> thanks dons
22:48:54 <dons> quick linux question, what would: syslinux /dev/sda1 do?
22:49:14 <dons> (I'm making a bootable linux usb key, so I can run linux apps when needed on my openbsd laptop)
22:50:28 <sieni> we need t-shirts with the text "WWSPJD?"
22:50:39 <dons> hehe
22:52:20 <Cale> hehe
23:14:29 <shapr> dons: Have you been carefully crafting April 1st entries for HWN?
23:15:51 <dons> ah. no. but perhaps I should!
23:15:55 <dons> thanks for the heads up..
23:16:15 <dons> lambdabot's going to disappear for a couple of minutes ...
23:16:28 <shapr> Perhaps we should ask for contributions from the peanut gallery, er, #haskell?
23:16:56 <dons> or maybe it won't disappear
23:17:00 <dons> ahah
23:17:08 <dons> yeah, contributions highly welcome!
23:17:24 <shapr> Oh, John Meacham showed me how to build Nokia 770 apps with anything JHC can compile! w00!
23:17:32 <dons> cool
23:17:43 <shapr> I've been trying to build Flippi.
23:21:13 <dons> ok. i've got linux on my usb key. now to boot from it. hmm :/
23:25:40 <dons> ?bot you there?
23:25:41 <lambdabot> :)
23:25:52 <dons> ah.
23:26:01 <dons> ?hylo f x = x
23:26:01 <lambdabot> f = hylo (_L :: Mu (Const v0)) g h
23:26:01 <lambdabot>  where g (x) = x
23:26:01 <lambdabot>    h x = (x)
23:31:40 <dons> ah, I know why its not booting
23:31:42 <dons> this is fun!
23:31:52 <dons> OS on a usb key for all!
23:32:38 <dons> shapr, I did some research today to list all known ray tracers written in Haskell
23:32:50 <dons> unless you saw above, how many did I find in 30 minutes?
23:41:38 <ncalexan> @index ExitSuccess
23:41:38 <lambdabot> System.Exit
